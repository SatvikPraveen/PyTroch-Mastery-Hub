{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6312b7ce",
   "metadata": {},
   "source": [
    "# Model Serving APIs: Production-Grade PyTorch Inference System\n",
    "\n",
    "**PyTorch Mastery Hub: Enterprise-Ready Model Deployment**\n",
    "\n",
    "**Authors:** ML Engineering Team  \n",
    "**Institution:** PyTorch Mastery Hub  \n",
    "**Module:** Production Deployment & MLOps  \n",
    "**Date:** August 2025\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a comprehensive implementation of production-grade model serving APIs for PyTorch models. We build a complete inference system with FastAPI, featuring async batch processing, authentication, monitoring, and deployment configurations optimized for enterprise environments.\n",
    "\n",
    "## Key Objectives\n",
    "1. Implement high-performance model serving with async batching\n",
    "2. Build comprehensive security and authentication systems\n",
    "3. Create production-ready FastAPI application with monitoring\n",
    "4. Develop extensive testing and load testing frameworks\n",
    "5. Generate complete deployment configurations (Docker, Kubernetes, Helm)\n",
    "6. Set up monitoring and alerting infrastructure\n",
    "7. Create CLI tools for deployment and management\n",
    "\n",
    "## 1. Setup and Environment Configuration\n",
    "\n",
    "```python\n",
    "# Import required libraries\n",
    "import asyncio\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import secrets\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any, Union\n",
    "from collections import defaultdict, deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core ML and data processing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# API and web frameworks\n",
    "try:\n",
    "    from fastapi import FastAPI, HTTPException, Depends, Request, status\n",
    "    from fastapi.security import HTTPBearer, APIKeyHeader\n",
    "    from fastapi.middleware.cors import CORSMiddleware\n",
    "    from fastapi.middleware.gzip import GZipMiddleware\n",
    "    from fastapi.responses import JSONResponse\n",
    "    from contextlib import asynccontextmanager\n",
    "    from pydantic import BaseModel\n",
    "    FASTAPI_AVAILABLE = True\n",
    "    print(\"‚úÖ FastAPI available\")\n",
    "except ImportError:\n",
    "    FASTAPI_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è FastAPI not available - using simulation mode\")\n",
    "    # Mock classes for demonstration\n",
    "    class BaseModel: pass\n",
    "    class FastAPI: pass\n",
    "\n",
    "# Monitoring and logging\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# Set up comprehensive logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Create results directory structure\n",
    "results_dir = Path('../../results/08_production/api_server')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Subdirectories for organized outputs\n",
    "(results_dir / 'deployment_configs').mkdir(exist_ok=True)\n",
    "(results_dir / 'monitoring').mkdir(exist_ok=True)\n",
    "(results_dir / 'scripts').mkdir(exist_ok=True)\n",
    "(results_dir / 'logs').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üöÄ MODEL SERVING API SYSTEM\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìÅ Results directory: {results_dir}\")\n",
    "print(f\"üîß FastAPI available: {FASTAPI_AVAILABLE}\")\n",
    "print(f\"üéØ CUDA available: {torch.cuda.is_available()}\")\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "```\n",
    "\n",
    "## 2. Data Models and Request/Response Schemas\n",
    "\n",
    "```python\n",
    "# Pydantic models for API request/response validation\n",
    "if FASTAPI_AVAILABLE:\n",
    "    \n",
    "    class PredictionRequest(BaseModel):\n",
    "        \"\"\"Request schema for single prediction.\"\"\"\n",
    "        image_data: Optional[str] = None  # Base64 encoded image\n",
    "        tensor_data: Optional[List[List[List[float]]]] = None  # Raw tensor data\n",
    "        return_probabilities: bool = True\n",
    "        confidence_threshold: float = 0.5\n",
    "        \n",
    "        class Config:\n",
    "            schema_extra = {\n",
    "                \"example\": {\n",
    "                    \"tensor_data\": [[[0.5] * 32] * 32] * 3,\n",
    "                    \"return_probabilities\": True,\n",
    "                    \"confidence_threshold\": 0.5\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    class PredictionResponse(BaseModel):\n",
    "        \"\"\"Response schema for single prediction.\"\"\"\n",
    "        predicted_class: int\n",
    "        class_name: str\n",
    "        confidence: float\n",
    "        probabilities: Optional[List[float]] = None\n",
    "        inference_time_ms: float\n",
    "        model_version: str\n",
    "        request_id: str\n",
    "        \n",
    "        class Config:\n",
    "            schema_extra = {\n",
    "                \"example\": {\n",
    "                    \"predicted_class\": 2,\n",
    "                    \"class_name\": \"corn\",\n",
    "                    \"confidence\": 0.95,\n",
    "                    \"probabilities\": [0.02, 0.03, 0.95],\n",
    "                    \"inference_time_ms\": 25.4,\n",
    "                    \"model_version\": \"1.0.0\",\n",
    "                    \"request_id\": \"req_123456\"\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    class BatchPredictionRequest(BaseModel):\n",
    "        \"\"\"Request schema for batch prediction.\"\"\"\n",
    "        batch: List[PredictionRequest]\n",
    "        batch_id: Optional[str] = None\n",
    "        \n",
    "        class Config:\n",
    "            schema_extra = {\n",
    "                \"example\": {\n",
    "                    \"batch\": [\n",
    "                        {\"tensor_data\": [[[0.5] * 32] * 32] * 3, \"return_probabilities\": True},\n",
    "                        {\"tensor_data\": [[[0.3] * 32] * 32] * 3, \"return_probabilities\": True}\n",
    "                    ],\n",
    "                    \"batch_id\": \"batch_001\"\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    class BatchPredictionResponse(BaseModel):\n",
    "        \"\"\"Response schema for batch prediction.\"\"\"\n",
    "        predictions: List[PredictionResponse]\n",
    "        batch_size: int\n",
    "        total_inference_time_ms: float\n",
    "        batch_id: str\n",
    "        \n",
    "        class Config:\n",
    "            schema_extra = {\n",
    "                \"example\": {\n",
    "                    \"predictions\": [\n",
    "                        {\"predicted_class\": 1, \"confidence\": 0.89},\n",
    "                        {\"predicted_class\": 2, \"confidence\": 0.94}\n",
    "                    ],\n",
    "                    \"batch_size\": 2,\n",
    "                    \"total_inference_time_ms\": 45.2,\n",
    "                    \"batch_id\": \"batch_001\"\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    class HealthResponse(BaseModel):\n",
    "        \"\"\"Health check response schema.\"\"\"\n",
    "        status: str\n",
    "        timestamp: str\n",
    "        version: str\n",
    "        uptime_seconds: float\n",
    "        model_loaded: bool\n",
    "        gpu_available: bool\n",
    "        queue_length: int\n",
    "        \n",
    "        class Config:\n",
    "            schema_extra = {\n",
    "                \"example\": {\n",
    "                    \"status\": \"healthy\",\n",
    "                    \"timestamp\": \"2025-08-13T10:30:00Z\",\n",
    "                    \"version\": \"1.0.0\",\n",
    "                    \"uptime_seconds\": 3600,\n",
    "                    \"model_loaded\": True,\n",
    "                    \"gpu_available\": False,\n",
    "                    \"queue_length\": 2\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    class ModelInfo(BaseModel):\n",
    "        \"\"\"Model information response schema.\"\"\"\n",
    "        model_name: str\n",
    "        model_version: str\n",
    "        model_type: str\n",
    "        input_shape: List[int]\n",
    "        output_classes: List[str]\n",
    "        parameters: int\n",
    "        model_size_mb: float\n",
    "        training_date: str\n",
    "        \n",
    "        class Config:\n",
    "            schema_extra = {\n",
    "                \"example\": {\n",
    "                    \"model_name\": \"SampleCNN\",\n",
    "                    \"model_version\": \"1.0.0\",\n",
    "                    \"model_type\": \"CNN\",\n",
    "                    \"input_shape\": [3, 32, 32],\n",
    "                    \"output_classes\": [\"class_0\", \"class_1\", \"class_2\"],\n",
    "                    \"parameters\": 25088,\n",
    "                    \"model_size_mb\": 0.1,\n",
    "                    \"training_date\": \"2025-08-13\"\n",
    "                }\n",
    "            }\n",
    "\n",
    "else:\n",
    "    # Dummy classes for when FastAPI is not available\n",
    "    class PredictionRequest: pass\n",
    "    class PredictionResponse: pass\n",
    "    class BatchPredictionRequest: pass\n",
    "    class BatchPredictionResponse: pass\n",
    "    class HealthResponse: pass\n",
    "    class ModelInfo: pass\n",
    "\n",
    "print(\"üìã DATA MODELS CONFIGURED\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Request/Response schemas defined\")\n",
    "print(\"‚úÖ Validation models ready\")\n",
    "print(\"üìä Example schemas with documentation\")\n",
    "```\n",
    "\n",
    "## 3. Sample Model and Core Inference Engine\n",
    "\n",
    "```python\n",
    "# Sample CNN model for demonstration\n",
    "class SampleCNN(nn.Module):\n",
    "    \"\"\"Sample CNN model for demonstration purposes.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SampleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((8, 8)),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((4, 4))\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Advanced Model Server with Async Batching\n",
    "class ModelServer:\n",
    "    \"\"\"High-performance model server with async batch processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, max_batch_size=32, batch_timeout=0.01):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.batch_timeout = batch_timeout\n",
    "        \n",
    "        # Request queue and processing\n",
    "        self.request_queue = asyncio.Queue()\n",
    "        self.response_futures = {}\n",
    "        self.batch_processor_task = None\n",
    "        self.is_running = False\n",
    "        \n",
    "        # Statistics tracking\n",
    "        self.stats = {\n",
    "            'total_requests': 0,\n",
    "            'successful_requests': 0,\n",
    "            'failed_requests': 0,\n",
    "            'total_inference_time': 0,\n",
    "            'total_batches': 0,\n",
    "            'batch_sizes': [],\n",
    "            'queue_lengths': [],\n",
    "            'start_time': time.time()\n",
    "        }\n",
    "        \n",
    "        # Model metadata\n",
    "        self.model_info = {\n",
    "            'name': 'SampleCNN',\n",
    "            'version': '1.0.0',\n",
    "            'type': 'CNN',\n",
    "            'input_shape': [3, 32, 32],\n",
    "            'output_classes': [f'class_{i}' for i in range(10)],\n",
    "            'parameters': sum(p.numel() for p in model.parameters()),\n",
    "            'model_size_mb': sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024),\n",
    "            'training_date': datetime.now().strftime('%Y-%m-%d')\n",
    "        }\n",
    "        \n",
    "        print(f\"ü§ñ Model Server initialized\")\n",
    "        print(f\"   üìä Parameters: {self.model_info['parameters']:,}\")\n",
    "        print(f\"   üíæ Size: {self.model_info['model_size_mb']:.2f} MB\")\n",
    "        print(f\"   üîÑ Max batch size: {max_batch_size}\")\n",
    "        print(f\"   ‚è±Ô∏è Batch timeout: {batch_timeout}s\")\n",
    "    \n",
    "    async def start_batch_processor(self):\n",
    "        \"\"\"Start the batch processing task.\"\"\"\n",
    "        if not self.is_running:\n",
    "            self.is_running = True\n",
    "            self.batch_processor_task = asyncio.create_task(self._batch_processor())\n",
    "            print(\"üöÄ Batch processor started\")\n",
    "    \n",
    "    async def stop_batch_processor(self):\n",
    "        \"\"\"Stop the batch processing task.\"\"\"\n",
    "        if self.is_running:\n",
    "            self.is_running = False\n",
    "            if self.batch_processor_task:\n",
    "                self.batch_processor_task.cancel()\n",
    "                try:\n",
    "                    await self.batch_processor_task\n",
    "                except asyncio.CancelledError:\n",
    "                    pass\n",
    "            print(\"üõë Batch processor stopped\")\n",
    "    \n",
    "    async def _batch_processor(self):\n",
    "        \"\"\"Main batch processing loop.\"\"\"\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                batch = []\n",
    "                batch_futures = []\n",
    "                \n",
    "                # Collect requests for batch\n",
    "                deadline = time.time() + self.batch_timeout\n",
    "                \n",
    "                while len(batch) < self.max_batch_size and time.time() < deadline:\n",
    "                    try:\n",
    "                        timeout = max(0.001, deadline - time.time())\n",
    "                        request_id, request_data, future = await asyncio.wait_for(\n",
    "                            self.request_queue.get(), timeout=timeout\n",
    "                        )\n",
    "                        batch.append((request_id, request_data))\n",
    "                        batch_futures.append(future)\n",
    "                    except asyncio.TimeoutError:\n",
    "                        break\n",
    "                \n",
    "                # Process batch if we have requests\n",
    "                if batch:\n",
    "                    await self._process_batch(batch, batch_futures)\n",
    "                \n",
    "                # Small delay to prevent tight loop\n",
    "                await asyncio.sleep(0.001)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch processor: {e}\")\n",
    "                await asyncio.sleep(0.1)\n",
    "    \n",
    "    async def _process_batch(self, batch, futures):\n",
    "        \"\"\"Process a batch of requests.\"\"\"\n",
    "        try:\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Prepare batch tensor\n",
    "            batch_tensors = []\n",
    "            for request_id, request_data in batch:\n",
    "                tensor = self._prepare_tensor(request_data)\n",
    "                batch_tensors.append(tensor)\n",
    "            \n",
    "            if not batch_tensors:\n",
    "                return\n",
    "            \n",
    "            # Stack tensors into batch\n",
    "            batch_tensor = torch.stack(batch_tensors).to(self.device)\n",
    "            \n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(batch_tensor)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                predictions = torch.argmax(probabilities, dim=1)\n",
    "            \n",
    "            batch_inference_time = (time.time() - batch_start_time) * 1000\n",
    "            \n",
    "            # Process results for each request\n",
    "            for i, ((request_id, request_data), future) in enumerate(zip(batch, futures)):\n",
    "                try:\n",
    "                    response = self._create_response(\n",
    "                        predictions[i].item(),\n",
    "                        probabilities[i].cpu().numpy(),\n",
    "                        batch_inference_time / len(batch),\n",
    "                        request_data,\n",
    "                        request_id\n",
    "                    )\n",
    "                    future.set_result(response)\n",
    "                    self.stats['successful_requests'] += 1\n",
    "                except Exception as e:\n",
    "                    future.set_exception(e)\n",
    "                    self.stats['failed_requests'] += 1\n",
    "            \n",
    "            # Update statistics\n",
    "            self.stats['total_batches'] += 1\n",
    "            self.stats['batch_sizes'].append(len(batch))\n",
    "            self.stats['total_inference_time'] += batch_inference_time\n",
    "            self.stats['queue_lengths'].append(self.request_queue.qsize())\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Set exception for all futures in case of batch failure\n",
    "            for future in futures:\n",
    "                if not future.done():\n",
    "                    future.set_exception(e)\n",
    "                    self.stats['failed_requests'] += 1\n",
    "    \n",
    "    def _prepare_tensor(self, request_data):\n",
    "        \"\"\"Prepare tensor from request data.\"\"\"\n",
    "        if 'tensor_data' in request_data and request_data['tensor_data'] is not None:\n",
    "            tensor = torch.tensor(request_data['tensor_data'], dtype=torch.float32)\n",
    "            if tensor.dim() == 3:  # Add batch dimension\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "            return tensor.squeeze(0)  # Remove batch dim for stacking later\n",
    "        else:\n",
    "            # Default random tensor for demonstration\n",
    "            return torch.randn(3, 32, 32)\n",
    "    \n",
    "    def _create_response(self, prediction, probabilities, inference_time, request_data, request_id):\n",
    "        \"\"\"Create response dictionary.\"\"\"\n",
    "        response = {\n",
    "            'predicted_class': prediction,\n",
    "            'class_name': self.model_info['output_classes'][prediction],\n",
    "            'confidence': float(probabilities[prediction]),\n",
    "            'inference_time_ms': inference_time,\n",
    "            'model_version': self.model_info['version'],\n",
    "            'request_id': request_id\n",
    "        }\n",
    "        \n",
    "        if request_data.get('return_probabilities', True):\n",
    "            response['probabilities'] = probabilities.tolist()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    async def predict(self, request_data):\n",
    "        \"\"\"Submit prediction request.\"\"\"\n",
    "        request_id = f\"req_{int(time.time() * 1000)}_{secrets.token_hex(4)}\"\n",
    "        future = asyncio.Future()\n",
    "        \n",
    "        await self.request_queue.put((request_id, request_data, future))\n",
    "        self.stats['total_requests'] += 1\n",
    "        \n",
    "        return await future\n",
    "    \n",
    "    def get_health_status(self):\n",
    "        \"\"\"Get server health status.\"\"\"\n",
    "        uptime = time.time() - self.stats['start_time']\n",
    "        return {\n",
    "            'status': 'healthy' if self.is_running else 'stopped',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'version': self.model_info['version'],\n",
    "            'uptime_seconds': uptime,\n",
    "            'model_loaded': True,\n",
    "            'gpu_available': torch.cuda.is_available(),\n",
    "            'queue_length': self.request_queue.qsize()\n",
    "        }\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model information.\"\"\"\n",
    "        return self.model_info.copy()\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get detailed server statistics.\"\"\"\n",
    "        avg_batch_size = np.mean(self.stats['batch_sizes']) if self.stats['batch_sizes'] else 0\n",
    "        avg_queue_length = np.mean(self.stats['queue_lengths']) if self.stats['queue_lengths'] else 0\n",
    "        avg_inference_time = (self.stats['total_inference_time'] / \n",
    "                            max(self.stats['total_batches'], 1))\n",
    "        \n",
    "        success_rate = (self.stats['successful_requests'] / \n",
    "                       max(self.stats['total_requests'], 1))\n",
    "        \n",
    "        return {\n",
    "            'total_requests': self.stats['total_requests'],\n",
    "            'successful_requests': self.stats['successful_requests'],\n",
    "            'failed_requests': self.stats['failed_requests'],\n",
    "            'success_rate': success_rate,\n",
    "            'total_batches': self.stats['total_batches'],\n",
    "            'avg_batch_size': avg_batch_size,\n",
    "            'avg_inference_time_ms': avg_inference_time,\n",
    "            'avg_queue_length': avg_queue_length,\n",
    "            'uptime_seconds': time.time() - self.stats['start_time'],\n",
    "            'queue_length': self.request_queue.qsize()\n",
    "        }\n",
    "\n",
    "# Initialize model and server\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SampleCNN(num_classes=10)\n",
    "model.eval()\n",
    "\n",
    "model_server = ModelServer(model, device, max_batch_size=16, batch_timeout=0.01)\n",
    "\n",
    "print(\"\\nü§ñ MODEL SERVER INITIALIZED\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úÖ Model: {model_server.model_info['name']}\")\n",
    "print(f\"üìä Parameters: {model_server.model_info['parameters']:,}\")\n",
    "print(f\"üíæ Size: {model_server.model_info['model_size_mb']:.2f} MB\")\n",
    "print(f\"üîß Device: {device}\")\n",
    "print(f\"üîÑ Max batch size: {model_server.max_batch_size}\")\n",
    "print(f\"‚è±Ô∏è Batch timeout: {model_server.batch_timeout}s\")\n",
    "```\n",
    "\n",
    "## 4. Authentication and Security System\n",
    "\n",
    "```python\n",
    "# Advanced Authentication and Rate Limiting\n",
    "class APIKeyManager:\n",
    "    \"\"\"Comprehensive API key management with tiered access.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # In production, use a proper database\n",
    "        self.api_keys = {\n",
    "            \"test_key_123\": {\n",
    "                \"name\": \"Test User\",\n",
    "                \"tier\": \"standard\",\n",
    "                \"rate_limit\": 100,  # requests per minute\n",
    "                \"created_at\": datetime.now(),\n",
    "                \"last_used\": None,\n",
    "                \"total_requests\": 0,\n",
    "                \"features\": [\"predict\", \"batch_predict\", \"model_info\"]\n",
    "            },\n",
    "            \"premium_key_456\": {\n",
    "                \"name\": \"Premium User\", \n",
    "                \"tier\": \"premium\",\n",
    "                \"rate_limit\": 1000,\n",
    "                \"created_at\": datetime.now(),\n",
    "                \"last_used\": None,\n",
    "                \"total_requests\": 0,\n",
    "                \"features\": [\"predict\", \"batch_predict\", \"model_info\", \"stats\", \"metrics\"]\n",
    "            },\n",
    "            \"enterprise_key_789\": {\n",
    "                \"name\": \"Enterprise User\",\n",
    "                \"tier\": \"enterprise\", \n",
    "                \"rate_limit\": 10000,\n",
    "                \"created_at\": datetime.now(),\n",
    "                \"last_used\": None,\n",
    "                \"total_requests\": 0,\n",
    "                \"features\": [\"predict\", \"batch_predict\", \"model_info\", \"stats\", \"metrics\", \"admin\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Rate limiting tracking with time windows\n",
    "        self.rate_limit_store = defaultdict(lambda: deque(maxlen=1000))\n",
    "    \n",
    "    def validate_api_key(self, api_key: str) -> Optional[Dict]:\n",
    "        \"\"\"Validate API key and return user info.\"\"\"\n",
    "        if api_key in self.api_keys:\n",
    "            user_info = self.api_keys[api_key].copy()\n",
    "            # Update usage stats\n",
    "            self.api_keys[api_key][\"last_used\"] = datetime.now()\n",
    "            self.api_keys[api_key][\"total_requests\"] += 1\n",
    "            return user_info\n",
    "        return None\n",
    "    \n",
    "    def check_rate_limit(self, api_key: str, rate_limit: int) -> tuple[bool, Dict]:\n",
    "        \"\"\"Check if request is within rate limits and return limit info.\"\"\"\n",
    "        now = time.time()\n",
    "        minute_ago = now - 60\n",
    "        \n",
    "        # Clean old entries\n",
    "        user_requests = self.rate_limit_store[api_key]\n",
    "        while user_requests and user_requests[0] < minute_ago:\n",
    "            user_requests.popleft()\n",
    "        \n",
    "        current_count = len(user_requests)\n",
    "        remaining = max(0, rate_limit - current_count)\n",
    "        \n",
    "        # Check if under limit\n",
    "        if current_count < rate_limit:\n",
    "            user_requests.append(now)\n",
    "            return True, {\n",
    "                'limit': rate_limit,\n",
    "                'remaining': remaining - 1,\n",
    "                'reset_time': int(now + 60),\n",
    "                'current_count': current_count + 1\n",
    "            }\n",
    "        \n",
    "        return False, {\n",
    "            'limit': rate_limit,\n",
    "            'remaining': 0,\n",
    "            'reset_time': int(now + 60),\n",
    "            'current_count': current_count\n",
    "        }\n",
    "    \n",
    "    def check_feature_access(self, api_key: str, feature: str) -> bool:\n",
    "        \"\"\"Check if API key has access to specific feature.\"\"\"\n",
    "        user_info = self.api_keys.get(api_key)\n",
    "        if not user_info:\n",
    "            return False\n",
    "        return feature in user_info.get('features', [])\n",
    "    \n",
    "    def get_usage_stats(self, api_key: str) -> Dict:\n",
    "        \"\"\"Get comprehensive usage statistics for API key.\"\"\"\n",
    "        if api_key not in self.api_keys:\n",
    "            return {}\n",
    "        \n",
    "        user_info = self.api_keys[api_key]\n",
    "        current_minute_requests = len([\n",
    "            req_time for req_time in self.rate_limit_store[api_key]\n",
    "            if req_time > time.time() - 60\n",
    "        ])\n",
    "        \n",
    "        return {\n",
    "            'user_name': user_info['name'],\n",
    "            'tier': user_info['tier'],\n",
    "            'total_requests': user_info['total_requests'],\n",
    "            'current_minute_requests': current_minute_requests,\n",
    "            'rate_limit': user_info['rate_limit'],\n",
    "            'features': user_info['features'],\n",
    "            'last_used': user_info['last_used'].isoformat() if user_info['last_used'] else None,\n",
    "            'created_at': user_info['created_at'].isoformat()\n",
    "        }\n",
    "\n",
    "class SecurityMiddleware:\n",
    "    \"\"\"Advanced security middleware with threat detection.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.blocked_ips = set()\n",
    "        self.suspicious_requests = defaultdict(int)\n",
    "        self.request_patterns = defaultdict(lambda: deque(maxlen=100))\n",
    "        self.threat_patterns = [\n",
    "            'script', 'javascript:', 'eval(', 'exec(', 'import os',\n",
    "            'subprocess', '__import__', 'system('\n",
    "        ]\n",
    "    \n",
    "    def validate_request_size(self, content_length: int, max_size: int = 10 * 1024 * 1024) -> bool:\n",
    "        \"\"\"Validate request size with configurable limits.\"\"\"\n",
    "        return content_length <= max_size\n",
    "    \n",
    "    def detect_suspicious_activity(self, client_ip: str, request_path: str, user_agent: str = \"\") -> Dict:\n",
    "        \"\"\"Advanced suspicious activity detection.\"\"\"\n",
    "        now = time.time()\n",
    "        \n",
    "        # Track request patterns\n",
    "        self.request_patterns[client_ip].append(now)\n",
    "        \n",
    "        threat_score = 0\n",
    "        threats_detected = []\n",
    "        \n",
    "        # Check for rapid requests (more than 50 requests in 10 seconds)\n",
    "        recent_requests = [\n",
    "            req_time for req_time in self.request_patterns[client_ip]\n",
    "            if req_time > now - 10\n",
    "        ]\n",
    "        \n",
    "        if len(recent_requests) > 50:\n",
    "            threat_score += 30\n",
    "            threats_detected.append(\"rapid_requests\")\n",
    "            self.suspicious_requests[client_ip] += 1\n",
    "        \n",
    "        # Check for suspicious patterns in request path\n",
    "        for pattern in self.threat_patterns:\n",
    "            if pattern.lower() in request_path.lower():\n",
    "                threat_score += 40\n",
    "                threats_detected.append(f\"suspicious_pattern_{pattern}\")\n",
    "        \n",
    "        # Check user agent\n",
    "        if not user_agent or len(user_agent) < 10:\n",
    "            threat_score += 10\n",
    "            threats_detected.append(\"suspicious_user_agent\")\n",
    "        \n",
    "        # Block if threat score is too high\n",
    "        is_blocked = threat_score >= 50 or self.suspicious_requests[client_ip] > 5\n",
    "        if is_blocked:\n",
    "            self.blocked_ips.add(client_ip)\n",
    "        \n",
    "        return {\n",
    "            'is_blocked': is_blocked or client_ip in self.blocked_ips,\n",
    "            'threat_score': threat_score,\n",
    "            'threats_detected': threats_detected,\n",
    "            'request_count_10s': len(recent_requests),\n",
    "            'total_suspicious_requests': self.suspicious_requests[client_ip]\n",
    "        }\n",
    "    \n",
    "    def sanitize_input(self, data: Any) -> Any:\n",
    "        \"\"\"Comprehensive input sanitization.\"\"\"\n",
    "        if isinstance(data, str):\n",
    "            # Remove potential script injections\n",
    "            for pattern in self.threat_patterns:\n",
    "                data = data.replace(pattern, '')\n",
    "            # Additional sanitization\n",
    "            data = data.replace('<script>', '').replace('</script>', '')\n",
    "            data = data.replace('javascript:', '').replace('data:', '')\n",
    "        elif isinstance(data, dict):\n",
    "            return {k: self.sanitize_input(v) for k, v in data.items()}\n",
    "        elif isinstance(data, list):\n",
    "            return [self.sanitize_input(item) for item in data]\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def get_security_stats(self) -> Dict:\n",
    "        \"\"\"Get security statistics.\"\"\"\n",
    "        return {\n",
    "            'blocked_ips_count': len(self.blocked_ips),\n",
    "            'blocked_ips': list(self.blocked_ips),\n",
    "            'suspicious_requests_count': sum(self.suspicious_requests.values()),\n",
    "            'unique_suspicious_ips': len(self.suspicious_requests),\n",
    "            'threat_patterns_count': len(self.threat_patterns)\n",
    "        }\n",
    "\n",
    "# Initialize security components\n",
    "api_key_manager = APIKeyManager()\n",
    "security_middleware = SecurityMiddleware()\n",
    "\n",
    "print(\"\\nüîê SECURITY SYSTEM CONFIGURED\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìã API Keys configured: {len(api_key_manager.api_keys)}\")\n",
    "print(\"üõ°Ô∏è Security middleware active\")\n",
    "print(\"‚ö° Rate limiting enabled\")\n",
    "print(\"üîç Threat detection ready\")\n",
    "\n",
    "# Test authentication and security\n",
    "print(\"\\nüß™ TESTING SECURITY SYSTEM\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Test API key validation\n",
    "for key, info in list(api_key_manager.api_keys.items())[:2]:\n",
    "    user_info = api_key_manager.validate_api_key(key)\n",
    "    print(f\"‚úÖ {user_info['name']} ({user_info['tier']}): {user_info['rate_limit']} req/min\")\n",
    "\n",
    "# Test rate limiting\n",
    "test_key = \"test_key_123\"\n",
    "is_allowed, limit_info = api_key_manager.check_rate_limit(test_key, 100)\n",
    "print(f\"‚ö° Rate limit test: {'‚úÖ Allowed' if is_allowed else '‚ùå Blocked'}\")\n",
    "print(f\"   Remaining: {limit_info['remaining']}, Reset: {limit_info['reset_time']}\")\n",
    "\n",
    "# Test security detection\n",
    "security_result = security_middleware.detect_suspicious_activity(\n",
    "    \"192.168.1.100\", \"/predict\", \"TestClient/1.0\"\n",
    ")\n",
    "print(f\"üõ°Ô∏è Security check: Score {security_result['threat_score']}, Blocked: {security_result['is_blocked']}\")\n",
    "```\n",
    "\n",
    "## 5. FastAPI Application Implementation\n",
    "\n",
    "```python\n",
    "# Complete FastAPI Application with All Features\n",
    "if FASTAPI_AVAILABLE:\n",
    "    \n",
    "    # Application lifecycle management\n",
    "    @asynccontextmanager\n",
    "    async def lifespan(app: FastAPI):\n",
    "        \"\"\"Manage application lifecycle with proper startup/shutdown.\"\"\"\n",
    "        # Startup\n",
    "        print(\"üöÄ Starting model server...\")\n",
    "        await model_server.start_batch_processor()\n",
    "        print(\"‚úÖ Model server ready!\")\n",
    "        \n",
    "        yield\n",
    "        \n",
    "        # Shutdown\n",
    "        print(\"üîÑ Shutting down model server...\")\n",
    "        await model_server.stop_batch_processor()\n",
    "        print(\"‚úÖ Model server stopped!\")\n",
    "    \n",
    "    # Initialize FastAPI app with comprehensive configuration\n",
    "    app = FastAPI(\n",
    "        title=\"PyTorch Model Serving API\",\n",
    "        description=\"\"\"\n",
    "        **Production-Grade PyTorch Model Serving**\n",
    "        \n",
    "        High-performance model inference API with:\n",
    "        - Async batch processing for optimal throughput\n",
    "        - Tiered authentication and rate limiting\n",
    "        - Comprehensive monitoring and metrics\n",
    "        - Enterprise security features\n",
    "        - Real-time health monitoring\n",
    "        \n",
    "        **Authentication**: Include `X-API-Key` header with your API key.\n",
    "        **Rate Limits**: Based on your subscription tier.\n",
    "        \"\"\",\n",
    "        version=\"1.0.0\",\n",
    "        docs_url=\"/docs\",\n",
    "        redoc_url=\"/redoc\",\n",
    "        lifespan=lifespan,\n",
    "        openapi_tags=[\n",
    "            {\"name\": \"health\", \"description\": \"Health and status endpoints\"},\n",
    "            {\"name\": \"prediction\", \"description\": \"Model inference endpoints\"},\n",
    "            {\"name\": \"monitoring\", \"description\": \"Metrics and monitoring\"},\n",
    "            {\"name\": \"admin\", \"description\": \"Administrative endpoints\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Add comprehensive middleware\n",
    "    app.add_middleware(\n",
    "        CORSMiddleware,\n",
    "        allow_origins=[\"*\"],  # Configure appropriately for production\n",
    "        allow_credentials=True,\n",
    "        allow_methods=[\"GET\", \"POST\"],\n",
    "        allow_headers=[\"*\"],\n",
    "    )\n",
    "    \n",
    "    app.add_middleware(GZipMiddleware, minimum_size=1000)\n",
    "    \n",
    "    # Custom middleware for request logging and security\n",
    "    @app.middleware(\"http\")\n",
    "    async def security_middleware_func(request: Request, call_next):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Security checks\n",
    "        client_ip = request.client.host\n",
    "        user_agent = request.headers.get(\"user-agent\", \"\")\n",
    "        content_length = int(request.headers.get(\"content-length\", 0))\n",
    "        \n",
    "        # Validate request size\n",
    "        if not security_middleware.validate_request_size(content_length):\n",
    "            return JSONResponse(\n",
    "                status_code=413,\n",
    "                content={\"error\": \"Request too large\", \"max_size_mb\": 10}\n",
    "            )\n",
    "        \n",
    "        # Check for suspicious activity\n",
    "        security_result = security_middleware.detect_suspicious_activity(\n",
    "            client_ip, str(request.url.path), user_agent\n",
    "        )\n",
    "        \n",
    "        if security_result['is_blocked']:\n",
    "            return JSONResponse(\n",
    "                status_code=429,\n",
    "                content={\n",
    "                    \"error\": \"Blocked due to suspicious activity\",\n",
    "                    \"threat_score\": security_result['threat_score'],\n",
    "                    \"contact\": \"support@example.com\"\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Process request\n",
    "        response = await call_next(request)\n",
    "        \n",
    "        # Add security headers\n",
    "        response.headers[\"X-Content-Type-Options\"] = \"nosniff\"\n",
    "        response.headers[\"X-Frame-Options\"] = \"DENY\"\n",
    "        response.headers[\"X-XSS-Protection\"] = \"1; mode=block\"\n",
    "        \n",
    "        # Add timing header\n",
    "        process_time = time.time() - start_time\n",
    "        response.headers[\"X-Process-Time\"] = str(process_time)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    # Security dependencies\n",
    "    api_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n",
    "    \n",
    "    async def get_api_key(api_key: str = Depends(api_key_header)):\n",
    "        \"\"\"Validate API key and return user info.\"\"\"\n",
    "        if not api_key:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "                detail=\"API key required\",\n",
    "                headers={\"WWW-Authenticate\": \"ApiKey\"}\n",
    "            )\n",
    "        \n",
    "        user_info = api_key_manager.validate_api_key(api_key)\n",
    "        if not user_info:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "                detail=\"Invalid API key\",\n",
    "                headers={\"WWW-Authenticate\": \"ApiKey\"}\n",
    "            )\n",
    "        \n",
    "        return user_info\n",
    "    \n",
    "    async def rate_limit_check(\n",
    "        request: Request,\n",
    "        user_info: dict = Depends(get_api_key)\n",
    "    ):\n",
    "        \"\"\"Check rate limits with detailed headers.\"\"\"\n",
    "        api_key = request.headers.get(\"X-API-Key\")\n",
    "        \n",
    "        is_allowed, limit_info = api_key_manager.check_rate_limit(api_key, user_info[\"rate_limit\"])\n",
    "        \n",
    "        if not is_allowed:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_429_TOO_MANY_REQUESTS,\n",
    "                detail=\"Rate limit exceeded\",\n",
    "                headers={\n",
    "                    \"X-RateLimit-Limit\": str(limit_info['limit']),\n",
    "                    \"X-RateLimit-Remaining\": str(limit_info['remaining']),\n",
    "                    \"X-RateLimit-Reset\": str(limit_info['reset_time']),\n",
    "                    \"Retry-After\": \"60\"\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return user_info, limit_info\n",
    "    \n",
    "    async def feature_access_check(feature: str):\n",
    "        \"\"\"Check feature access for the user.\"\"\"\n",
    "        def _check(request: Request, user_info: dict = Depends(get_api_key)):\n",
    "            api_key = request.headers.get(\"X-API-Key\")\n",
    "            if not api_key_manager.check_feature_access(api_key, feature):\n",
    "                raise HTTPException(\n",
    "                    status_code=status.HTTP_403_FORBIDDEN,\n",
    "                    detail=f\"Access to '{feature}' not available in {user_info['tier']} tier\"\n",
    "                )\n",
    "            return user_info\n",
    "        return _check\n",
    "    \n",
    "    # API Endpoints\n",
    "    @app.get(\"/\", tags=[\"health\"])\n",
    "    async def root():\n",
    "        \"\"\"Root endpoint with comprehensive API information.\"\"\"\n",
    "        return {\n",
    "            \"service\": \"PyTorch Model Serving API\",\n",
    "            \"version\": \"1.0.0\",\n",
    "            \"status\": \"running\",\n",
    "            \"features\": {\n",
    "                \"async_batching\": True,\n",
    "                \"rate_limiting\": True,\n",
    "                \"authentication\": True,\n",
    "                \"monitoring\": True,\n",
    "                \"auto_scaling\": True\n",
    "            },\n",
    "            \"endpoints\": {\n",
    "                \"health\": \"/health\",\n",
    "                \"model_info\": \"/model/info\", \n",
    "                \"predict\": \"/predict\",\n",
    "                \"batch_predict\": \"/batch_predict\",\n",
    "                \"stats\": \"/stats\",\n",
    "                \"metrics\": \"/metrics\",\n",
    "                \"docs\": \"/docs\"\n",
    "            },\n",
    "            \"authentication\": {\n",
    "                \"type\": \"API Key\",\n",
    "                \"header\": \"X-API-Key\",\n",
    "                \"tiers\": [\"standard\", \"premium\", \"enterprise\"]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    @app.get(\"/health\", response_model=HealthResponse, tags=[\"health\"])\n",
    "    async def health_check():\n",
    "        \"\"\"Comprehensive health check endpoint.\"\"\"\n",
    "        return model_server.get_health_status()\n",
    "    \n",
    "    @app.get(\"/model/info\", response_model=ModelInfo, tags=[\"prediction\"])\n",
    "    async def get_model_info(user_info: dict = Depends(get_api_key)):\n",
    "        \"\"\"Get detailed model information.\"\"\"\n",
    "        return model_server.get_model_info()\n",
    "    \n",
    "    @app.get(\"/stats\", tags=[\"monitoring\"])\n",
    "    async def get_stats(\n",
    "        user_info: dict = Depends(feature_access_check(\"stats\"))\n",
    "    ):\n",
    "        \"\"\"Get comprehensive server statistics.\"\"\"\n",
    "        server_stats = model_server.get_stats()\n",
    "        \n",
    "        # Get API key from request to show usage stats\n",
    "        # In real implementation, extract from request context\n",
    "        api_key = \"test_key_123\"  # Placeholder\n",
    "        usage_stats = api_key_manager.get_usage_stats(api_key)\n",
    "        security_stats = security_middleware.get_security_stats()\n",
    "        \n",
    "        return {\n",
    "            \"server\": server_stats,\n",
    "            \"user\": usage_stats,\n",
    "            \"security\": security_stats,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    @app.post(\"/predict\", response_model=PredictionResponse, tags=[\"prediction\"])\n",
    "    async def predict(\n",
    "        request_data: PredictionRequest,\n",
    "        req: Request,\n",
    "        rate_check: tuple = Depends(rate_limit_check)\n",
    "    ):\n",
    "        \"\"\"Single prediction endpoint with async processing.\"\"\"\n",
    "        user_info, limit_info = rate_check\n",
    "        \n",
    "        try:\n",
    "            # Sanitize input data\n",
    "            request_dict = security_middleware.sanitize_input(request_data.dict())\n",
    "            \n",
    "            # Make prediction using async batch processor\n",
    "            result = await model_server.predict(request_dict)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise HTTPException(\n",
    "                status_code=500, \n",
    "                detail=f\"Prediction failed: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    @app.post(\"/batch_predict\", response_model=BatchPredictionResponse, tags=[\"prediction\"])\n",
    "    async def batch_predict(\n",
    "        request: BatchPredictionRequest,\n",
    "        req: Request,\n",
    "        rate_check: tuple = Depends(rate_limit_check)\n",
    "    ):\n",
    "        \"\"\"Batch prediction endpoint for high-throughput processing.\"\"\"\n",
    "        user_info, limit_info = rate_check\n",
    "        \n",
    "        # Check batch size limits based on tier\n",
    "        max_batch_sizes = {\"standard\": 10, \"premium\": 50, \"enterprise\": 100}\n",
    "        max_batch = max_batch_sizes.get(user_info['tier'], 10)\n",
    "        \n",
    "        if len(request.batch) > max_batch:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Batch size {len(request.batch)} exceeds limit of {max_batch} for {user_info['tier']} tier\"\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            batch_id = request.batch_id or f\"batch_{int(time.time() * 1000)}_{secrets.token_hex(4)}\"\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Process batch requests\n",
    "            predictions = []\n",
    "            for item in request.batch:\n",
    "                item_dict = security_middleware.sanitize_input(item.dict())\n",
    "                result = await model_server.predict(item_dict)\n",
    "                predictions.append(result)\n",
    "            \n",
    "            total_time = (time.time() - batch_start_time) * 1000\n",
    "            \n",
    "            return {\n",
    "                \"predictions\": predictions,\n",
    "                \"batch_size\": len(predictions),\n",
    "                \"total_inference_time_ms\": total_time,\n",
    "                \"batch_id\": batch_id\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise HTTPException(\n",
    "                status_code=500, \n",
    "                detail=f\"Batch prediction failed: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    @app.get(\"/metrics\", tags=[\"monitoring\"])\n",
    "    async def get_metrics(\n",
    "        user_info: dict = Depends(feature_access_check(\"metrics\"))\n",
    "    ):\n",
    "        \"\"\"Prometheus-style metrics endpoint.\"\"\"\n",
    "        stats = model_server.get_stats()\n",
    "        \n",
    "        metrics = {\n",
    "            # Counter metrics\n",
    "            \"model_requests_total\": stats[\"total_requests\"],\n",
    "            \"model_requests_successful_total\": stats[\"successful_requests\"],\n",
    "            \"model_requests_failed_total\": stats[\"failed_requests\"],\n",
    "            \"model_batches_processed_total\": stats[\"total_batches\"],\n",
    "            \n",
    "            # Gauge metrics\n",
    "            \"model_queue_length\": stats[\"queue_length\"],\n",
    "            \"model_uptime_seconds\": stats[\"uptime_seconds\"],\n",
    "            \n",
    "            # Histogram metrics\n",
    "            \"model_inference_duration_seconds\": stats[\"avg_inference_time_ms\"] / 1000,\n",
    "            \"model_batch_size_avg\": stats[\"avg_batch_size\"],\n",
    "            \n",
    "            # Rate metrics\n",
    "            \"model_success_rate\": stats[\"success_rate\"],\n",
    "            \"model_requests_per_second\": stats[\"total_requests\"] / max(stats[\"uptime_seconds\"], 1)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    @app.get(\"/admin/security\", tags=[\"admin\"])\n",
    "    async def get_security_info(\n",
    "        user_info: dict = Depends(feature_access_check(\"admin\"))\n",
    "    ):\n",
    "        \"\"\"Administrative security information.\"\"\"\n",
    "        return security_middleware.get_security_stats()\n",
    "    \n",
    "    # Comprehensive error handlers\n",
    "    @app.exception_handler(HTTPException)\n",
    "    async def http_exception_handler(request: Request, exc: HTTPException):\n",
    "        \"\"\"Custom HTTP exception handler with detailed error info.\"\"\"\n",
    "        return JSONResponse(\n",
    "            status_code=exc.status_code,\n",
    "            content={\n",
    "                \"error\": {\n",
    "                    \"message\": exc.detail,\n",
    "                    \"status_code\": exc.status_code,\n",
    "                    \"type\": \"HTTPException\",\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"path\": str(request.url.path),\n",
    "                    \"method\": request.method\n",
    "                },\n",
    "                \"support\": {\n",
    "                    \"docs\": \"/docs\",\n",
    "                    \"contact\": \"support@example.com\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    @app.exception_handler(Exception)\n",
    "    async def general_exception_handler(request: Request, exc: Exception):\n",
    "        \"\"\"General exception handler for unexpected errors.\"\"\"\n",
    "        error_id = secrets.token_hex(8)\n",
    "        \n",
    "        # Log error details (in production, use proper logging)\n",
    "        print(f\"Error {error_id}: {str(exc)}\")\n",
    "        \n",
    "        return JSONResponse(\n",
    "            status_code=500,\n",
    "            content={\n",
    "                \"error\": {\n",
    "                    \"message\": \"Internal server error\",\n",
    "                    \"status_code\": 500,\n",
    "                    \"type\": \"InternalServerError\",\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"path\": str(request.url.path),\n",
    "                    \"error_id\": error_id\n",
    "                },\n",
    "                \"support\": {\n",
    "                    \"docs\": \"/docs\",\n",
    "                    \"contact\": \"support@example.com\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(\"\\nüåê FASTAPI APPLICATION CONFIGURED\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"‚úÖ All endpoints registered\")\n",
    "    print(\"üîê Authentication middleware active\")\n",
    "    print(\"‚ö° Rate limiting with tier-based limits\")\n",
    "    print(\"üõ°Ô∏è Security checks and threat detection\")\n",
    "    print(\"üìä Comprehensive metrics and monitoring\")\n",
    "    print(\"üö® Custom error handlers\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è FastAPI not available - API server simulation only\")\n",
    "    app = None\n",
    "```\n",
    "\n",
    "## 6. API Testing and Load Testing Framework\n",
    "\n",
    "```python\n",
    "# Comprehensive API Client and Load Testing Framework\n",
    "class ModelAPIClient:\n",
    "    \"\"\"Production-ready API client with comprehensive features.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, api_key: str):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        # Request tracking and statistics\n",
    "        self.request_count = 0\n",
    "        self.error_count = 0\n",
    "        self.total_latency = 0\n",
    "        self.response_times = []\n",
    "        self.error_log = []\n",
    "        \n",
    "        # Session configuration (would use aiohttp/httpx in production)\n",
    "        self.timeout = 30\n",
    "        self.retry_attempts = 3\n",
    "        \n",
    "    def _get_headers(self) -> Dict[str, str]:\n",
    "        \"\"\"Get comprehensive request headers.\"\"\"\n",
    "        return {\n",
    "            \"X-API-Key\": self.api_key,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"User-Agent\": \"PyTorch-Model-Client/1.0\",\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"Cache-Control\": \"no-cache\"\n",
    "        }\n",
    "    \n",
    "    async def health_check(self) -> Dict:\n",
    "        \"\"\"Check API health with detailed response.\"\"\"\n",
    "        # Simulated response for demonstration\n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"version\": \"1.0.0\",\n",
    "            \"uptime_seconds\": 3600,\n",
    "            \"model_loaded\": True,\n",
    "            \"gpu_available\": torch.cuda.is_available(),\n",
    "            \"queue_length\": 2\n",
    "        }\n",
    "    \n",
    "    async def get_model_info(self) -> Dict:\n",
    "        \"\"\"Get detailed model information.\"\"\"\n",
    "        return model_server.get_model_info()\n",
    "    \n",
    "    async def predict(\n",
    "        self,\n",
    "        image_data: Optional[str] = None,\n",
    "        tensor_data: Optional[List] = None,\n",
    "        return_probabilities: bool = True,\n",
    "        confidence_threshold: float = 0.5\n",
    "    ) -> Dict:\n",
    "        \"\"\"Make single prediction with comprehensive error handling.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Prepare request data\n",
    "            request_data = {\n",
    "                \"image_data\": image_data,\n",
    "                \"tensor_data\": tensor_data,\n",
    "                \"return_probabilities\": return_probabilities,\n",
    "                \"confidence_threshold\": confidence_threshold\n",
    "            }\n",
    "            \n",
    "            # Make prediction using model server directly (simulated)\n",
    "            result = await model_server.predict(request_data)\n",
    "            \n",
    "            # Track metrics\n",
    "            latency = time.time() - start_time\n",
    "            self.request_count += 1\n",
    "            self.total_latency += latency\n",
    "            self.response_times.append(latency)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.error_count += 1\n",
    "            error_info = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'error': str(e),\n",
    "                'latency': time.time() - start_time\n",
    "            }\n",
    "            self.error_log.append(error_info)\n",
    "            raise Exception(f\"Prediction failed: {str(e)}\")\n",
    "    \n",
    "    async def batch_predict(self, batch_requests: List[Dict], batch_id: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Make batch prediction with performance tracking.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            predictions = []\n",
    "            for request_data in batch_requests:\n",
    "                result = await model_server.predict(request_data)\n",
    "                predictions.append(result)\n",
    "            \n",
    "            batch_id = batch_id or f\"batch_{int(time.time() * 1000)}_{secrets.token_hex(4)}\"\n",
    "            total_time = (time.time() - start_time) * 1000\n",
    "            \n",
    "            # Track batch metrics\n",
    "            self.request_count += len(batch_requests)\n",
    "            self.total_latency += time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"predictions\": predictions,\n",
    "                \"batch_size\": len(predictions),\n",
    "                \"total_inference_time_ms\": total_time,\n",
    "                \"batch_id\": batch_id\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.error_count += len(batch_requests)\n",
    "            error_info = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'error': str(e),\n",
    "                'batch_size': len(batch_requests),\n",
    "                'latency': time.time() - start_time\n",
    "            }\n",
    "            self.error_log.append(error_info)\n",
    "            raise Exception(f\"Batch prediction failed: {str(e)}\")\n",
    "    \n",
    "    def get_client_stats(self) -> Dict:\n",
    "        \"\"\"Get comprehensive client-side statistics.\"\"\"\n",
    "        if self.request_count == 0:\n",
    "            return {\"message\": \"No requests made yet\"}\n",
    "        \n",
    "        avg_latency = self.total_latency / self.request_count\n",
    "        success_rate = (self.request_count - self.error_count) / self.request_count\n",
    "        \n",
    "        stats = {\n",
    "            \"total_requests\": self.request_count,\n",
    "            \"successful_requests\": self.request_count - self.error_count,\n",
    "            \"error_count\": self.error_count,\n",
    "            \"success_rate\": success_rate,\n",
    "            \"average_latency_ms\": avg_latency * 1000,\n",
    "            \"total_latency_seconds\": self.total_latency\n",
    "        }\n",
    "        \n",
    "        if self.response_times:\n",
    "            stats.update({\n",
    "                \"min_latency_ms\": min(self.response_times) * 1000,\n",
    "                \"max_latency_ms\": max(self.response_times) * 1000,\n",
    "                \"p95_latency_ms\": np.percentile(self.response_times, 95) * 1000,\n",
    "                \"p99_latency_ms\": np.percentile(self.response_times, 99) * 1000\n",
    "            })\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Advanced Load Testing Framework\n",
    "class LoadTester:\n",
    "    \"\"\"Comprehensive load testing framework with detailed analytics.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: ModelAPIClient):\n",
    "        self.client = client\n",
    "        self.results = []\n",
    "        self.errors = []\n",
    "        \n",
    "    async def single_request_test(self, num_requests: int = 100):\n",
    "        \"\"\"Test single request performance with detailed metrics.\"\"\"\n",
    "        print(f\"\\nüß™ SINGLE REQUEST LOAD TEST\")\n",
    "        print(f\"Target: {num_requests} requests\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        latencies = []\n",
    "        errors = []\n",
    "        \n",
    "        for i in range(num_requests):\n",
    "            try:\n",
    "                request_start = time.time()\n",
    "                \n",
    "                # Generate realistic test data\n",
    "                test_tensor = torch.randn(3, 32, 32).tolist()\n",
    "                result = await self.client.predict(\n",
    "                    tensor_data=test_tensor,\n",
    "                    return_probabilities=True,\n",
    "                    confidence_threshold=0.5\n",
    "                )\n",
    "                \n",
    "                latency = time.time() - request_start\n",
    "                latencies.append(latency)\n",
    "                \n",
    "                # Progress updates\n",
    "                if (i + 1) % max(1, num_requests // 10) == 0:\n",
    "                    progress = (i + 1) / num_requests * 100\n",
    "                    avg_latency = np.mean(latencies[-10:]) if latencies else 0\n",
    "                    print(f\"Progress: {progress:.0f}% | Recent avg latency: {avg_latency*1000:.1f}ms\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                errors.append({\n",
    "                    'request_id': i,\n",
    "                    'error': str(e),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate comprehensive statistics\n",
    "        results = self._calculate_performance_stats(\n",
    "            \"single_requests\", latencies, errors, total_time, num_requests\n",
    "        )\n",
    "        \n",
    "        self.results.append(results)\n",
    "        self._print_test_results(results)\n",
    "        return results\n",
    "    \n",
    "    async def batch_request_test(self, num_batches: int = 20, batch_size: int = 8):\n",
    "        \"\"\"Test batch request performance with throughput analysis.\"\"\"\n",
    "        print(f\"\\nüß™ BATCH REQUEST LOAD TEST\")\n",
    "        print(f\"Target: {num_batches} batches of {batch_size} items each\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        batch_latencies = []\n",
    "        item_latencies = []\n",
    "        errors = []\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            try:\n",
    "                batch_start = time.time()\n",
    "                \n",
    "                # Generate batch of test data\n",
    "                batch_requests = []\n",
    "                for _ in range(batch_size):\n",
    "                    test_tensor = torch.randn(3, 32, 32).tolist()\n",
    "                    batch_requests.append({\n",
    "                        \"tensor_data\": test_tensor,\n",
    "                        \"return_probabilities\": True,\n",
    "                        \"confidence_threshold\": 0.5\n",
    "                    })\n",
    "                \n",
    "                result = await self.client.batch_predict(batch_requests)\n",
    "                \n",
    "                batch_latency = time.time() - batch_start\n",
    "                batch_latencies.append(batch_latency)\n",
    "                \n",
    "                # Calculate per-item latency\n",
    "                item_latency = batch_latency / batch_size\n",
    "                item_latencies.extend([item_latency] * batch_size)\n",
    "                \n",
    "                print(f\"Batch {i+1}/{num_batches}: {batch_size} items in {batch_latency*1000:.1f}ms ({item_latency*1000:.1f}ms/item)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                errors.append({\n",
    "                    'batch_id': i,\n",
    "                    'error': str(e),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        total_items = num_batches * batch_size\n",
    "        \n",
    "        # Calculate batch-specific statistics\n",
    "        results = {\n",
    "            \"test_type\": \"batch_requests\",\n",
    "            \"total_batches\": num_batches,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"total_items\": total_items,\n",
    "            \"successful_batches\": len(batch_latencies),\n",
    "            \"failed_batches\": len(errors),\n",
    "            \"total_time_seconds\": total_time,\n",
    "            \"throughput_items_per_second\": len(item_latencies) / total_time if total_time > 0 else 0,\n",
    "            \"throughput_batches_per_second\": len(batch_latencies) / total_time if total_time > 0 else 0\n",
    "        }\n",
    "        \n",
    "        if batch_latencies:\n",
    "            results.update({\n",
    "                \"avg_batch_latency_ms\": np.mean(batch_latencies) * 1000,\n",
    "                \"min_batch_latency_ms\": np.min(batch_latencies) * 1000,\n",
    "                \"max_batch_latency_ms\": np.max(batch_latencies) * 1000,\n",
    "                \"p95_batch_latency_ms\": np.percentile(batch_latencies, 95) * 1000\n",
    "            })\n",
    "        \n",
    "        if item_latencies:\n",
    "            results.update({\n",
    "                \"avg_item_latency_ms\": np.mean(item_latencies) * 1000,\n",
    "                \"p95_item_latency_ms\": np.percentile(item_latencies, 95) * 1000\n",
    "            })\n",
    "        \n",
    "        results[\"error_rate\"] = len(errors) / num_batches if num_batches > 0 else 0\n",
    "        \n",
    "        self.results.append(results)\n",
    "        self._print_test_results(results)\n",
    "        return results\n",
    "    \n",
    "    async def concurrent_test(self, num_concurrent: int = 10, requests_per_client: int = 10):\n",
    "        \"\"\"Test concurrent request handling with detailed analysis.\"\"\"\n",
    "        print(f\"\\nüß™ CONCURRENT LOAD TEST\")\n",
    "        print(f\"Target: {num_concurrent} concurrent clients, {requests_per_client} requests each\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        async def client_worker(client_id: int):\n",
    "            \"\"\"Individual client worker with error tracking.\"\"\"\n",
    "            latencies = []\n",
    "            errors = []\n",
    "            \n",
    "            for i in range(requests_per_client):\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    test_tensor = torch.randn(3, 32, 32).tolist()\n",
    "                    await self.client.predict(tensor_data=test_tensor)\n",
    "                    latencies.append(time.time() - start_time)\n",
    "                except Exception as e:\n",
    "                    errors.append({\n",
    "                        'client_id': client_id,\n",
    "                        'request_id': i,\n",
    "                        'error': str(e),\n",
    "                        'timestamp': datetime.now().isoformat()\n",
    "                    })\n",
    "            \n",
    "            return latencies, errors\n",
    "        \n",
    "        # Run concurrent workers (simulated)\n",
    "        start_time = time.time()\n",
    "        all_latencies = []\n",
    "        all_errors = []\n",
    "        \n",
    "        # In real implementation, use asyncio.gather for true concurrency\n",
    "        for client_id in range(num_concurrent):\n",
    "            latencies, errors = await client_worker(client_id)\n",
    "            all_latencies.extend(latencies)\n",
    "            all_errors.extend(errors)\n",
    "            \n",
    "            # Progress update\n",
    "            progress = (client_id + 1) / num_concurrent * 100\n",
    "            print(f\"Client {client_id + 1}/{num_concurrent} completed ({progress:.0f}%)\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        total_requests = num_concurrent * requests_per_client\n",
    "        \n",
    "        # Calculate comprehensive concurrent statistics\n",
    "        results = self._calculate_performance_stats(\n",
    "            \"concurrent_requests\", all_latencies, all_errors, total_time, total_requests\n",
    "        )\n",
    "        \n",
    "        results.update({\n",
    "            \"num_concurrent_clients\": num_concurrent,\n",
    "            \"requests_per_client\": requests_per_client\n",
    "        })\n",
    "        \n",
    "        self.results.append(results)\n",
    "        self._print_test_results(results)\n",
    "        return results\n",
    "    \n",
    "    def _calculate_performance_stats(self, test_type: str, latencies: List[float], \n",
    "                                   errors: List[Dict], total_time: float, \n",
    "                                   total_requests: int) -> Dict:\n",
    "        \"\"\"Calculate comprehensive performance statistics.\"\"\"\n",
    "        successful_requests = len(latencies)\n",
    "        failed_requests = len(errors)\n",
    "        \n",
    "        results = {\n",
    "            \"test_type\": test_type,\n",
    "            \"total_requests\": total_requests,\n",
    "            \"successful_requests\": successful_requests,\n",
    "            \"failed_requests\": failed_requests,\n",
    "            \"total_time_seconds\": total_time,\n",
    "            \"error_rate\": failed_requests / total_requests if total_requests > 0 else 0,\n",
    "            \"success_rate\": successful_requests / total_requests if total_requests > 0 else 0\n",
    "        }\n",
    "        \n",
    "        if latencies:\n",
    "            results.update({\n",
    "                \"throughput_rps\": successful_requests / total_time if total_time > 0 else 0,\n",
    "                \"avg_latency_ms\": np.mean(latencies) * 1000,\n",
    "                \"min_latency_ms\": np.min(latencies) * 1000,\n",
    "                \"max_latency_ms\": np.max(latencies) * 1000,\n",
    "                \"median_latency_ms\": np.median(latencies) * 1000,\n",
    "                \"p95_latency_ms\": np.percentile(latencies, 95) * 1000,\n",
    "                \"p99_latency_ms\": np.percentile(latencies, 99) * 1000,\n",
    "                \"std_latency_ms\": np.std(latencies) * 1000\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _print_test_results(self, results: Dict):\n",
    "        \"\"\"Print formatted test results.\"\"\"\n",
    "        print(f\"\\nüìä {results['test_type'].replace('_', ' ').title()} Results:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Total Requests: {results['total_requests']:,}\")\n",
    "        print(f\"Successful: {results['successful_requests']:,}\")\n",
    "        print(f\"Failed: {results['failed_requests']:,}\")\n",
    "        print(f\"Success Rate: {results['success_rate']:.1%}\")\n",
    "        print(f\"Total Time: {results['total_time_seconds']:.1f}s\")\n",
    "        \n",
    "        if 'throughput_rps' in results:\n",
    "            print(f\"Throughput: {results['throughput_rps']:.1f} RPS\")\n",
    "        \n",
    "        if 'avg_latency_ms' in results:\n",
    "            print(f\"Avg Latency: {results['avg_latency_ms']:.1f}ms\")\n",
    "            print(f\"P95 Latency: {results['p95_latency_ms']:.1f}ms\")\n",
    "            print(f\"P99 Latency: {results['p99_latency_ms']:.1f}ms\")\n",
    "        \n",
    "        if results['test_type'] == 'batch_requests':\n",
    "            if 'throughput_items_per_second' in results:\n",
    "                print(f\"Item Throughput: {results['throughput_items_per_second']:.1f} items/sec\")\n",
    "            if 'avg_batch_latency_ms' in results:\n",
    "                print(f\"Avg Batch Latency: {results['avg_batch_latency_ms']:.1f}ms\")\n",
    "    \n",
    "    def generate_comprehensive_report(self) -> str:\n",
    "        \"\"\"Generate detailed performance analysis report.\"\"\"\n",
    "        if not self.results:\n",
    "            return \"No test results available\"\n",
    "        \n",
    "        report = \"\\nüìä COMPREHENSIVE LOAD TEST REPORT\\n\"\n",
    "        report += \"=\" * 60 + \"\\n\\n\"\n",
    "        \n",
    "        # Executive Summary\n",
    "        report += \"üéØ EXECUTIVE SUMMARY\\n\"\n",
    "        report += \"-\" * 30 + \"\\n\"\n",
    "        \n",
    "        total_requests = sum(r['total_requests'] for r in self.results)\n",
    "        total_successful = sum(r['successful_requests'] for r in self.results)\n",
    "        overall_success_rate = total_successful / total_requests if total_requests > 0 else 0\n",
    "        \n",
    "        report += f\"Total Requests Processed: {total_requests:,}\\n\"\n",
    "        report += f\"Overall Success Rate: {overall_success_rate:.1%}\\n\"\n",
    "        report += f\"Tests Conducted: {len(self.results)}\\n\\n\"\n",
    "        \n",
    "        # Detailed Results\n",
    "        for i, result in enumerate(self.results, 1):\n",
    "            report += f\"üìã Test {i}: {result['test_type'].replace('_', ' ').title()}\\n\"\n",
    "            report += \"-\" * 40 + \"\\n\"\n",
    "            \n",
    "            if result['test_type'] == 'single_requests':\n",
    "                report += f\"Requests: {result['total_requests']:,}\\n\"\n",
    "                report += f\"Success Rate: {result['success_rate']:.1%}\\n\"\n",
    "                report += f\"Throughput: {result.get('throughput_rps', 0):.1f} RPS\\n\"\n",
    "                report += f\"Avg Latency: {result.get('avg_latency_ms', 0):.1f}ms\\n\"\n",
    "                report += f\"P95 Latency: {result.get('p95_latency_ms', 0):.1f}ms\\n\"\n",
    "                \n",
    "            elif result['test_type'] == 'batch_requests':\n",
    "                report += f\"Batches: {result['total_batches']:,}\\n\"\n",
    "                report += f\"Batch Size: {result['batch_size']}\\n\"\n",
    "                report += f\"Total Items: {result['total_items']:,}\\n\"\n",
    "                report += f\"Item Throughput: {result.get('throughput_items_per_second', 0):.1f} items/sec\\n\"\n",
    "                report += f\"Avg Batch Latency: {result.get('avg_batch_latency_ms', 0):.1f}ms\\n\"\n",
    "                report += f\"Avg Item Latency: {result.get('avg_item_latency_ms', 0):.1f}ms\\n\"\n",
    "                \n",
    "            elif result['test_type'] == 'concurrent_requests':\n",
    "                report += f\"Concurrent Clients: {result['num_concurrent_clients']}\\n\"\n",
    "                report += f\"Requests per Client: {result['requests_per_client']}\\n\"\n",
    "                report += f\"Total Requests: {result['total_requests']:,}\\n\"\n",
    "                report += f\"Throughput: {result.get('throughput_rps', 0):.1f} RPS\\n\"\n",
    "                report += f\"Avg Latency: {result.get('avg_latency_ms', 0):.1f}ms\\n\"\n",
    "                report += f\"P95 Latency: {result.get('p95_latency_ms', 0):.1f}ms\\n\"\n",
    "            \n",
    "            report += \"\\n\"\n",
    "        \n",
    "        # Performance Recommendations\n",
    "        report += \"üí° PERFORMANCE RECOMMENDATIONS\\n\"\n",
    "        report += \"-\" * 40 + \"\\n\"\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        for result in self.results:\n",
    "            if result.get('error_rate', 0) > 0.05:\n",
    "                recommendations.append(f\"High error rate in {result['test_type']} ({result['error_rate']:.1%}) - investigate error handling\")\n",
    "            \n",
    "            if result.get('p95_latency_ms', 0) > 1000:\n",
    "                recommendations.append(f\"High P95 latency in {result['test_type']} ({result['p95_latency_ms']:.0f}ms) - consider optimization\")\n",
    "            \n",
    "            if result.get('throughput_rps', 0) < 50:\n",
    "                recommendations.append(f\"Low throughput in {result['test_type']} ({result['throughput_rps']:.1f} RPS) - increase batch size or optimize inference\")\n",
    "        \n",
    "        if not recommendations:\n",
    "            recommendations.append(\"Performance looks good! System is handling load well.\")\n",
    "        \n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            report += f\"{i}. {rec}\\n\"\n",
    "        \n",
    "        report += \"\\n\"\n",
    "        return report\n",
    "\n",
    "# Initialize client and load tester\n",
    "print(\"\\nüîß INITIALIZING TEST FRAMEWORK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "client = ModelAPIClient(\"http://localhost:8000\", \"test_key_123\")\n",
    "load_tester = LoadTester(client)\n",
    "\n",
    "print(\"‚úÖ API client initialized\")\n",
    "print(\"üß™ Load testing framework ready\")\n",
    "print(\"üìä Comprehensive metrics tracking enabled\")\n",
    "```\n",
    "\n",
    "## 7. Running Comprehensive Load Tests\n",
    "\n",
    "```python\n",
    "# Execute comprehensive performance testing\n",
    "async def run_comprehensive_tests():\n",
    "    \"\"\"Run complete test suite with detailed analysis.\"\"\"\n",
    "    \n",
    "    print(\"\\nüöÄ STARTING COMPREHENSIVE LOAD TEST SUITE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Start the model server batch processor\n",
    "    await model_server.start_batch_processor()\n",
    "    \n",
    "    try:\n",
    "        # Test 1: Single Request Performance\n",
    "        print(\"\\nüéØ Phase 1: Single Request Performance Analysis\")\n",
    "        single_results = await load_tester.single_request_test(num_requests=50)\n",
    "        \n",
    "        # Test 2: Batch Processing Efficiency\n",
    "        print(\"\\nüéØ Phase 2: Batch Processing Efficiency Analysis\")\n",
    "        batch_results = await load_tester.batch_request_test(num_batches=10, batch_size=8)\n",
    "        \n",
    "        # Test 3: Concurrent Load Handling\n",
    "        print(\"\\nüéØ Phase 3: Concurrent Load Handling Analysis\")\n",
    "        concurrent_results = await load_tester.concurrent_test(num_concurrent=5, requests_per_client=10)\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        print(\"\\nüìä GENERATING COMPREHENSIVE PERFORMANCE REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        report = load_tester.generate_comprehensive_report()\n",
    "        print(report)\n",
    "        \n",
    "        # Save test results\n",
    "        test_results = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'test_configuration': {\n",
    "                'model_name': model_server.model_info['name'],\n",
    "                'device': str(device),\n",
    "                'max_batch_size': model_server.max_batch_size,\n",
    "                'batch_timeout': model_server.batch_timeout\n",
    "            },\n",
    "            'results': load_tester.results,\n",
    "            'client_stats': client.get_client_stats()\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        with open(results_dir / 'load_test_results.json', 'w') as f:\n",
    "            json.dump(test_results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\nüíæ Test results saved to {results_dir / 'load_test_results.json'}\")\n",
    "        \n",
    "        # Display server statistics\n",
    "        server_stats = model_server.get_stats()\n",
    "        print(\"\\nüìà SERVER PERFORMANCE SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total requests processed: {server_stats['total_requests']:,}\")\n",
    "        print(f\"Success rate: {server_stats['success_rate']:.1%}\")\n",
    "        print(f\"Average inference time: {server_stats['avg_inference_time_ms']:.1f}ms\")\n",
    "        print(f\"Average batch size: {server_stats['avg_batch_size']:.1f}\")\n",
    "        print(f\"Total batches processed: {server_stats['total_batches']}\")\n",
    "        print(f\"Current queue length: {server_stats['queue_length']}\")\n",
    "        print(f\"Uptime: {server_stats['uptime_seconds']:.1f}s\")\n",
    "        \n",
    "        return test_results\n",
    "        \n",
    "    finally:\n",
    "        # Stop batch processor\n",
    "        await model_server.stop_batch_processor()\n",
    "\n",
    "# Run the comprehensive test suite\n",
    "test_results = await run_comprehensive_tests()\n",
    "```\n",
    "\n",
    "## 8. Deployment Configuration Generation\n",
    "\n",
    "```python\n",
    "class DeploymentConfigGenerator:\n",
    "    \"\"\"Generate production-ready deployment configurations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.configs = {}\n",
    "    \n",
    "    def generate_dockerfile(self) -> str:\n",
    "        \"\"\"Generate optimized multi-stage Dockerfile.\"\"\"\n",
    "        \n",
    "        dockerfile_content = '''# Multi-stage Dockerfile for PyTorch model serving\n",
    "FROM python:3.9-slim as base\n",
    "\n",
    "# Set environment variables for optimization\n",
    "ENV PYTHONUNBUFFERED=1 \\\\\n",
    "    PYTHONDONTWRITEBYTECODE=1 \\\\\n",
    "    PIP_NO_CACHE_DIR=1 \\\\\n",
    "    PIP_DISABLE_PIP_VERSION_CHECK=1 \\\\\n",
    "    PYTHONHASHSEED=random\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    build-essential \\\\\n",
    "    curl \\\\\n",
    "    git \\\\\n",
    "    && rm -rf /var/lib/apt/lists/* \\\\\n",
    "    && apt-get clean\n",
    "\n",
    "# Create app user for security\n",
    "RUN groupadd -r appuser && useradd -r -g appuser appuser\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements first for better Docker layer caching\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies with optimizations\n",
    "RUN pip install --no-cache-dir --upgrade pip \\\\\n",
    "    && pip install --no-cache-dir -r requirements.txt \\\\\n",
    "    && pip install --no-cache-dir uvicorn[standard] gunicorn\n",
    "\n",
    "# Copy application code\n",
    "COPY src/ ./src/\n",
    "COPY web_app/ ./web_app/\n",
    "COPY models/ ./models/\n",
    "COPY config/ ./config/\n",
    "\n",
    "# Create necessary directories with proper permissions\n",
    "RUN mkdir -p /app/logs /app/data /app/results /app/temp \\\\\n",
    "    && chown -R appuser:appuser /app\n",
    "\n",
    "# Switch to non-root user\n",
    "USER appuser\n",
    "\n",
    "# Health check configuration\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Expose application port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Use uvicorn for production ASGI server\n",
    "CMD [\"uvicorn\", \"web_app.api_server:app\", \\\\\n",
    "     \"--host\", \"0.0.0.0\", \\\\\n",
    "     \"--port\", \"8000\", \\\\\n",
    "     \"--workers\", \"1\", \\\\\n",
    "     \"--access-log\", \\\\\n",
    "     \"--log-config\", \"config/logging.yaml\"]\n",
    "        '''\n",
    "        \n",
    "        return dockerfile_content\n",
    "    \n",
    "    def generate_docker_compose(self) -> str:\n",
    "        \"\"\"Generate comprehensive Docker Compose configuration.\"\"\"\n",
    "        \n",
    "        compose_content = '''version: '3.8'\n",
    "\n",
    "services:\n",
    "  pytorch-api:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "      target: base\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - MODEL_PATH=/app/models/model.pth\n",
    "      - LOG_LEVEL=INFO\n",
    "      - MAX_BATCH_SIZE=32\n",
    "      - BATCH_TIMEOUT=0.01\n",
    "      - DEVICE=cpu\n",
    "      - WORKERS=1\n",
    "      - HOST=0.0.0.0\n",
    "      - PORT=8000\n",
    "    volumes:\n",
    "      - ./models:/app/models:ro\n",
    "      - ./logs:/app/logs\n",
    "      - ./data:/app/data:ro\n",
    "      - ./config:/app/config:ro\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      start_period: 40s\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          cpus: '2.0'\n",
    "          memory: 4G\n",
    "        reservations:\n",
    "          cpus: '1.0'\n",
    "          memory: 2G\n",
    "    networks:\n",
    "      - pytorch-network\n",
    "    labels:\n",
    "      - \"traefik.enable=true\"\n",
    "      - \"traefik.http.routers.pytorch-api.rule=Host(`api.pytorch.local`)\"\n",
    "\n",
    "  nginx:\n",
    "    image: nginx:alpine\n",
    "    ports:\n",
    "      - \"80:80\"\n",
    "      - \"443:443\"\n",
    "    volumes:\n",
    "      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n",
    "      - ./ssl:/etc/nginx/ssl:ro\n",
    "      - ./logs/nginx:/var/log/nginx\n",
    "    depends_on:\n",
    "      - pytorch-api\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - pytorch-network\n",
    "\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro\n",
    "      - ./monitoring/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro\n",
    "      - prometheus_data:/prometheus\n",
    "    command:\n",
    "      - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "      - '--storage.tsdb.path=/prometheus'\n",
    "      - '--web.console.libraries=/etc/prometheus/console_libraries'\n",
    "      - '--web.console.templates=/etc/prometheus/consoles'\n",
    "      - '--storage.tsdb.retention.time=200h'\n",
    "      - '--web.enable-lifecycle'\n",
    "      - '--web.enable-admin-api'\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - pytorch-network\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin123\n",
    "      - GF_USERS_ALLOW_SIGN_UP=false\n",
    "      - GF_INSTALL_PLUGINS=grafana-piechart-panel\n",
    "    volumes:\n",
    "      - grafana_data:/var/lib/grafana\n",
    "      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro\n",
    "      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro\n",
    "    depends_on:\n",
    "      - prometheus\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - pytorch-network\n",
    "\n",
    "  alertmanager:\n",
    "    image: prom/alertmanager:latest\n",
    "    ports:\n",
    "      - \"9093:9093\"\n",
    "    volumes:\n",
    "      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro\n",
    "      - alertmanager_data:/alertmanager\n",
    "    command:\n",
    "      - '--config.file=/etc/alertmanager/alertmanager.yml'\n",
    "      - '--storage.path=/alertmanager'\n",
    "      - '--web.external-url=http://localhost:9093'\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - pytorch-network\n",
    "\n",
    "  redis:\n",
    "    image: redis:alpine\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - pytorch-network\n",
    "    command: redis-server --appendonly yes\n",
    "\n",
    "volumes:\n",
    "  prometheus_data:\n",
    "  grafana_data:\n",
    "  alertmanager_data:\n",
    "  redis_data:\n",
    "\n",
    "networks:\n",
    "  pytorch-network:\n",
    "    driver: bridge\n",
    "    ipam:\n",
    "      config:\n",
    "        - subnet: 172.20.0.0/16\n",
    "        '''\n",
    "        \n",
    "        return compose_content\n",
    "    \n",
    "    def generate_kubernetes_manifests(self) -> Dict[str, str]:\n",
    "        \"\"\"Generate comprehensive Kubernetes deployment manifests.\"\"\"\n",
    "        \n",
    "        # Namespace\n",
    "        namespace_yaml = '''apiVersion: v1\n",
    "kind: Namespace\n",
    "metadata:\n",
    "  name: pytorch-ml\n",
    "  labels:\n",
    "    name: pytorch-ml\n",
    "    environment: production\n",
    "'''\n",
    "        \n",
    "        # ConfigMap\n",
    "        configmap_yaml = '''apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: pytorch-config\n",
    "  namespace: pytorch-ml\n",
    "data:\n",
    "  MAX_BATCH_SIZE: \"32\"\n",
    "  BATCH_TIMEOUT: \"0.01\"\n",
    "  LOG_LEVEL: \"INFO\"\n",
    "  WORKERS: \"1\"\n",
    "  DEVICE: \"cpu\"\n",
    "  PROMETHEUS_METRICS: \"true\"\n",
    "'''\n",
    "        \n",
    "        # Deployment\n",
    "        deployment_yaml = '''apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: pytorch-model-server\n",
    "  namespace: pytorch-ml\n",
    "  labels:\n",
    "    app: pytorch-model-server\n",
    "    version: v1.0\n",
    "    component: inference\n",
    "spec:\n",
    "  replicas: 3\n",
    "  strategy:\n",
    "    type: RollingUpdate\n",
    "    rollingUpdate:\n",
    "      maxSurge: 1\n",
    "      maxUnavailable: 0\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: pytorch-model-server\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: pytorch-model-server\n",
    "        version: v1.0\n",
    "        component: inference\n",
    "      annotations:\n",
    "        prometheus.io/scrape: \"true\"\n",
    "        prometheus.io/port: \"8000\"\n",
    "        prometheus.io/path: \"/metrics\"\n",
    "    spec:\n",
    "      serviceAccountName: pytorch-service-account\n",
    "      securityContext:\n",
    "        runAsNonRoot: true\n",
    "        runAsUser: 1000\n",
    "        fsGroup: 2000\n",
    "      containers:\n",
    "      - name: pytorch-api\n",
    "        image: pytorch-model-server:v1.0\n",
    "        imagePullPolicy: IfNotPresent\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "          name: http\n",
    "          protocol: TCP\n",
    "        env:\n",
    "        - name: MODEL_PATH\n",
    "          value: \"/app/models/model.pth\"\n",
    "        - name: POD_NAME\n",
    "          valueFrom:\n",
    "            fieldRef:\n",
    "              fieldPath: metadata.name\n",
    "        - name: POD_NAMESPACE\n",
    "          valueFrom:\n",
    "            fieldRef:\n",
    "              fieldPath: metadata.namespace\n",
    "        envFrom:\n",
    "        - configMapRef:\n",
    "            name: pytorch-config\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"2Gi\"\n",
    "            cpu: \"1000m\"\n",
    "            ephemeral-storage: \"1Gi\"\n",
    "          limits:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2000m\"\n",
    "            ephemeral-storage: \"2Gi\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "            scheme: HTTP\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "          timeoutSeconds: 5\n",
    "          failureThreshold: 3\n",
    "          successThreshold: 1\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "            scheme: HTTP\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 5\n",
    "          timeoutSeconds: 3\n",
    "          failureThreshold: 2\n",
    "          successThreshold: 1\n",
    "        startupProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "            scheme: HTTP\n",
    "          initialDelaySeconds: 10\n",
    "          periodSeconds: 10\n",
    "          timeoutSeconds: 5\n",
    "          failureThreshold: 30\n",
    "          successThreshold: 1\n",
    "        volumeMounts:\n",
    "        - name: model-storage\n",
    "          mountPath: /app/models\n",
    "          readOnly: true\n",
    "        - name: log-storage\n",
    "          mountPath: /app/logs\n",
    "        - name: tmp-storage\n",
    "          mountPath: /tmp\n",
    "        securityContext:\n",
    "          allowPrivilegeEscalation: false\n",
    "          readOnlyRootFilesystem: true\n",
    "          capabilities:\n",
    "            drop:\n",
    "            - ALL\n",
    "      volumes:\n",
    "      - name: model-storage\n",
    "        persistentVolumeClaim:\n",
    "          claimName: model-pvc\n",
    "      - name: log-storage\n",
    "        emptyDir:\n",
    "          sizeLimit: 1Gi\n",
    "      - name: tmp-storage\n",
    "        emptyDir:\n",
    "          sizeLimit: 1Gi\n",
    "      imagePullSecrets:\n",
    "      - name: registry-secret\n",
    "      affinity:\n",
    "        podAntiAffinity:\n",
    "          preferredDuringSchedulingIgnoredDuringExecution:\n",
    "          - weight: 100\n",
    "            podAffinityTerm:\n",
    "              labelSelector:\n",
    "                matchExpressions:\n",
    "                - key: app\n",
    "                  operator: In\n",
    "                  values:\n",
    "                  - pytorch-model-server\n",
    "              topologyKey: kubernetes.io/hostname\n",
    "'''\n",
    "        \n",
    "        # Service\n",
    "        service_yaml = '''apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: pytorch-model-service\n",
    "  namespace: pytorch-ml\n",
    "  labels:\n",
    "    app: pytorch-model-server\n",
    "  annotations:\n",
    "    service.beta.kubernetes.io/aws-load-balancer-type: nlb\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8000\n",
    "    protocol: TCP\n",
    "    name: http\n",
    "  selector:\n",
    "    app: pytorch-model-server\n",
    "  sessionAffinity: None\n",
    "'''\n",
    "        \n",
    "        # Ingress\n",
    "        ingress_yaml = '''apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: pytorch-model-ingress\n",
    "  namespace: pytorch-ml\n",
    "  annotations:\n",
    "    kubernetes.io/ingress.class: nginx\n",
    "    nginx.ingress.kubernetes.io/rewrite-target: /\n",
    "    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n",
    "    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n",
    "    cert-manager.io/cluster-issuer: letsencrypt-prod\n",
    "    nginx.ingress.kubernetes.io/rate-limit: \"100\"\n",
    "    nginx.ingress.kubernetes.io/rate-limit-window: \"1m\"\n",
    "    nginx.ingress.kubernetes.io/rate-limit-connections: \"10\"\n",
    "    nginx.ingress.kubernetes.io/upstream-hash-by: \"$remote_addr\"\n",
    "spec:\n",
    "  tls:\n",
    "  - hosts:\n",
    "    - api.yourmodel.com\n",
    "    secretName: pytorch-api-tls\n",
    "  rules:\n",
    "  - host: api.yourmodel.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: pytorch-model-service\n",
    "            port:\n",
    "              number: 80\n",
    "'''\n",
    "        \n",
    "        # HorizontalPodAutoscaler\n",
    "        hpa_yaml = '''apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: pytorch-model-hpa\n",
    "  namespace: pytorch-ml\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: pytorch-model-server\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "  - type: Pods\n",
    "    pods:\n",
    "      metric:\n",
    "        name: http_requests_per_second\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"50\"\n",
    "  behavior:\n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 300\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 50\n",
    "        periodSeconds: 60\n",
    "      - type: Pods\n",
    "        value: 2\n",
    "        periodSeconds: 60\n",
    "      selectPolicy: Min\n",
    "    scaleUp:\n",
    "      stabilizationWindowSeconds: 60\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 100\n",
    "        periodSeconds: 15\n",
    "      - type: Pods\n",
    "        value: 4\n",
    "        periodSeconds: 60\n",
    "      selectPolicy: Max\n",
    "'''\n",
    "        \n",
    "        # PersistentVolumeClaim\n",
    "        pvc_yaml = '''apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: model-pvc\n",
    "  namespace: pytorch-ml\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadOnlyMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 10Gi\n",
    "  storageClassName: fast-ssd\n",
    "'''\n",
    "        \n",
    "        # ServiceAccount\n",
    "        serviceaccount_yaml = '''apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: pytorch-service-account\n",
    "  namespace: pytorch-ml\n",
    "  labels:\n",
    "    app: pytorch-model-server\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRole\n",
    "metadata:\n",
    "  name: pytorch-cluster-role\n",
    "rules:\n",
    "- apiGroups: [\"\"]\n",
    "  resources: [\"pods\", \"services\", \"endpoints\"]\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "- apiGroups: [\"apps\"]\n",
    "  resources: [\"deployments\", \"replicasets\"]\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRoleBinding\n",
    "metadata:\n",
    "  name: pytorch-cluster-role-binding\n",
    "roleRef:\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "  kind: ClusterRole\n",
    "  name: pytorch-cluster-role\n",
    "subjects:\n",
    "- kind: ServiceAccount\n",
    "  name: pytorch-service-account\n",
    "  namespace: pytorch-ml\n",
    "'''\n",
    "        \n",
    "        return {\n",
    "            'namespace': namespace_yaml,\n",
    "            'configmap': configmap_yaml,\n",
    "            'deployment': deployment_yaml,\n",
    "            'service': service_yaml,\n",
    "            'ingress': ingress_yaml,\n",
    "            'hpa': hpa_yaml,\n",
    "            'pvc': pvc_yaml,\n",
    "            'serviceaccount': serviceaccount_yaml\n",
    "        }\n",
    "    \n",
    "    def generate_helm_chart(self) -> Dict[str, str]:\n",
    "        \"\"\"Generate comprehensive Helm chart for deployment.\"\"\"\n",
    "        \n",
    "        # Chart.yaml\n",
    "        chart_yaml = '''apiVersion: v2\n",
    "name: pytorch-model-server\n",
    "description: Production-grade PyTorch model serving API with comprehensive features\n",
    "type: application\n",
    "version: 1.0.0\n",
    "appVersion: \"1.0.0\"\n",
    "home: https://github.com/your-org/pytorch-model-server\n",
    "sources:\n",
    "  - https://github.com/your-org/pytorch-model-server\n",
    "keywords:\n",
    "  - pytorch\n",
    "  - machine-learning\n",
    "  - api\n",
    "  - model-serving\n",
    "  - inference\n",
    "  - mlops\n",
    "maintainers:\n",
    "  - name: ML Engineering Team\n",
    "    email: ml-team@yourcompany.com\n",
    "  - name: DevOps Team\n",
    "    email: devops@yourcompany.com\n",
    "dependencies:\n",
    "  - name: prometheus\n",
    "    version: \"15.x.x\"\n",
    "    repository: https://prometheus-community.github.io/helm-charts\n",
    "    condition: monitoring.prometheus.enabled\n",
    "  - name: grafana\n",
    "    version: \"6.x.x\"\n",
    "    repository: https://grafana.github.io/helm-charts\n",
    "    condition: monitoring.grafana.enabled\n",
    "annotations:\n",
    "  category: MachineLearning\n",
    "'''\n",
    "        \n",
    "        # values.yaml\n",
    "        values_yaml = '''# Default values for pytorch-model-server\n",
    "# This is a YAML-formatted file.\n",
    "\n",
    "# Image configuration\n",
    "image:\n",
    "  repository: pytorch-model-server\n",
    "  pullPolicy: IfNotPresent\n",
    "  tag: \"v1.0.0\"\n",
    "\n",
    "# Image pull secrets\n",
    "imagePullSecrets:\n",
    "  - name: registry-secret\n",
    "\n",
    "# Service account\n",
    "serviceAccount:\n",
    "  create: true\n",
    "  annotations: {}\n",
    "  name: \"\"\n",
    "\n",
    "# Pod security context\n",
    "podSecurityContext:\n",
    "  runAsNonRoot: true\n",
    "  runAsUser: 1000\n",
    "  fsGroup: 2000\n",
    "\n",
    "# Container security context\n",
    "securityContext:\n",
    "  allowPrivilegeEscalation: false\n",
    "  readOnlyRootFilesystem: true\n",
    "  capabilities:\n",
    "    drop:\n",
    "    - ALL\n",
    "\n",
    "# Deployment configuration\n",
    "replicaCount: 3\n",
    "\n",
    "# Rolling update strategy\n",
    "strategy:\n",
    "  type: RollingUpdate\n",
    "  rollingUpdate:\n",
    "    maxSurge: 1\n",
    "    maxUnavailable: 0\n",
    "\n",
    "# Pod annotations\n",
    "podAnnotations:\n",
    "  prometheus.io/scrape: \"true\"\n",
    "  prometheus.io/port: \"8000\"\n",
    "  prometheus.io/path: \"/metrics\"\n",
    "\n",
    "# Service configuration\n",
    "service:\n",
    "  type: ClusterIP\n",
    "  port: 80\n",
    "  targetPort: 8000\n",
    "  annotations: {}\n",
    "\n",
    "# Ingress configuration\n",
    "ingress:\n",
    "  enabled: true\n",
    "  className: \"nginx\"\n",
    "  annotations:\n",
    "    nginx.ingress.kubernetes.io/rewrite-target: /\n",
    "    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n",
    "    cert-manager.io/cluster-issuer: letsencrypt-prod\n",
    "    nginx.ingress.kubernetes.io/rate-limit: \"100\"\n",
    "    nginx.ingress.kubernetes.io/rate-limit-window: \"1m\"\n",
    "  hosts:\n",
    "    - host: api.yourmodel.com\n",
    "      paths:\n",
    "        - path: /\n",
    "          pathType: Prefix\n",
    "  tls:\n",
    "    - secretName: pytorch-api-tls\n",
    "      hosts:\n",
    "        - api.yourmodel.com\n",
    "\n",
    "# Resource limits and requests\n",
    "resources:\n",
    "  limits:\n",
    "    cpu: 2000m\n",
    "    memory: 4Gi\n",
    "    ephemeral-storage: 2Gi\n",
    "  requests:\n",
    "    cpu: 1000m\n",
    "    memory: 2Gi\n",
    "    ephemeral-storage: 1Gi\n",
    "\n",
    "# Autoscaling configuration\n",
    "autoscaling:\n",
    "  enabled: true\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  targetCPUUtilizationPercentage: 70\n",
    "  targetMemoryUtilizationPercentage: 80\n",
    "  customMetrics:\n",
    "    - type: Pods\n",
    "      pods:\n",
    "        metric:\n",
    "          name: http_requests_per_second\n",
    "        target:\n",
    "          type: AverageValue\n",
    "          averageValue: \"50\"\n",
    "\n",
    "# Health check configuration\n",
    "healthCheck:\n",
    "  livenessProbe:\n",
    "    initialDelaySeconds: 30\n",
    "    periodSeconds: 10\n",
    "    timeoutSeconds: 5\n",
    "    failureThreshold: 3\n",
    "  readinessProbe:\n",
    "    initialDelaySeconds: 5\n",
    "    periodSeconds: 5\n",
    "    timeoutSeconds: 3\n",
    "    failureThreshold: 2\n",
    "  startupProbe:\n",
    "    initialDelaySeconds: 10\n",
    "    periodSeconds: 10\n",
    "    timeoutSeconds: 5\n",
    "    failureThreshold: 30\n",
    "\n",
    "# Node selector\n",
    "nodeSelector: {}\n",
    "\n",
    "# Tolerations\n",
    "tolerations: []\n",
    "\n",
    "# Affinity rules\n",
    "affinity:\n",
    "  podAntiAffinity:\n",
    "    preferredDuringSchedulingIgnoredDuringExecution:\n",
    "    - weight: 100\n",
    "      podAffinityTerm:\n",
    "        labelSelector:\n",
    "          matchExpressions:\n",
    "          - key: app.kubernetes.io/name\n",
    "            operator: In\n",
    "            values:\n",
    "            - pytorch-model-server\n",
    "        topologyKey: kubernetes.io/hostname\n",
    "\n",
    "# Model server configuration\n",
    "modelServer:\n",
    "  model:\n",
    "    path: \"/app/models/model.pth\"\n",
    "    maxBatchSize: 32\n",
    "    batchTimeout: 0.01\n",
    "    device: \"cpu\"\n",
    "  \n",
    "  api:\n",
    "    workers: 1\n",
    "    logLevel: \"INFO\"\n",
    "    \n",
    "  features:\n",
    "    prometheusMetrics: true\n",
    "    healthChecks: true\n",
    "    rateLimiting: true\n",
    "\n",
    "# Storage configuration\n",
    "storage:\n",
    "  models:\n",
    "    enabled: true\n",
    "    storageClass: \"fast-ssd\"\n",
    "    size: \"10Gi\"\n",
    "    accessMode: \"ReadOnlyMany\"\n",
    "  \n",
    "  logs:\n",
    "    enabled: true\n",
    "    size: \"1Gi\"\n",
    "\n",
    "# Monitoring configuration\n",
    "monitoring:\n",
    "  enabled: true\n",
    "  \n",
    "  prometheus:\n",
    "    enabled: true\n",
    "    serviceMonitor:\n",
    "      enabled: true\n",
    "      interval: 30s\n",
    "      path: /metrics\n",
    "      labels: {}\n",
    "  \n",
    "  grafana:\n",
    "    enabled: true\n",
    "    dashboards:\n",
    "      enabled: true\n",
    "  \n",
    "  alerting:\n",
    "    enabled: true\n",
    "    rules:\n",
    "      highErrorRate:\n",
    "        enabled: true\n",
    "        threshold: 0.1\n",
    "      highLatency:\n",
    "        enabled: true\n",
    "        threshold: 1000\n",
    "      highMemoryUsage:\n",
    "        enabled: true\n",
    "        threshold: 3000\n",
    "\n",
    "# Environment variables\n",
    "env:\n",
    "  - name: MODEL_PATH\n",
    "    value: \"/app/models/model.pth\"\n",
    "  - name: MAX_BATCH_SIZE\n",
    "    value: \"32\"\n",
    "  - name: BATCH_TIMEOUT\n",
    "    value: \"0.01\"\n",
    "  - name: LOG_LEVEL\n",
    "    value: \"INFO\"\n",
    "  - name: PROMETHEUS_METRICS\n",
    "    value: \"true\"\n",
    "\n",
    "# Additional labels\n",
    "labels: {}\n",
    "\n",
    "# Additional annotations\n",
    "annotations: {}\n",
    "'''\n",
    "        \n",
    "        # templates/deployment.yaml\n",
    "        deployment_template = '''{{- $fullName := include \"pytorch-model-server.fullname\" . -}}\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: {{ $fullName }}\n",
    "  labels:\n",
    "    {{- include \"pytorch-model-server.labels\" . | nindent 4 }}\n",
    "spec:\n",
    "  {{- if not .Values.autoscaling.enabled }}\n",
    "  replicas: {{ .Values.replicaCount }}\n",
    "  {{- end }}\n",
    "  strategy:\n",
    "    {{- toYaml .Values.strategy | nindent 4 }}\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      {{- include \"pytorch-model-server.selectorLabels\" . | nindent 6 }}\n",
    "  template:\n",
    "    metadata:\n",
    "      annotations:\n",
    "        {{- with .Values.podAnnotations }}\n",
    "        {{- toYaml . | nindent 8 }}\n",
    "        {{- end }}\n",
    "      labels:\n",
    "        {{- include \"pytorch-model-server.selectorLabels\" . | nindent 8 }}\n",
    "    spec:\n",
    "      {{- with .Values.imagePullSecrets }}\n",
    "      imagePullSecrets:\n",
    "        {{- toYaml . | nindent 8 }}\n",
    "      {{- end }}\n",
    "      serviceAccountName: {{ include \"pytorch-model-server.serviceAccountName\" . }}\n",
    "      securityContext:\n",
    "        {{- toYaml .Values.podSecurityContext | nindent 8 }}\n",
    "      containers:\n",
    "        - name: {{ .Chart.Name }}\n",
    "          securityContext:\n",
    "            {{- toYaml .Values.securityContext | nindent 12 }}\n",
    "          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n",
    "          imagePullPolicy: {{ .Values.image.pullPolicy }}\n",
    "          ports:\n",
    "            - name: http\n",
    "              containerPort: 8000\n",
    "              protocol: TCP\n",
    "          env:\n",
    "            {{- toYaml .Values.env | nindent 12 }}\n",
    "          livenessProbe:\n",
    "            httpGet:\n",
    "              path: /health\n",
    "              port: http\n",
    "            {{- toYaml .Values.healthCheck.livenessProbe | nindent 12 }}\n",
    "          readinessProbe:\n",
    "            httpGet:\n",
    "              path: /health\n",
    "              port: http\n",
    "            {{- toYaml .Values.healthCheck.readinessProbe | nindent 12 }}\n",
    "          startupProbe:\n",
    "            httpGet:\n",
    "              path: /health\n",
    "              port: http\n",
    "            {{- toYaml .Values.healthCheck.startupProbe | nindent 12 }}\n",
    "          resources:\n",
    "            {{- toYaml .Values.resources | nindent 12 }}\n",
    "          volumeMounts:\n",
    "          {{- if .Values.storage.models.enabled }}\n",
    "            - name: model-storage\n",
    "              mountPath: /app/models\n",
    "              readOnly: true\n",
    "          {{- end }}\n",
    "          {{- if .Values.storage.logs.enabled }}\n",
    "            - name: log-storage\n",
    "              mountPath: /app/logs\n",
    "          {{- end }}\n",
    "            - name: tmp-storage\n",
    "              mountPath: /tmp\n",
    "      volumes:\n",
    "      {{- if .Values.storage.models.enabled }}\n",
    "        - name: model-storage\n",
    "          persistentVolumeClaim:\n",
    "            claimName: {{ $fullName }}-models\n",
    "      {{- end }}\n",
    "      {{- if .Values.storage.logs.enabled }}\n",
    "        - name: log-storage\n",
    "          emptyDir:\n",
    "            sizeLimit: {{ .Values.storage.logs.size }}\n",
    "      {{- end }}\n",
    "        - name: tmp-storage\n",
    "          emptyDir:\n",
    "            sizeLimit: 1Gi\n",
    "      {{- with .Values.nodeSelector }}\n",
    "      nodeSelector:\n",
    "        {{- toYaml . | nindent 8 }}\n",
    "      {{- end }}\n",
    "      {{- with .Values.affinity }}\n",
    "      affinity:\n",
    "        {{- toYaml . | nindent 8 }}\n",
    "      {{- end }}\n",
    "      {{- with .Values.tolerations }}\n",
    "      tolerations:\n",
    "        {{- toYaml . | nindent 8 }}\n",
    "      {{- end }}\n",
    "'''\n",
    "        \n",
    "        return {\n",
    "            'Chart.yaml': chart_yaml,\n",
    "            'values.yaml': values_yaml,\n",
    "            'templates/deployment.yaml': deployment_template\n",
    "        }\n",
    "\n",
    "# Generate all deployment configurations\n",
    "print(\"\\nüîß GENERATING DEPLOYMENT CONFIGURATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "config_generator = DeploymentConfigGenerator()\n",
    "\n",
    "# Generate configurations\n",
    "dockerfile = config_generator.generate_dockerfile()\n",
    "docker_compose = config_generator.generate_docker_compose()\n",
    "k8s_manifests = config_generator.generate_kubernetes_manifests()\n",
    "helm_chart = config_generator.generate_helm_chart()\n",
    "\n",
    "# Save configurations to files\n",
    "configs_dir = results_dir / 'deployment_configs'\n",
    "configs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save Dockerfile\n",
    "with open(configs_dir / 'Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile)\n",
    "\n",
    "# Save Docker Compose\n",
    "with open(configs_dir / 'docker-compose.yml', 'w') as f:\n",
    "    f.write(docker_compose)\n",
    "\n",
    "# Save Kubernetes manifests\n",
    "k8s_dir = configs_dir / 'kubernetes'\n",
    "k8s_dir.mkdir(exist_ok=True)\n",
    "for name, content in k8s_manifests.items():\n",
    "    with open(k8s_dir / f'{name}.yaml', 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Save Helm chart\n",
    "helm_dir = configs_dir / 'helm' / 'pytorch-model-server'\n",
    "helm_dir.mkdir(parents=True, exist_ok=True)\n",
    "for name, content in helm_chart.items():\n",
    "    file_path = helm_dir / name\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"‚úÖ Dockerfile generated\")\n",
    "print(\"‚úÖ Docker Compose configuration created\")\n",
    "print(\"‚úÖ Kubernetes manifests generated (8 files)\")\n",
    "print(\"‚úÖ Helm chart created with templates\")\n",
    "print(f\"üìÅ All configurations saved to: {configs_dir}\")\n",
    "```\n",
    "\n",
    "## 9. Production Monitoring and Alerting Setup\n",
    "\n",
    "```python\n",
    "class MonitoringConfigGenerator:\n",
    "    \"\"\"Generate comprehensive monitoring and alerting configurations.\"\"\"\n",
    "    \n",
    "    def generate_prometheus_config(self) -> str:\n",
    "        \"\"\"Generate Prometheus configuration with comprehensive scraping.\"\"\"\n",
    "        \n",
    "        prometheus_yml = '''# Prometheus configuration for PyTorch model serving\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "  evaluation_interval: 15s\n",
    "  external_labels:\n",
    "    cluster: 'pytorch-production'\n",
    "    environment: 'production'\n",
    "\n",
    "rule_files:\n",
    "  - \"alert_rules.yml\"\n",
    "  - \"recording_rules.yml\"\n",
    "\n",
    "scrape_configs:\n",
    "  # PyTorch model server metrics\n",
    "  - job_name: 'pytorch-model-server'\n",
    "    static_configs:\n",
    "      - targets: ['pytorch-api:8000']\n",
    "    metrics_path: '/metrics'\n",
    "    scrape_interval: 5s\n",
    "    scrape_timeout: 3s\n",
    "    honor_labels: true\n",
    "    params:\n",
    "      format: ['prometheus']\n",
    "\n",
    "  # Prometheus self-monitoring\n",
    "  - job_name: 'prometheus'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:9090']\n",
    "    scrape_interval: 30s\n",
    "\n",
    "  # Node exporter for system metrics\n",
    "  - job_name: 'node-exporter'\n",
    "    static_configs:\n",
    "      - targets: ['node-exporter:9100']\n",
    "    scrape_interval: 15s\n",
    "\n",
    "  # Nginx metrics\n",
    "  - job_name: 'nginx'\n",
    "    static_configs:\n",
    "      - targets: ['nginx:9113']\n",
    "    scrape_interval: 15s\n",
    "\n",
    "  # Redis metrics\n",
    "  - job_name: 'redis'\n",
    "    static_configs:\n",
    "      - targets: ['redis-exporter:9121']\n",
    "    scrape_interval: 15s\n",
    "\n",
    "alerting:\n",
    "  alertmanagers:\n",
    "    - static_configs:\n",
    "        - targets:\n",
    "          - alertmanager:9093\n",
    "      timeout: 10s\n",
    "      api_version: v2\n",
    "\n",
    "# Remote write for long-term storage (optional)\n",
    "# remote_write:\n",
    "#   - url: \"https://prometheus.example.com/api/v1/write\"\n",
    "#     basic_auth:\n",
    "#       username: \"prometheus\"\n",
    "#       password: \"password\"\n",
    "        '''\n",
    "        \n",
    "        return prometheus_yml\n",
    "    \n",
    "    def generate_alert_rules(self) -> str:\n",
    "        \"\"\"Generate comprehensive Prometheus alert rules.\"\"\"\n",
    "        \n",
    "        alert_rules_yml = '''# Alert rules for PyTorch model serving\n",
    "groups:\n",
    "- name: pytorch-model-server-alerts\n",
    "  interval: 30s\n",
    "  rules:\n",
    "  \n",
    "  # Service availability alerts\n",
    "  - alert: ModelServerDown\n",
    "    expr: up{job=\"pytorch-model-server\"} == 0\n",
    "    for: 1m\n",
    "    labels:\n",
    "      severity: critical\n",
    "      service: pytorch-model-server\n",
    "    annotations:\n",
    "      summary: \"PyTorch model server is down\"\n",
    "      description: \"PyTorch model server {{ $labels.instance }} has been down for more than 1 minute.\"\n",
    "      runbook_url: \"https://wiki.example.com/runbooks/pytorch-server-down\"\n",
    "\n",
    "  # Error rate alerts\n",
    "  - alert: HighErrorRate\n",
    "    expr: |\n",
    "      (\n",
    "        rate(model_requests_failed_total[5m]) / \n",
    "        rate(model_requests_total[5m])\n",
    "      ) > 0.1\n",
    "    for: 2m\n",
    "    labels:\n",
    "      severity: warning\n",
    "      service: pytorch-model-server\n",
    "    annotations:\n",
    "      summary: \"High error rate detected\"\n",
    "      description: \"Error rate is {{ $value | humanizePercentage }} over the last 5 minutes on {{ $labels.instance }}.\"\n",
    "\n",
    "  - alert: CriticalErrorRate\n",
    "    expr: |\n",
    "      (\n",
    "        rate(model_requests_failed_total[5m]) / \n",
    "        rate(model_requests_total[5m])\n",
    "      ) > 0.25\n",
    "    for: 1m\n",
    "    labels:\n",
    "      severity: critical\n",
    "      service: pytorch-model-server\n",
    "    annotations:\n",
    "      summary: \"Critical error rate detected\"\n",
    "      description: \"Error rate is {{ $value | humanizePercentage }} over the last 5 minutes on {{ $labels.instance }}.\"\n",
    "\n",
    "  # Latency alerts\n",
    "  - alert: HighLatency\n",
    "    expr: |\n",
    "      histogram_quantile(0.95, \n",
    "        rate(model_inference_duration_seconds_bucket[5m])\n",
    "      ) > 0.1\n",
    "    for: 3m\n",
    "    labels:\n",
    "      severity: warning\n",
    "      service: pytorch-model-server\n",
    "    annotations:\n",
    "      summary: \"High inference latency\"\n",
    "      description: \"95th percentile latency is {{ $value }}s over the last 5 minutes on {{ $labels.instance }}.\"\n",
    "\n",
    "  - alert: ExtremeLatency\n",
    "    expr: |\n",
    "      histogram_quantile(0.95, \n",
    "        rate(model_inference_duration_seconds_bucket[5m])\n",
    "      ) > 0.5\n",
    "    for: 1m\n",
    "    labels:\n",
    "      severity: critical\n",
    "      service: pytorch-model-server\n",
    "    annotations:\n",
    "      summary: \"Extreme inference latency\"\n",
    "      description: \"95th percentile latency is {{ $value }}s over the last 5 minutes on {{ $labels.instance }}.\"\n",
    "\n",
    "  # Resource usage alerts\n",
    "  - alert: HighMemoryUsage\n",
    "    expr: process_resident_memory_bytes / (1024 * 1024) > 3000\n",
    "    for: 5m\n",
    "    labels:\n",
    "      severity: warning\n",
    "      service: pytorch-model-server\n",
    "    annotations:\n",
    "      summary: \"High memory usage\"\n",
    "      description: \"Memory usage is {{ $value }}MB on {{ $labels.instance }}, above 3GB threshold.\"\n",
    "\n",
    "  - alert: CriticalMemoryUsage\n",
    "    expr: process_resident_memory_bytes / (1024 * 1024) > 4500\n",
    "    for: 2m\n",
    "    labels:\n",
    "      severity: critical\n",
    "      service: pytorch-model-server\n",
    "    annotations:\n",
    "      summary: \"Critical memory usage\"\n",
    "      description: \"Memory usage is {{ $value }}MB on {{ $labels.instance }}, approaching limit.\"\n",
    "\n",
    "  - alert: HighCPUUsage\n",
    "    expr: rate(process_cpu_seconds_total[5m]) * 100 > 80\n",
    "    for: 5m\n",
    "    labels:\n",
    "      severity: warning\n",
    "      service: pytorch-model-server\n",
    "    annotations:\n",
    "      summary: \"High CPU usage\"\n",
    "      description: \"CPU usage is {{ $value }}% over the last 5 minutes on {{ $labels.instance }}.\"\n",
    "\n",
    "  # Throughput alerts\n",
    "  - alert: LowThroughput\n",
    "    expr: rate(model_requests_total[5m]) < 1\n",
    "    for: 10m\n",
    "    labels:\n",
    "      severity: info\n",
    "      service: pytorch-model-server\n",
    "    annotations:\n",
    "      summary: \"Low request throughput\"\n",
    "      description: \"Request rate is {{ $value }} requests/second over the last 5 minutes on {{ $labels.instance }}.\"\n",
    "\n",
    "  # Queue length alerts\n",
    "  - alert: HighQueueLength\n",
    "    expr: model_queue_length > 50\n",
    "    for: 2m\n",
    "    labels:\n",
    "      severity: warning\n",
    "      service: pytorch-model-server\n",
    "    annotations:\n",
    "      summary: \"High request queue length\"\n",
    "      description: \"Request queue length is {{ $value }} on {{ $labels.instance }}.\"\n",
    "\n",
    "  # Batch processing alerts\n",
    "  - alert: LowBatchEfficiency\n",
    "    expr: model_batch_size_avg < 4\n",
    "    for: 10m\n",
    "    labels:\n",
    "      severity: info\n",
    "      service: pytorch-model-server\n",
    "    annotations:\n",
    "      summary: \"Low batch processing efficiency\"\n",
    "      description: \"Average batch size is {{ $value }} on {{ $labels.instance }}, consider tuning batch parameters.\"\n",
    "\n",
    "- name: infrastructure-alerts\n",
    "  interval: 30s\n",
    "  rules:\n",
    "  \n",
    "  # System resource alerts\n",
    "  - alert: HighSystemLoad\n",
    "    expr: node_load1 > 4\n",
    "    for: 5m\n",
    "    labels:\n",
    "      severity: warning\n",
    "      service: infrastructure\n",
    "    annotations:\n",
    "      summary: \"High system load\"\n",
    "      description: \"System load is {{ $value }} on {{ $labels.instance }}.\"\n",
    "\n",
    "  - alert: LowDiskSpace\n",
    "    expr: |\n",
    "      (\n",
    "        node_filesystem_avail_bytes{mountpoint=\"/\"} / \n",
    "        node_filesystem_size_bytes{mountpoint=\"/\"}\n",
    "      ) * 100 < 20\n",
    "    for: 5m\n",
    "    labels:\n",
    "      severity: warning\n",
    "      service: infrastructure\n",
    "    annotations:\n",
    "      summary: \"Low disk space\"\n",
    "      description: \"Disk space is {{ $value }}% available on {{ $labels.instance }}.\"\n",
    "\n",
    "  - alert: HighNetworkLatency\n",
    "    expr: avg_over_time(probe_duration_seconds[5m]) > 0.1\n",
    "    for: 3m\n",
    "    labels:\n",
    "      severity: warning\n",
    "      service: infrastructure\n",
    "    annotations:\n",
    "      summary: \"High network latency\"\n",
    "      description: \"Network latency is {{ $value }}s to {{ $labels.instance }}.\"\n",
    "        '''\n",
    "        \n",
    "        return alert_rules_yml\n",
    "    \n",
    "    def generate_grafana_dashboard(self) -> str:\n",
    "        \"\"\"Generate comprehensive Grafana dashboard configuration.\"\"\"\n",
    "        \n",
    "        dashboard_json = '''{\n",
    "  \"dashboard\": {\n",
    "    \"id\": null,\n",
    "    \"title\": \"PyTorch Model Server - Production Dashboard\",\n",
    "    \"tags\": [\"pytorch\", \"ml\", \"api\", \"production\"],\n",
    "    \"timezone\": \"browser\",\n",
    "    \"refresh\": \"5s\",\n",
    "    \"time\": {\n",
    "      \"from\": \"now-1h\",\n",
    "      \"to\": \"now\"\n",
    "    },\n",
    "    \"panels\": [\n",
    "      {\n",
    "        \"id\": 1,\n",
    "        \"title\": \"Request Rate\",\n",
    "        \"type\": \"stat\",\n",
    "        \"targets\": [\n",
    "          {\n",
    "            \"expr\": \"rate(model_requests_total[5m])\",\n",
    "            \"legendFormat\": \"Requests/sec\",\n",
    "            \"refId\": \"A\"\n",
    "          }\n",
    "        ],\n",
    "        \"fieldConfig\": {\n",
    "          \"defaults\": {\n",
    "            \"unit\": \"reqps\",\n",
    "            \"color\": {\n",
    "              \"mode\": \"thresholds\"\n",
    "            },\n",
    "            \"thresholds\": {\n",
    "              \"steps\": [\n",
    "                {\"color\": \"red\", \"value\": 0},\n",
    "                {\"color\": \"yellow\", \"value\": 10},\n",
    "                {\"color\": \"green\", \"value\": 50}\n",
    "              ]\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"gridPos\": {\"h\": 8, \"w\": 6, \"x\": 0, \"y\": 0}\n",
    "      },\n",
    "      {\n",
    "        \"id\": 2,\n",
    "        \"title\": \"Error Rate\",\n",
    "        \"type\": \"stat\",\n",
    "        \"targets\": [\n",
    "          {\n",
    "            \"expr\": \"rate(model_requests_failed_total[5m]) / rate(model_requests_total[5m])\",\n",
    "            \"legendFormat\": \"Error Rate\",\n",
    "            \"refId\": \"A\"\n",
    "          }\n",
    "        ],\n",
    "        \"fieldConfig\": {\n",
    "          \"defaults\": {\n",
    "            \"unit\": \"percentunit\",\n",
    "            \"color\": {\n",
    "              \"mode\": \"thresholds\"\n",
    "            },\n",
    "            \"thresholds\": {\n",
    "              \"steps\": [\n",
    "                {\"color\": \"green\", \"value\": 0},\n",
    "                {\"color\": \"yellow\", \"value\": 0.05},\n",
    "                {\"color\": \"red\", \"value\": 0.1}\n",
    "              ]\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"gridPos\": {\"h\": 8, \"w\": 6, \"x\": 6, \"y\": 0}\n",
    "      },\n",
    "      {\n",
    "        \"id\": 3,\n",
    "        \"title\": \"Response Time\",\n",
    "        \"type\": \"stat\",\n",
    "        \"targets\": [\n",
    "          {\n",
    "            \"expr\": \"histogram_quantile(0.95, rate(model_inference_duration_seconds_bucket[5m]))\",\n",
    "            \"legendFormat\": \"P95 Latency\",\n",
    "            \"refId\": \"A\"\n",
    "          }\n",
    "        ],\n",
    "        \"fieldConfig\": {\n",
    "          \"defaults\": {\n",
    "            \"unit\": \"s\",\n",
    "            \"color\": {\n",
    "              \"mode\": \"thresholds\"\n",
    "            },\n",
    "            \"thresholds\": {\n",
    "              \"steps\": [\n",
    "                {\"color\": \"green\", \"value\": 0},\n",
    "                {\"color\": \"yellow\", \"value\": 0.1},\n",
    "                {\"color\": \"red\", \"value\": 0.5}\n",
    "              ]\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"gridPos\": {\"h\": 8, \"w\": 6, \"x\": 12, \"y\": 0}\n",
    "      },\n",
    "      {\n",
    "        \"id\": 4,\n",
    "        \"title\": \"Active Connections\",\n",
    "        \"type\": \"stat\",\n",
    "        \"targets\": [\n",
    "          {\n",
    "            \"expr\": \"model_queue_length\",\n",
    "            \"legendFormat\": \"Queue Length\",\n",
    "            \"refId\": \"A\"\n",
    "          }\n",
    "        ],\n",
    "        \"fieldConfig\": {\n",
    "          \"defaults\": {\n",
    "            \"unit\": \"short\",\n",
    "            \"color\": {\n",
    "              \"mode\": \"thresholds\"\n",
    "            },\n",
    "            \"thresholds\": {\n",
    "              \"steps\": [\n",
    "                {\"color\": \"green\", \"value\": 0},\n",
    "                {\"color\": \"yellow\", \"value\": 20},\n",
    "                {\"color\": \"red\", \"value\": 50}\n",
    "              ]\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"gridPos\": {\"h\": 8, \"w\": 6, \"x\": 18, \"y\": 0}\n",
    "      },\n",
    "      {\n",
    "        \"id\": 5,\n",
    "        \"title\": \"Request Rate Over Time\",\n",
    "        \"type\": \"graph\",\n",
    "        \"targets\": [\n",
    "          {\n",
    "            \"expr\": \"rate(model_requests_total[5m])\",\n",
    "            \"legendFormat\": \"Total Requests/sec\",\n",
    "            \"refId\": \"A\"\n",
    "          },\n",
    "          {\n",
    "            \"expr\": \"rate(model_requests_successful_total[5m])\",\n",
    "            \"legendFormat\": \"Successful Requests/sec\",\n",
    "            \"refId\": \"B\"\n",
    "          },\n",
    "          {\n",
    "            \"expr\": \"rate(model_requests_failed_total[5m])\",\n",
    "            \"legendFormat\": \"Failed Requests/sec\",\n",
    "            \"refId\": \"C\"\n",
    "          }\n",
    "        ],\n",
    "        \"yAxes\": [\n",
    "          {\n",
    "            \"label\": \"Requests/sec\",\n",
    "            \"min\": 0\n",
    "          }\n",
    "        ],\n",
    "        \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 0, \"y\": 8}\n",
    "      },\n",
    "      {\n",
    "        \"id\": 6,\n",
    "        \"title\": \"Latency Percentiles\",\n",
    "        \"type\": \"graph\",\n",
    "        \"targets\": [\n",
    "          {\n",
    "            \"expr\": \"histogram_quantile(0.50, rate(model_inference_duration_seconds_bucket[5m]))\",\n",
    "            \"legendFormat\": \"P50\",\n",
    "            \"refId\": \"A\"\n",
    "          },\n",
    "          {\n",
    "            \"expr\": \"histogram_quantile(0.95, rate(model_inference_duration_seconds_bucket[5m]))\",\n",
    "            \"legendFormat\": \"P95\",\n",
    "            \"refId\": \"B\"\n",
    "          },\n",
    "          {\n",
    "            \"expr\": \"histogram_quantile(0.99, rate(model_inference_duration_seconds_bucket[5m]))\",\n",
    "            \"legendFormat\": \"P99\",\n",
    "            \"refId\": \"C\"\n",
    "          }\n",
    "        ],\n",
    "        \"yAxes\": [\n",
    "          {\n",
    "            \"label\": \"Seconds\",\n",
    "            \"min\": 0\n",
    "          }\n",
    "        ],\n",
    "        \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 12, \"y\": 8}\n",
    "      },\n",
    "      {\n",
    "        \"id\": 7,\n",
    "        \"title\": \"Resource Usage\",\n",
    "        \"type\": \"graph\",\n",
    "        \"targets\": [\n",
    "          {\n",
    "            \"expr\": \"process_resident_memory_bytes / (1024 * 1024)\",\n",
    "            \"legendFormat\": \"Memory (MB)\",\n",
    "            \"refId\": \"A\"\n",
    "          },\n",
    "          {\n",
    "            \"expr\": \"rate(process_cpu_seconds_total[5m]) * 100\",\n",
    "            \"legendFormat\": \"CPU (%)\",\n",
    "            \"refId\": \"B\"\n",
    "          }\n",
    "        ],\n",
    "        \"yAxes\": [\n",
    "          {\n",
    "            \"label\": \"Usage\",\n",
    "            \"min\": 0\n",
    "          }\n",
    "        ],\n",
    "        \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 0, \"y\": 16}\n",
    "      },\n",
    "      {\n",
    "        \"id\": 8,\n",
    "        \"title\": \"Batch Processing Efficiency\",\n",
    "        \"type\": \"graph\",\n",
    "        \"targets\": [\n",
    "          {\n",
    "            \"expr\": \"model_batch_size_avg\",\n",
    "            \"legendFormat\": \"Avg Batch Size\",\n",
    "            \"refId\": \"A\"\n",
    "          },\n",
    "          {\n",
    "            \"expr\": \"rate(model_batches_processed_total[5m])\",\n",
    "            \"legendFormat\": \"Batches/sec\",\n",
    "            \"refId\": \"B\"\n",
    "          }\n",
    "        ],\n",
    "        \"yAxes\": [\n",
    "          {\n",
    "            \"label\": \"Count\",\n",
    "            \"min\": 0\n",
    "          }\n",
    "        ],\n",
    "        \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 12, \"y\": 16}\n",
    "      }\n",
    "    ],\n",
    "    \"templating\": {\n",
    "      \"list\": [\n",
    "        {\n",
    "          \"name\": \"instance\",\n",
    "          \"type\": \"query\",\n",
    "          \"query\": \"label_values(up{job=\\\"pytorch-model-server\\\"}, instance)\",\n",
    "          \"refresh\": 1,\n",
    "          \"includeAll\": true,\n",
    "          \"multi\": true\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"annotations\": {\n",
    "      \"list\": [\n",
    "        {\n",
    "          \"name\": \"Deployments\",\n",
    "          \"datasource\": \"Prometheus\",\n",
    "          \"expr\": \"resets(process_start_time_seconds[1h]) > 0\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}'''\n",
    "        \n",
    "        return dashboard_json\n",
    "    \n",
    "    def generate_alertmanager_config(self) -> str:\n",
    "        \"\"\"Generate Alertmanager configuration for notification routing.\"\"\"\n",
    "        \n",
    "        alertmanager_yml = '''# Alertmanager configuration\n",
    "global:\n",
    "  smtp_smarthost: 'localhost:587'\n",
    "  smtp_from: 'alerts@yourcompany.com'\n",
    "  slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'\n",
    "\n",
    "# Templates for notifications\n",
    "templates:\n",
    "  - '/etc/alertmanager/templates/*.tmpl'\n",
    "\n",
    "# Routing tree\n",
    "route:\n",
    "  group_by: ['alertname', 'service']\n",
    "  group_wait: 10s\n",
    "  group_interval: 10s\n",
    "  repeat_interval: 1h\n",
    "  receiver: 'default'\n",
    "  routes:\n",
    "  \n",
    "  # Critical alerts - immediate notification\n",
    "  - match:\n",
    "      severity: critical\n",
    "    receiver: 'critical-alerts'\n",
    "    group_wait: 0s\n",
    "    repeat_interval: 5m\n",
    "    \n",
    "  # Warning alerts - standard notification\n",
    "  - match:\n",
    "      severity: warning\n",
    "    receiver: 'warning-alerts'\n",
    "    repeat_interval: 30m\n",
    "    \n",
    "  # Info alerts - low priority\n",
    "  - match:\n",
    "      severity: info\n",
    "    receiver: 'info-alerts'\n",
    "    repeat_interval: 4h\n",
    "\n",
    "# Notification receivers\n",
    "receivers:\n",
    "- name: 'default'\n",
    "  slack_configs:\n",
    "  - channel: '#alerts'\n",
    "    title: 'PyTorch Model Server Alert'\n",
    "    text: '{{ range .Alerts }}{{ .Annotations.summary }}\\\\n{{ .Annotations.description }}{{ end }}'\n",
    "\n",
    "- name: 'critical-alerts'\n",
    "  email_configs:\n",
    "  - to: 'oncall@yourcompany.com'\n",
    "    subject: '[CRITICAL] PyTorch Model Server Alert'\n",
    "    body: |\n",
    "      {{ range .Alerts }}\n",
    "      Alert: {{ .Annotations.summary }}\n",
    "      Description: {{ .Annotations.description }}\n",
    "      Severity: {{ .Labels.severity }}\n",
    "      Service: {{ .Labels.service }}\n",
    "      Instance: {{ .Labels.instance }}\n",
    "      {{ end }}\n",
    "  slack_configs:\n",
    "  - channel: '#critical-alerts'\n",
    "    title: 'üö® CRITICAL: PyTorch Model Server'\n",
    "    text: |\n",
    "      {{ range .Alerts }}\n",
    "      *{{ .Annotations.summary }}*\n",
    "      {{ .Annotations.description }}\n",
    "      Severity: {{ .Labels.severity }}\n",
    "      {{ end }}\n",
    "    color: 'danger'\n",
    "  pagerduty_configs:\n",
    "  - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'\n",
    "    description: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'\n",
    "\n",
    "- name: 'warning-alerts'\n",
    "  slack_configs:\n",
    "  - channel: '#alerts'\n",
    "    title: '‚ö†Ô∏è WARNING: PyTorch Model Server'\n",
    "    text: |\n",
    "      {{ range .Alerts }}\n",
    "      *{{ .Annotations.summary }}*\n",
    "      {{ .Annotations.description }}\n",
    "      {{ end }}\n",
    "    color: 'warning'\n",
    "\n",
    "- name: 'info-alerts'\n",
    "  slack_configs:\n",
    "  - channel: '#monitoring'\n",
    "    title: '‚ÑπÔ∏è INFO: PyTorch Model Server'\n",
    "    text: |\n",
    "      {{ range .Alerts }}\n",
    "      {{ .Annotations.summary }}\n",
    "      {{ end }}\n",
    "    color: 'good'\n",
    "\n",
    "# Inhibition rules to reduce noise\n",
    "inhibit_rules:\n",
    "- source_match:\n",
    "    severity: 'critical'\n",
    "  target_match:\n",
    "    severity: 'warning'\n",
    "  equal: ['alertname', 'service', 'instance']\n",
    "\n",
    "- source_match:\n",
    "    alertname: 'ModelServerDown'\n",
    "  target_match_re:\n",
    "    alertname: 'High.*'\n",
    "  equal: ['service', 'instance']\n",
    "        '''\n",
    "        \n",
    "        return alertmanager_yml\n",
    "\n",
    "# Generate monitoring configurations\n",
    "print(\"\\nüìä GENERATING MONITORING CONFIGURATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "monitoring_generator = MonitoringConfigGenerator()\n",
    "\n",
    "# Generate configurations\n",
    "prometheus_config = monitoring_generator.generate_prometheus_config()\n",
    "alert_rules = monitoring_generator.generate_alert_rules()\n",
    "grafana_dashboard = monitoring_generator.generate_grafana_dashboard()\n",
    "alertmanager_config = monitoring_generator.generate_alertmanager_config()\n",
    "\n",
    "# Save monitoring configurations\n",
    "monitoring_dir = results_dir / 'monitoring'\n",
    "monitoring_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save Prometheus config\n",
    "with open(monitoring_dir / 'prometheus.yml', 'w') as f:\n",
    "    f.write(prometheus_config)\n",
    "\n",
    "# Save alert rules\n",
    "with open(monitoring_dir / 'alert_rules.yml', 'w') as f:\n",
    "    f.write(alert_rules)\n",
    "\n",
    "# Save Grafana dashboard\n",
    "with open(monitoring_dir / 'grafana_dashboard.json', 'w') as f:\n",
    "    f.write(grafana_dashboard)\n",
    "\n",
    "# Save Alertmanager config\n",
    "with open(monitoring_dir / 'alertmanager.yml', 'w') as f:\n",
    "    f.write(alertmanager_config)\n",
    "\n",
    "print(\"‚úÖ Prometheus configuration generated\")\n",
    "print(\"‚úÖ Comprehensive alert rules created (20+ alerts)\")\n",
    "print(\"‚úÖ Grafana dashboard with 8 panels created\")\n",
    "print(\"‚úÖ Alertmanager configuration with routing generated\")\n",
    "print(f\"üìÅ Monitoring configs saved to: {monitoring_dir}\")\n",
    "```\n",
    "\n",
    "## 10. CLI Tools and Management Scripts\n",
    "\n",
    "```python\n",
    "class CLIToolsGenerator:\n",
    "    \"\"\"Generate comprehensive CLI tools for deployment and management.\"\"\"\n",
    "    \n",
    "    def generate_deployment_script(self) -> str:\n",
    "        \"\"\"Generate advanced deployment automation script.\"\"\"\n",
    "        \n",
    "        deploy_script = '''#!/bin/bash\n",
    "\n",
    "# PyTorch Model Server Deployment Script v2.0\n",
    "# Comprehensive deployment automation with health checks and rollback\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "# Configuration variables\n",
    "readonly SCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n",
    "readonly PROJECT_ROOT=\"$(cd \"${SCRIPT_DIR}/..\" && pwd)\"\n",
    "readonly DOCKER_IMAGE=\"pytorch-model-server\"\n",
    "readonly DOCKER_TAG=\"${DOCKER_TAG:-v1.0}\"\n",
    "readonly NAMESPACE=\"${NAMESPACE:-pytorch-ml}\"\n",
    "readonly RELEASE_NAME=\"${RELEASE_NAME:-pytorch-model-server}\"\n",
    "readonly TIMEOUT=\"${TIMEOUT:-300}\"\n",
    "\n",
    "# Colors for output\n",
    "readonly RED='\\\\033[0;31m'\n",
    "readonly GREEN='\\\\033[0;32m'\n",
    "readonly YELLOW='\\\\033[1;33m'\n",
    "readonly BLUE='\\\\033[0;34m'\n",
    "readonly NC='\\\\033[0m' # No Color\n",
    "\n",
    "# Logging functions\n",
    "log_info() {\n",
    "    echo -e \"${GREEN}[INFO]${NC} $(date '+%Y-%m-%d %H:%M:%S') $1\"\n",
    "}\n",
    "\n",
    "log_warn() {\n",
    "    echo -e \"${YELLOW}[WARN]${NC} $(date '+%Y-%m-%d %H:%M:%S') $1\"\n",
    "}\n",
    "\n",
    "log_error() {\n",
    "    echo -e \"${RED}[ERROR]${NC} $(date '+%Y-%m-%d %H:%M:%S') $1\"\n",
    "}\n",
    "\n",
    "log_debug() {\n",
    "    if [[ \"${DEBUG:-}\" == \"true\" ]]; then\n",
    "        echo -e \"${BLUE}[DEBUG]${NC} $(date '+%Y-%m-%d %H:%M:%S') $1\"\n",
    "    fi\n",
    "}\n",
    "\n",
    "# Error handling\n",
    "trap 'log_error \"Script failed at line $LINENO\"' ERR\n",
    "\n",
    "# Help function\n",
    "show_help() {\n",
    "    cat << EOF\n",
    "PyTorch Model Server Deployment Script\n",
    "\n",
    "USAGE:\n",
    "    $0 COMMAND [OPTIONS]\n",
    "\n",
    "COMMANDS:\n",
    "    docker                  Deploy using Docker Compose\n",
    "    kubernetes|k8s          Deploy to Kubernetes cluster\n",
    "    helm                    Deploy using Helm chart\n",
    "    test                    Run load tests\n",
    "    health                  Check service health\n",
    "    logs                    View service logs\n",
    "    cleanup                 Clean up deployments\n",
    "    help                    Show this help message\n",
    "\n",
    "OPTIONS:\n",
    "    --tag TAG              Docker image tag (default: v1.0)\n",
    "    --namespace NS         Kubernetes namespace (default: pytorch-ml)\n",
    "    --timeout SECONDS      Deployment timeout (default: 300)\n",
    "    --debug               Enable debug logging\n",
    "    --dry-run             Show what would be done without executing\n",
    "\n",
    "EXAMPLES:\n",
    "    $0 docker --tag v1.1\n",
    "    $0 kubernetes --namespace production --timeout 600\n",
    "    $0 helm --dry-run\n",
    "    $0 test --requests 1000\n",
    "\n",
    "ENVIRONMENT VARIABLES:\n",
    "    DOCKER_TAG            Override default Docker tag\n",
    "    NAMESPACE             Override default Kubernetes namespace\n",
    "    KUBECONFIG            Path to Kubernetes config file\n",
    "    DEBUG                 Enable debug mode (true/false)\n",
    "\n",
    "EOF\n",
    "}\n",
    "\n",
    "# Check prerequisites\n",
    "check_prerequisites() {\n",
    "    log_info \"Checking prerequisites...\"\n",
    "    \n",
    "    local missing_tools=()\n",
    "    \n",
    "    if ! command -v docker &> /dev/null; then\n",
    "        missing_tools+=(\"docker\")\n",
    "    fi\n",
    "    \n",
    "    if [[ \"$1\" == \"kubernetes\" || \"$1\" == \"k8s\" || \"$1\" == \"helm\" ]]; then\n",
    "        if ! command -v kubectl &> /dev/null; then\n",
    "            missing_tools+=(\"kubectl\")\n",
    "        fi\n",
    "        \n",
    "        if [[ \"$1\" == \"helm\" ]] && ! command -v helm &> /dev/null; then\n",
    "            missing_tools+=(\"helm\")\n",
    "        fi\n",
    "    fi\n",
    "    \n",
    "    if [[ ${#missing_tools[@]} -gt 0 ]]; then\n",
    "        log_error \"Missing required tools: ${missing_tools[*]}\"\n",
    "        log_info \"Please install the missing tools and try again\"\n",
    "        exit 1\n",
    "    fi\n",
    "    \n",
    "    log_info \"Prerequisites check passed\"\n",
    "}\n",
    "\n",
    "# Build Docker image with optimizations\n",
    "build_image() {\n",
    "    log_info \"Building Docker image: ${DOCKER_IMAGE}:${DOCKER_TAG}\"\n",
    "    \n",
    "    # Check if Dockerfile exists\n",
    "    if [[ ! -f \"${PROJECT_ROOT}/Dockerfile\" ]]; then\n",
    "        log_error \"Dockerfile not found in ${PROJECT_ROOT}\"\n",
    "        exit 1\n",
    "    fi\n",
    "    \n",
    "    # Build with build cache and multi-stage optimization\n",
    "    docker build \\\\\n",
    "        --build-arg BUILDKIT_INLINE_CACHE=1 \\\\\n",
    "        --cache-from \"${DOCKER_IMAGE}:latest\" \\\\\n",
    "        --tag \"${DOCKER_IMAGE}:${DOCKER_TAG}\" \\\\\n",
    "        --tag \"${DOCKER_IMAGE}:latest\" \\\\\n",
    "        \"${PROJECT_ROOT}\" || {\n",
    "        log_error \"Docker build failed\"\n",
    "        exit 1\n",
    "    }\n",
    "    \n",
    "    # Get image size\n",
    "    local image_size\n",
    "    image_size=$(docker images \"${DOCKER_IMAGE}:${DOCKER_TAG}\" --format \"table {{.Size}}\" | tail -n 1)\n",
    "    log_info \"Docker image built successfully (Size: ${image_size})\"\n",
    "}\n",
    "\n",
    "# Deploy with Docker Compose\n",
    "deploy_docker_compose() {\n",
    "    log_info \"Deploying with Docker Compose...\"\n",
    "    \n",
    "    cd \"${PROJECT_ROOT}\"\n",
    "    \n",
    "    # Check if docker-compose.yml exists\n",
    "    if [[ ! -f \"docker-compose.yml\" ]]; then\n",
    "        log_error \"docker-compose.yml not found\"\n",
    "        exit 1\n",
    "    fi\n",
    "    \n",
    "    # Stop existing services\n",
    "    log_info \"Stopping existing services...\"\n",
    "    docker-compose down --remove-orphans || true\n",
    "    \n",
    "    # Deploy services\n",
    "    log_info \"Starting services...\"\n",
    "    docker-compose up -d --build\n",
    "    \n",
    "    # Wait for health check\n",
    "    log_info \"Waiting for service to be healthy...\"\n",
    "    local max_attempts=30\n",
    "    local attempt=1\n",
    "    \n",
    "    while [[ ${attempt} -le ${max_attempts} ]]; do\n",
    "        if curl -f http://localhost:8000/health &> /dev/null; then\n",
    "            log_info \"Service is healthy after ${attempt} attempts\"\n",
    "            break\n",
    "        fi\n",
    "        \n",
    "        if [[ ${attempt} -eq ${max_attempts} ]]; then\n",
    "            log_error \"Service failed to become healthy after ${max_attempts} attempts\"\n",
    "            docker-compose logs pytorch-api\n",
    "            exit 1\n",
    "        fi\n",
    "        \n",
    "        log_debug \"Health check attempt ${attempt}/${max_attempts} failed, retrying...\"\n",
    "        sleep 5\n",
    "        ((attempt++))\n",
    "    done\n",
    "    \n",
    "    log_info \"Docker Compose deployment completed successfully\"\n",
    "    docker-compose ps\n",
    "}\n",
    "\n",
    "# Deploy to Kubernetes\n",
    "deploy_kubernetes() {\n",
    "    log_info \"Deploying to Kubernetes...\"\n",
    "    \n",
    "    # Check cluster connectivity\n",
    "    if ! kubectl cluster-info &> /dev/null; then\n",
    "        log_error \"Cannot connect to Kubernetes cluster\"\n",
    "        log_info \"Please check your KUBECONFIG and cluster connectivity\"\n",
    "        exit 1\n",
    "    fi\n",
    "    \n",
    "    # Create namespace if it doesn't exist\n",
    "    log_info \"Creating namespace: ${NAMESPACE}\"\n",
    "    kubectl create namespace \"${NAMESPACE}\" --dry-run=client -o yaml | kubectl apply -f -\n",
    "    \n",
    "    # Apply Kubernetes manifests\n",
    "    local manifests_dir=\"${PROJECT_ROOT}/deployment_configs/kubernetes\"\n",
    "    \n",
    "    if [[ ! -d \"${manifests_dir}\" ]]; then\n",
    "        log_error \"Kubernetes manifests not found in ${manifests_dir}\"\n",
    "        exit 1\n",
    "    fi\n",
    "    \n",
    "    log_info \"Applying Kubernetes manifests...\"\n",
    "    kubectl apply -f \"${manifests_dir}/\" --namespace=\"${NAMESPACE}\"\n",
    "    \n",
    "    # Wait for deployment to be ready\n",
    "    log_info \"Waiting for deployment to be ready...\"\n",
    "    kubectl rollout status deployment/pytorch-model-server \\\\\n",
    "        --namespace=\"${NAMESPACE}\" \\\\\n",
    "        --timeout=\"${TIMEOUT}s\" || {\n",
    "        log_error \"Deployment failed to become ready\"\n",
    "        kubectl describe deployment pytorch-model-server --namespace=\"${NAMESPACE}\"\n",
    "        exit 1\n",
    "    }\n",
    "    \n",
    "    log_info \"Kubernetes deployment completed successfully\"\n",
    "    kubectl get pods --namespace=\"${NAMESPACE}\" -l app=pytorch-model-server\n",
    "}\n",
    "\n",
    "# Deploy using Helm\n",
    "deploy_helm() {\n",
    "    log_info \"Deploying using Helm...\"\n",
    "    \n",
    "    # Check if Helm chart exists\n",
    "    local chart_dir=\"${PROJECT_ROOT}/deployment_configs/helm/pytorch-model-server\"\n",
    "    \n",
    "    if [[ ! -d \"${chart_dir}\" ]]; then\n",
    "        log_error \"Helm chart not found in ${chart_dir}\"\n",
    "        exit 1\n",
    "    fi\n",
    "    \n",
    "    # Validate chart\n",
    "    log_info \"Validating Helm chart...\"\n",
    "    helm lint \"${chart_dir}\" || {\n",
    "        log_error \"Helm chart validation failed\"\n",
    "        exit 1\n",
    "    }\n",
    "    \n",
    "    # Deploy or upgrade\n",
    "    log_info \"Installing/upgrading Helm release: ${RELEASE_NAME}\"\n",
    "    helm upgrade --install \"${RELEASE_NAME}\" \"${chart_dir}\" \\\\\n",
    "        --namespace \"${NAMESPACE}\" \\\\\n",
    "        --create-namespace \\\\\n",
    "        --set image.tag=\"${DOCKER_TAG}\" \\\\\n",
    "        --wait \\\\\n",
    "        --timeout=\"${TIMEOUT}s\" || {\n",
    "        log_error \"Helm deployment failed\"\n",
    "        helm status \"${RELEASE_NAME}\" --namespace=\"${NAMESPACE}\"\n",
    "        exit 1\n",
    "    }\n",
    "    \n",
    "    log_info \"Helm deployment completed successfully\"\n",
    "    helm status \"${RELEASE_NAME}\" --namespace=\"${NAMESPACE}\"\n",
    "}\n",
    "\n",
    "# Health check function\n",
    "health_check() {\n",
    "    local deployment_type=\"$1\"\n",
    "    log_info \"Performing health check for ${deployment_type} deployment...\"\n",
    "    \n",
    "    local endpoint=\"\"\n",
    "    case \"${deployment_type}\" in\n",
    "        \"docker\")\n",
    "            endpoint=\"http://localhost:8000/health\"\n",
    "            ;;\n",
    "        \"kubernetes\"|\"k8s\"|\"helm\")\n",
    "            # Get ingress or service endpoint\n",
    "            local ingress_host\n",
    "            ingress_host=$(kubectl get ingress pytorch-model-ingress \\\\\n",
    "                --namespace=\"${NAMESPACE}\" \\\\\n",
    "                -o jsonpath='{.spec.rules[0].host}' 2>/dev/null || echo \"\")\n",
    "            \n",
    "            if [[ -n \"${ingress_host}\" ]]; then\n",
    "                endpoint=\"https://${ingress_host}/health\"\n",
    "            else\n",
    "                # Use port-forward as fallback\n",
    "                log_info \"Using port-forward for health check...\"\n",
    "                kubectl port-forward service/pytorch-model-service 8080:80 \\\\\n",
    "                    --namespace=\"${NAMESPACE}\" &\n",
    "                local pf_pid=$!\n",
    "                sleep 5\n",
    "                endpoint=\"http://localhost:8080/health\"\n",
    "                trap \"kill ${pf_pid} 2>/dev/null || true\" EXIT\n",
    "            fi\n",
    "            ;;\n",
    "        *)\n",
    "            log_error \"Unknown deployment type: ${deployment_type}\"\n",
    "            exit 1\n",
    "            ;;\n",
    "    esac\n",
    "    \n",
    "    # Perform health check\n",
    "    local response\n",
    "    response=$(curl -s \"${endpoint}\" 2>/dev/null || echo \"FAILED\")\n",
    "    \n",
    "    if echo \"${response}\" | grep -q \"healthy\"; then\n",
    "        log_info \"Health check passed ‚úÖ\"\n",
    "        echo \"${response}\" | python3 -m json.tool 2>/dev/null || echo \"${response}\"\n",
    "    else\n",
    "        log_error \"Health check failed ‚ùå\"\n",
    "        log_error \"Response: ${response}\"\n",
    "        exit 1\n",
    "    fi\n",
    "}\n",
    "\n",
    "# Load testing function\n",
    "run_load_test() {\n",
    "    log_info \"Running load test...\"\n",
    "    \n",
    "    local requests=\"${REQUESTS:-100}\"\n",
    "    local concurrent=\"${CONCURRENT:-10}\"\n",
    "    local endpoint=\"${ENDPOINT:-http://localhost:8000}\"\n",
    "    \n",
    "    # Check if Python and required packages are available\n",
    "    if ! python3 -c \"import requests, asyncio\" &> /dev/null; then\n",
    "        log_error \"Python 3 with 'requests' and 'asyncio' packages required for load testing\"\n",
    "        exit 1\n",
    "    fi\n",
    "    \n",
    "    # Run load test using Python\n",
    "    python3 << EOF\n",
    "import asyncio\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import statistics\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import sys\n",
    "\n",
    "def single_request(endpoint, api_key=\"test_key_123\"):\n",
    "    \"\"\"Make a single request.\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            f\"{endpoint}/predict\",\n",
    "            headers={\"X-API-Key\": api_key, \"Content-Type\": \"application/json\"},\n",
    "            json={\n",
    "                \"tensor_data\": [[[0.5] * 32] * 32] * 3,\n",
    "                \"return_probabilities\": True\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        latency = time.time() - start_time\n",
    "        return latency if response.status_code == 200 else None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def run_load_test(endpoint, num_requests=100, num_workers=10):\n",
    "    \"\"\"Run comprehensive load test.\"\"\"\n",
    "    print(f\"Running load test against: {endpoint}\")\n",
    "    print(f\"Requests: {num_requests}, Concurrent workers: {num_workers}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        start_time = time.time()\n",
    "        futures = [executor.submit(single_request, endpoint) for _ in range(num_requests)]\n",
    "        results = [future.result() for future in futures]\n",
    "        total_time = time.time() - start_time\n",
    "    \n",
    "    # Analyze results\n",
    "    successful = [r for r in results if r is not None]\n",
    "    failed = len(results) - len(successful)\n",
    "    \n",
    "    if successful:\n",
    "        avg_latency = statistics.mean(successful)\n",
    "        min_latency = min(successful)\n",
    "        max_latency = max(successful)\n",
    "        p95_latency = statistics.quantiles(successful, n=20)[18] if len(successful) > 5 else 0\n",
    "        throughput = len(successful) / total_time\n",
    "    else:\n",
    "        avg_latency = min_latency = max_latency = p95_latency = throughput = 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Load Test Results:\")\n",
    "    print(f\"  Total Requests: {num_requests}\")\n",
    "    print(f\"  Successful: {len(successful)}\")\n",
    "    print(f\"  Failed: {failed}\")\n",
    "    print(f\"  Success Rate: {len(successful)/num_requests*100:.1f}%\")\n",
    "    print(f\"  Total Time: {total_time:.2f}s\")\n",
    "    print(f\"  Throughput: {throughput:.1f} RPS\")\n",
    "    print(f\"  Avg Latency: {avg_latency*1000:.1f}ms\")\n",
    "    print(f\"  Min Latency: {min_latency*1000:.1f}ms\")\n",
    "    print(f\"  Max Latency: {max_latency*1000:.1f}ms\")\n",
    "    print(f\"  P95 Latency: {p95_latency*1000:.1f}ms\")\n",
    "    \n",
    "    # Exit with error code if too many failures\n",
    "    if failed / num_requests > 0.1:\n",
    "        print(f\"\\\\nERROR: High failure rate ({failed/num_requests*100:.1f}%)\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_load_test(\"${endpoint}\", ${requests}, ${concurrent})\n",
    "EOF\n",
    "    \n",
    "    local exit_code=$?\n",
    "    if [[ ${exit_code} -eq 0 ]]; then\n",
    "        log_info \"Load test completed successfully\"\n",
    "    else\n",
    "        log_error \"Load test failed\"\n",
    "        exit 1\n",
    "    fi\n",
    "}\n",
    "\n",
    "# View logs function\n",
    "view_logs() {\n",
    "    local deployment_type=\"$1\"\n",
    "    log_info \"Viewing logs for ${deployment_type} deployment...\"\n",
    "    \n",
    "    case \"${deployment_type}\" in\n",
    "        \"docker\")\n",
    "            docker-compose logs -f pytorch-api\n",
    "            ;;\n",
    "        \"kubernetes\"|\"k8s\"|\"helm\")\n",
    "            kubectl logs -f deployment/pytorch-model-server \\\\\n",
    "                --namespace=\"${NAMESPACE}\" \\\\\n",
    "                --max-log-requests=10\n",
    "            ;;\n",
    "        *)\n",
    "            log_error \"Unknown deployment type: ${deployment_type}\"\n",
    "            exit 1\n",
    "            ;;\n",
    "    esac\n",
    "}\n",
    "\n",
    "# Cleanup function\n",
    "cleanup_deployment() {\n",
    "    local deployment_type=\"$1\"\n",
    "    log_info \"Cleaning up ${deployment_type} deployment...\"\n",
    "    \n",
    "    case \"${deployment_type}\" in\n",
    "        \"docker\")\n",
    "            docker-compose down --volumes --remove-orphans\n",
    "            docker system prune -f\n",
    "            ;;\n",
    "        \"kubernetes\"|\"k8s\")\n",
    "            kubectl delete namespace \"${NAMESPACE}\" --ignore-not-found=true\n",
    "            ;;\n",
    "        \"helm\")\n",
    "            helm uninstall \"${RELEASE_NAME}\" --namespace=\"${NAMESPACE}\" || true\n",
    "            kubectl delete namespace \"${NAMESPACE}\" --ignore-not-found=true\n",
    "            ;;\n",
    "        *)\n",
    "            log_error \"Unknown deployment type: ${deployment_type}\"\n",
    "            exit 1\n",
    "            ;;\n",
    "    esac\n",
    "    \n",
    "    log_info \"Cleanup completed\"\n",
    "}\n",
    "\n",
    "# Parse command line arguments\n",
    "parse_args() {\n",
    "    local command=\"\"\n",
    "    \n",
    "    while [[ $# -gt 0 ]]; do\n",
    "        case $1 in\n",
    "            docker|kubernetes|k8s|helm|test|health|logs|cleanup|help)\n",
    "                command=\"$1\"\n",
    "                shift\n",
    "                ;;\n",
    "            --tag)\n",
    "                DOCKER_TAG=\"$2\"\n",
    "                shift 2\n",
    "                ;;\n",
    "            --namespace)\n",
    "                NAMESPACE=\"$2\"\n",
    "                shift 2\n",
    "                ;;\n",
    "            --timeout)\n",
    "                TIMEOUT=\"$2\"\n",
    "                shift 2\n",
    "                ;;\n",
    "            --requests)\n",
    "                REQUESTS=\"$2\"\n",
    "                shift 2\n",
    "                ;;\n",
    "            --concurrent)\n",
    "                CONCURRENT=\"$2\"\n",
    "                shift 2\n",
    "                ;;\n",
    "            --endpoint)\n",
    "                ENDPOINT=\"$2\"\n",
    "                shift 2\n",
    "                ;;\n",
    "            --debug)\n",
    "                DEBUG=\"true\"\n",
    "                shift\n",
    "                ;;\n",
    "            --dry-run)\n",
    "                DRY_RUN=\"true\"\n",
    "                shift\n",
    "                ;;\n",
    "            *)\n",
    "                log_error \"Unknown option: $1\"\n",
    "                show_help\n",
    "                exit 1\n",
    "                ;;\n",
    "        esac\n",
    "    done\n",
    "    \n",
    "    echo \"${command}\"\n",
    "}\n",
    "\n",
    "# Main execution function\n",
    "main() {\n",
    "    local command\n",
    "    command=$(parse_args \"$@\")\n",
    "    \n",
    "    if [[ -z \"${command}\" ]]; then\n",
    "        show_help\n",
    "        exit 1\n",
    "    fi\n",
    "    \n",
    "    log_info \"Starting deployment script for: ${command}\"\n",
    "    log_info \"Configuration: Docker tag=${DOCKER_TAG}, Namespace=${NAMESPACE}, Timeout=${TIMEOUT}s\"\n",
    "    \n",
    "    if [[ \"${DRY_RUN:-}\" == \"true\" ]]; then\n",
    "        log_info \"DRY RUN MODE - No actual changes will be made\"\n",
    "    fi\n",
    "    \n",
    "    case \"${command}\" in\n",
    "        \"docker\")\n",
    "            check_prerequisites docker\n",
    "            if [[ \"${DRY_RUN:-}\" != \"true\" ]]; then\n",
    "                build_image\n",
    "                deploy_docker_compose\n",
    "                health_check docker\n",
    "            fi\n",
    "            ;;\n",
    "        \"kubernetes\"|\"k8s\")\n",
    "            check_prerequisites kubernetes\n",
    "            if [[ \"${DRY_RUN:-}\" != \"true\" ]]; then\n",
    "                build_image\n",
    "                deploy_kubernetes\n",
    "                health_check kubernetes\n",
    "            fi\n",
    "            ;;\n",
    "        \"helm\")\n",
    "            check_prerequisites helm\n",
    "            if [[ \"${DRY_RUN:-}\" != \"true\" ]]; then\n",
    "                build_image\n",
    "                deploy_helm\n",
    "                health_check helm\n",
    "            fi\n",
    "            ;;\n",
    "        \"test\")\n",
    "            if [[ \"${DRY_RUN:-}\" != \"true\" ]]; then\n",
    "                run_load_test\n",
    "            fi\n",
    "            ;;\n",
    "        \"health\")\n",
    "            local deploy_type=\"${DEPLOY_TYPE:-docker}\"\n",
    "            if [[ \"${DRY_RUN:-}\" != \"true\" ]]; then\n",
    "                health_check \"${deploy_type}\"\n",
    "            fi\n",
    "            ;;\n",
    "        \"logs\")\n",
    "            local deploy_type=\"${DEPLOY_TYPE:-docker}\"\n",
    "            if [[ \"${DRY_RUN:-}\" != \"true\" ]]; then\n",
    "                view_logs \"${deploy_type}\"\n",
    "            fi\n",
    "            ;;\n",
    "        \"cleanup\")\n",
    "            local deploy_type=\"${DEPLOY_TYPE:-docker}\"\n",
    "            if [[ \"${DRY_RUN:-}\" != \"true\" ]]; then\n",
    "                cleanup_deployment \"${deploy_type}\"\n",
    "            fi\n",
    "            ;;\n",
    "        \"help\")\n",
    "            show_help\n",
    "            ;;\n",
    "        *)\n",
    "            log_error \"Unknown command: ${command}\"\n",
    "            show_help\n",
    "            exit 1\n",
    "            ;;\n",
    "    esac\n",
    "    \n",
    "    log_info \"Deployment script completed successfully\"\n",
    "}\n",
    "\n",
    "# Run main function with all arguments\n",
    "main \"$@\"\n",
    "        '''\n",
    "        \n",
    "        return deploy_script\n",
    "    \n",
    "    def generate_model_management_script(self) -> str:\n",
    "        \"\"\"Generate comprehensive model management CLI tool.\"\"\"\n",
    "        \n",
    "        model_mgmt_script = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "PyTorch Model Management CLI Tool v2.0\n",
    "Comprehensive model deployment, monitoring, and management utilities.\n",
    "\n",
    "Features:\n",
    "- Health monitoring and alerting\n",
    "- Performance benchmarking\n",
    "- Model versioning and deployment\n",
    "- Real-time metrics visualization\n",
    "- Automated testing and validation\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import statistics\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "# Third-party imports (with fallbacks)\n",
    "try:\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    HAS_VISUALIZATION = True\n",
    "except ImportError:\n",
    "    HAS_VISUALIZATION = False\n",
    "    print(\"Warning: Visualization libraries not available (install pandas, matplotlib, seaborn)\")\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "    HAS_YAML = True\n",
    "except ImportError:\n",
    "    HAS_YAML = False\n",
    "\n",
    "class Colors:\n",
    "    \"\"\"Terminal color codes for better output formatting.\"\"\"\n",
    "    RED = '\\\\033[91m'\n",
    "    GREEN = '\\\\033[92m'\n",
    "    YELLOW = '\\\\033[93m'\n",
    "    BLUE = '\\\\033[94m'\n",
    "    PURPLE = '\\\\033[95m'\n",
    "    CYAN = '\\\\033[96m'\n",
    "    WHITE = '\\\\033[97m'\n",
    "    BOLD = '\\\\033[1m'\n",
    "    UNDERLINE = '\\\\033[4m'\n",
    "    END = '\\\\033[0m'\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Advanced CLI tool for model management and monitoring.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_url: str = \"http://localhost:8000\", api_key: str = \"test_key_123\", \n",
    "                 config_file: Optional[str] = None):\n",
    "        self.api_url = api_url.rstrip('/')\n",
    "        self.api_key = api_key\n",
    "        self.timeout = 30\n",
    "        self.retry_attempts = 3\n",
    "        self.session = None\n",
    "        \n",
    "        # Load configuration from file if provided\n",
    "        if config_file and os.path.exists(config_file):\n",
    "            self._load_config(config_file)\n",
    "        \n",
    "        # Initialize session\n",
    "        self._init_session()\n",
    "        \n",
    "        # Metrics storage\n",
    "        self.metrics_history = []\n",
    "        \n",
    "    def _load_config(self, config_file: str):\n",
    "        \"\"\"Load configuration from YAML or JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(config_file, 'r') as f:\n",
    "                if config_file.endswith('.yaml') or config_file.endswith('.yml'):\n",
    "                    if HAS_YAML:\n",
    "                        config = yaml.safe_load(f)\n",
    "                    else:\n",
    "                        print(f\"{Colors.YELLOW}Warning: YAML support not available{Colors.END}\")\n",
    "                        return\n",
    "                else:\n",
    "                    config = json.load(f)\n",
    "            \n",
    "            self.api_url = config.get('api_url', self.api_url)\n",
    "            self.api_key = config.get('api_key', self.api_key)\n",
    "            self.timeout = config.get('timeout', self.timeout)\n",
    "            self.retry_attempts = config.get('retry_attempts', self.retry_attempts)\n",
    "            \n",
    "            print(f\"{Colors.GREEN}‚úì Configuration loaded from {config_file}{Colors.END}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{Colors.RED}Error loading config: {e}{Colors.END}\")\n",
    "    \n",
    "    def _init_session(self):\n",
    "        \"\"\"Initialize HTTP session with proper headers and settings.\"\"\"\n",
    "        if 'requests' in sys.modules:\n",
    "            self.session = requests.Session()\n",
    "            self.session.headers.update({\n",
    "                \"X-API-Key\": self.api_key,\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"User-Agent\": \"PyTorch-Model-Manager/2.0\",\n",
    "                \"Accept\": \"application/json\"\n",
    "            })\n",
    "    \n",
    "    def _make_request(self, method: str, endpoint: str, **kwargs) -> requests.Response:\n",
    "        \"\"\"Make HTTP request with retry logic and error handling.\"\"\"\n",
    "        url = f\"{self.api_url}{endpoint}\"\n",
    "        \n",
    "        for attempt in range(self.retry_attempts):\n",
    "            try:\n",
    "                if self.session:\n",
    "                    response = self.session.request(method, url, timeout=self.timeout, **kwargs)\n",
    "                else:\n",
    "                    # Fallback for when requests is not available\n",
    "                    print(f\"{Colors.YELLOW}Making simulated request to {url}{Colors.END}\")\n",
    "                    return type('Response', (), {'status_code': 200, 'json': lambda: {'status': 'simulated'}})()\n",
    "                \n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt == self.retry_attempts - 1:\n",
    "                    raise\n",
    "                print(f\"{Colors.YELLOW}Request failed (attempt {attempt + 1}/{self.retry_attempts}): {e}{Colors.END}\")\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        \n",
    "        raise Exception(\"Max retry attempts exceeded\")\n",
    "    \n",
    "    def health_check(self) -> Dict:\n",
    "        \"\"\"Perform comprehensive health check.\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = self._make_request('GET', '/health')\n",
    "            latency = (time.time() - start_time) * 1000\n",
    "            \n",
    "            health_data = response.json()\n",
    "            health_data['response_time_ms'] = latency\n",
    "            \n",
    "            # Determine health status\n",
    "            status = health_data.get('status', 'unknown')\n",
    "            if status == 'healthy' and latency < 1000:\n",
    "                print(f\"{Colors.GREEN}‚úì Service is healthy (response time: {latency:.1f}ms){Colors.END}\")\n",
    "            elif status == 'healthy':\n",
    "                print(f\"{Colors.YELLOW}‚ö† Service is healthy but slow (response time: {latency:.1f}ms){Colors.END}\")\n",
    "            else:\n",
    "                print(f\"{Colors.RED}‚úó Service is unhealthy{Colors.END}\")\n",
    "            \n",
    "            return health_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{Colors.RED}Health check failed: {e}{Colors.END}\")\n",
    "            return {'status': 'error', 'error': str(e)}\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        \"\"\"Get comprehensive model information.\"\"\"\n",
    "        try:\n",
    "            response = self._make_request('GET', '/model/info')\n",
    "            model_info = response.json()\n",
    "            \n",
    "            print(f\"{Colors.BLUE}Model Information:{Colors.END}\")\n",
    "            print(f\"  Name: {model_info.get('model_name', 'Unknown')}\")\n",
    "            print(f\"  Version: {model_info.get('model_version', 'Unknown')}\")\n",
    "            print(f\"  Type: {model_info.get('model_type', 'Unknown')}\")\n",
    "            print(f\"  Parameters: {model_info.get('parameters', 0):,}\")\n",
    "            print(f\"  Size: {model_info.get('model_size_mb', 0):.2f} MB\")\n",
    "            print(f\"  Input Shape: {model_info.get('input_shape', 'Unknown')}\")\n",
    "            print(f\"  Classes: {len(model_info.get('output_classes', []))}\")\n",
    "            \n",
    "            return model_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{Colors.RED}Failed to get model info: {e}{Colors.END}\")\n",
    "            return {}\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get comprehensive server statistics.\"\"\"\n",
    "        try:\n",
    "            response = self._make_request('GET', '/stats')\n",
    "            stats = response.json()\n",
    "            \n",
    "            # Display server stats\n",
    "            server_stats = stats.get('server', {})\n",
    "            print(f\"{Colors.BLUE}Server Statistics:{Colors.END}\")\n",
    "            print(f\"  Total Requests: {server_stats.get('total_requests', 0):,}\")\n",
    "            print(f\"  Success Rate: {server_stats.get('success_rate', 0):.1%}\")\n",
    "            print(f\"  Avg Inference Time: {server_stats.get('avg_inference_time_ms', 0):.1f}ms\")\n",
    "            print(f\"  Avg Batch Size: {server_stats.get('avg_batch_size', 0):.1f}\")\n",
    "            print(f\"  Queue Length: {server_stats.get('queue_length', 0)}\")\n",
    "            print(f\"  Uptime: {server_stats.get('uptime_seconds', 0):.0f}s\")\n",
    "            \n",
    "            # Display user stats if available\n",
    "            user_stats = stats.get('user', {})\n",
    "            if user_stats:\n",
    "                print(f\"{Colors.CYAN}User Statistics:{Colors.END}\")\n",
    "                print(f\"  Tier: {user_stats.get('tier', 'Unknown')}\")\n",
    "                print(f\"  Total Requests: {user_stats.get('total_requests', 0):,}\")\n",
    "                print(f\"  Rate Limit: {user_stats.get('rate_limit', 0)}\")\n",
    "                print(f\"  Current Minute: {user_stats.get('current_minute_requests', 0)}\")\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{Colors.RED}Failed to get stats: {e}{Colors.END}\")\n",
    "            return {}\n",
    "    \n",
    "    def test_prediction(self, test_data_path: Optional[str] = None, num_tests: int = 1) -> Dict:\n",
    "        \"\"\"Test model prediction with comprehensive validation.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        print(f\"{Colors.BLUE}Running {num_tests} prediction test(s)...{Colors.END}\")\n",
    "        \n",
    "        for i in range(num_tests):\n",
    "            try:\n",
    "                # Generate or load test data\n",
    "                if test_data_path and os.path.exists(test_data_path):\n",
    "                    # Load from file (would need proper implementation based on format)\n",
    "                    test_data = {\"tensor_data\": [[[0.5] * 32] * 32] * 3}\n",
    "                else:\n",
    "                    # Generate random test data\n",
    "                    import random\n",
    "                    test_data = {\n",
    "                        \"tensor_data\": [[[random.random() for _ in range(32)] for _ in range(32)] for _ in range(3)],\n",
    "                        \"return_probabilities\": True,\n",
    "                        \"confidence_threshold\": 0.5\n",
    "                    }\n",
    "                \n",
    "                start_time = time.time()\n",
    "                response = self._make_request('POST', '/predict', json=test_data)\n",
    "                latency = (time.time() - start_time) * 1000\n",
    "                \n",
    "                result = response.json()\n",
    "                result['client_latency_ms'] = latency\n",
    "                results.append(result)\n",
    "                \n",
    "                if num_tests == 1:\n",
    "                    print(f\"  Predicted Class: {result.get('predicted_class', 'Unknown')}\")\n",
    "                    print(f\"  Class Name: {result.get('class_name', 'Unknown')}\")\n",
    "                    print(f\"  Confidence: {result.get('confidence', 0):.3f}\")\n",
    "                    print(f\"  Inference Time: {result.get('inference_time_ms', 0):.1f}ms\")\n",
    "                    print(f\"  Client Latency: {latency:.1f}ms\")\n",
    "                else:\n",
    "                    if (i + 1) % max(1, num_tests // 10) == 0:\n",
    "                        print(f\"  Completed {i + 1}/{num_tests} tests\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"{Colors.RED}Prediction test {i + 1} failed: {e}{Colors.END}\")\n",
    "                results.append({'error': str(e)})\n",
    "        \n",
    "        # Calculate summary statistics for multiple tests\n",
    "        if num_tests > 1:\n",
    "            successful_results = [r for r in results if 'error' not in r]\n",
    "            if successful_results:\n",
    "                latencies = [r['client_latency_ms'] for r in successful_results]\n",
    "                confidences = [r.get('confidence', 0) for r in successful_results]\n",
    "                \n",
    "                print(f\"{Colors.GREEN}Test Summary:{Colors.END}\")\n",
    "                print(f\"  Successful: {len(successful_results)}/{num_tests}\")\n",
    "                print(f\"  Avg Latency: {statistics.mean(latencies):.1f}ms\")\n",
    "                print(f\"  P95 Latency: {statistics.quantiles(latencies, n=20)[18]:.1f}ms\" if len(latencies) > 5 else \"\")\n",
    "                print(f\"  Avg Confidence: {statistics.mean(confidences):.3f}\")\n",
    "        \n",
    "        return {'results': results, 'summary': {'total': num_tests, 'successful': len([r for r in results if 'error' not in r])}}\n",
    "    \n",
    "    def benchmark(self, num_requests: int = 100, concurrent: int = 1, duration: Optional[int] = None) -> Dict:\n",
    "        \"\"\"Run comprehensive benchmark test.\"\"\"\n",
    "        print(f\"{Colors.BLUE}Running benchmark: {num_requests} requests, {concurrent} concurrent{Colors.END}\")\n",
    "        if duration:\n",
    "            print(f\"Duration limit: {duration}s\")\n",
    "        \n",
    "        results = []\n",
    "        errors = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        def worker():\n",
    "            \"\"\"Worker function for concurrent execution.\"\"\"\n",
    "            requests_per_worker = num_requests // concurrent\n",
    "            \n",
    "            for _ in range(requests_per_worker):\n",
    "                if duration and (time.time() - start_time) > duration:\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    test_data = {\n",
    "                        \"tensor_data\": [[[0.5] * 32] * 32] * 3,\n",
    "                        \"return_probabilities\": True\n",
    "                    }\n",
    "                    \n",
    "                    request_start = time.time()\n",
    "                    response = self._make_request('POST', '/predict', json=test_data)\n",
    "                    latency = (time.time() - request_start) * 1000\n",
    "                    \n",
    "                    result = response.json()\n",
    "                    results.append({\n",
    "                        'latency_ms': latency,\n",
    "                        'confidence': result.get('confidence', 0),\n",
    "                        'timestamp': time.time()\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    errors.append({\n",
    "                        'error': str(e),\n",
    "                        'timestamp': time.time()\n",
    "                    })\n",
    "        \n",
    "        # Run concurrent workers\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent) as executor:\n",
    "            futures = [executor.submit(worker) for _ in range(concurrent)]\n",
    "            concurrent.futures.wait(futures)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate comprehensive statistics\n",
    "        if results:\n",
    "            latencies = [r['latency_ms'] for r in results]\n",
    "            confidences = [r['confidence'] for r in results]\n",
    "            \n",
    "            benchmark_stats = {\n",
    "                \"total_requests\": len(results) + len(errors),\n",
    "                \"successful_requests\": len(results),\n",
    "                \"failed_requests\": len(errors),\n",
    "                \"success_rate\": len(results) / (len(results) + len(errors)),\n",
    "                \"total_time_seconds\": total_time,\n",
    "                \"throughput_rps\": len(results) / total_time,\n",
    "                \"latency_stats\": {\n",
    "                    \"mean_ms\": statistics.mean(latencies),\n",
    "                    \"median_ms\": statistics.median(latencies),\n",
    "                    \"min_ms\": min(latencies),\n",
    "                    \"max_ms\": max(latencies),\n",
    "                    \"std_ms\": statistics.stdev(latencies) if len(latencies) > 1 else 0\n",
    "                },\n",
    "                \"confidence_stats\": {\n",
    "                    \"mean\": statistics.mean(confidences),\n",
    "                    \"median\": statistics.median(confidences),\n",
    "                    \"min\": min(confidences),\n",
    "                    \"max\": max(confidences)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            if len(latencies) > 5:\n",
    "                percentiles = statistics.quantiles(latencies, n=100)\n",
    "                benchmark_stats[\"latency_stats\"].update({\n",
    "                    \"p50_ms\": percentiles[49],\n",
    "                    \"p95_ms\": percentiles[94],\n",
    "                    \"p99_ms\": percentiles[98]\n",
    "                })\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"{Colors.GREEN}Benchmark Results:{Colors.END}\")\n",
    "            print(f\"  Requests: {benchmark_stats['successful_requests']:,}/{benchmark_stats['total_requests']:,}\")\n",
    "            print(f\"  Success Rate: {benchmark_stats['success_rate']:.1%}\")\n",
    "            print(f\"  Throughput: {benchmark_stats['throughput_rps']:.1f} RPS\")\n",
    "            print(f\"  Avg Latency: {benchmark_stats['latency_stats']['mean_ms']:.1f}ms\")\n",
    "            print(f\"  P95 Latency: {benchmark_stats['latency_stats'].get('p95_ms', 0):.1f}ms\")\n",
    "            print(f\"  P99 Latency: {benchmark_stats['latency_stats'].get('p99_ms', 0):.1f}ms\")\n",
    "            \n",
    "            return benchmark_stats\n",
    "        else:\n",
    "            print(f\"{Colors.RED}All requests failed{Colors.END}\")\n",
    "            return {\"error\": \"All requests failed\", \"errors\": errors}\n",
    "    \n",
    "    def monitor(self, interval: int = 30, duration: int = 300, save_data: bool = False):\n",
    "        \"\"\"Monitor API metrics with real-time display.\"\"\"\n",
    "        print(f\"{Colors.BLUE}Monitoring for {duration}s (interval: {interval}s){Colors.END}\")\n",
    "        print(f\"{'Time':<12} {'RPS':<8} {'Latency':<10} {'Success%':<9} {'Queue':<8} {'Memory':<10}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        end_time = time.time() + duration\n",
    "        prev_stats = None\n",
    "        monitoring_data = []\n",
    "        \n",
    "        try:\n",
    "            while time.time() < end_time:\n",
    "                try:\n",
    "                    stats = self.get_stats()\n",
    "                    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "                    \n",
    "                    server_stats = stats.get('server', {})\n",
    "                    \n",
    "                    # Calculate RPS\n",
    "                    rps = 0\n",
    "                    if prev_stats:\n",
    "                        req_delta = server_stats.get('total_requests', 0) - prev_stats.get('server', {}).get('total_requests', 0)\n",
    "                        rps = req_delta / interval\n",
    "                    \n",
    "                    # Format display\n",
    "                    latency = server_stats.get('avg_inference_time_ms', 0)\n",
    "                    success_rate = server_stats.get('success_rate', 0) * 100\n",
    "                    queue_length = server_stats.get('queue_length', 0)\n",
    "                    memory_mb = server_stats.get('memory_usage_mb', 0)\n",
    "                    \n",
    "                    print(f\"{current_time:<12} {rps:<8.1f} {latency:<10.1f} {success_rate:<9.1f} {queue_length:<8} {memory_mb:<10.1f}\")\n",
    "                    \n",
    "                    # Save monitoring data\n",
    "                    if save_data:\n",
    "                        monitoring_data.append({\n",
    "                            'timestamp': datetime.now().isoformat(),\n",
    "                            'rps': rps,\n",
    "                            'latency_ms': latency,\n",
    "                            'success_rate': success_rate,\n",
    "                            'queue_length': queue_length,\n",
    "                            'memory_mb': memory_mb\n",
    "                        })\n",
    "                    \n",
    "                    prev_stats = stats\n",
    "                    time.sleep(interval)\n",
    "                    \n",
    "                except KeyboardInterrupt:\n",
    "                    print(f\"\\\\n{Colors.YELLOW}Monitoring stopped by user{Colors.END}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"{Colors.RED}Monitoring error: {e}{Colors.END}\")\n",
    "                    time.sleep(interval)\n",
    "        \n",
    "        finally:\n",
    "            # Save monitoring data if requested\n",
    "            if save_data and monitoring_data:\n",
    "                filename = f\"monitoring_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "                with open(filename, 'w') as f:\n",
    "                    json.dump(monitoring_data, f, indent=2)\n",
    "                print(f\"{Colors.GREEN}Monitoring data saved to {filename}{Colors.END}\")\n",
    "    \n",
    "    def generate_report(self, output_file: Optional[str] = None):\n",
    "        \"\"\"Generate comprehensive system report.\"\"\"\n",
    "        print(f\"{Colors.BLUE}Generating comprehensive system report...{Colors.END}\")\n",
    "        \n",
    "        report_data = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'health': self.health_check(),\n",
    "            'model_info': self.get_model_info(),\n",
    "            'stats': self.get_stats(),\n",
    "            'benchmark': self.benchmark(num_requests=50, concurrent=5)\n",
    "        }\n",
    "        \n",
    "        # Format report\n",
    "        report = f\"\"\"\n",
    "# PyTorch Model Server Report\n",
    "Generated: {report_data['timestamp']}\n",
    "\n",
    "## Health Status\n",
    "Status: {report_data['health'].get('status', 'Unknown')}\n",
    "Response Time: {report_data['health'].get('response_time_ms', 0):.1f}ms\n",
    "Uptime: {report_data['health'].get('uptime_seconds', 0):.0f}s\n",
    "\n",
    "## Model Information\n",
    "Name: {report_data['model_info'].get('model_name', 'Unknown')}\n",
    "Version: {report_data['model_info'].get('model_version', 'Unknown')}\n",
    "Parameters: {report_data['model_info'].get('parameters', 0):,}\n",
    "Size: {report_data['model_info'].get('model_size_mb', 0):.2f} MB\n",
    "\n",
    "## Performance Metrics\n",
    "Total Requests: {report_data['stats'].get('server', {}).get('total_requests', 0):,}\n",
    "Success Rate: {report_data['stats'].get('server', {}).get('success_rate', 0):.1%}\n",
    "Avg Inference Time: {report_data['stats'].get('server', {}).get('avg_inference_time_ms', 0):.1f}ms\n",
    "Current Queue Length: {report_data['stats'].get('server', {}).get('queue_length', 0)}\n",
    "\n",
    "## Benchmark Results\n",
    "Throughput: {report_data['benchmark'].get('throughput_rps', 0):.1f} RPS\n",
    "Avg Latency: {report_data['benchmark'].get('latency_stats', {}).get('mean_ms', 0):.1f}ms\n",
    "P95 Latency: {report_data['benchmark'].get('latency_stats', {}).get('p95_ms', 0):.1f}ms\n",
    "Success Rate: {report_data['benchmark'].get('success_rate', 0):.1%}\n",
    "        \"\"\"\n",
    "        \n",
    "        if output_file:\n",
    "            with open(output_file, 'w') as f:\n",
    "                f.write(report)\n",
    "            print(f\"{Colors.GREEN}Report saved to {output_file}{Colors.END}\")\n",
    "        else:\n",
    "            print(report)\n",
    "        \n",
    "        # Save raw data\n",
    "        data_file = f\"report_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(data_file, 'w') as f:\n",
    "            json.dump(report_data, f, indent=2, default=str)\n",
    "        print(f\"{Colors.GREEN}Raw data saved to {data_file}{Colors.END}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main CLI interface.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"PyTorch Model Management CLI v2.0\",\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        epilog='''\n",
    "Examples:\n",
    "  %(prog)s health --api-url http://prod.example.com:8000\n",
    "  %(prog)s benchmark --requests 1000 --concurrent 20\n",
    "  %(prog)s monitor --duration 600 --interval 10 --save-data\n",
    "  %(prog)s test --num-tests 10 --test-data ./test_image.jpg\n",
    "  %(prog)s report --output system_report.md\n",
    "        '''\n",
    "    )\n",
    "    \n",
    "    # Global options\n",
    "    parser.add_argument(\"--api-url\", default=\"http://localhost:8000\", help=\"API URL\")\n",
    "    parser.add_argument(\"--api-key\", default=\"test_key_123\", help=\"API key\")\n",
    "    parser.add_argument(\"--config\", help=\"Configuration file (YAML/JSON)\")\n",
    "    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Verbose output\")\n",
    "    \n",
    "    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n",
    "    \n",
    "    # Health check command\n",
    "    health_parser = subparsers.add_parser(\"health\", help=\"Check API health\")\n",
    "    \n",
    "    # Model info command\n",
    "    info_parser = subparsers.add_parser(\"info\", help=\"Get model information\")\n",
    "    \n",
    "    # Statistics command\n",
    "    stats_parser = subparsers.add_parser(\"stats\", help=\"Get server statistics\")\n",
    "    \n",
    "    # Test prediction command\n",
    "    test_parser = subparsers.add_parser(\"test\", help=\"Test model prediction\")\n",
    "    test_parser.add_argument(\"--test-data\", help=\"Path to test data file\")\n",
    "    test_parser.add_argument(\"--num-tests\", type=int, default=1, help=\"Number of tests to run\")\n",
    "    \n",
    "    # Benchmark command\n",
    "    bench_parser = subparsers.add_parser(\"benchmark\", help=\"Run benchmark test\")\n",
    "    bench_parser.add_argument(\"--requests\", type=int, default=100, help=\"Number of requests\")\n",
    "    bench_parser.add_argument(\"--concurrent\", type=int, default=1, help=\"Concurrent workers\")\n",
    "    bench_parser.add_argument(\"--duration\", type=int, help=\"Duration limit in seconds\")\n",
    "    \n",
    "    # Monitor command\n",
    "    monitor_parser = subparsers.add_parser(\"monitor\", help=\"Monitor API metrics\")\n",
    "    monitor_parser.add_argument(\"--interval\", type=int, default=30, help=\"Monitoring interval\")\n",
    "    monitor_parser.add_argument(\"--duration\", type=int, default=300, help=\"Monitoring duration\")\n",
    "    monitor_parser.add_argument(\"--save-data\", action=\"store_true\", help=\"Save monitoring data\")\n",
    "    \n",
    "    # Report command\n",
    "    report_parser = subparsers.add_parser(\"report\", help=\"Generate comprehensive report\")\n",
    "    report_parser.add_argument(\"--output\", help=\"Output file path\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if not args.command:\n",
    "        parser.print_help()\n",
    "        return\n",
    "    \n",
    "    # Initialize manager\n",
    "    manager = ModelManager(args.api_url, args.api_key, args.config)\n",
    "    \n",
    "    # Execute command\n",
    "    try:\n",
    "        if args.command == \"health\":\n",
    "            manager.health_check()\n",
    "        elif args.command == \"info\":\n",
    "            manager.get_model_info()\n",
    "        elif args.command == \"stats\":\n",
    "            manager.get_stats()\n",
    "        elif args.command == \"test\":\n",
    "            manager.test_prediction(args.test_data, args.num_tests)\n",
    "        elif args.command == \"benchmark\":\n",
    "            manager.benchmark(args.requests, args.concurrent, getattr(args, 'duration', None))\n",
    "        elif args.command == \"monitor\":\n",
    "            manager.monitor(args.interval, args.duration, args.save_data)\n",
    "        elif args.command == \"report\":\n",
    "            manager.generate_report(args.output)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\\\n{Colors.YELLOW}Operation cancelled by user{Colors.END}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{Colors.RED}Error: {e}{Colors.END}\")\n",
    "        if args.verbose:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "        '''\n",
    "        \n",
    "        return model_mgmt_script\n",
    "\n",
    "# Generate CLI tools\n",
    "print(\"\\nüîß GENERATING CLI TOOLS AND SCRIPTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cli_generator = CLIToolsGenerator()\n",
    "\n",
    "# Generate scripts\n",
    "deploy_script = cli_generator.generate_deployment_script()\n",
    "model_mgmt_script = cli_generator.generate_model_management_script()\n",
    "\n",
    "# Save CLI tools\n",
    "scripts_dir = results_dir / 'scripts'\n",
    "scripts_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save deployment script\n",
    "with open(scripts_dir / 'deploy.sh', 'w') as f:\n",
    "    f.write(deploy_script)\n",
    "os.chmod(scripts_dir / 'deploy.sh', 0o755)\n",
    "\n",
    "# Save model management script\n",
    "with open(scripts_dir / 'model_manager.py', 'w') as f:\n",
    "    f.write(model_mgmt_script)\n",
    "os.chmod(scripts_dir / 'model_manager.py', 0o755)\n",
    "\n",
    "print(\"‚úÖ Advanced deployment script generated (deploy.sh)\")\n",
    "print(\"‚úÖ Comprehensive model management CLI created (model_manager.py)\")\n",
    "print(f\"üìÅ CLI tools saved to: {scripts_dir}\")\n",
    "\n",
    "# Display usage examples\n",
    "print(f\"\\nüí° USAGE EXAMPLES\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Deployment Script:\")\n",
    "print(\"  ./deploy.sh docker --tag v1.1 --debug\")\n",
    "print(\"  ./deploy.sh kubernetes --namespace production --timeout 600\")\n",
    "print(\"  ./deploy.sh helm --dry-run\")\n",
    "print(\"  ./deploy.sh test --requests 1000 --concurrent 20\")\n",
    "print()\n",
    "print(\"Model Manager:\")\n",
    "print(\"  ./model_manager.py health --api-url http://prod.example.com:8000\")\n",
    "print(\"  ./model_manager.py benchmark --requests 1000 --concurrent 10\")\n",
    "print(\"  ./model_manager.py monitor --duration 600 --save-data\")\n",
    "print(\"  ./model_manager.py report --output system_report.md\")\n",
    "```\n",
    "\n",
    "## 11. Comprehensive Summary and Production Readiness\n",
    "\n",
    "```python\n",
    "def generate_comprehensive_summary():\n",
    "    \"\"\"Generate final summary of the complete model serving system.\"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'system_overview': {\n",
    "            'name': 'PyTorch Model Serving APIs',\n",
    "            'version': '1.0.0',\n",
    "            'description': 'Production-grade machine learning inference system',\n",
    "            'features': [\n",
    "                'High-performance async batch processing',\n",
    "                'Tiered authentication and rate limiting',\n",
    "                'Comprehensive monitoring and alerting',\n",
    "                'Enterprise security features',\n",
    "                'Container-native deployment',\n",
    "                'Auto-scaling and load balancing',\n",
    "                'CLI tools for management'\n",
    "            ]\n",
    "        },\n",
    "        'performance_characteristics': {\n",
    "            'throughput': '100+ RPS with optimal batching',\n",
    "            'latency': 'Sub-50ms for single requests',\n",
    "            'batch_efficiency': 'Automatic request aggregation',\n",
    "            'scalability': 'Kubernetes HPA with demand-responsive scaling',\n",
    "            'reliability': '99.9% uptime with proper deployment'\n",
    "        },\n",
    "        'security_features': [\n",
    "            'API key-based authentication with tiers',\n",
    "            'Rate limiting per user tier',\n",
    "            'Request validation and sanitization',\n",
    "            'Threat detection and blocking',\n",
    "            'Security headers and SSL termination',\n",
    "            'Input sanitization and validation'\n",
    "        ],\n",
    "        'monitoring_capabilities': [\n",
    "            'Prometheus metrics collection',\n",
    "            'Grafana dashboards with 8+ panels',\n",
    "            'Comprehensive alert rules (20+ alerts)',\n",
    "            'Real-time health monitoring',\n",
    "            'Performance tracking and analytics',\n",
    "            'Custom metrics and logging'\n",
    "        ],\n",
    "        'deployment_options': [\n",
    "            'Docker Compose for development',\n",
    "            'Kubernetes manifests for production',\n",
    "            'Helm charts for easy deployment',\n",
    "            'Multi-stage Docker builds',\n",
    "            'Nginx load balancing and SSL',\n",
    "            'Auto-scaling configurations'\n",
    "        ],\n",
    "        'files_generated': [\n",
    "            'FastAPI application with comprehensive endpoints',\n",
    "            'Docker and Kubernetes deployment configs',\n",
    "            'Monitoring and alerting configurations',\n",
    "            'CLI tools for deployment and management',\n",
    "            'Load testing framework',\n",
    "            'Security and authentication systems'\n",
    "        ],\n",
    "        'production_readiness': {\n",
    "            'infrastructure': '‚úÖ Complete',\n",
    "            'security': '‚úÖ Enterprise-grade',\n",
    "            'monitoring': '‚úÖ Comprehensive',\n",
    "            'deployment': '‚úÖ Container-native',\n",
    "            'testing': '‚úÖ Load testing included',\n",
    "            'documentation': '‚úÖ Comprehensive',\n",
    "            'management': '‚úÖ CLI tools provided'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate final summary\n",
    "final_summary = generate_comprehensive_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ PYTORCH MODEL SERVING APIS - COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìã SYSTEM OVERVIEW\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Name: {final_summary['system_overview']['name']}\")\n",
    "print(f\"Version: {final_summary['system_overview']['version']}\")\n",
    "print(f\"Description: {final_summary['system_overview']['description']}\")\n",
    "\n",
    "print(f\"\\n‚ö° KEY FEATURES\")\n",
    "print(\"-\" * 40)\n",
    "for i, feature in enumerate(final_summary['system_overview']['features'], 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "print(f\"\\nüìà PERFORMANCE CHARACTERISTICS\")\n",
    "print(\"-\" * 40)\n",
    "for key, value in final_summary['performance_characteristics'].items():\n",
    "    print(f\"‚Ä¢ {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\nüîê SECURITY FEATURES\")\n",
    "print(\"-\" * 40)\n",
    "for i, feature in enumerate(final_summary['security_features'], 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "print(f\"\\nüìä MONITORING CAPABILITIES\")\n",
    "print(\"-\" * 40)\n",
    "for i, capability in enumerate(final_summary['monitoring_capabilities'], 1):\n",
    "    print(f\"{i:2d}. {capability}\")\n",
    "\n",
    "print(f\"\\nüöÄ DEPLOYMENT OPTIONS\")\n",
    "print(\"-\" * 40)\n",
    "for i, option in enumerate(final_summary['deployment_options'], 1):\n",
    "    print(f\"{i:2d}. {option}\")\n",
    "\n",
    "print(f\"\\nüìÅ GENERATED ASSETS\")\n",
    "print(\"-\" * 40)\n",
    "for i, asset in enumerate(final_summary['files_generated'], 1):\n",
    "    print(f\"{i:2d}. {asset}\")\n",
    "\n",
    "print(f\"\\n‚úÖ PRODUCTION READINESS CHECKLIST\")\n",
    "print(\"-\" * 40)\n",
    "for category, status in final_summary['production_readiness'].items():\n",
    "    print(f\"‚Ä¢ {category.replace('_', ' ').title()}: {status}\")\n",
    "\n",
    "# Count generated files\n",
    "total_files = 0\n",
    "for item in results_dir.rglob('*'):\n",
    "    if item.is_file():\n",
    "        total_files += 1\n",
    "\n",
    "print(f\"\\nüìä SYSTEM STATISTICS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"‚Ä¢ Total files generated: {total_files}\")\n",
    "print(f\"‚Ä¢ Lines of code: 2,500+ (estimated)\")\n",
    "print(f\"‚Ä¢ Docker configurations: 3 (Dockerfile, Compose, Multi-stage)\")\n",
    "print(f\"‚Ä¢ Kubernetes manifests: 8 files\")\n",
    "print(f\"‚Ä¢ Monitoring configs: 4 files\")\n",
    "print(f\"‚Ä¢ CLI tools: 2 scripts\")\n",
    "print(f\"‚Ä¢ Test frameworks: Load testing included\")\n",
    "\n",
    "print(f\"\\nüéØ NEXT STEPS\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. Review and customize configurations for your environment\")\n",
    "print(\"2. Build and test Docker images\")\n",
    "print(\"3. Deploy to staging environment\")\n",
    "print(\"4. Run load tests and monitor performance\")\n",
    "print(\"5. Configure production monitoring and alerting\")\n",
    "print(\"6. Deploy to production with proper security\")\n",
    "print(\"7. Set up CI/CD pipelines for automated deployment\")\n",
    "\n",
    "print(f\"\\nüìö ADDITIONAL RESOURCES\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"‚Ä¢ All configurations saved to: {results_dir}\")\n",
    "print(f\"‚Ä¢ Documentation: See README files in each directory\")\n",
    "print(f\"‚Ä¢ Examples: Usage examples included in CLI tools\")\n",
    "print(f\"‚Ä¢ Monitoring: Grafana dashboards ready to import\")\n",
    "print(f\"‚Ä¢ Security: Review security configurations before production\")\n",
    "\n",
    "# Save final summary\n",
    "with open(results_dir / 'system_summary.json', 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Complete system summary saved to: {results_dir / 'system_summary.json'}\")\n",
    "\n",
    "# List all generated directories and files\n",
    "print(f\"\\nüìÇ COMPLETE DIRECTORY STRUCTURE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def print_tree(directory, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    \"\"\"Print directory tree structure.\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    items = list(directory.iterdir())\n",
    "    items.sort(key=lambda x: (x.is_file(), x.name))\n",
    "    \n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == len(items) - 1\n",
    "        current_prefix = \"‚îî‚îÄ‚îÄ \" if is_last else \"‚îú‚îÄ‚îÄ \"\n",
    "        print(f\"{prefix}{current_prefix}{item.name}\")\n",
    "        \n",
    "        if item.is_dir() and current_depth < max_depth - 1:\n",
    "            extension_prefix = \"    \" if is_last else \"‚îÇ   \"\n",
    "            print_tree(item, prefix + extension_prefix, max_depth, current_depth + 1)\n",
    "\n",
    "print_tree(results_dir)\n",
    "\n",
    "print(f\"\\nüéâ PYTORCH MODEL SERVING API SYSTEM COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Your production-ready machine learning inference system is now available.\")\n",
    "print(\"All configurations, scripts, and tools have been generated and are ready for deployment.\")\n",
    "print(f\"\\nüìÅ Location: {results_dir}\")\n",
    "print(\"üöÄ Ready for production deployment!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
