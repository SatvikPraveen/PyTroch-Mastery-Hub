{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17fe24a",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron from Scratch: PyTorch Mastery Hub\n",
    "\n",
    "**Building Neural Networks from First Principles**\n",
    "\n",
    "**Authors:** PyTorch Mastery Hub Team  \n",
    "**Institution:** Deep Learning Education Initiative  \n",
    "**Course:** Neural Networks Fundamentals  \n",
    "**Date:** December 2024\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a comprehensive implementation of Multi-Layer Perceptrons (MLPs) from scratch using PyTorch. We build neural networks from fundamental components to develop deep understanding of forward propagation, backpropagation, and training dynamics. This hands-on approach bridges the gap between theory and implementation.\n",
    "\n",
    "## Key Objectives\n",
    "1. Implement linear layers and activation functions from scratch\n",
    "2. Build complete MLP architectures with custom components\n",
    "3. Develop manual backpropagation algorithms\n",
    "4. Create comprehensive training systems with monitoring\n",
    "5. Compare custom implementations with PyTorch built-ins\n",
    "6. Visualize training dynamics and network behavior\n",
    "\n",
    "## 1. Setup and Imports\n",
    "\n",
    "```python\n",
    "# Essential imports for neural network implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional, Callable\n",
    "import time\n",
    "import math\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom utilities\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "try:\n",
    "    from src.utils.device_utils import get_device\n",
    "    from src.utils.data_utils import create_synthetic_dataset\n",
    "    from src.visualization.training_viz import TrainingVisualizer\n",
    "    from src.utils.logging_utils import setup_logger\n",
    "except ImportError:\n",
    "    print(\"Warning: Custom utilities not found. Using fallback implementations.\")\n",
    "    def get_device():\n",
    "        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def setup_logger(name):\n",
    "        import logging\n",
    "        return logging.getLogger(name)\n",
    "\n",
    "# Set up environment\n",
    "device = get_device()\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "logger = setup_logger('MLP_Tutorial')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "\n",
    "# Create results directory\n",
    "results_dir = os.path.join('results', 'mlp_from_scratch')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "print(\"✅ Environment setup complete!\")\n",
    "print(f\"📱 Device: {device}\")\n",
    "print(f\"🎨 PyTorch version: {torch.__version__}\")\n",
    "print(f\"📁 Results will be saved to: {results_dir}\")\n",
    "```\n",
    "\n",
    "## 2. Linear Layer Implementation\n",
    "\n",
    "```python\n",
    "class LinearLayer:\n",
    "    \"\"\"Linear (fully connected) layer implementation from scratch.\n",
    "    \n",
    "    This class implements the fundamental building block of neural networks:\n",
    "    a linear transformation y = xW + b with optional bias term.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int, use_bias: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize linear layer with Xavier/Glorot initialization.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            output_size: Number of output features\n",
    "            use_bias: Whether to include bias term\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        # Xavier/Glorot initialization for better gradient flow\n",
    "        std = math.sqrt(2.0 / (input_size + output_size))\n",
    "        self.weight = torch.randn(input_size, output_size) * std\n",
    "        self.weight.requires_grad_(True)\n",
    "        \n",
    "        if use_bias:\n",
    "            self.bias = torch.zeros(output_size)\n",
    "            self.bias.requires_grad_(True)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "        # Store intermediate values for analysis\n",
    "        self.last_input = None\n",
    "        self.last_output = None\n",
    "        \n",
    "        print(f\"📊 Created LinearLayer: {input_size} → {output_size}\")\n",
    "        print(f\"   Weight shape: {self.weight.shape}\")\n",
    "        print(f\"   Bias: {'Enabled' if use_bias else 'Disabled'}\")\n",
    "        print(f\"   Parameters: {self.weight.numel() + (self.bias.numel() if use_bias else 0):,}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: y = xW + b\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, input_size]\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor [batch_size, output_size]\n",
    "        \"\"\"\n",
    "        self.last_input = x\n",
    "        \n",
    "        # Linear transformation\n",
    "        output = torch.mm(x, self.weight)\n",
    "        \n",
    "        # Add bias if enabled\n",
    "        if self.use_bias:\n",
    "            output = output + self.bias\n",
    "        \n",
    "        self.last_output = output\n",
    "        return output\n",
    "    \n",
    "    def parameters(self) -> List[torch.Tensor]:\n",
    "        \"\"\"Return list of trainable parameters.\"\"\"\n",
    "        params = [self.weight]\n",
    "        if self.use_bias:\n",
    "            params.append(self.bias)\n",
    "        return params\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero out parameter gradients.\"\"\"\n",
    "        if self.weight.grad is not None:\n",
    "            self.weight.grad.zero_()\n",
    "        if self.use_bias and self.bias.grad is not None:\n",
    "            self.bias.grad.zero_()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"LinearLayer({self.input_size}, {self.output_size}, bias={self.use_bias})\"\n",
    "\n",
    "# Test linear layer implementation\n",
    "print(\"🧪 Testing Linear Layer Implementation:\")\n",
    "layer = LinearLayer(4, 3)\n",
    "test_input = torch.randn(2, 4)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "output = layer.forward(test_input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output sample:\\n{output}\")\n",
    "\n",
    "# Verify against PyTorch implementation\n",
    "pytorch_layer = nn.Linear(4, 3)\n",
    "pytorch_layer.weight.data = layer.weight.data.T  # PyTorch uses transposed weights\n",
    "pytorch_layer.bias.data = layer.bias.data\n",
    "\n",
    "pytorch_output = pytorch_layer(test_input)\n",
    "print(f\"✅ Implementation matches PyTorch: {torch.allclose(output, pytorch_output, atol=1e-6)}\")\n",
    "```\n",
    "\n",
    "## 3. Activation Functions\n",
    "\n",
    "```python\n",
    "class ActivationFunction:\n",
    "    \"\"\"Base class for activation functions with forward and derivative methods.\"\"\"\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute derivative for backpropagation.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    \"\"\"Rectified Linear Unit: f(x) = max(0, x)\"\"\"\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.last_input = x\n",
    "        return torch.clamp(x, min=0)\n",
    "    \n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return (x > 0).float()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU()\"\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    \"\"\"Sigmoid: f(x) = 1 / (1 + exp(-x))\"\"\"\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Numerical stability: clip extreme values\n",
    "        x_clipped = torch.clamp(x, -500, 500)\n",
    "        output = 1.0 / (1.0 + torch.exp(-x_clipped))\n",
    "        self.last_output = output\n",
    "        return output\n",
    "    \n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        sigmoid_x = self.forward(x)\n",
    "        return sigmoid_x * (1 - sigmoid_x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Sigmoid()\"\n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    \"\"\"Hyperbolic tangent: f(x) = tanh(x)\"\"\"\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        output = torch.tanh(x)\n",
    "        self.last_output = output\n",
    "        return output\n",
    "    \n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        tanh_x = self.forward(x)\n",
    "        return 1 - tanh_x ** 2\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Tanh()\"\n",
    "\n",
    "class LeakyReLU(ActivationFunction):\n",
    "    \"\"\"Leaky ReLU: f(x) = max(alpha*x, x)\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.01):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.last_input = x\n",
    "        return torch.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.where(x > 0, torch.ones_like(x), self.alpha * torch.ones_like(x))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"LeakyReLU(alpha={self.alpha})\"\n",
    "\n",
    "# Test and visualize activation functions\n",
    "def visualize_activation_functions():\n",
    "    \"\"\"Create comprehensive visualization of activation functions.\"\"\"\n",
    "    \n",
    "    print(\"🎨 Analyzing Activation Functions:\")\n",
    "    \n",
    "    x_test = torch.linspace(-3, 3, 100)\n",
    "    activations = {\n",
    "        'ReLU': ReLU(),\n",
    "        'Sigmoid': Sigmoid(),\n",
    "        'Tanh': Tanh(),\n",
    "        'LeakyReLU': LeakyReLU(0.1)\n",
    "    }\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = [ax1, ax2, ax3, ax4]\n",
    "    \n",
    "    for i, (name, activation) in enumerate(activations.items()):\n",
    "        with torch.no_grad():\n",
    "            y = activation.forward(x_test)\n",
    "            dy = activation.derivative(x_test)\n",
    "        \n",
    "        ax = axes[i]\n",
    "        ax.plot(x_test, y, label=f'{name}', linewidth=2, color='blue')\n",
    "        ax.plot(x_test, dy, label=f\"{name} derivative\", linewidth=2, color='red', alpha=0.7)\n",
    "        ax.set_title(f'{name} Activation Function', fontweight='bold')\n",
    "        ax.set_xlabel('Input (x)')\n",
    "        ax.set_ylabel('Output f(x)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "        ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "    \n",
    "    plt.suptitle('Activation Functions and Their Derivatives', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'activation_functions.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print numerical properties\n",
    "    print(f\"\\n📊 Activation Function Properties at x=1:\")\n",
    "    for name, activation in activations.items():\n",
    "        test_val = torch.tensor(1.0)\n",
    "        output = activation.forward(test_val)\n",
    "        derivative = activation.derivative(test_val)\n",
    "        print(f\"  {name:12} | f(1) = {output.item():.4f} | f'(1) = {derivative.item():.4f}\")\n",
    "\n",
    "visualize_activation_functions()\n",
    "```\n",
    "\n",
    "## 4. Multi-Layer Perceptron Architecture\n",
    "\n",
    "```python\n",
    "class MLP:\n",
    "    \"\"\"Multi-Layer Perceptron implementation from scratch.\n",
    "    \n",
    "    This class combines linear layers and activation functions to create\n",
    "    a complete feedforward neural network architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes: List[int], activations: List[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize MLP with specified architecture.\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes: List of layer sizes [input, hidden1, hidden2, ..., output]\n",
    "            activations: List of activation function names for each layer\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes) - 1\n",
    "        \n",
    "        # Default activations: ReLU for hidden layers, none for output\n",
    "        if activations is None:\n",
    "            activations = ['relu'] * (self.num_layers - 1) + ['none']\n",
    "        \n",
    "        assert len(activations) == self.num_layers, \"Number of activations must match number of layers\"\n",
    "        \n",
    "        # Create linear layers\n",
    "        self.layers = []\n",
    "        for i in range(self.num_layers):\n",
    "            layer = LinearLayer(layer_sizes[i], layer_sizes[i + 1])\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        # Create activation functions\n",
    "        self.activations = []\n",
    "        activation_map = {\n",
    "            'relu': ReLU(),\n",
    "            'sigmoid': Sigmoid(),\n",
    "            'tanh': Tanh(),\n",
    "            'leaky_relu': LeakyReLU(),\n",
    "            'none': None\n",
    "        }\n",
    "        \n",
    "        for activation_name in activations:\n",
    "            self.activations.append(activation_map[activation_name.lower()])\n",
    "        \n",
    "        # Storage for intermediate values during forward pass\n",
    "        self.layer_outputs = []\n",
    "        self.layer_inputs = []\n",
    "        \n",
    "        # Print architecture summary\n",
    "        print(f\"🧠 MLP Architecture Created:\")\n",
    "        for i, (size_in, size_out, act) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:], activations)):\n",
    "            act_name = act if act != 'none' else 'Linear'\n",
    "            print(f\"   Layer {i+1}: {size_in:4d} → {size_out:4d} ({act_name})\")\n",
    "        \n",
    "        total_params = sum(sum(p.numel() for p in layer.parameters()) for layer in self.layers)\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward propagation through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, input_size]\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor [batch_size, output_size]\n",
    "        \"\"\"\n",
    "        self.layer_inputs = [x]\n",
    "        self.layer_outputs = []\n",
    "        \n",
    "        current_input = x\n",
    "        \n",
    "        for i, (layer, activation) in enumerate(zip(self.layers, self.activations)):\n",
    "            # Linear transformation\n",
    "            linear_output = layer.forward(current_input)\n",
    "            \n",
    "            # Apply activation function\n",
    "            if activation is not None:\n",
    "                activated_output = activation.forward(linear_output)\n",
    "            else:\n",
    "                activated_output = linear_output\n",
    "            \n",
    "            self.layer_outputs.append(activated_output)\n",
    "            \n",
    "            # Prepare input for next layer\n",
    "            current_input = activated_output\n",
    "            if i < len(self.layers) - 1:\n",
    "                self.layer_inputs.append(current_input)\n",
    "        \n",
    "        return current_input\n",
    "    \n",
    "    def parameters(self) -> List[torch.Tensor]:\n",
    "        \"\"\"Get all trainable parameters.\"\"\"\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero all parameter gradients.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grad()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        lines = [\"MLP(\"]\n",
    "        for i, (layer, activation) in enumerate(zip(self.layers, self.activations)):\n",
    "            act_str = f\" + {activation}\" if activation else \"\"\n",
    "            lines.append(f\"  ({i}): {layer}{act_str}\")\n",
    "        lines.append(\")\")\n",
    "        return \"\\\\n\".join(lines)\n",
    "\n",
    "# Test MLP implementation\n",
    "def test_mlp_architecture():\n",
    "    \"\"\"Test MLP with various configurations.\"\"\"\n",
    "    \n",
    "    print(\"🧪 Testing MLP Architecture:\")\n",
    "    \n",
    "    # Create test MLP\n",
    "    mlp = MLP(\n",
    "        layer_sizes=[4, 8, 6, 3],\n",
    "        activations=['relu', 'tanh', 'none']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\nArchitecture Details:\")\n",
    "    print(mlp)\n",
    "    \n",
    "    # Test forward pass\n",
    "    test_input = torch.randn(5, 4)\n",
    "    print(f\"\\\\n📊 Forward Pass Analysis:\")\n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "    \n",
    "    output = mlp.forward(test_input)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    # Analyze layer-by-layer transformations\n",
    "    print(f\"\\\\n🔍 Layer-by-layer Transformations:\")\n",
    "    print(f\"Input: {test_input.shape}\")\n",
    "    for i, layer_output in enumerate(mlp.layer_outputs):\n",
    "        print(f\"Layer {i+1} output: {layer_output.shape}\")\n",
    "    \n",
    "    print(f\"✅ MLP forward propagation successful!\")\n",
    "\n",
    "test_mlp_architecture()\n",
    "```\n",
    "\n",
    "## 5. Loss Functions Implementation\n",
    "\n",
    "```python\n",
    "class LossFunction:\n",
    "    \"\"\"Base class for loss functions with forward and backward methods.\"\"\"\n",
    "    \n",
    "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute loss value.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute gradient of loss w.r.t. predictions.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MeanSquaredError(LossFunction):\n",
    "    \"\"\"Mean Squared Error: L = (1/n) * Σ(y_pred - y_true)²\"\"\"\n",
    "    \n",
    "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        self.predictions = predictions\n",
    "        self.targets = targets\n",
    "        diff = predictions - targets\n",
    "        return torch.mean(diff ** 2)\n",
    "    \n",
    "    def backward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = predictions.shape[0]\n",
    "        return 2 * (predictions - targets) / batch_size\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"MeanSquaredError()\"\n",
    "\n",
    "class CrossEntropyLoss(LossFunction):\n",
    "    \"\"\"Cross-Entropy Loss for multi-class classification.\"\"\"\n",
    "    \n",
    "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # Numerically stable softmax\n",
    "        exp_preds = torch.exp(predictions - torch.max(predictions, dim=1, keepdim=True)[0])\n",
    "        softmax_preds = exp_preds / torch.sum(exp_preds, dim=1, keepdim=True)\n",
    "        \n",
    "        # Store for backward pass\n",
    "        self.softmax_preds = softmax_preds\n",
    "        self.targets = targets\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        batch_size = predictions.shape[0]\n",
    "        log_probs = torch.log(softmax_preds + 1e-8)\n",
    "        \n",
    "        if targets.dtype == torch.long:  # Class indices\n",
    "            loss = -torch.sum(log_probs[range(batch_size), targets]) / batch_size\n",
    "        else:  # One-hot encoded\n",
    "            loss = -torch.sum(targets * log_probs) / batch_size\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = predictions.shape[0]\n",
    "        \n",
    "        if targets.dtype == torch.long:  # Class indices\n",
    "            # Convert to one-hot encoding\n",
    "            num_classes = predictions.shape[1]\n",
    "            targets_one_hot = torch.zeros_like(predictions)\n",
    "            targets_one_hot[range(batch_size), targets] = 1\n",
    "        else:\n",
    "            targets_one_hot = targets\n",
    "        \n",
    "        return (self.softmax_preds - targets_one_hot) / batch_size\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"CrossEntropyLoss()\"\n",
    "\n",
    "class BinaryCrossEntropyLoss(LossFunction):\n",
    "    \"\"\"Binary Cross-Entropy Loss for binary classification.\"\"\"\n",
    "    \n",
    "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply sigmoid activation\n",
    "        sigmoid_preds = torch.sigmoid(predictions)\n",
    "        self.sigmoid_preds = sigmoid_preds\n",
    "        self.targets = targets\n",
    "        \n",
    "        # Numerical stability clipping\n",
    "        sigmoid_preds = torch.clamp(sigmoid_preds, 1e-8, 1 - 1e-8)\n",
    "        \n",
    "        loss = -(targets * torch.log(sigmoid_preds) + \n",
    "                (1 - targets) * torch.log(1 - sigmoid_preds))\n",
    "        return torch.mean(loss)\n",
    "    \n",
    "    def backward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = predictions.shape[0]\n",
    "        return (self.sigmoid_preds - targets) / batch_size\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"BinaryCrossEntropyLoss()\"\n",
    "\n",
    "# Test loss function implementations\n",
    "def test_loss_functions():\n",
    "    \"\"\"Comprehensive testing of loss function implementations.\"\"\"\n",
    "    \n",
    "    print(\"📊 Testing Loss Function Implementations:\")\n",
    "    \n",
    "    # Test Mean Squared Error\n",
    "    mse_loss = MeanSquaredError()\n",
    "    pred_reg = torch.randn(3, 2, requires_grad=True)\n",
    "    target_reg = torch.randn(3, 2)\n",
    "    \n",
    "    loss_mse = mse_loss.forward(pred_reg, target_reg)\n",
    "    print(f\"\\\\n📈 MSE Loss Test:\")\n",
    "    print(f\"  Custom MSE: {loss_mse.item():.6f}\")\n",
    "    \n",
    "    # Compare with PyTorch implementation\n",
    "    pytorch_mse = F.mse_loss(pred_reg, target_reg)\n",
    "    print(f\"  PyTorch MSE: {pytorch_mse.item():.6f}\")\n",
    "    print(f\"  ✅ Implementation matches: {torch.allclose(loss_mse, pytorch_mse)}\")\n",
    "    \n",
    "    # Test Cross-Entropy Loss\n",
    "    ce_loss = CrossEntropyLoss()\n",
    "    pred_class = torch.randn(4, 3, requires_grad=True)\n",
    "    target_class = torch.randint(0, 3, (4,))\n",
    "    \n",
    "    loss_ce = ce_loss.forward(pred_class, target_class)\n",
    "    print(f\"\\\\n🎯 Cross-Entropy Loss Test:\")\n",
    "    print(f\"  Custom CE: {loss_ce.item():.6f}\")\n",
    "    \n",
    "    # Compare with PyTorch implementation\n",
    "    pytorch_ce = F.cross_entropy(pred_class, target_class)\n",
    "    print(f\"  PyTorch CE: {pytorch_ce.item():.6f}\")\n",
    "    print(f\"  ✅ Implementation matches: {torch.allclose(loss_ce, pytorch_ce, atol=1e-6)}\")\n",
    "    \n",
    "    print(f\"\\\\n🎓 All loss functions implemented successfully!\")\n",
    "\n",
    "test_loss_functions()\n",
    "```\n",
    "\n",
    "## 6. Manual Backpropagation Implementation\n",
    "\n",
    "```python\n",
    "class BackpropagationEngine:\n",
    "    \"\"\"Manual backpropagation implementation for educational purposes.\n",
    "    \n",
    "    This class demonstrates how gradients flow backward through the network\n",
    "    using the chain rule of calculus.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mlp: MLP):\n",
    "        self.mlp = mlp\n",
    "        self.gradients = {}\n",
    "    \n",
    "    def backward(self, loss_gradient: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Perform backpropagation manually using the chain rule.\n",
    "        \n",
    "        Args:\n",
    "            loss_gradient: Gradient of loss w.r.t. network output\n",
    "        \"\"\"\n",
    "        # Initialize gradient flowing backward\n",
    "        current_grad = loss_gradient\n",
    "        \n",
    "        # Process layers in reverse order\n",
    "        for layer_idx in range(len(self.mlp.layers) - 1, -1, -1):\n",
    "            layer = self.mlp.layers[layer_idx]\n",
    "            activation = self.mlp.activations[layer_idx]\n",
    "            \n",
    "            # Get input to this layer\n",
    "            if layer_idx == 0:\n",
    "                layer_input = self.mlp.layer_inputs[0]  # Original network input\n",
    "            else:\n",
    "                layer_input = self.mlp.layer_outputs[layer_idx - 1]\n",
    "            \n",
    "            # Get linear output (pre-activation)\n",
    "            linear_output = layer.last_output\n",
    "            \n",
    "            # Gradient through activation function\n",
    "            if activation is not None:\n",
    "                activation_grad = activation.derivative(linear_output)\n",
    "                grad_after_activation = current_grad * activation_grad\n",
    "            else:\n",
    "                grad_after_activation = current_grad\n",
    "            \n",
    "            # Compute parameter gradients\n",
    "            # dL/dW = input.T @ grad_output\n",
    "            weight_grad = torch.mm(layer_input.T, grad_after_activation)\n",
    "            \n",
    "            if layer.use_bias:\n",
    "                # dL/db = sum(grad_output, dim=0)\n",
    "                bias_grad = torch.sum(grad_after_activation, dim=0)\n",
    "                \n",
    "                # Accumulate bias gradient\n",
    "                if layer.bias.grad is None:\n",
    "                    layer.bias.grad = bias_grad\n",
    "                else:\n",
    "                    layer.bias.grad += bias_grad\n",
    "            \n",
    "            # Accumulate weight gradient\n",
    "            if layer.weight.grad is None:\n",
    "                layer.weight.grad = weight_grad\n",
    "            else:\n",
    "                layer.weight.grad += weight_grad\n",
    "            \n",
    "            # Compute gradient w.r.t. input for next layer\n",
    "            if layer_idx > 0:\n",
    "                current_grad = torch.mm(grad_after_activation, layer.weight.T)\n",
    "        \n",
    "        print(f\"🔄 Backpropagation completed for {len(self.mlp.layers)} layers\")\n",
    "\n",
    "def validate_backpropagation():\n",
    "    \"\"\"Validate manual backpropagation against PyTorch's autograd.\"\"\"\n",
    "    \n",
    "    print(\"🧪 Validating Manual Backpropagation:\")\n",
    "    \n",
    "    # Create simple test network\n",
    "    torch.manual_seed(42)\n",
    "    test_mlp = MLP([2, 4, 1], ['relu', 'none'])\n",
    "    test_x = torch.randn(5, 2)\n",
    "    test_y = torch.randn(5, 1)\n",
    "    mse_loss = MeanSquaredError()\n",
    "    \n",
    "    print(f\"\\\\n📊 Validation Setup:\")\n",
    "    print(f\"  Network: {test_mlp.layer_sizes}\")\n",
    "    print(f\"  Input shape: {test_x.shape}\")\n",
    "    print(f\"  Target shape: {test_y.shape}\")\n",
    "    \n",
    "    # Forward pass and manual backpropagation\n",
    "    test_mlp.zero_grad()\n",
    "    initial_pred = test_mlp.forward(test_x)\n",
    "    initial_loss = mse_loss.forward(initial_pred, test_y)\n",
    "    print(f\"  Initial loss: {initial_loss.item():.6f}\")\n",
    "    \n",
    "    # Manual backpropagation\n",
    "    loss_grad = mse_loss.backward(initial_pred, test_y)\n",
    "    backprop_engine = BackpropagationEngine(test_mlp)\n",
    "    backprop_engine.backward(loss_grad)\n",
    "    \n",
    "    # Store manual gradients\n",
    "    manual_grads = {}\n",
    "    for i, layer in enumerate(test_mlp.layers):\n",
    "        manual_grads[f'layer_{i}_weight'] = layer.weight.grad.clone()\n",
    "        if layer.use_bias:\n",
    "            manual_grads[f'layer_{i}_bias'] = layer.bias.grad.clone()\n",
    "    \n",
    "    print(f\"\\\\n🔍 Manual Gradient Norms:\")\n",
    "    for name, grad in manual_grads.items():\n",
    "        print(f\"  {name}: {grad.norm().item():.6f}\")\n",
    "    \n",
    "    # Compare with PyTorch autograd\n",
    "    print(f\"\\\\n⚖️ PyTorch Autograd Comparison:\")\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    pytorch_mlp = nn.Sequential(\n",
    "        nn.Linear(2, 4),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4, 1)\n",
    "    )\n",
    "    \n",
    "    # Copy weights to ensure identical starting conditions\n",
    "    with torch.no_grad():\n",
    "        pytorch_mlp[0].weight.copy_(test_mlp.layers[0].weight.T)\n",
    "        pytorch_mlp[0].bias.copy_(test_mlp.layers[0].bias)\n",
    "        pytorch_mlp[2].weight.copy_(test_mlp.layers[1].weight.T)\n",
    "        pytorch_mlp[2].bias.copy_(test_mlp.layers[1].bias)\n",
    "    \n",
    "    pytorch_pred = pytorch_mlp(test_x)\n",
    "    pytorch_loss = F.mse_loss(pytorch_pred, test_y)\n",
    "    pytorch_loss.backward()\n",
    "    \n",
    "    print(f\"  PyTorch Gradient Norms:\")\n",
    "    print(f\"    layer_0_weight: {pytorch_mlp[0].weight.grad.norm().item():.6f}\")\n",
    "    print(f\"    layer_0_bias: {pytorch_mlp[0].bias.grad.norm().item():.6f}\")\n",
    "    print(f\"    layer_1_weight: {pytorch_mlp[2].weight.grad.norm().item():.6f}\")\n",
    "    print(f\"    layer_1_bias: {pytorch_mlp[2].bias.grad.norm().item():.6f}\")\n",
    "    \n",
    "    # Validate gradient matching (accounting for weight transpose)\n",
    "    weight_0_match = torch.allclose(manual_grads['layer_0_weight'], pytorch_mlp[0].weight.grad.T, atol=1e-6)\n",
    "    bias_0_match = torch.allclose(manual_grads['layer_0_bias'], pytorch_mlp[0].bias.grad, atol=1e-6)\n",
    "    weight_1_match = torch.allclose(manual_grads['layer_1_weight'], pytorch_mlp[2].weight.grad.T, atol=1e-6)\n",
    "    bias_1_match = torch.allclose(manual_grads['layer_1_bias'], pytorch_mlp[2].bias.grad, atol=1e-6)\n",
    "    \n",
    "    print(f\"\\\\n✅ Gradient Validation Results:\")\n",
    "    print(f\"  Layer 0 weights: {weight_0_match}\")\n",
    "    print(f\"  Layer 0 bias: {bias_0_match}\")\n",
    "    print(f\"  Layer 1 weights: {weight_1_match}\")\n",
    "    print(f\"  Layer 1 bias: {bias_1_match}\")\n",
    "    \n",
    "    all_match = weight_0_match and bias_0_match and weight_1_match and bias_1_match\n",
    "    print(f\"\\\\n🎉 Manual backpropagation: {'✅ VALIDATED' if all_match else '❌ NEEDS DEBUGGING'}\")\n",
    "\n",
    "validate_backpropagation()\n",
    "```\n",
    "\n",
    "## 7. Complete Training System\n",
    "\n",
    "```python\n",
    "class MLPTrainer:\n",
    "    \"\"\"Comprehensive training system for MLP with monitoring and analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, mlp: MLP, loss_fn: LossFunction, learning_rate: float = 0.01):\n",
    "        self.mlp = mlp\n",
    "        self.loss_fn = loss_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Training metrics storage\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        self.gradient_norms = []\n",
    "        self.weight_norms = []\n",
    "        \n",
    "        print(f\"🚀 MLPTrainer Configuration:\")\n",
    "        print(f\"   Learning rate: {learning_rate}\")\n",
    "        print(f\"   Loss function: {loss_fn}\")\n",
    "        print(f\"   Network layers: {len(mlp.layers)}\")\n",
    "    \n",
    "    def train_step(self, x: torch.Tensor, y: torch.Tensor) -> Tuple[float, float]:\n",
    "        \"\"\"Execute single training step with manual backpropagation.\"\"\"\n",
    "        \n",
    "        # Zero gradients\n",
    "        self.mlp.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        predictions = self.mlp.forward(x)\n",
    "        loss = self.loss_fn.forward(predictions, y)\n",
    "        \n",
    "        # Manual backpropagation\n",
    "        loss_gradient = self.loss_fn.backward(predictions, y)\n",
    "        backprop_engine = BackpropagationEngine(self.mlp)\n",
    "        backprop_engine.backward(loss_gradient)\n",
    "        \n",
    "        # Compute gradient norm for monitoring\n",
    "        total_grad_norm = 0\n",
    "        for layer in self.mlp.layers:\n",
    "            if layer.weight.grad is not None:\n",
    "                total_grad_norm += layer.weight.grad.norm().item() ** 2\n",
    "            if layer.use_bias and layer.bias.grad is not None:\n",
    "                total_grad_norm += layer.bias.grad.norm().item() ** 2\n",
    "        total_grad_norm = math.sqrt(total_grad_norm)\n",
    "        \n",
    "        # Parameter updates using gradient descent\n",
    "        with torch.no_grad():\n",
    "            for layer in self.mlp.layers:\n",
    "                if layer.weight.grad is not None:\n",
    "                    layer.weight -= self.learning_rate * layer.weight.grad\n",
    "                if layer.use_bias and layer.bias.grad is not None:\n",
    "                    layer.bias -= self.learning_rate * layer.bias.grad\n",
    "        \n",
    "        return loss.item(), total_grad_norm\n",
    "    \n",
    "    def evaluate(self, x: torch.Tensor, y: torch.Tensor, is_classification: bool = False) -> Tuple[float, float]:\n",
    "        \"\"\"Evaluate model performance on given data.\"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = self.mlp.forward(x)\n",
    "            loss = self.loss_fn.forward(predictions, y)\n",
    "            \n",
    "            # Compute accuracy for classification tasks\n",
    "            if is_classification:\n",
    "                if y.dtype == torch.long:  # Class indices\n",
    "                    pred_classes = torch.argmax(predictions, dim=1)\n",
    "                    accuracy = (pred_classes == y).float().mean().item()\n",
    "                else:  # Probability outputs\n",
    "                    pred_classes = (predictions > 0.5).float()\n",
    "                    accuracy = (pred_classes == y).float().mean().item()\n",
    "            else:\n",
    "                accuracy = 0.0  # Not applicable for regression\n",
    "        \n",
    "        return loss.item(), accuracy\n",
    "    \n",
    "    def train_epoch(self, train_x: torch.Tensor, train_y: torch.Tensor, \n",
    "                   val_x: torch.Tensor = None, val_y: torch.Tensor = None, \n",
    "                   batch_size: int = 32, is_classification: bool = False):\n",
    "        \"\"\"Train model for one complete epoch.\"\"\"\n",
    "        \n",
    "        n_samples = train_x.shape[0]\n",
    "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "        \n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_grad_norm = 0.0\n",
    "        \n",
    "        # Mini-batch training loop\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, n_samples)\n",
    "            \n",
    "            batch_x = train_x[start_idx:end_idx]\n",
    "            batch_y = train_y[start_idx:end_idx]\n",
    "            \n",
    "            batch_loss, batch_grad_norm = self.train_step(batch_x, batch_y)\n",
    "            epoch_train_loss += batch_loss\n",
    "            epoch_grad_norm += batch_grad_norm\n",
    "        \n",
    "        # Average metrics over batches\n",
    "        epoch_train_loss /= n_batches\n",
    "        epoch_grad_norm /= n_batches\n",
    "        \n",
    "        # Validation evaluation\n",
    "        if val_x is not None and val_y is not None:\n",
    "            val_loss, val_accuracy = self.evaluate(val_x, val_y, is_classification)\n",
    "        else:\n",
    "            val_loss, val_accuracy = 0.0, 0.0\n",
    "        \n",
    "        # Training evaluation\n",
    "        train_loss, train_accuracy = self.evaluate(train_x, train_y, is_classification)\n",
    "        \n",
    "        # Store training metrics\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.train_accuracies.append(train_accuracy)\n",
    "        self.val_accuracies.append(val_accuracy)\n",
    "        self.gradient_norms.append(epoch_grad_norm)\n",
    "        \n",
    "        # Compute weight norms for monitoring\n",
    "        total_weight_norm = 0\n",
    "        for layer in self.mlp.layers:\n",
    "            total_weight_norm += layer.weight.norm().item() ** 2\n",
    "        self.weight_norms.append(math.sqrt(total_weight_norm))\n",
    "        \n",
    "        return train_loss, val_loss, train_accuracy, val_accuracy\n",
    "    \n",
    "    def train(self, train_x: torch.Tensor, train_y: torch.Tensor, \n",
    "             val_x: torch.Tensor = None, val_y: torch.Tensor = None,\n",
    "             epochs: int = 100, batch_size: int = 32, \n",
    "             is_classification: bool = False, verbose: bool = True):\n",
    "        \"\"\"Execute complete training procedure.\"\"\"\n",
    "        \n",
    "        print(f\"\\\\n🚀 Training Configuration:\")\n",
    "        print(f\"   Epochs: {epochs}\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        print(f\"   Task type: {'Classification' if is_classification else 'Regression'}\")\n",
    "        print(f\"   Training samples: {len(train_x)}\")\n",
    "        print(f\"   Validation samples: {len(val_x) if val_x is not None else 'None'}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss, val_loss, train_acc, val_acc = self.train_epoch(\n",
    "                train_x, train_y, val_x, val_y, batch_size, is_classification\n",
    "            )\n",
    "            \n",
    "            if verbose and (epoch + 1) % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1:3d} | \"\n",
    "                      f\"Train Loss: {train_loss:.4f} | \"\n",
    "                      f\"Val Loss: {val_loss:.4f} | \"\n",
    "                      f\"Train Acc: {train_acc:.3f} | \"\n",
    "                      f\"Val Acc: {val_acc:.3f}\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"\\\\n✅ Training completed in {training_time:.2f} seconds\")\n",
    "        \n",
    "        return self.train_losses, self.val_losses, self.train_accuracies, self.val_accuracies\n",
    "\n",
    "def create_synthetic_data(task='classification', n_samples=1000, noise=0.1):\n",
    "    \"\"\"Generate synthetic datasets for testing.\"\"\"\n",
    "    \n",
    "    if task == 'classification':\n",
    "        X, y = make_classification(\n",
    "            n_samples=n_samples,\n",
    "            n_features=10,\n",
    "            n_informative=8,\n",
    "            n_redundant=2,\n",
    "            n_classes=3,\n",
    "            random_state=42\n",
    "        )\n",
    "    else:  # regression\n",
    "        X, y = make_regression(\n",
    "            n_samples=n_samples,\n",
    "            n_features=10,\n",
    "            n_informative=8,\n",
    "            noise=noise,\n",
    "            random_state=42\n",
    "        )\n",
    "        y = y.reshape(-1, 1)\n",
    "    \n",
    "    # Feature normalization\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    \n",
    "    if task == 'classification':\n",
    "        y_train = torch.LongTensor(y_train)\n",
    "        y_test = torch.LongTensor(y_test)\n",
    "    else:\n",
    "        y_train = torch.FloatTensor(y_train)\n",
    "        y_test = torch.FloatTensor(y_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Demonstrate complete training pipeline\n",
    "def demonstrate_training_pipeline():\n",
    "    \"\"\"Comprehensive demonstration of the training system.\"\"\"\n",
    "    \n",
    "    print(\"🎯 Demonstrating Complete Training Pipeline:\")\n",
    "    \n",
    "    # Classification experiment\n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    print(\"CLASSIFICATION EXPERIMENT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    X_train_cls, X_test_cls, y_train_cls, y_test_cls = create_synthetic_data('classification')\n",
    "    \n",
    "    mlp_cls = MLP([10, 16, 8, 3], ['relu', 'relu', 'none'])\n",
    "    loss_fn_cls = CrossEntropyLoss()\n",
    "    trainer_cls = MLPTrainer(mlp_cls, loss_fn_cls, learning_rate=0.01)\n",
    "    \n",
    "    train_losses_cls, val_losses_cls, train_accs_cls, val_accs_cls = trainer_cls.train(\n",
    "        X_train_cls, y_train_cls, X_test_cls, y_test_cls,\n",
    "        epochs=100, batch_size=32, is_classification=True, verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\n📊 Classification Results:\")\n",
    "    print(f\"   Final training accuracy: {train_accs_cls[-1]:.3f}\")\n",
    "    print(f\"   Final validation accuracy: {val_accs_cls[-1]:.3f}\")\n",
    "    \n",
    "    # Regression experiment\n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    print(\"REGRESSION EXPERIMENT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = create_synthetic_data('regression')\n",
    "    \n",
    "    mlp_reg = MLP([10, 16, 8, 1], ['relu', 'tanh', 'none'])\n",
    "    loss_fn_reg = MeanSquaredError()\n",
    "    trainer_reg = MLPTrainer(mlp_reg, loss_fn_reg, learning_rate=0.01)\n",
    "    \n",
    "    train_losses_reg, val_losses_reg, _, _ = trainer_reg.train(\n",
    "        X_train_reg, y_train_reg, X_test_reg, y_test_reg,\n",
    "        epochs=100, batch_size=32, is_classification=False, verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\n📊 Regression Results:\")\n",
    "    print(f\"   Final training MSE: {train_losses_reg[-1]:.6f}\")\n",
    "    print(f\"   Final validation MSE: {val_losses_reg[-1]:.6f}\")\n",
    "    \n",
    "    return trainer_cls, trainer_reg\n",
    "\n",
    "# Execute demonstration\n",
    "classification_trainer, regression_trainer = demonstrate_training_pipeline()\n",
    "```\n",
    "\n",
    "## 8. Training Results Visualization\n",
    "\n",
    "```python\n",
    "def create_comprehensive_training_dashboard(cls_trainer, reg_trainer):\n",
    "    \"\"\"Generate comprehensive visualization of training results.\"\"\"\n",
    "    \n",
    "    print(\"📊 Creating Training Results Dashboard:\")\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Classification results\n",
    "    epochs_cls = range(1, len(cls_trainer.train_losses) + 1)\n",
    "    \n",
    "    ax1.plot(epochs_cls, cls_trainer.train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(epochs_cls, cls_trainer.val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title('Classification: Training Progress', fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Cross-Entropy Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(epochs_cls, cls_trainer.train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(epochs_cls, cls_trainer.val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_title('Classification: Accuracy Evolution', fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Regression results\n",
    "    epochs_reg = range(1, len(reg_trainer.train_losses) + 1)\n",
    "    \n",
    "    ax3.semilogy(epochs_reg, reg_trainer.train_losses, 'b-', label='Training MSE', linewidth=2)\n",
    "    ax3.semilogy(epochs_reg, reg_trainer.val_losses, 'r-', label='Validation MSE', linewidth=2)\n",
    "    ax3.set_title('Regression: Training Progress', fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('MSE Loss (log scale)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training dynamics\n",
    "    ax4_twin = ax4.twinx()\n",
    "    \n",
    "    ax4.plot(epochs_cls, cls_trainer.gradient_norms, 'g-', label='Gradient Norm', linewidth=2)\n",
    "    ax4_twin.plot(epochs_cls, cls_trainer.weight_norms, 'orange', label='Weight Norm', linewidth=2)\n",
    "    \n",
    "    ax4.set_title('Training Dynamics (Classification)', fontweight='bold')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Gradient Norm', color='g')\n",
    "    ax4_twin.set_ylabel('Weight Norm', color='orange')\n",
    "    ax4.tick_params(axis='y', labelcolor='g')\n",
    "    ax4_twin.tick_params(axis='y', labelcolor='orange')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Combined legend\n",
    "    lines1, labels1 = ax4.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax4_twin.get_legend_handles_labels()\n",
    "    ax4.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    \n",
    "    plt.suptitle('MLP from Scratch: Complete Training Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'training_dashboard.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print performance summary\n",
    "    print(f\"\\\\n📈 Training Performance Summary:\")\n",
    "    print(f\"\\\\nClassification Task:\")\n",
    "    print(f\"   Final training accuracy: {cls_trainer.train_accuracies[-1]:.3f}\")\n",
    "    print(f\"   Final validation accuracy: {cls_trainer.val_accuracies[-1]:.3f}\")\n",
    "    print(f\"   Final gradient norm: {cls_trainer.gradient_norms[-1]:.6f}\")\n",
    "    \n",
    "    print(f\"\\\\nRegression Task:\")\n",
    "    print(f\"   Final training MSE: {reg_trainer.train_losses[-1]:.6f}\")\n",
    "    print(f\"   Final validation MSE: {reg_trainer.val_losses[-1]:.6f}\")\n",
    "    print(f\"   Final gradient norm: {reg_trainer.gradient_norms[-1]:.6f}\")\n",
    "\n",
    "create_comprehensive_training_dashboard(classification_trainer, regression_trainer)\n",
    "```\n",
    "\n",
    "## 9. PyTorch Comparison Analysis\n",
    "\n",
    "```python\n",
    "def compare_with_pytorch_implementation():\n",
    "    \"\"\"Comprehensive comparison with PyTorch's built-in functionality.\"\"\"\n",
    "    \n",
    "    print(\"⚖️ PyTorch Implementation Comparison:\")\n",
    "    \n",
    "    # Create identical architectures\n",
    "    print(f\"\\\\n🔧 Architecture Comparison:\")\n",
    "    \n",
    "    # Our implementation\n",
    "    our_mlp = MLP([10, 32, 16, 3], ['relu', 'relu', 'none'])\n",
    "    \n",
    "    # PyTorch equivalent\n",
    "    pytorch_mlp = nn.Sequential(\n",
    "        nn.Linear(10, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 16), \n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 3)\n",
    "    )\n",
    "    \n",
    "    # Parameter comparison\n",
    "    our_params = sum(sum(p.numel() for p in layer.parameters()) for layer in our_mlp.layers)\n",
    "    pytorch_params = sum(p.numel() for p in pytorch_mlp.parameters())\n",
    "    \n",
    "    print(f\"   Our implementation parameters: {our_params:,}\")\n",
    "    print(f\"   PyTorch implementation parameters: {pytorch_params:,}\")\n",
    "    print(f\"   ✅ Parameter count matches: {our_params == pytorch_params}\")\n",
    "    \n",
    "    # Performance timing comparison\n",
    "    print(f\"\\\\n⏱️ Performance Timing (1000 forward passes):\")\n",
    "    \n",
    "    test_input = torch.randn(32, 10)\n",
    "    \n",
    "    # Time our implementation\n",
    "    start_time = time.time()\n",
    "    for _ in range(1000):\n",
    "        _ = our_mlp.forward(test_input)\n",
    "    our_time = time.time() - start_time\n",
    "    \n",
    "    # Time PyTorch implementation\n",
    "    start_time = time.time()\n",
    "    for _ in range(1000):\n",
    "        _ = pytorch_mlp(test_input)\n",
    "    pytorch_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   Our implementation: {our_time*1000:.2f} ms\")\n",
    "    print(f\"   PyTorch implementation: {pytorch_time*1000:.2f} ms\") \n",
    "    print(f\"   Speed ratio: {our_time/pytorch_time:.1f}x slower\")\n",
    "    \n",
    "    # Feature comparison matrix\n",
    "    print(f\"\\\\n📋 Feature Comparison Matrix:\")\n",
    "    \n",
    "    feature_comparison = {\n",
    "        'Feature': [\n",
    "            'Forward Propagation',\n",
    "            'Backward Propagation',\n",
    "            'Automatic Differentiation',\n",
    "            'GPU Acceleration',\n",
    "            'Memory Optimization',\n",
    "            'Numerical Stability',\n",
    "            'Educational Value',\n",
    "            'Production Ready',\n",
    "            'Debugging Capability',\n",
    "            'Extensibility'\n",
    "        ],\n",
    "        'Our Implementation': [\n",
    "            '✅ Manual & Transparent',\n",
    "            '✅ Manual Implementation',\n",
    "            '❌ Manual Only',\n",
    "            '❌ CPU Only',\n",
    "            '⚠️ Basic',\n",
    "            '⚠️ Limited',\n",
    "            '✅ Excellent',\n",
    "            '❌ Educational Only',\n",
    "            '✅ Full Visibility',\n",
    "            '✅ Highly Modular'\n",
    "        ],\n",
    "        'PyTorch': [\n",
    "            '✅ Optimized C++',\n",
    "            '✅ Automatic',\n",
    "            '✅ Built-in Autograd',\n",
    "            '✅ CUDA Support',\n",
    "            '✅ Highly Optimized',\n",
    "            '✅ Production Grade',\n",
    "            '⚠️ Black Box',\n",
    "            '✅ Production Ready',\n",
    "            '⚠️ Limited Visibility',\n",
    "            '✅ Rich Ecosystem'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(f\"{'Feature':<25} {'Our Implementation':<25} {'PyTorch':<25}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for i in range(len(feature_comparison['Feature'])):\n",
    "        feature = feature_comparison['Feature'][i]\n",
    "        ours = feature_comparison['Our Implementation'][i]\n",
    "        pytorch = feature_comparison['PyTorch'][i]\n",
    "        print(f\"{feature:<25} {ours:<25} {pytorch:<25}\")\n",
    "    \n",
    "    print(f\"\\\\n💡 Key Insights:\")\n",
    "    insights = [\n",
    "        \"• Our implementation provides deep understanding of neural network internals\",\n",
    "        \"• PyTorch offers optimized performance and production-ready features\",\n",
    "        \"• Manual implementation enables complete control and debugging capability\",\n",
    "        \"• PyTorch's automatic differentiation prevents gradient computation errors\",\n",
    "        \"• Both approaches serve complementary educational and practical purposes\"\n",
    "    ]\n",
    "    \n",
    "    for insight in insights:\n",
    "        print(f\"  {insight}\")\n",
    "\n",
    "compare_with_pytorch_implementation()\n",
    "```\n",
    "\n",
    "## Summary and Key Findings\n",
    "\n",
    "This comprehensive notebook has successfully demonstrated:\n",
    "\n",
    "### 🎯 **Implementation Achievements**\n",
    "- **Complete MLP Architecture**: Built from fundamental linear layers and activation functions\n",
    "- **Manual Backpropagation**: Implemented gradient computation using chain rule\n",
    "- **Training System**: Created comprehensive training pipeline with monitoring\n",
    "- **Loss Functions**: Developed MSE, Cross-Entropy, and Binary Cross-Entropy from scratch\n",
    "- **Validation**: Verified implementations against PyTorch's built-in functionality\n",
    "\n",
    "### 📊 **Key Learning Outcomes**\n",
    "- **Deep Understanding**: Gained insight into neural network internals and gradient flow\n",
    "- **Mathematical Foundation**: Applied calculus and linear algebra in practical implementation  \n",
    "- **Algorithm Implementation**: Translated mathematical concepts into working code\n",
    "- **Performance Analysis**: Evaluated training dynamics and convergence behavior\n",
    "- **Debugging Skills**: Developed ability to trace and fix neural network issues\n",
    "\n",
    "### 🔧 **Technical Accomplishments**\n",
    "- **Forward Propagation**: Matrix operations with activation function applications\n",
    "- **Backward Propagation**: Chain rule implementation for gradient computation\n",
    "- **Parameter Updates**: Gradient descent optimization with learning rate scheduling\n",
    "- **Training Monitoring**: Comprehensive metrics collection and visualization\n",
    "- **Comparative Validation**: Benchmarking against production implementations\n",
    "\n",
    "### 💡 **Educational Value**\n",
    "- **Transparency**: Complete visibility into every computational step\n",
    "- **Modularity**: Clean, extensible code architecture for further exploration  \n",
    "- **Verification**: Systematic validation against established implementations\n",
    "- **Visualization**: Clear presentation of training dynamics and performance\n",
    "- **Foundation**: Solid base for understanding advanced architectures and techniques\n",
    "\n",
    "### 🚀 **Next Steps**\n",
    "- **Advanced Architectures**: Apply these principles to CNNs, RNNs, and Transformers\n",
    "- **Optimization Techniques**: Implement momentum, Adam, and other advanced optimizers\n",
    "- **Regularization Methods**: Add dropout, batch normalization, and other regularization\n",
    "- **Hardware Acceleration**: Extend implementation for GPU computation\n",
    "- **Production Deployment**: Scale implementation for real-world applications\n",
    "\n",
    "**This implementation provides the fundamental understanding necessary for mastering advanced deep learning concepts and architectures.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
