{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1535e962",
   "metadata": {},
   "source": [
    "# Advanced Training Techniques: Master the Art of Neural Network Training\n",
    "\n",
    "**PyTorch Mastery Hub - Advanced Training Module**\n",
    "\n",
    "**Topics Covered:** Regularization, Advanced Optimizers, Learning Rate Scheduling, Data Augmentation, Training Debugging  \n",
    "**Prerequisites:** Neural network fundamentals, gradient descent, PyTorch basics  \n",
    "**Difficulty Level:** Intermediate to Advanced  \n",
    "**Estimated Time:** 3-4 hours\n",
    "\n",
    "## Overview\n",
    "\n",
    "This comprehensive notebook explores the sophisticated training techniques that separate good models from great ones. We'll dive deep into regularization methods, modern optimization algorithms, learning rate scheduling strategies, and advanced data augmentation techniques that are essential for training robust, high-performance neural networks.\n",
    "\n",
    "## Key Objectives\n",
    "1. Master advanced regularization techniques to prevent overfitting\n",
    "2. Implement and compare modern optimization algorithms (Adam, AdamW, LAMB, SAM)\n",
    "3. Design sophisticated learning rate scheduling and warm-up strategies\n",
    "4. Apply cutting-edge data augmentation methods (RandAugment, MixUp, CutMix)\n",
    "5. Develop training stability monitoring and debugging capabilities\n",
    "6. Build comprehensive training pipelines with best practices\n",
    "7. Analyze training dynamics and performance trade-offs\n",
    "8. Generate detailed training reports and visualizations\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Environment Configuration\n",
    "\n",
    "```python\n",
    "# Essential imports for advanced training techniques\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional, Callable, Any\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "from collections import defaultdict, OrderedDict\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our utilities\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "try:\n",
    "    from src.utils.device_utils import get_device\n",
    "    from src.utils.model_utils import count_parameters\n",
    "    from src.utils.data_utils import create_synthetic_dataset\n",
    "    from src.utils.logging_utils import setup_logger\n",
    "except ImportError:\n",
    "    print(\"Warning: Custom utilities not found. Using fallback implementations.\")\n",
    "    def get_device():\n",
    "        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    def setup_logger(name):\n",
    "        import logging\n",
    "        return logging.getLogger(name)\n",
    "\n",
    "# Set up environment\n",
    "device = get_device()\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "logger = setup_logger('Training_Techniques')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "\n",
    "# Create results directory\n",
    "results_dir = os.path.join('results', 'training_techniques')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "print(\"üéØ PyTorch Mastery Hub - Advanced Training Techniques\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üì± Device: {device}\")\n",
    "print(f\"üé® PyTorch version: {torch.__version__}\")\n",
    "print(f\"üìÅ Results directory: {results_dir}\")\n",
    "print(f\"‚ú® Ready to master advanced training techniques!\\n\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Advanced Regularization Techniques\n",
    "\n",
    "### 2.1 Comprehensive Regularization Toolkit Implementation\n",
    "\n",
    "```python\n",
    "print(\"=== 2.1 Advanced Regularization Methods ===\\n\")\n",
    "\n",
    "class DropBlock2D(nn.Module):\n",
    "    \"\"\"DropBlock regularization for 2D feature maps - superior to standard dropout for CNNs.\"\"\"\n",
    "    \n",
    "    def __init__(self, drop_rate: float = 0.1, block_size: int = 7):\n",
    "        super().__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.training or self.drop_rate == 0:\n",
    "            return x\n",
    "        \n",
    "        # Calculate gamma (probability of dropping a block)\n",
    "        gamma = self.drop_rate / (self.block_size ** 2)\n",
    "        \n",
    "        # Generate random mask\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        \n",
    "        # Sample mask\n",
    "        mask = torch.rand((batch_size, channels, height - self.block_size + 1, \n",
    "                          width - self.block_size + 1), device=x.device)\n",
    "        mask = (mask < gamma).float()\n",
    "        \n",
    "        # Expand mask to block size\n",
    "        mask = F.max_pool2d(mask, kernel_size=self.block_size, stride=1, \n",
    "                           padding=self.block_size // 2)\n",
    "        \n",
    "        # Ensure mask has same size as input\n",
    "        if mask.shape[-2:] != x.shape[-2:]:\n",
    "            mask = F.interpolate(mask, size=x.shape[-2:], mode='nearest')\n",
    "        \n",
    "        # Apply mask\n",
    "        mask = 1 - mask\n",
    "        normalize_scale = mask.numel() / (mask.sum() + 1e-7)\n",
    "        \n",
    "        return x * mask * normalize_scale\n",
    "\n",
    "class StochasticDepth(nn.Module):\n",
    "    \"\"\"Stochastic depth for training deeper networks efficiently.\"\"\"\n",
    "    \n",
    "    def __init__(self, survival_prob: float = 0.8):\n",
    "        super().__init__()\n",
    "        self.survival_prob = survival_prob\n",
    "    \n",
    "    def forward(self, x, residual):\n",
    "        if not self.training:\n",
    "            return x + residual\n",
    "        \n",
    "        # Random dropout of entire residual path\n",
    "        if torch.rand(1).item() < self.survival_prob:\n",
    "            return x + residual / self.survival_prob  # Scale for expected value\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class MixUp:\n",
    "    \"\"\"MixUp data augmentation for improved generalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.2):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def __call__(self, x, y):\n",
    "        if self.alpha <= 0:\n",
    "            return x, y\n",
    "        \n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        batch_size = x.size(0)\n",
    "        index = torch.randperm(batch_size)\n",
    "        \n",
    "        mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "        y_a, y_b = y, y[index]\n",
    "        \n",
    "        return mixed_x, (y_a, y_b, lam)\n",
    "\n",
    "class CutMix:\n",
    "    \"\"\"CutMix data augmentation combining regional replacement with mixing.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 1.0):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def __call__(self, x, y):\n",
    "        if self.alpha <= 0:\n",
    "            return x, y\n",
    "        \n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        batch_size = x.size(0)\n",
    "        index = torch.randperm(batch_size)\n",
    "        \n",
    "        # Generate random box\n",
    "        W, H = x.size(2), x.size(3)\n",
    "        cut_rat = np.sqrt(1. - lam)\n",
    "        cut_w = int(W * cut_rat)\n",
    "        cut_h = int(H * cut_rat)\n",
    "        \n",
    "        cx = np.random.randint(W)\n",
    "        cy = np.random.randint(H)\n",
    "        \n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "        \n",
    "        # Apply cutmix\n",
    "        x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
    "        \n",
    "        # Adjust lambda\n",
    "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "        \n",
    "        y_a, y_b = y, y[index]\n",
    "        return x, (y_a, y_b, lam)\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"Label smoothing regularization to prevent overconfident predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int, smoothing: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1.0 - smoothing\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.num_classes - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=-1))\n",
    "\n",
    "print(\"‚úÖ Advanced regularization toolkit implemented!\")\n",
    "print(\"   ‚Ä¢ DropBlock2D: Block-wise dropout for convolutional layers\")\n",
    "print(\"   ‚Ä¢ StochasticDepth: Random layer skipping for deeper networks\")\n",
    "print(\"   ‚Ä¢ MixUp: Linear interpolation between training examples\")\n",
    "print(\"   ‚Ä¢ CutMix: Regional replacement data augmentation\")\n",
    "print(\"   ‚Ä¢ LabelSmoothing: Soft target regularization\")\n",
    "```\n",
    "\n",
    "### 2.2 Regularization Testing and Analysis\n",
    "\n",
    "```python\n",
    "# Test regularization techniques with comprehensive analysis\n",
    "print(\"üõ°Ô∏è Testing Regularization Techniques:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create test data\n",
    "test_input = torch.randn(4, 3, 32, 32)\n",
    "test_labels = torch.randint(0, 10, (4,))\n",
    "\n",
    "# Test DropBlock\n",
    "dropblock = DropBlock2D(drop_rate=0.1, block_size=7)\n",
    "dropblock.train()\n",
    "dropblock_output = dropblock(test_input)\n",
    "\n",
    "print(f\"\\nüìä DropBlock Analysis:\")\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {dropblock_output.shape}\")\n",
    "print(f\"  Dropout ratio: {(dropblock_output == 0).float().mean().item():.3f}\")\n",
    "print(f\"  Mean activation: {dropblock_output.mean().item():.4f}\")\n",
    "print(f\"  Std activation: {dropblock_output.std().item():.4f}\")\n",
    "\n",
    "# Test MixUp\n",
    "mixup = MixUp(alpha=0.2)\n",
    "mixed_input, mixed_labels = mixup(test_input, test_labels)\n",
    "\n",
    "print(f\"\\nüîÄ MixUp Analysis:\")\n",
    "print(f\"  Original labels: {test_labels.tolist()}\")\n",
    "print(f\"  Mixed labels A: {mixed_labels[0].tolist()}\")\n",
    "print(f\"  Mixed labels B: {mixed_labels[1].tolist()}\")\n",
    "print(f\"  Lambda (mixing ratio): {mixed_labels[2]:.3f}\")\n",
    "print(f\"  Input similarity: {F.cosine_similarity(test_input.flatten(), mixed_input.flatten(), dim=0).item():.3f}\")\n",
    "\n",
    "# Test CutMix\n",
    "cutmix = CutMix(alpha=1.0)\n",
    "cut_input, cut_labels = cutmix(test_input.clone(), test_labels.clone())\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è CutMix Analysis:\")\n",
    "print(f\"  Original labels: {test_labels.tolist()}\")\n",
    "print(f\"  Cut labels A: {cut_labels[0].tolist()}\")\n",
    "print(f\"  Cut labels B: {cut_labels[1].tolist()}\")\n",
    "print(f\"  Lambda (area ratio): {cut_labels[2]:.3f}\")\n",
    "print(f\"  Pixels changed: {(test_input != cut_input).float().mean().item():.3f}\")\n",
    "\n",
    "# Test Label Smoothing\n",
    "label_smoothing = LabelSmoothing(num_classes=10, smoothing=0.1)\n",
    "test_pred = torch.randn(4, 10)\n",
    "smooth_loss = label_smoothing(test_pred, test_labels)\n",
    "ce_loss = F.cross_entropy(test_pred, test_labels)\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Label Smoothing Analysis:\")\n",
    "print(f\"  Standard CE loss: {ce_loss.item():.4f}\")\n",
    "print(f\"  Label smoothed loss: {smooth_loss.item():.4f}\")\n",
    "print(f\"  Loss difference: {(smooth_loss - ce_loss).item():.4f}\")\n",
    "print(f\"  Regularization strength: {abs(smooth_loss - ce_loss).item() / ce_loss.item() * 100:.1f}%\")\n",
    "\n",
    "# Comprehensive regularization comparison\n",
    "class RegularizedNet(nn.Module):\n",
    "    \"\"\"Network with configurable regularization techniques.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10, regularization_config: Dict = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = regularization_config or {}\n",
    "        \n",
    "        # Base architecture\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Regularization layers\n",
    "        self.dropout1 = nn.Dropout2d(self.config.get('dropout', 0.0))\n",
    "        self.dropout2 = nn.Dropout2d(self.config.get('dropout', 0.0))\n",
    "        \n",
    "        if self.config.get('dropblock', False):\n",
    "            self.dropblock1 = DropBlock2D(self.config.get('dropblock_rate', 0.1))\n",
    "            self.dropblock2 = DropBlock2D(self.config.get('dropblock_rate', 0.1))\n",
    "        \n",
    "        # Batch normalization\n",
    "        if self.config.get('batch_norm', True):\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            self.bn2 = nn.BatchNorm2d(128)\n",
    "            self.bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Stochastic depth for residual connections\n",
    "        if self.config.get('stochastic_depth', False):\n",
    "            self.stoch_depth1 = StochasticDepth(self.config.get('survival_prob', 0.8))\n",
    "            self.stoch_depth2 = StochasticDepth(self.config.get('survival_prob', 0.8))\n",
    "        \n",
    "        # Classifier\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "        self.final_dropout = nn.Dropout(self.config.get('final_dropout', 0.0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = self.conv1(x)\n",
    "        if hasattr(self, 'bn1'):\n",
    "            x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        if hasattr(self, 'dropblock1'):\n",
    "            x = self.dropblock1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Second block\n",
    "        x = self.conv2(x)\n",
    "        if hasattr(self, 'bn2'):\n",
    "            x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        if hasattr(self, 'dropblock2'):\n",
    "            x = self.dropblock2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Third block\n",
    "        x = self.conv3(x)\n",
    "        if hasattr(self, 'bn3'):\n",
    "            x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.final_dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def compare_regularization_methods():\n",
    "    \"\"\"Compare different regularization techniques systematically.\"\"\"\n",
    "    \n",
    "    print(f\"\\nüî¨ Regularization Methods Comparison:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Define different regularization configurations\n",
    "    configs = {\n",
    "        'baseline': {},\n",
    "        'dropout_only': {'dropout': 0.3, 'final_dropout': 0.5, 'batch_norm': False},\n",
    "        'batch_norm_only': {'batch_norm': True, 'dropout': 0.0},\n",
    "        'dropblock_enhanced': {'dropblock': True, 'dropblock_rate': 0.1, 'batch_norm': True},\n",
    "        'full_regularization': {\n",
    "            'dropout': 0.2,\n",
    "            'batch_norm': True,\n",
    "            'dropblock': True,\n",
    "            'dropblock_rate': 0.05,\n",
    "            'stochastic_depth': True,\n",
    "            'survival_prob': 0.9,\n",
    "            'final_dropout': 0.3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create and analyze models\n",
    "    models = {}\n",
    "    analysis_results = {}\n",
    "    \n",
    "    test_input = torch.randn(8, 3, 32, 32)\n",
    "    \n",
    "    print(f\"{'Method':<20} {'Parameters':<12} {'Train Var':<12} {'Eval Var':<12} {'Reg Effect':<12}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for name, config in configs.items():\n",
    "        model = RegularizedNet(num_classes=10, regularization_config=config)\n",
    "        models[name] = model\n",
    "        \n",
    "        total_params = count_parameters(model)\n",
    "        \n",
    "        # Training mode variance\n",
    "        model.train()\n",
    "        with torch.no_grad():\n",
    "            train_outputs = []\n",
    "            for _ in range(5):  # Multiple forward passes\n",
    "                train_outputs.append(model(test_input))\n",
    "            train_var = torch.var(torch.stack(train_outputs)).item()\n",
    "        \n",
    "        # Evaluation mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            eval_output = model(test_input)\n",
    "            eval_var = torch.var(eval_output).item()\n",
    "        \n",
    "        # Regularization effect (difference in variance)\n",
    "        reg_effect = train_var / (eval_var + 1e-8)\n",
    "        \n",
    "        analysis_results[name] = {\n",
    "            'parameters': total_params,\n",
    "            'train_variance': train_var,\n",
    "            'eval_variance': eval_var,\n",
    "            'regularization_effect': reg_effect,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        print(f\"{name:<20} {total_params:<12,} {train_var:<12.4f} {eval_var:<12.4f} {reg_effect:<12.2f}\")\n",
    "    \n",
    "    return models, analysis_results\n",
    "\n",
    "reg_models, reg_analysis = compare_regularization_methods()\n",
    "```\n",
    "\n",
    "### 2.3 Regularization Effects Visualization\n",
    "\n",
    "```python\n",
    "def visualize_regularization_effects(models, analysis_results):\n",
    "    \"\"\"Create comprehensive regularization analysis visualizations.\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Parameter overhead analysis\n",
    "    method_names = list(analysis_results.keys())\n",
    "    param_counts = [analysis_results[name]['parameters'] for name in method_names]\n",
    "    baseline_params = param_counts[0]\n",
    "    param_overhead = [(p - baseline_params) / baseline_params * 100 for p in param_counts]\n",
    "    \n",
    "    bars = ax1.bar(method_names, param_overhead, alpha=0.8, \n",
    "                   color=['gray', 'blue', 'green', 'orange', 'red'])\n",
    "    ax1.set_title('Parameter Overhead by Regularization Method', fontweight='bold')\n",
    "    ax1.set_ylabel('Overhead (%)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, overhead in zip(bars, param_overhead):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{overhead:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Regularization strength analysis\n",
    "    reg_effects = [analysis_results[name]['regularization_effect'] for name in method_names]\n",
    "    colors = ['gray', 'lightblue', 'lightgreen', 'orange', 'lightcoral']\n",
    "    \n",
    "    bars = ax2.bar(method_names, reg_effects, alpha=0.8, color=colors)\n",
    "    ax2.set_title('Regularization Strength (Train/Eval Variance Ratio)', fontweight='bold')\n",
    "    ax2.set_ylabel('Regularization Effect')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='No regularization')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, effect in zip(bars, reg_effects):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{effect:.2f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Plot 3: Dropout rate effectiveness (simulated data)\n",
    "    dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "    train_acc = [95, 94, 92, 89, 85, 80, 75, 68]\n",
    "    val_acc = [85, 87, 89, 88, 86, 83, 78, 72]\n",
    "    \n",
    "    ax3.plot(dropout_rates, train_acc, 'bo-', label='Training Accuracy', linewidth=3, markersize=8)\n",
    "    ax3.plot(dropout_rates, val_acc, 'ro-', label='Validation Accuracy', linewidth=3, markersize=8)\n",
    "    ax3.set_title('Dropout Rate vs Model Performance', fontweight='bold')\n",
    "    ax3.set_xlabel('Dropout Rate')\n",
    "    ax3.set_ylabel('Accuracy (%)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark optimal point\n",
    "    optimal_idx = np.argmax(val_acc)\n",
    "    ax3.axvline(x=dropout_rates[optimal_idx], color='green', linestyle='--', \n",
    "               linewidth=2, label=f'Optimal: {dropout_rates[optimal_idx]}')\n",
    "    ax3.annotate(f'Optimal\\n{dropout_rates[optimal_idx]}', \n",
    "                xy=(dropout_rates[optimal_idx], val_acc[optimal_idx]),\n",
    "                xytext=(dropout_rates[optimal_idx]+0.1, val_acc[optimal_idx]+3),\n",
    "                arrowprops=dict(arrowstyle='->', color='green'),\n",
    "                fontweight='bold', color='green')\n",
    "    \n",
    "    # Plot 4: Training dynamics simulation\n",
    "    epochs = range(1, 51)\n",
    "    # Simulated loss curves\n",
    "    no_reg_train = [2.3 - 2.0 * (1 - np.exp(-e/10)) + 0.1 * np.random.random() for e in epochs]\n",
    "    no_reg_val = [2.3 - 1.5 * (1 - np.exp(-e/15)) + 0.2 * np.random.random() for e in epochs]\n",
    "    \n",
    "    with_reg_train = [2.3 - 1.8 * (1 - np.exp(-e/12)) + 0.1 * np.random.random() for e in epochs]\n",
    "    with_reg_val = [2.3 - 1.7 * (1 - np.exp(-e/12)) + 0.15 * np.random.random() for e in epochs]\n",
    "    \n",
    "    ax4.plot(epochs, no_reg_train, '--', label='No Regularization (Train)', alpha=0.8, color='blue', linewidth=2)\n",
    "    ax4.plot(epochs, no_reg_val, '--', label='No Regularization (Val)', alpha=0.8, color='red', linewidth=2)\n",
    "    ax4.plot(epochs, with_reg_train, '-', label='With Regularization (Train)', linewidth=3, color='blue')\n",
    "    ax4.plot(epochs, with_reg_val, '-', label='With Regularization (Val)', linewidth=3, color='red')\n",
    "    \n",
    "    ax4.set_title('Training Dynamics: Regularization Impact', fontweight='bold')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Loss')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight generalization gap\n",
    "    final_train_gap = no_reg_train[-1] - with_reg_train[-1]\n",
    "    final_val_gap = no_reg_val[-1] - with_reg_val[-1]\n",
    "    ax4.annotate(f'Reduced\\nGeneralization Gap', \n",
    "                xy=(45, (with_reg_train[-1] + with_reg_val[-1])/2),\n",
    "                xytext=(35, 1.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='green'),\n",
    "                fontweight='bold', color='green',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.7))\n",
    "    \n",
    "    plt.suptitle('Comprehensive Regularization Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'regularization_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_regularization_effects(reg_models, reg_analysis)\n",
    "\n",
    "print(f\"\\nüí° Regularization Key Insights:\")\n",
    "regularization_insights = [\n",
    "    \"‚Ä¢ Dropout (0.2-0.3): Simple and effective, optimal rate varies by architecture\",\n",
    "    \"‚Ä¢ Batch Normalization: Accelerates training and provides implicit regularization\",\n",
    "    \"‚Ä¢ DropBlock: Superior to standard dropout for convolutional architectures\",\n",
    "    \"‚Ä¢ Data Augmentation: Often the most effective regularization technique\",\n",
    "    \"‚Ä¢ Label Smoothing: Prevents overconfident predictions and improves calibration\",\n",
    "    \"‚Ä¢ Combined Techniques: Multiple regularization methods work synergistically\",\n",
    "    \"‚Ä¢ Architecture-Specific: Tailor regularization strategy to your model design\"\n",
    "]\n",
    "\n",
    "for insight in regularization_insights:\n",
    "    print(f\"  {insight}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Regularization analysis complete!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Advanced Optimization Algorithms\n",
    "\n",
    "### 3.1 Modern Optimizer Implementations\n",
    "\n",
    "```python\n",
    "print(\"=== 3.1 Advanced Optimization Algorithms ===\\n\")\n",
    "\n",
    "class AdamW(optim.Optimizer):\n",
    "    \"\"\"AdamW optimizer with decoupled weight decay for improved generalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                grad = p.grad.data\n",
    "                if grad.dtype in {torch.float16, torch.bfloat16}:\n",
    "                    grad = grad.float()\n",
    "                \n",
    "                state = self.state[p]\n",
    "                \n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data).float()\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data).float()\n",
    "                \n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "                \n",
    "                state['step'] += 1\n",
    "                \n",
    "                # Exponential moving average of gradient values\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                # Exponential moving average of squared gradient values\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "                \n",
    "                denom = (exp_avg_sq.sqrt() / math.sqrt(1 - beta2 ** state['step'])).add_(group['eps'])\n",
    "                step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                \n",
    "                # Decoupled weight decay (key difference from Adam)\n",
    "                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n",
    "                \n",
    "                # Update parameters\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "class LAMB(optim.Optimizer):\n",
    "    \"\"\"LAMB optimizer for large batch training and distributed settings.\"\"\"\n",
    "    \n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.01):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super(LAMB, self).__init__(params, defaults)\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "                \n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                \n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "                \n",
    "                state['step'] += 1\n",
    "                \n",
    "                # Add weight decay\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(p.data, alpha=group['weight_decay'])\n",
    "                \n",
    "                # Exponential moving average of gradient values\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                # Exponential moving average of squared gradient values\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "                \n",
    "                # Bias correction\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                \n",
    "                # Compute update\n",
    "                update = (exp_avg / bias_correction1) / ((exp_avg_sq / bias_correction2).sqrt() + group['eps'])\n",
    "                \n",
    "                # Layer-wise adaptation (LAMB's key innovation)\n",
    "                weight_norm = p.data.norm()\n",
    "                update_norm = update.norm()\n",
    "                \n",
    "                if weight_norm > 0 and update_norm > 0:\n",
    "                    trust_ratio = weight_norm / update_norm\n",
    "                    trust_ratio = min(trust_ratio, 10.0)  # Clip trust ratio\n",
    "                else:\n",
    "                    trust_ratio = 1.0\n",
    "                \n",
    "                # Update parameters\n",
    "                p.data.add_(update, alpha=-group['lr'] * trust_ratio)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "class SAM(optim.Optimizer):\n",
    "    \"\"\"Sharpness-Aware Minimization for finding flatter minima.\"\"\"\n",
    "    \n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho: {rho}\"\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "        \n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "            \n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # Climb to the local maximum \"w + e(w)\"\n",
    "        \n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p.data = self.state[p][\"old_p\"]  # Get back to \"w\" from \"w + e(w)\"\n",
    "        \n",
    "        self.base_optimizer.step()  # Do the actual \"sharpness-aware\" update\n",
    "        \n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "    \n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device\n",
    "        norm = torch.norm(\n",
    "            torch.stack([\n",
    "                ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(dtype=torch.float32)\n",
    "                for group in self.param_groups for p in group[\"params\"]\n",
    "                if p.grad is not None\n",
    "            ]),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        return norm\n",
    "\n",
    "print(\"‚úÖ Advanced optimizers implemented!\")\n",
    "print(\"   ‚Ä¢ AdamW: Decoupled weight decay for better generalization\")\n",
    "print(\"   ‚Ä¢ LAMB: Layer-wise adaptive moments for large batch training\")\n",
    "print(\"   ‚Ä¢ SAM: Sharpness-aware minimization for flatter minima\")\n",
    "```\n",
    "\n",
    "### 3.2 Optimizer Performance Comparison\n",
    "\n",
    "```python\n",
    "def compare_optimizers():\n",
    "    \"\"\"Comprehensive comparison of optimization algorithms.\"\"\"\n",
    "    \n",
    "    print(\"‚öôÔ∏è Optimizer Performance Comparison:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create standardized test model\n",
    "    class OptimizationTestNet(nn.Module):\n",
    "        def __init__(self, input_size=100, hidden_size=50, num_classes=10):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "            self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "            self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.fc1(x)))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.bn2(self.fc2(x)))\n",
    "            x = self.dropout(x)\n",
    "            return self.fc3(x)\n",
    "    \n",
    "    # Generate synthetic dataset\n",
    "    torch.manual_seed(42)\n",
    "    x_train = torch.randn(1000, 100)\n",
    "    y_train = torch.randint(0, 10, (1000,))\n",
    "    x_val = torch.randn(200, 100)\n",
    "    y_val = torch.randint(0, 10, (200,))\n",
    "    \n",
    "    # Define optimizers to test\n",
    "    optimizers_config = {\n",
    "        'SGD': lambda params: optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=1e-4),\n",
    "        'Adam': lambda params: optim.Adam(params, lr=0.001, weight_decay=1e-4),\n",
    "        'AdamW': lambda params: AdamW(params, lr=0.001, weight_decay=0.01),\n",
    "        'LAMB': lambda params: LAMB(params, lr=0.001, weight_decay=0.01),\n",
    "        'RMSprop': lambda params: optim.RMSprop(params, lr=0.001, weight_decay=1e-4)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    num_epochs = 50\n",
    "    \n",
    "    for opt_name, opt_fn in optimizers_config.items():\n",
    "        print(f\"\\n  üî¨ Testing {opt_name}...\")\n",
    "        \n",
    "        # Initialize model and optimizer\n",
    "        model = OptimizationTestNet()\n",
    "        optimizer = opt_fn(model.parameters())\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Training metrics\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        convergence_epochs = []\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            train_output = model(x_train)\n",
    "            train_loss = criterion(train_output, y_train)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            with torch.no_grad():\n",
    "                train_pred = train_output.argmax(dim=1)\n",
    "                train_acc = (train_pred == y_train).float().mean().item()\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_output = model(x_val)\n",
    "                val_loss = criterion(val_output, y_val)\n",
    "                val_pred = val_output.argmax(dim=1)\n",
    "                val_acc = (val_pred == y_val).float().mean().item()\n",
    "            \n",
    "            train_losses.append(train_loss.item())\n",
    "            val_losses.append(val_loss.item())\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_accuracies.append(val_acc)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early convergence detection\n",
    "            if epoch > 10 and patience_counter == 0:\n",
    "                convergence_epochs.append(epoch)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        final_train_loss = train_losses[-1]\n",
    "        final_val_loss = val_losses[-1]\n",
    "        best_val_acc = max(val_accuracies)\n",
    "        convergence_speed = len(convergence_epochs) / num_epochs if convergence_epochs else 0\n",
    "        \n",
    "        results[opt_name] = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'final_train_loss': final_train_loss,\n",
    "            'final_val_loss': final_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'convergence_rate': train_losses[0] / final_train_loss,\n",
    "            'generalization_gap': final_train_loss - final_val_loss,\n",
    "            'convergence_speed': convergence_speed\n",
    "        }\n",
    "        \n",
    "        print(f\"    Final train loss: {final_train_loss:.4f}\")\n",
    "        print(f\"    Final val loss: {final_val_loss:.4f}\")\n",
    "        print(f\"    Best val accuracy: {best_val_acc:.3f}\")\n",
    "        print(f\"    Convergence ratio: {train_losses[0] / final_train_loss:.2f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "optimizer_results = compare_optimizers()\n",
    "```\n",
    "\n",
    "### 3.3 Optimizer Analysis Visualization\n",
    "\n",
    "```python\n",
    "def plot_optimizer_analysis(results):\n",
    "    \"\"\"Create comprehensive optimizer comparison visualizations.\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Training loss curves\n",
    "    for opt_name, data in results.items():\n",
    "        epochs = range(len(data['train_losses']))\n",
    "        ax1.plot(epochs, data['train_losses'], label=f\"{opt_name} (Train)\", linewidth=2)\n",
    "        ax1.plot(epochs, data['val_losses'], label=f\"{opt_name} (Val)\", linewidth=2, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    ax1.set_title('Training and Validation Loss Convergence', fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Plot 2: Validation accuracy comparison\n",
    "    for opt_name, data in results.items():\n",
    "        epochs = range(len(data['val_accuracies']))\n",
    "        ax2.plot(epochs, data['val_accuracies'], label=opt_name, linewidth=3, marker='o', markersize=4)\n",
    "    \n",
    "    ax2.set_title('Validation Accuracy Progress', fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Final performance metrics\n",
    "    opt_names = list(results.keys())\n",
    "    final_val_losses = [results[name]['final_val_loss'] for name in opt_names]\n",
    "    best_val_accs = [results[name]['best_val_accuracy'] for name in opt_names]\n",
    "    \n",
    "    x = np.arange(len(opt_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax3.bar(x - width/2, final_val_losses, width, label='Final Val Loss', alpha=0.8, color='lightcoral')\n",
    "    ax3_twin = ax3.twinx()\n",
    "    bars2 = ax3_twin.bar(x + width/2, best_val_accs, width, label='Best Val Accuracy', alpha=0.8, color='lightblue')\n",
    "    \n",
    "    ax3.set_xlabel('Optimizer')\n",
    "    ax3.set_ylabel('Final Validation Loss', color='red')\n",
    "    ax3_twin.set_ylabel('Best Validation Accuracy', color='blue')\n",
    "    ax3.set_title('Final Performance Comparison', fontweight='bold')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(opt_names, rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars1, final_val_losses):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    for bar, val in zip(bars2, best_val_accs):\n",
    "        height = bar.get_height()\n",
    "        ax3_twin.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{val:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Plot 4: Convergence and generalization analysis\n",
    "    convergence_rates = [results[name]['convergence_rate'] for name in opt_names]\n",
    "    gen_gaps = [abs(results[name]['generalization_gap']) for name in opt_names]\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']\n",
    "    \n",
    "    scatter = ax4.scatter(convergence_rates, gen_gaps, s=200, alpha=0.7, c=colors[:len(opt_names)])\n",
    "    \n",
    "    for i, name in enumerate(opt_names):\n",
    "        ax4.annotate(name, (convergence_rates[i], gen_gaps[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax4.set_xlabel('Convergence Rate (Initial/Final Loss)')\n",
    "    ax4.set_ylabel('Generalization Gap')\n",
    "    ax4.set_title('Convergence vs Generalization Trade-off', fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add quadrant labels\n",
    "    ax4.axhline(y=np.median(gen_gaps), color='gray', linestyle='--', alpha=0.5)\n",
    "    ax4.axvline(x=np.median(convergence_rates), color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.suptitle('Comprehensive Optimizer Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'optimizer_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_optimizer_analysis(optimizer_results)\n",
    "\n",
    "print(f\"\\nüí° Optimizer Performance Insights:\")\n",
    "optimizer_insights = [\n",
    "    \"‚Ä¢ AdamW: Superior generalization compared to Adam due to decoupled weight decay\",\n",
    "    \"‚Ä¢ LAMB: Excellent for large batch training and distributed scenarios\",\n",
    "    \"‚Ä¢ SAM: Finds flatter minima leading to better generalization (requires 2x forward passes)\",\n",
    "    \"‚Ä¢ Adam variants: Generally faster convergence than SGD with less hyperparameter tuning\",\n",
    "    \"‚Ä¢ SGD + Momentum: Often achieves best final performance with proper learning rate scheduling\",\n",
    "    \"‚Ä¢ RMSprop: Good middle ground between Adam and SGD for many applications\",\n",
    "    \"‚Ä¢ Context Matters: Optimal choice depends on model architecture, dataset, and constraints\"\n",
    "]\n",
    "\n",
    "for insight in optimizer_insights:\n",
    "    print(f\"  {insight}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Advanced optimizers analysis complete!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Learning Rate Scheduling and Warm-up Strategies\n",
    "\n",
    "### 4.1 Advanced Learning Rate Schedulers\n",
    "\n",
    "```python\n",
    "print(\"=== 4.1 Learning Rate Scheduling Techniques ===\\n\")\n",
    "\n",
    "class WarmupScheduler:\n",
    "    \"\"\"Learning rate warmup scheduler for stable training initialization.\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_epochs: int, base_lr: float, target_lr: float):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.base_lr = base_lr\n",
    "        self.target_lr = target_lr\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def step(self):\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            lr = self.base_lr + (self.target_lr - self.base_lr) * (self.current_epoch / self.warmup_epochs)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        \n",
    "        self.current_epoch += 1\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "class CosineAnnealingWarmRestarts:\n",
    "    \"\"\"Cosine annealing with warm restarts (SGDR) for escaping local minima.\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, T_0: int, T_mult: int = 1, eta_min: float = 0):\n",
    "        self.optimizer = optimizer\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_min = eta_min\n",
    "        self.T_cur = 0\n",
    "        self.T_i = T_0\n",
    "        self.eta_max = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    def step(self):\n",
    "        self.T_cur += 1\n",
    "        \n",
    "        if self.T_cur >= self.T_i:\n",
    "            # Restart\n",
    "            self.T_cur = 0\n",
    "            self.T_i *= self.T_mult\n",
    "        \n",
    "        # Cosine annealing\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * self.T_cur / self.T_i)) / 2\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        return lr\n",
    "\n",
    "class OneCycleLR:\n",
    "    \"\"\"One Cycle learning rate policy for super-convergence.\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, max_lr: float, total_steps: int, \n",
    "                 pct_start: float = 0.3, div_factor: float = 25.0, final_div_factor: float = 1e4):\n",
    "        self.optimizer = optimizer\n",
    "        self.max_lr = max_lr\n",
    "        self.total_steps = total_steps\n",
    "        self.pct_start = pct_start\n",
    "        self.div_factor = div_factor\n",
    "        self.final_div_factor = final_div_factor\n",
    "        \n",
    "        self.initial_lr = max_lr / div_factor\n",
    "        self.final_lr = self.initial_lr / final_div_factor\n",
    "        self.step_up = int(total_steps * pct_start)\n",
    "        self.step_down = total_steps - self.step_up\n",
    "        \n",
    "        self.current_step = 0\n",
    "    \n",
    "    def step(self):\n",
    "        if self.current_step <= self.step_up:\n",
    "            # Warmup phase\n",
    "            lr = self.initial_lr + (self.max_lr - self.initial_lr) * (self.current_step / self.step_up)\n",
    "        else:\n",
    "            # Cooldown phase\n",
    "            progress = (self.current_step - self.step_up) / self.step_down\n",
    "            lr = self.max_lr - (self.max_lr - self.final_lr) * progress\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        self.current_step += 1\n",
    "        return lr\n",
    "\n",
    "class PolynomialLR:\n",
    "    \"\"\"Polynomial learning rate decay scheduler.\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, total_steps: int, power: float = 0.9, min_lr: float = 0):\n",
    "        self.optimizer = optimizer\n",
    "        self.total_steps = total_steps\n",
    "        self.power = power\n",
    "        self.min_lr = min_lr\n",
    "        self.initial_lr = optimizer.param_groups[0]['lr']\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def step(self):\n",
    "        if self.current_step < self.total_steps:\n",
    "            decay_factor = (1 - self.current_step / self.total_steps) ** self.power\n",
    "            lr = (self.initial_lr - self.min_lr) * decay_factor + self.min_lr\n",
    "        else:\n",
    "            lr = self.min_lr\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        self.current_step += 1\n",
    "        return lr\n",
    "\n",
    "def test_lr_schedules():\n",
    "    \"\"\"Test and analyze different learning rate schedules.\"\"\"\n",
    "    \n",
    "    print(\"üìà Testing Learning Rate Schedules:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create dummy model and optimizer for testing\n",
    "    model = nn.Linear(10, 1)\n",
    "    \n",
    "    # Test different schedulers\n",
    "    schedules = {}\n",
    "    \n",
    "    # Standard PyTorch schedulers\n",
    "    optimizer1 = optim.SGD(model.parameters(), lr=0.1)\n",
    "    step_lr = StepLR(optimizer1, step_size=30, gamma=0.1)\n",
    "    schedules['StepLR'] = (optimizer1, step_lr)\n",
    "    \n",
    "    optimizer2 = optim.SGD(model.parameters(), lr=0.1)\n",
    "    exp_lr = ExponentialLR(optimizer2, gamma=0.95)\n",
    "    schedules['ExponentialLR'] = (optimizer2, exp_lr)\n",
    "    \n",
    "    optimizer3 = optim.SGD(model.parameters(), lr=0.1)\n",
    "    cosine_lr = CosineAnnealingLR(optimizer3, T_max=100)\n",
    "    schedules['CosineAnnealing'] = (optimizer3, cosine_lr)\n",
    "    \n",
    "    # Custom advanced schedulers\n",
    "    optimizer4 = optim.SGD(model.parameters(), lr=0.001)\n",
    "    warmup_lr = WarmupScheduler(optimizer4, warmup_epochs=20, base_lr=0.001, target_lr=0.1)\n",
    "    schedules['Warmup'] = (optimizer4, warmup_lr)\n",
    "    \n",
    "    optimizer5 = optim.SGD(model.parameters(), lr=0.1)\n",
    "    sgdr_lr = CosineAnnealingWarmRestarts(optimizer5, T_0=25, T_mult=2)\n",
    "    schedules['SGDR'] = (optimizer5, sgdr_lr)\n",
    "    \n",
    "    optimizer6 = optim.SGD(model.parameters(), lr=0.001)\n",
    "    onecycle_lr = OneCycleLR(optimizer6, max_lr=0.1, total_steps=100)\n",
    "    schedules['OneCycle'] = (optimizer6, onecycle_lr)\n",
    "    \n",
    "    optimizer7 = optim.SGD(model.parameters(), lr=0.1)\n",
    "    poly_lr = PolynomialLR(optimizer7, total_steps=100, power=0.9)\n",
    "    schedules['PolynomialLR'] = (optimizer7, poly_lr)\n",
    "    \n",
    "    # Simulate training and collect learning rates\n",
    "    lr_histories = {}\n",
    "    epochs = 100\n",
    "    \n",
    "    for name, (optimizer, scheduler) in schedules.items():\n",
    "        lr_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            lr_history.append(current_lr)\n",
    "            \n",
    "            # Step scheduler\n",
    "            if hasattr(scheduler, 'step'):\n",
    "                scheduler.step()\n",
    "        \n",
    "        lr_histories[name] = lr_history\n",
    "        print(f\"  {name}: LR range [{min(lr_history):.6f}, {max(lr_history):.6f}]\")\n",
    "        print(f\"    Final LR: {lr_history[-1]:.6f}\")\n",
    "        print(f\"    LR variance: {np.var(lr_history):.6f}\")\n",
    "    \n",
    "    return lr_histories\n",
    "\n",
    "lr_histories = test_lr_schedules()\n",
    "```\n",
    "\n",
    "### 4.2 Learning Rate Schedule Visualization and Analysis\n",
    "\n",
    "```python\n",
    "def plot_lr_schedules_comprehensive(lr_histories):\n",
    "    \"\"\"Create comprehensive learning rate schedule analysis.\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: All schedules comparison\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(lr_histories)))\n",
    "    \n",
    "    for i, (name, lr_history) in enumerate(lr_histories.items()):\n",
    "        epochs = range(len(lr_history))\n",
    "        ax1.plot(epochs, lr_history, label=name, linewidth=2.5, color=colors[i])\n",
    "    \n",
    "    ax1.set_title('Learning Rate Schedule Comparison', fontweight='bold', fontsize=14)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Learning Rate')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Plot 2: Focus on specific advanced schedules\n",
    "    advanced_schedules = ['SGDR', 'OneCycle', 'Warmup']\n",
    "    colors_advanced = ['red', 'green', 'blue']\n",
    "    \n",
    "    for schedule_name, color in zip(advanced_schedules, colors_advanced):\n",
    "        if schedule_name in lr_histories:\n",
    "            lr_history = lr_histories[schedule_name]\n",
    "            epochs = range(len(lr_history))\n",
    "            ax2.plot(epochs, lr_history, color=color, linewidth=3, label=schedule_name)\n",
    "            \n",
    "            # Add annotations for key features\n",
    "            if schedule_name == 'SGDR':\n",
    "                # Find restart points\n",
    "                restarts = []\n",
    "                for i in range(1, len(lr_history)):\n",
    "                    if lr_history[i] > lr_history[i-1] * 1.5:  # Significant jump\n",
    "                        restarts.append(i)\n",
    "                \n",
    "                for restart in restarts[:2]:  # Annotate first 2 restarts\n",
    "                    ax2.annotate('Restart', xy=(restart, lr_history[restart]), \n",
    "                               xytext=(restart+5, lr_history[restart]*1.5),\n",
    "                               arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                               fontsize=10, color='red', fontweight='bold')\n",
    "            \n",
    "            elif schedule_name == 'OneCycle':\n",
    "                max_idx = lr_history.index(max(lr_history))\n",
    "                ax2.annotate(f'Peak LR\\\\n({max(lr_history):.3f})', \n",
    "                           xy=(max_idx, max(lr_history)), \n",
    "                           xytext=(max_idx+10, max(lr_history)*0.8),\n",
    "                           arrowprops=dict(arrowstyle='->', color='green'),\n",
    "                           fontsize=10, color='green', fontweight='bold')\n",
    "            \n",
    "            elif schedule_name == 'Warmup':\n",
    "                # Find end of warmup\n",
    "                warmup_end = 20  # Known from implementation\n",
    "                ax2.annotate('Warmup End', xy=(warmup_end, lr_history[warmup_end]), \n",
    "                           xytext=(warmup_end+10, lr_history[warmup_end]*2),\n",
    "                           arrowprops=dict(arrowstyle='->', color='blue'),\n",
    "                           fontsize=10, color='blue', fontweight='bold')\n",
    "    \n",
    "    ax2.set_title('Advanced Learning Rate Strategies', fontweight='bold', fontsize=14)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Learning Rate')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Schedule characteristics analysis\n",
    "    schedule_names = list(lr_histories.keys())\n",
    "    \n",
    "    # Calculate schedule characteristics\n",
    "    characteristics = {\n",
    "        'max_lr': [max(lr_histories[name]) for name in schedule_names],\n",
    "        'min_lr': [min(lr_histories[name]) for name in schedule_names],\n",
    "        'lr_range': [max(lr_histories[name]) - min(lr_histories[name]) for name in schedule_names],\n",
    "        'variance': [np.var(lr_histories[name]) for name in schedule_names]\n",
    "    }\n",
    "    \n",
    "    x = np.arange(len(schedule_names))\n",
    "    width = 0.2\n",
    "    \n",
    "    bars1 = ax3.bar(x - 1.5*width, characteristics['max_lr'], width, label='Max LR', alpha=0.8, color='lightcoral')\n",
    "    bars2 = ax3.bar(x - 0.5*width, characteristics['min_lr'], width, label='Min LR', alpha=0.8, color='lightblue')\n",
    "    bars3 = ax3.bar(x + 0.5*width, characteristics['lr_range'], width, label='LR Range', alpha=0.8, color='lightgreen')\n",
    "    bars4 = ax3.bar(x + 1.5*width, [v*1000 for v in characteristics['variance']], width, label='Variance√ó1000', alpha=0.8, color='lightyellow')\n",
    "    \n",
    "    ax3.set_xlabel('Learning Rate Schedule')\n",
    "    ax3.set_ylabel('Value')\n",
    "    ax3.set_title('Schedule Characteristics Comparison', fontweight='bold', fontsize=14)\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels([name.replace('LR', '') for name in schedule_names], rotation=45, ha='right')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 4: Training phase analysis for OneCycle\n",
    "    if 'OneCycle' in lr_histories:\n",
    "        onecycle_history = lr_histories['OneCycle']\n",
    "        epochs = range(len(onecycle_history))\n",
    "        \n",
    "        ax4.plot(epochs, onecycle_history, 'g-', linewidth=4, label='OneCycle LR')\n",
    "        \n",
    "        # Identify phases\n",
    "        max_idx = onecycle_history.index(max(onecycle_history))\n",
    "        warmup_phase = epochs[:max_idx]\n",
    "        cooldown_phase = epochs[max_idx:]\n",
    "        \n",
    "        # Color-code phases\n",
    "        ax4.fill_between(warmup_phase, [onecycle_history[i] for i in warmup_phase], \n",
    "                        alpha=0.3, color='orange', label='Warmup Phase')\n",
    "        ax4.fill_between(cooldown_phase, [onecycle_history[i] for i in cooldown_phase], \n",
    "                        alpha=0.3, color='blue', label='Cooldown Phase')\n",
    "        \n",
    "        # Add phase annotations\n",
    "        ax4.annotate('Fast Learning\\\\n(High LR)', xy=(max_idx//2, onecycle_history[max_idx//2]), \n",
    "                   xytext=(max_idx//2, max(onecycle_history)*0.7),\n",
    "                   arrowprops=dict(arrowstyle='->', color='orange'),\n",
    "                   fontsize=10, fontweight='bold', ha='center')\n",
    "        \n",
    "        ax4.annotate('Fine-tuning\\\\n(Low LR)', xy=(max_idx + (len(epochs)-max_idx)//2, onecycle_history[max_idx + (len(epochs)-max_idx)//2]), \n",
    "                   xytext=(max_idx + (len(epochs)-max_idx)//2, max(onecycle_history)*0.4),\n",
    "                   arrowprops=dict(arrowstyle='->', color='blue'),\n",
    "                   fontsize=10, fontweight='bold', ha='center')\n",
    "        \n",
    "        ax4.set_title('OneCycle LR: Phase Analysis', fontweight='bold', fontsize=14)\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Learning Rate')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # Alternative visualization for learning rate evolution\n",
    "        sample_schedules = ['StepLR', 'ExponentialLR', 'CosineAnnealing']\n",
    "        for i, schedule in enumerate(sample_schedules):\n",
    "            if schedule in lr_histories:\n",
    "                epochs = range(len(lr_histories[schedule]))\n",
    "                ax4.plot(epochs, lr_histories[schedule], linewidth=3, label=schedule, alpha=0.8)\n",
    "        \n",
    "        ax4.set_title('Traditional Schedule Comparison', fontweight='bold', fontsize=14)\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Learning Rate')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Comprehensive Learning Rate Schedule Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'lr_schedules_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_lr_schedules_comprehensive(lr_histories)\n",
    "\n",
    "print(f\"\\nüí° Learning Rate Schedule Insights:\")\n",
    "lr_insights = [\n",
    "    \"‚Ä¢ Warmup: Prevents early divergence, especially critical for large models and batch sizes\",\n",
    "    \"‚Ä¢ SGDR: Helps escape local minima with periodic restarts, enables ensemble-like behavior\",\n",
    "    \"‚Ä¢ OneCycle: Achieves super-convergence with single peak LR, faster training\",\n",
    "    \"‚Ä¢ Cosine Annealing: Smooth decay often outperforms step decay for final performance\",\n",
    "    \"‚Ä¢ Exponential: Simple and effective for many tasks, good baseline choice\",\n",
    "    \"‚Ä¢ Polynomial: Provides controlled decay rate, good for fine-tuning scenarios\",\n",
    "    \"‚Ä¢ Schedule Selection: Should match training duration and convergence patterns\"\n",
    "]\n",
    "\n",
    "for insight in lr_insights:\n",
    "    print(f\"  {insight}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Learning rate scheduling analysis complete!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Advanced Data Augmentation Techniques\n",
    "\n",
    "### 5.1 Cutting-edge Augmentation Methods\n",
    "\n",
    "```python\n",
    "print(\"=== 5.1 Advanced Data Augmentation Strategies ===\\n\")\n",
    "\n",
    "class RandAugment:\n",
    "    \"\"\"RandAugment: automated augmentation policy learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, n: int = 2, m: int = 10):\n",
    "        self.n = n  # Number of augmentation transformations to apply\n",
    "        self.m = m  # Magnitude for all the transformations\n",
    "        \n",
    "        # Define available transformations\n",
    "        self.transforms = [\n",
    "            self.auto_contrast,\n",
    "            self.equalize,\n",
    "            self.rotate,\n",
    "            self.posterize,\n",
    "            self.solarize,\n",
    "            self.color,\n",
    "            self.contrast,\n",
    "            self.brightness,\n",
    "            self.sharpness,\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        \"\"\"Apply n random transformations with magnitude m.\"\"\"\n",
    "        ops = np.random.choice(self.transforms, self.n, replace=False)\n",
    "        for op in ops:\n",
    "            if np.random.random() < 0.5:  # 50% chance to apply each operation\n",
    "                img = op(img)\n",
    "        return img\n",
    "    \n",
    "    def auto_contrast(self, img):\n",
    "        if hasattr(transforms.functional, 'autocontrast'):\n",
    "            return transforms.functional.autocontrast(img)\n",
    "        return img\n",
    "    \n",
    "    def equalize(self, img):\n",
    "        if hasattr(transforms.functional, 'equalize'):\n",
    "            return transforms.functional.equalize(img)\n",
    "        return img\n",
    "    \n",
    "    def rotate(self, img):\n",
    "        degrees = self.m * 30 / 10  # Scale magnitude to degrees\n",
    "        angle = np.random.uniform(-degrees, degrees)\n",
    "        return transforms.functional.rotate(img, angle)\n",
    "    \n",
    "    def posterize(self, img):\n",
    "        bits = int(8 - (self.m / 10) * 4)  # 4-8 bits\n",
    "        bits = max(1, min(8, bits))\n",
    "        if hasattr(transforms.functional, 'posterize'):\n",
    "            return transforms.functional.posterize(img, bits)\n",
    "        return img\n",
    "    \n",
    "    def solarize(self, img):\n",
    "        threshold = int(256 - (self.m / 10) * 128)  # 128-256\n",
    "        if hasattr(transforms.functional, 'solarize'):\n",
    "            return transforms.functional.solarize(img, threshold)\n",
    "        return img\n",
    "    \n",
    "    def color(self, img):\n",
    "        factor = 1 + (self.m / 10) * 0.8  # 1.0-1.8\n",
    "        return transforms.functional.adjust_saturation(img, factor)\n",
    "    \n",
    "    def contrast(self, img):\n",
    "        factor = 1 + (self.m / 10) * 0.8  # 1.0-1.8\n",
    "        return transforms.functional.adjust_contrast(img, factor)\n",
    "    \n",
    "    def brightness(self, img):\n",
    "        factor = 1 + (self.m / 10) * 0.8  # 1.0-1.8\n",
    "        return transforms.functional.adjust_brightness(img, factor)\n",
    "    \n",
    "    def sharpness(self, img):\n",
    "        factor = 1 + (self.m / 10) * 0.8  # 1.0-1.8\n",
    "        return transforms.functional.adjust_sharpness(img, factor)\n",
    "\n",
    "class TrivialAugmentWide:\n",
    "    \"\"\"TrivialAugment-Wide: simplified yet effective augmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.transforms = [\n",
    "            'Identity', 'Rotate', 'Brightness', 'Contrast', 'Sharpness',\n",
    "            'ShearX', 'ShearY', 'TranslateX', 'TranslateY'\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Randomly select one transformation\n",
    "        transform = np.random.choice(self.transforms)\n",
    "        \n",
    "        # Randomly select magnitude\n",
    "        magnitude = np.random.randint(0, 31)  # 0-30\n",
    "        \n",
    "        # Apply transformation\n",
    "        return self.apply_transform(img, transform, magnitude)\n",
    "    \n",
    "    def apply_transform(self, img, transform, magnitude):\n",
    "        \"\"\"Apply specific transformation with given magnitude.\"\"\"\n",
    "        if transform == 'Identity':\n",
    "            return img\n",
    "        elif transform == 'Rotate':\n",
    "            angle = magnitude * 30 / 30  # 0-30 degrees\n",
    "            return transforms.functional.rotate(img, angle)\n",
    "        elif transform == 'Brightness':\n",
    "            factor = 1 + magnitude * 0.9 / 30  # 1.0-1.9\n",
    "            return transforms.functional.adjust_brightness(img, factor)\n",
    "        elif transform == 'Contrast':\n",
    "            factor = 1 + magnitude * 0.9 / 30  # 1.0-1.9\n",
    "            return transforms.functional.adjust_contrast(img, factor)\n",
    "        elif transform == 'Sharpness':\n",
    "            factor = 1 + magnitude * 0.9 / 30  # 1.0-1.9\n",
    "            return transforms.functional.adjust_sharpness(img, factor)\n",
    "        elif transform in ['ShearX', 'ShearY', 'TranslateX', 'TranslateY']:\n",
    "            # Simplified geometric transformations\n",
    "            return img  # Would implement full geometric transforms in production\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "class GeometricAugmentations:\n",
    "    \"\"\"Advanced geometric augmentation techniques.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_erasing(img, p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)):\n",
    "        \"\"\"Random erasing augmentation for occlusion robustness.\"\"\"\n",
    "        if np.random.random() > p:\n",
    "            return img\n",
    "        \n",
    "        if isinstance(img, torch.Tensor):\n",
    "            c, h, w = img.shape\n",
    "            img = img.clone()\n",
    "        else:\n",
    "            # PIL Image\n",
    "            w, h = img.size\n",
    "            img = transforms.functional.to_tensor(img)\n",
    "            c, h, w = img.shape\n",
    "        \n",
    "        area = h * w\n",
    "        target_area = np.random.uniform(scale[0], scale[1]) * area\n",
    "        aspect_ratio = np.random.uniform(ratio[0], ratio[1])\n",
    "        \n",
    "        h_erased = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "        w_erased = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "        \n",
    "        if h_erased < h and w_erased < w:\n",
    "            x1 = np.random.randint(0, h - h_erased)\n",
    "            y1 = np.random.randint(0, w - w_erased)\n",
    "            img[:, x1:x1+h_erased, y1:y1+w_erased] = torch.randn(c, h_erased, w_erased)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    @staticmethod\n",
    "    def grid_shuffle(img, grid_size=2):\n",
    "        \"\"\"Grid shuffle augmentation for spatial reasoning.\"\"\"\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            c, h, w = img.shape\n",
    "        else:\n",
    "            img = transforms.functional.to_tensor(img)\n",
    "            c, h, w = img.shape\n",
    "        \n",
    "        # Create grid\n",
    "        grid_h, grid_w = h // grid_size, w // grid_size\n",
    "        \n",
    "        # Extract patches\n",
    "        patches = []\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                patch = img[:, i*grid_h:(i+1)*grid_h, j*grid_w:(j+1)*grid_w]\n",
    "                patches.append(patch)\n",
    "        \n",
    "        # Shuffle patches\n",
    "        np.random.shuffle(patches)\n",
    "        \n",
    "        # Reconstruct image\n",
    "        result = torch.zeros_like(img)\n",
    "        idx = 0\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                result[:, i*grid_h:(i+1)*grid_h, j*grid_w:(j+1)*grid_w] = patches[idx]\n",
    "                idx += 1\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"‚úÖ Advanced augmentation techniques implemented!\")\n",
    "print(\"   ‚Ä¢ RandAugment: Automated policy with configurable magnitude\")\n",
    "print(\"   ‚Ä¢ TrivialAugmentWide: Simplified single-transform approach\")\n",
    "print(\"   ‚Ä¢ GeometricAugmentations: Random erasing and grid shuffle\")\n",
    "```\n",
    "\n",
    "### 5.2 Augmentation Testing and Analysis\n",
    "\n",
    "```python\n",
    "def test_augmentation_techniques():\n",
    "    \"\"\"Comprehensive testing of augmentation methods.\"\"\"\n",
    "    \n",
    "    print(\"üé® Testing Advanced Augmentation Techniques:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create synthetic test image\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create a more complex test image with patterns\n",
    "    img_size = 64\n",
    "    img_array = np.zeros((img_size, img_size, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Add geometric patterns for better visualization\n",
    "    for i in range(img_size):\n",
    "        for j in range(img_size):\n",
    "            img_array[i, j, 0] = int(255 * (i / img_size))  # Red gradient\n",
    "            img_array[i, j, 1] = int(255 * (j / img_size))  # Green gradient\n",
    "            img_array[i, j, 2] = int(255 * ((i + j) / (2 * img_size)))  # Blue diagonal\n",
    "    \n",
    "    # Add some geometric shapes\n",
    "    center = img_size // 2\n",
    "    for i in range(img_size):\n",
    "        for j in range(img_size):\n",
    "            if (i - center)**2 + (j - center)**2 < (img_size // 4)**2:\n",
    "                img_array[i, j] = [255, 255, 255]  # White circle\n",
    "    \n",
    "    img = Image.fromarray(img_array)\n",
    "    \n",
    "    # Test different augmentations\n",
    "    augmentations = {\n",
    "        'Original': transforms.Compose([]),\n",
    "        'Standard': transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "        ]),\n",
    "        'RandAugment': RandAugment(n=2, m=10),\n",
    "        'TrivialAugment': TrivialAugmentWide(),\n",
    "    }\n",
    "    \n",
    "    # Test augmentations and collect statistics\n",
    "    augmentation_stats = {}\n",
    "    \n",
    "    print(f\"\\nüìä Augmentation Method Analysis:\")\n",
    "    print(f\"{'Method':<20} {'Status':<15} {'Avg Pixel Change':<20} {'Intensity Variance':<20}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, aug in augmentations.items():\n",
    "        try:\n",
    "            # Apply augmentation multiple times for statistics\n",
    "            pixel_changes = []\n",
    "            intensity_vars = []\n",
    "            original_tensor = transforms.functional.to_tensor(img)\n",
    "            \n",
    "            for _ in range(10):  # Test 10 times for statistics\n",
    "                augmented_img = aug(img)\n",
    "                if not isinstance(augmented_img, torch.Tensor):\n",
    "                    augmented_tensor = transforms.functional.to_tensor(augmented_img)\n",
    "                else:\n",
    "                    augmented_tensor = augmented_img\n",
    "                \n",
    "                # Calculate pixel change\n",
    "                pixel_change = torch.mean(torch.abs(original_tensor - augmented_tensor)).item()\n",
    "                pixel_changes.append(pixel_change)\n",
    "                \n",
    "                # Calculate intensity variance\n",
    "                intensity_var = torch.var(augmented_tensor).item()\n",
    "                intensity_vars.append(intensity_var)\n",
    "            \n",
    "            avg_pixel_change = np.mean(pixel_changes)\n",
    "            avg_intensity_var = np.mean(intensity_vars)\n",
    "            \n",
    "            augmentation_stats[name] = {\n",
    "                'avg_pixel_change': avg_pixel_change,\n",
    "                'avg_intensity_variance': avg_intensity_var,\n",
    "                'pixel_change_std': np.std(pixel_changes),\n",
    "                'intensity_var_std': np.std(intensity_vars),\n",
    "                'status': 'Success'\n",
    "            }\n",
    "            \n",
    "            print(f\"{name:<20} {'‚úÖ Success':<15} {avg_pixel_change:<20.4f} {avg_intensity_var:<20.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            augmentation_stats[name] = {\n",
    "                'status': 'Error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "            print(f\"{name:<20} {'‚ùå Error':<15} {'-':<20} {'-':<20}\")\n",
    "    \n",
    "    # Test geometric augmentations\n",
    "    tensor_img = transforms.functional.to_tensor(img)\n",
    "    \n",
    "    print(f\"\\nüîç Geometric Augmentation Tests:\")\n",
    "    \n",
    "    # Random Erasing\n",
    "    erased_img = GeometricAugmentations.random_erasing(tensor_img, p=1.0)  # Force application\n",
    "    erased_ratio = (erased_img != tensor_img).float().mean().item()\n",
    "    print(f\"  Random Erasing:\")\n",
    "    print(f\"    Pixels modified: {erased_ratio:.1%}\")\n",
    "    print(f\"    Original mean: {tensor_img.mean().item():.4f}\")\n",
    "    print(f\"    Erased mean: {erased_img.mean().item():.4f}\")\n",
    "    \n",
    "    # Grid Shuffle\n",
    "    shuffled_img = GeometricAugmentations.grid_shuffle(tensor_img, grid_size=4)\n",
    "    shuffle_diff = torch.mean(torch.abs(tensor_img - shuffled_img)).item()\n",
    "    print(f\"  Grid Shuffle:\")\n",
    "    print(f\"    Mean pixel difference: {shuffle_diff:.4f}\")\n",
    "    print(f\"    Spatial correlation change: {torch.corrcoef(tensor_img.flatten().unsqueeze(0), shuffled_img.flatten().unsqueeze(0))[0,1].item():.4f}\")\n",
    "    \n",
    "    return augmentation_stats, {\n",
    "        'original': tensor_img,\n",
    "        'erased': erased_img,\n",
    "        'shuffled': shuffled_img\n",
    "    }\n",
    "\n",
    "augmentation_stats, sample_images = test_augmentation_techniques()\n",
    "```\n",
    "\n",
    "### 5.3 Augmentation Strategy Analysis\n",
    "\n",
    "```python\n",
    "def analyze_augmentation_strategies():\n",
    "    \"\"\"Analyze the effectiveness and trade-offs of different augmentation strategies.\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìà Augmentation Strategy Effectiveness Analysis:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    strategies = {\n",
    "        'None': 'Baseline - no augmentation',\n",
    "        'Basic': 'Flip + Rotation + ColorJitter',\n",
    "        'RandAugment': 'Random selection from policy space',\n",
    "        'TrivialAugment': 'Single random transformation per image',\n",
    "        'AutoAugment': 'Learned augmentation policies',\n",
    "        'MixUp': 'Linear combination of image pairs',\n",
    "        'CutMix': 'Regional replacement between images',\n",
    "        'Combined': 'Multiple strategies together'\n",
    "    }\n",
    "    \n",
    "    # Simulated effectiveness data based on research literature\n",
    "    effectiveness_data = {\n",
    "        'Strategy': list(strategies.keys()),\n",
    "        'Accuracy_Gain': [0, 2.1, 3.5, 2.8, 4.2, 4.2, 3.9, 5.1],\n",
    "        'Robustness_Score': [5.0, 6.5, 8.2, 7.5, 8.8, 9.1, 8.7, 9.5],\n",
    "        'Training_Time_Multiplier': [1.0, 1.1, 1.3, 1.2, 1.4, 1.4, 1.3, 1.6],\n",
    "        'Implementation_Complexity': [1, 3, 7, 5, 9, 6, 7, 8],\n",
    "        'Memory_Overhead': [1.0, 1.1, 1.2, 1.1, 1.3, 1.5, 1.4, 1.7]\n",
    "    }\n",
    "    \n",
    "    print(f\"{'Strategy':<15} {'Acc Gain':<10} {'Robustness':<12} {'Time Cost':<12} {'Complexity':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i in range(len(effectiveness_data['Strategy'])):\n",
    "        strategy = effectiveness_data['Strategy'][i]\n",
    "        acc_gain = effectiveness_data['Accuracy_Gain'][i]\n",
    "        robustness = effectiveness_data['Robustness_Score'][i]\n",
    "        time_cost = effectiveness_data['Training_Time_Multiplier'][i]\n",
    "        complexity = effectiveness_data['Implementation_Complexity'][i]\n",
    "        \n",
    "        print(f\"{strategy:<15} {acc_gain:<10.1f}% {robustness:<12.1f} {time_cost:<12.1f}x {complexity:<12}/10\")\n",
    "    \n",
    "    return effectiveness_data, strategies\n",
    "\n",
    "def visualize_augmentation_analysis(effectiveness_data, strategies, augmentation_stats):\n",
    "    \"\"\"Create comprehensive augmentation strategy visualizations.\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Effectiveness vs Implementation Cost\n",
    "    strategies_list = effectiveness_data['Strategy']\n",
    "    accuracy_gains = effectiveness_data['Accuracy_Gain']\n",
    "    complexity_scores = effectiveness_data['Implementation_Complexity']\n",
    "    robustness_scores = effectiveness_data['Robustness_Score']\n",
    "    \n",
    "    scatter = ax1.scatter(complexity_scores, accuracy_gains, \n",
    "                         s=[score*20 for score in robustness_scores], \n",
    "                         alpha=0.7, c=range(len(strategies_list)), cmap='viridis')\n",
    "    \n",
    "    for i, strategy in enumerate(strategies_list):\n",
    "        ax1.annotate(strategy, (complexity_scores[i], accuracy_gains[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax1.set_xlabel('Implementation Complexity (1-10)')\n",
    "    ax1.set_ylabel('Accuracy Gain (%)')\n",
    "    ax1.set_title('Augmentation Strategy Trade-offs', fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add quadrant lines\n",
    "    ax1.axhline(y=np.median(accuracy_gains), color='red', linestyle='--', alpha=0.5)\n",
    "    ax1.axvline(x=np.median(complexity_scores), color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Plot 2: Multi-metric comparison\n",
    "    training_times = effectiveness_data['Training_Time_Multiplier']\n",
    "    memory_overhead = effectiveness_data['Memory_Overhead']\n",
    "    \n",
    "    x = range(len(strategies_list))\n",
    "    width = 0.25\n",
    "    \n",
    "    bars1 = ax2.bar([i - width for i in x], accuracy_gains, width, label='Accuracy Gain (%)', alpha=0.8, color='lightgreen')\n",
    "    bars2 = ax2.bar(x, [r/2 for r in robustness_scores], width, label='Robustness/2', alpha=0.8, color='lightblue')\n",
    "    bars3 = ax2.bar([i + width for i in x], [t*3 for t in training_times], width, label='Time Cost√ó3', alpha=0.8, color='lightcoral')\n",
    "    \n",
    "    ax2.set_xlabel('Augmentation Strategy')\n",
    "    ax2.set_ylabel('Normalized Score')\n",
    "    ax2.set_title('Multi-Metric Strategy Comparison', fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([s[:8] for s in strategies_list], rotation=45, ha='right')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 3: Pixel-level augmentation effects (from our tests)\n",
    "    if augmentation_stats:\n",
    "        tested_methods = [name for name, stats in augmentation_stats.items() if stats.get('status') == 'Success']\n",
    "        pixel_changes = [augmentation_stats[name]['avg_pixel_change'] for name in tested_methods]\n",
    "        intensity_vars = [augmentation_stats[name]['avg_intensity_variance'] for name in tested_methods]\n",
    "        \n",
    "        colors_methods = ['gray', 'blue', 'green', 'orange'][:len(tested_methods)]\n",
    "        \n",
    "        bars = ax3.bar(tested_methods, pixel_changes, alpha=0.8, color=colors_methods)\n",
    "        ax3.set_title('Pixel-Level Augmentation Effects', fontweight='bold')\n",
    "        ax3.set_ylabel('Average Pixel Change')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        ax3.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, change in zip(bars, pixel_changes):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + max(pixel_changes)*0.01,\n",
    "                     f'{change:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Plot 4: Augmentation recommendation matrix\n",
    "    use_cases = ['Small Dataset', 'Large Dataset', 'Mobile Deploy', 'Research']\n",
    "    recommended_strategies = ['Combined', 'RandAugment', 'TrivialAugment', 'AutoAugment']\n",
    "    \n",
    "    # Create recommendation scores (higher = better fit)\n",
    "    recommendation_matrix = [\n",
    "        [9, 7, 4, 8],  # Small Dataset\n",
    "        [6, 9, 7, 8],  # Large Dataset  \n",
    "        [4, 6, 9, 5],  # Mobile Deploy\n",
    "        [8, 8, 6, 9]   # Research\n",
    "    ]\n",
    "    \n",
    "    im = ax4.imshow(recommendation_matrix, cmap='RdYlGn', aspect='auto', vmin=3, vmax=10)\n",
    "    ax4.set_xticks(range(len(use_cases)))\n",
    "    ax4.set_xticklabels(use_cases, fontsize=10)\n",
    "    ax4.set_yticks(range(len(recommended_strategies)))\n",
    "    ax4.set_yticklabels(recommended_strategies, fontsize=10)\n",
    "    ax4.set_title('Augmentation Strategy Recommendations', fontweight='bold')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(recommended_strategies)):\n",
    "        for j in range(len(use_cases)):\n",
    "            text = ax4.text(j, i, f'{recommendation_matrix[i][j]}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\", fontweight='bold', fontsize=12)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax4, shrink=0.8)\n",
    "    cbar.set_label('Recommendation Score (3-10)')\n",
    "    \n",
    "    plt.suptitle('Comprehensive Data Augmentation Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'augmentation_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Run comprehensive augmentation analysis\n",
    "effectiveness_data, strategies = analyze_augmentation_strategies()\n",
    "visualize_augmentation_analysis(effectiveness_data, strategies, augmentation_stats)\n",
    "\n",
    "print(f\"\\nüí° Data Augmentation Strategy Insights:\")\n",
    "augmentation_insights = [\n",
    "    \"‚Ä¢ RandAugment: Automated policy search with strong empirical results across domains\",\n",
    "    \"‚Ä¢ TrivialAugment: Simpler alternative with competitive performance and lower complexity\",\n",
    "    \"‚Ä¢ MixUp/CutMix: Powerful regularization, especially effective for small datasets\",\n",
    "    \"‚Ä¢ Combined Strategies: Best performance but increased computational and memory costs\",\n",
    "    \"‚Ä¢ Domain-Specific: Tailor augmentation policies to your specific data characteristics\",\n",
    "    \"‚Ä¢ Training Budget: Consider computational overhead vs. performance gains\",\n",
    "    \"‚Ä¢ Robustness vs. Accuracy: Some augmentations improve robustness more than accuracy\"\n",
    "]\n",
    "\n",
    "for insight in augmentation_insights:\n",
    "    print(f\"  {insight}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data augmentation analysis complete!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Training Stability and Debugging\n",
    "\n",
    "### 6.1 Training Monitoring and Diagnostics\n",
    "\n",
    "```python\n",
    "print(\"=== 6.1 Training Stability and Debugging System ===\\n\")\n",
    "\n",
    "class TrainingMonitor:\n",
    "    \"\"\"Comprehensive training monitoring system for debugging and optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.metrics = {\n",
    "            'losses': [],\n",
    "            'gradient_norms': [],\n",
    "            'weight_norms': [],\n",
    "            'learning_rates': [],\n",
    "            'batch_times': [],\n",
    "            'gradient_ratios': [],  # grad_norm / weight_norm\n",
    "            'loss_smoothness': [],  # Moving average smoothness\n",
    "            'gradient_variance': []  # Gradient variance tracking\n",
    "        }\n",
    "        self.layer_stats = {}\n",
    "        self.issue_history = []\n",
    "        self.moving_window = 10\n",
    "    \n",
    "    def log_iteration(self, loss, optimizer, batch_time, epoch=None):\n",
    "        \"\"\"Log comprehensive metrics for one training iteration.\"\"\"\n",
    "        \n",
    "        current_loss = loss.item() if torch.is_tensor(loss) else loss\n",
    "        self.metrics['losses'].append(current_loss)\n",
    "        self.metrics['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "        self.metrics['batch_times'].append(batch_time)\n",
    "        \n",
    "        # Compute gradient and weight norms\n",
    "        total_grad_norm = 0\n",
    "        total_weight_norm = 0\n",
    "        layer_grad_norms = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norm = param.grad.data.norm(2).item()\n",
    "                weight_norm = param.data.norm(2).item()\n",
    "                \n",
    "                total_grad_norm += grad_norm ** 2\n",
    "                total_weight_norm += weight_norm ** 2\n",
    "                layer_grad_norms.append(grad_norm)\n",
    "                \n",
    "                # Store layer-specific stats\n",
    "                if name not in self.layer_stats:\n",
    "                    self.layer_stats[name] = {\n",
    "                        'grad_norms': [],\n",
    "                        'weight_norms': [],\n",
    "                        'grad_to_weight_ratios': []\n",
    "                    }\n",
    "                \n",
    "                self.layer_stats[name]['grad_norms'].append(grad_norm)\n",
    "                self.layer_stats[name]['weight_norms'].append(weight_norm)\n",
    "                \n",
    "                if weight_norm > 0:\n",
    "                    self.layer_stats[name]['grad_to_weight_ratios'].append(grad_norm / weight_norm)\n",
    "        \n",
    "        total_grad_norm = math.sqrt(total_grad_norm)\n",
    "        total_weight_norm = math.sqrt(total_weight_norm)\n",
    "        \n",
    "        self.metrics['gradient_norms'].append(total_grad_norm)\n",
    "        self.metrics['weight_norms'].append(total_weight_norm)\n",
    "        \n",
    "        # Gradient to weight ratio\n",
    "        if total_weight_norm > 0:\n",
    "            grad_ratio = total_grad_norm / total_weight_norm\n",
    "            self.metrics['gradient_ratios'].append(grad_ratio)\n",
    "        else:\n",
    "            self.metrics['gradient_ratios'].append(0)\n",
    "        \n",
    "        # Loss smoothness (moving average deviation)\n",
    "        if len(self.metrics['losses']) >= self.moving_window:\n",
    "            recent_losses = self.metrics['losses'][-self.moving_window:]\n",
    "            loss_smoothness = np.std(recent_losses) / (np.mean(recent_losses) + 1e-8)\n",
    "            self.metrics['loss_smoothness'].append(loss_smoothness)\n",
    "        else:\n",
    "            self.metrics['loss_smoothness'].append(0)\n",
    "        \n",
    "        # Gradient variance\n",
    "        if len(layer_grad_norms) > 1:\n",
    "            grad_variance = np.var(layer_grad_norms)\n",
    "            self.metrics['gradient_variance'].append(grad_variance)\n",
    "        else:\n",
    "            self.metrics['gradient_variance'].append(0)\n",
    "    \n",
    "    def detect_training_issues(self):\n",
    "        \"\"\"Detect common training issues automatically.\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        if len(self.metrics['losses']) < self.moving_window:\n",
    "            return issues\n",
    "        \n",
    "        recent_losses = self.metrics['losses'][-self.moving_window:]\n",
    "        recent_grads = self.metrics['gradient_norms'][-self.moving_window:]\n",
    "        recent_ratios = self.metrics['gradient_ratios'][-self.moving_window:]\n",
    "        recent_smoothness = self.metrics['loss_smoothness'][-5:] if len(self.metrics['loss_smoothness']) >= 5 else []\n",
    "        \n",
    "        # 1. Exploding gradients\n",
    "        if any(grad > 10.0 for grad in recent_grads):\n",
    "            max_grad = max(recent_grads)\n",
    "            issues.append({\n",
    "                'type': 'exploding_gradients',\n",
    "                'severity': 'high',\n",
    "                'message': f\"üö® Exploding gradients detected (max: {max_grad:.2f})\",\n",
    "                'recommendation': 'Apply gradient clipping, reduce learning rate'\n",
    "            })\n",
    "        \n",
    "        # 2. Vanishing gradients\n",
    "        if all(grad < 1e-6 for grad in recent_grads):\n",
    "            avg_grad = np.mean(recent_grads)\n",
    "            issues.append({\n",
    "                'type': 'vanishing_gradients',\n",
    "                'severity': 'high',\n",
    "                'message': f\"üö® Vanishing gradients detected (avg: {avg_grad:.2e})\",\n",
    "                'recommendation': 'Add skip connections, better initialization, different activation'\n",
    "            })\n",
    "        \n",
    "        # 3. Loss explosion\n",
    "        if len(recent_losses) >= 2:\n",
    "            if any(loss > recent_losses[0] * 10 for loss in recent_losses[1:]):\n",
    "                issues.append({\n",
    "                    'type': 'loss_explosion',\n",
    "                    'severity': 'critical',\n",
    "                    'message': \"üö® Loss explosion detected\",\n",
    "                    'recommendation': 'Drastically reduce learning rate, check loss function'\n",
    "                })\n",
    "        \n",
    "        # 4. Learning stagnation\n",
    "        if len(recent_losses) >= 5:\n",
    "            loss_std = np.std(recent_losses[-5:])\n",
    "            loss_mean = np.mean(recent_losses[-5:])\n",
    "            if loss_std / (loss_mean + 1e-8) < 1e-4:\n",
    "                issues.append({\n",
    "                    'type': 'learning_stagnation',\n",
    "                    'severity': 'medium',\n",
    "                    'message': \"‚ö†Ô∏è Learning stagnation detected\",\n",
    "                    'recommendation': 'Increase learning rate, add learning rate scheduling'\n",
    "                })\n",
    "        \n",
    "        # 5. Oscillating loss\n",
    "        if recent_smoothness and np.mean(recent_smoothness) > 0.1:\n",
    "            issues.append({\n",
    "                'type': 'oscillating_loss',\n",
    "                'severity': 'medium', \n",
    "                'message': f\"‚ö†Ô∏è High loss oscillation (smoothness: {np.mean(recent_smoothness):.3f})\",\n",
    "                'recommendation': 'Reduce learning rate, increase batch size'\n",
    "            })\n",
    "        \n",
    "        # 6. Gradient-to-weight ratio issues\n",
    "        if recent_ratios:\n",
    "            avg_ratio = np.mean(recent_ratios)\n",
    "            if avg_ratio > 1.0:\n",
    "                issues.append({\n",
    "                    'type': 'high_gradient_ratio',\n",
    "                    'severity': 'medium',\n",
    "                    'message': f\"‚ö†Ô∏è High gradient-to-weight ratios (avg: {avg_ratio:.3f})\",\n",
    "                    'recommendation': 'Reduce learning rate or apply gradient clipping'\n",
    "                })\n",
    "            elif avg_ratio < 1e-4:\n",
    "                issues.append({\n",
    "                    'type': 'low_gradient_ratio',\n",
    "                    'severity': 'medium',\n",
    "                    'message': f\"‚ö†Ô∏è Very low gradient-to-weight ratios (avg: {avg_ratio:.2e})\",\n",
    "                    'recommendation': 'Increase learning rate or check for frozen layers'\n",
    "                })\n",
    "        \n",
    "        # Store issues in history\n",
    "        if issues:\n",
    "            self.issue_history.append({\n",
    "                'iteration': len(self.metrics['losses']),\n",
    "                'issues': issues\n",
    "            })\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def get_training_health_score(self):\n",
    "        \"\"\"Calculate overall training health score (0-100).\"\"\"\n",
    "        if len(self.metrics['losses']) < self.moving_window:\n",
    "            return 50  # Neutral score for insufficient data\n",
    "        \n",
    "        score = 100\n",
    "        recent_window = min(20, len(self.metrics['losses']))\n",
    "        \n",
    "        # Factor 1: Loss trend (30 points)\n",
    "        recent_losses = self.metrics['losses'][-recent_window:]\n",
    "        if len(recent_losses) >= 2:\n",
    "            loss_trend = (recent_losses[0] - recent_losses[-1]) / (recent_losses[0] + 1e-8)\n",
    "            if loss_trend > 0:  # Decreasing loss is good\n",
    "                score += min(30, loss_trend * 100)\n",
    "            else:  # Increasing loss is bad\n",
    "                score += max(-30, loss_trend * 100)\n",
    "        \n",
    "        # Factor 2: Gradient stability (25 points)\n",
    "        recent_grads = self.metrics['gradient_norms'][-recent_window:]\n",
    "        grad_stability = 1 - (np.std(recent_grads) / (np.mean(recent_grads) + 1e-8))\n",
    "        score += grad_stability * 25\n",
    "        \n",
    "        # Factor 3: Loss smoothness (25 points)\n",
    "        if self.metrics['loss_smoothness']:\n",
    "            recent_smoothness = self.metrics['loss_smoothness'][-min(10, len(self.metrics['loss_smoothness'])):]\n",
    "            smoothness_score = max(0, 1 - np.mean(recent_smoothness))\n",
    "            score += smoothness_score * 25\n",
    "        \n",
    "        # Factor 4: Learning rate appropriateness (20 points)\n",
    "        recent_ratios = self.metrics['gradient_ratios'][-recent_window:]\n",
    "        if recent_ratios:\n",
    "            avg_ratio = np.mean(recent_ratios)\n",
    "            # Ideal ratio is around 0.001 to 0.1\n",
    "            if 0.001 <= avg_ratio <= 0.1:\n",
    "                score += 20\n",
    "            elif 0.0001 <= avg_ratio <= 1.0:\n",
    "                score += 10\n",
    "            else:\n",
    "                score -= 10\n",
    "        \n",
    "        return max(0, min(100, score))\n",
    "    \n",
    "    def generate_training_report(self):\n",
    "        \"\"\"Generate comprehensive training report.\"\"\"\n",
    "        if len(self.metrics['losses']) < 2:\n",
    "            return \"Insufficient data for report generation\"\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"üìä TRAINING DIAGNOSTICS REPORT\")\n",
    "        report.append(\"=\" * 50)\n",
    "        \n",
    "        # Basic metrics\n",
    "        total_iterations = len(self.metrics['losses'])\n",
    "        current_loss = self.metrics['losses'][-1]\n",
    "        initial_loss = self.metrics['losses'][0]\n",
    "        loss_reduction = (initial_loss - current_loss) / initial_loss * 100\n",
    "        \n",
    "        report.append(f\"\\nüéØ Training Progress:\")\n",
    "        report.append(f\"   Total iterations: {total_iterations}\")\n",
    "        report.append(f\"   Initial loss: {initial_loss:.6f}\")\n",
    "        report.append(f\"   Current loss: {current_loss:.6f}\")\n",
    "        report.append(f\"   Loss reduction: {loss_reduction:.2f}%\")\n",
    "        \n",
    "        # Health score\n",
    "        health_score = self.get_training_health_score()\n",
    "        health_status = \"üü¢ Excellent\" if health_score >= 80 else \"üü° Good\" if health_score >= 60 else \"üü† Fair\" if health_score >= 40 else \"üî¥ Poor\"\n",
    "        report.append(f\"\\nüíö Training Health: {health_score:.1f}/100 ({health_status})\")\n",
    "        \n",
    "        # Recent issues\n",
    "        recent_issues = self.detect_training_issues()\n",
    "        if recent_issues:\n",
    "            report.append(f\"\\n‚ö†Ô∏è Current Issues ({len(recent_issues)}):\")\n",
    "            for issue in recent_issues:\n",
    "                report.append(f\"   ‚Ä¢ {issue['message']}\")\n",
    "                report.append(f\"     ‚Üí {issue['recommendation']}\")\n",
    "        else:\n",
    "            report.append(f\"\\n‚úÖ No current issues detected\")\n",
    "        \n",
    "        # Performance metrics\n",
    "        if self.metrics['batch_times']:\n",
    "            avg_batch_time = np.mean(self.metrics['batch_times'][-100:])  # Last 100 iterations\n",
    "            report.append(f\"\\n‚ö° Performance:\")\n",
    "            report.append(f\"   Avg batch time: {avg_batch_time:.3f}s\")\n",
    "            report.append(f\"   Estimated throughput: {1/avg_batch_time:.1f} batches/sec\")\n",
    "        \n",
    "        # Gradient analysis\n",
    "        if self.metrics['gradient_norms']:\n",
    "            recent_grad_norm = np.mean(self.metrics['gradient_norms'][-10:])\n",
    "            recent_weight_norm = np.mean(self.metrics['weight_norms'][-10:])\n",
    "            report.append(f\"\\nüìê Gradient Analysis:\")\n",
    "            report.append(f\"   Avg gradient norm: {recent_grad_norm:.6f}\")\n",
    "            report.append(f\"   Avg weight norm: {recent_weight_norm:.6f}\")\n",
    "            report.append(f\"   Gradient/weight ratio: {recent_grad_norm/recent_weight_norm:.6f}\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "class GradientClipper:\n",
    "    \"\"\"Advanced gradient clipping utilities with adaptive strategies.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def clip_grad_norm(parameters, max_norm, norm_type=2, adaptive=False):\n",
    "        \"\"\"Clip gradients by global norm with optional adaptive scaling.\"\"\"\n",
    "        if isinstance(parameters, torch.Tensor):\n",
    "            parameters = [parameters]\n",
    "        parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "        \n",
    "        if len(parameters) == 0:\n",
    "            return 0\n",
    "        \n",
    "        device = parameters[0].grad.device\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) \n",
    "                                           for p in parameters]), norm_type)\n",
    "        \n",
    "        if adaptive:\n",
    "            # Adaptive clipping based on gradient history\n",
    "            # This would maintain a running average of gradient norms\n",
    "            # and adjust clipping threshold accordingly\n",
    "            pass\n",
    "        \n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        \n",
    "        if clip_coef < 1:\n",
    "            for p in parameters:\n",
    "                p.grad.detach().mul_(clip_coef.to(p.grad.device))\n",
    "        \n",
    "        return total_norm.item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def clip_grad_value(parameters, clip_value):\n",
    "        \"\"\"Clip gradients by absolute value.\"\"\"\n",
    "        if isinstance(parameters, torch.Tensor):\n",
    "            parameters = [parameters]\n",
    "        \n",
    "        clipped_count = 0\n",
    "        for p in filter(lambda p: p.grad is not None, parameters):\n",
    "            original_grad = p.grad.data.clone()\n",
    "            p.grad.data.clamp_(min=-clip_value, max=clip_value)\n",
    "            if not torch.equal(original_grad, p.grad.data):\n",
    "                clipped_count += 1\n",
    "        \n",
    "        return clipped_count\n",
    "\n",
    "print(\"‚úÖ Training monitoring and debugging system implemented!\")\n",
    "print(\"   ‚Ä¢ TrainingMonitor: Comprehensive metrics tracking and issue detection\")\n",
    "print(\"   ‚Ä¢ GradientClipper: Advanced gradient clipping with adaptive options\")\n",
    "print(\"   ‚Ä¢ Automated issue detection with severity levels and recommendations\")\n",
    "print(\"   ‚Ä¢ Health scoring system for training quality assessment\")\n",
    "```\n",
    "\n",
    "### 6.2 Training Scenario Testing\n",
    "\n",
    "```python\n",
    "def simulate_training_scenarios():\n",
    "    \"\"\"Simulate various training scenarios to test monitoring capabilities.\"\"\"\n",
    "    \n",
    "    print(\"üî¨ Testing Training Monitoring System:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create test model\n",
    "    class TestNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(100, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 10)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "    \n",
    "    # Define different training scenarios\n",
    "    scenarios = {\n",
    "        'healthy_training': {\n",
    "            'lr_multiplier': 1.0,\n",
    "            'noise_multiplier': 1.0,\n",
    "            'loss_scaling': 1.0,\n",
    "            'description': 'Normal healthy training'\n",
    "        },\n",
    "        'exploding_gradients': {\n",
    "            'lr_multiplier': 100.0,\n",
    "            'noise_multiplier': 1.0,\n",
    "            'loss_scaling': 1.0,\n",
    "            'description': 'High learning rate causing exploding gradients'\n",
    "        },\n",
    "        'vanishing_gradients': {\n",
    "            'lr_multiplier': 0.00001,\n",
    "            'noise_multiplier': 1.0,\n",
    "            'loss_scaling': 1.0,\n",
    "            'description': 'Very low learning rate causing vanishing gradients'\n",
    "        },\n",
    "        'noisy_training': {\n",
    "            'lr_multiplier': 1.0,\n",
    "            'noise_multiplier': 10.0,\n",
    "            'loss_scaling': 1.0,\n",
    "            'description': 'High noise causing training instability'\n",
    "        },\n",
    "        'loss_plateau': {\n",
    "            'lr_multiplier': 0.01,\n",
    "            'noise_multiplier': 0.1,\n",
    "            'loss_scaling': 1.0,\n",
    "            'description': 'Low learning rate causing learning stagnation'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    scenario_results = {}\n",
    "    \n",
    "    for scenario_name, params in scenarios.items():\n",
    "        print(f\"\\nüìä Testing Scenario: {scenario_name}\")\n",
    "        print(f\"   Description: {params['description']}\")\n",
    "        \n",
    "        # Initialize model and optimizer\n",
    "        model = TestNet()\n",
    "        base_lr = 0.001\n",
    "        optimizer = optim.Adam(model.parameters(), lr=base_lr * params['lr_multiplier'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        monitor = TrainingMonitor(model)\n",
    "        \n",
    "        # Training simulation\n",
    "        num_iterations = 50\n",
    "        issues_detected = []\n",
    "        health_scores = []\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            # Generate synthetic data with controlled noise\n",
    "            batch_size = 32\n",
    "            x = torch.randn(batch_size, 100) * params['noise_multiplier']\n",
    "            y = torch.randint(0, 10, (batch_size,))\n",
    "            \n",
    "            # Forward pass\n",
    "            start_time = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y) * params['loss_scaling']\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply gradient clipping for exploding scenario\n",
    "            if scenario_name == 'exploding_gradients' and iteration > 10:\n",
    "                GradientClipper.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            batch_time = time.time() - start_time\n",
    "            \n",
    "            # Monitor training\n",
    "            monitor.log_iteration(loss, optimizer, batch_time)\n",
    "            \n",
    "            # Detect issues every 10 iterations\n",
    "            if iteration % 10 == 0:\n",
    "                current_issues = monitor.detect_training_issues()\n",
    "                if current_issues:\n",
    "                    issues_detected.extend(current_issues)\n",
    "            \n",
    "            # Track health score\n",
    "            health_score = monitor.get_training_health_score()\n",
    "            health_scores.append(health_score)\n",
    "        \n",
    "        # Generate final report\n",
    "        final_report = monitor.generate_training_report()\n",
    "        \n",
    "        # Store results\n",
    "        scenario_results[scenario_name] = {\n",
    "            'monitor': monitor,\n",
    "            'issues': issues_detected,\n",
    "            'health_scores': health_scores,\n",
    "            'final_health': health_scores[-1] if health_scores else 0,\n",
    "            'report': final_report,\n",
    "            'final_loss': monitor.metrics['losses'][-1],\n",
    "            'loss_reduction': (monitor.metrics['losses'][0] - monitor.metrics['losses'][-1]) / monitor.metrics['losses'][0] * 100\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        unique_issue_types = set(issue['type'] for issue in issues_detected)\n",
    "        print(f\"   Final loss: {monitor.metrics['losses'][-1]:.6f}\")\n",
    "        print(f\"   Health score: {health_scores[-1]:.1f}/100\")\n",
    "        print(f\"   Issues detected: {len(unique_issue_types)} types\")\n",
    "        if unique_issue_types:\n",
    "            print(f\"   Issue types: {', '.join(unique_issue_types)}\")\n",
    "    \n",
    "    return scenario_results\n",
    "\n",
    "# Run scenario testing\n",
    "scenario_results = simulate_training_scenarios()\n",
    "```\n",
    "\n",
    "### 6.3 Training Diagnostics Visualization\n",
    "\n",
    "```python\n",
    "def visualize_training_diagnostics(scenario_results):\n",
    "    \"\"\"Create comprehensive training diagnostics visualizations.\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Loss curves for all scenarios\n",
    "    for scenario_name, results in scenario_results.items():\n",
    "        monitor = results['monitor']\n",
    "        iterations = range(len(monitor.metrics['losses']))\n",
    "        \n",
    "        # Use different line styles for different scenarios\n",
    "        line_styles = {\n",
    "            'healthy_training': '-',\n",
    "            'exploding_gradients': '--',\n",
    "            'vanishing_gradients': '-.',\n",
    "            'noisy_training': ':',\n",
    "            'loss_plateau': '-'\n",
    "        }\n",
    "        \n",
    "        ax1.plot(iterations, monitor.metrics['losses'], \n",
    "                line_styles.get(scenario_name, '-'), \n",
    "                linewidth=2.5, label=scenario_name.replace('_', ' ').title(), alpha=0.8)\n",
    "    \n",
    "    ax1.set_title('Training Loss Curves by Scenario', fontweight='bold')\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Plot 2: Health scores over time\n",
    "    for scenario_name, results in scenario_results.items():\n",
    "        health_scores = results['health_scores']\n",
    "        iterations = range(len(health_scores))\n",
    "        \n",
    "        ax2.plot(iterations, health_scores, linewidth=2.5, \n",
    "                label=scenario_name.replace('_', ' ').title(), alpha=0.8)\n",
    "    \n",
    "    ax2.set_title('Training Health Score Evolution', fontweight='bold')\n",
    "    ax2.set_xlabel('Iteration')\n",
    "    ax2.set_ylabel('Health Score (0-100)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(y=80, color='green', linestyle='--', alpha=0.5, label='Excellent threshold')\n",
    "    ax2.axhline(y=60, color='orange', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "    ax2.axhline(y=40, color='red', linestyle='--', alpha=0.5, label='Poor threshold')\n",
    "    \n",
    "    # Plot 3: Final performance comparison\n",
    "    scenario_names = list(scenario_results.keys())\n",
    "    final_losses = [scenario_results[name]['final_loss'] for name in scenario_names]\n",
    "    final_health = [scenario_results[name]['final_health'] for name in scenario_names]\n",
    "    loss_reductions = [scenario_results[name]['loss_reduction'] for name in scenario_names]\n",
    "    \n",
    "    x = np.arange(len(scenario_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    bars1 = ax3.bar(x - width, [100 - h for h in final_health], width, \n",
    "                   label='Health Issues (100-score)', alpha=0.8, color='lightcoral')\n",
    "    bars2 = ax3.bar(x, [max(0, lr) for lr in loss_reductions], width, \n",
    "                   label='Loss Reduction %', alpha=0.8, color='lightgreen')\n",
    "    bars3 = ax3.bar(x + width, [min(fl*1000, 100) for fl in final_losses], width, \n",
    "                   label='Final Loss√ó1000', alpha=0.8, color='lightblue')\n",
    "    \n",
    "    ax3.set_xlabel('Training Scenario')\n",
    "    ax3.set_ylabel('Score/Percentage')\n",
    "    ax3.set_title('Final Training Performance Metrics', fontweight='bold')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels([name.replace('_', '\\n') for name in scenario_names], fontsize=9)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 4: Issue detection summary\n",
    "    issue_counts = {}\n",
    "    all_issue_types = set()\n",
    "    \n",
    "    for scenario_name, results in scenario_results.items():\n",
    "        scenario_issues = {}\n",
    "        for issue in results['issues']:\n",
    "            issue_type = issue['type']\n",
    "            all_issue_types.add(issue_type)\n",
    "            scenario_issues[issue_type] = scenario_issues.get(issue_type, 0) + 1\n",
    "        issue_counts[scenario_name] = scenario_issues\n",
    "    \n",
    "    # Create heatmap of issues\n",
    "    issue_types_list = sorted(list(all_issue_types))\n",
    "    issue_matrix = []\n",
    "    \n",
    "    for scenario in scenario_names:\n",
    "        row = [issue_counts[scenario].get(issue_type, 0) for issue_type in issue_types_list]\n",
    "        issue_matrix.append(row)\n",
    "    \n",
    "    if issue_matrix and issue_types_list:\n",
    "        im = ax4.imshow(issue_matrix, cmap='Reds', aspect='auto')\n",
    "        ax4.set_xticks(range(len(issue_types_list)))\n",
    "        ax4.set_xticklabels([it.replace('_', '\\n') for it in issue_types_list], fontsize=9, rotation=45, ha='right')\n",
    "        ax4.set_yticks(range(len(scenario_names)))\n",
    "        ax4.set_yticklabels([name.replace('_', '\\n') for name in scenario_names], fontsize=9)\n",
    "        ax4.set_title('Training Issues Detection Matrix', fontweight='bold')\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(len(scenario_names)):\n",
    "            for j in range(len(issue_types_list)):\n",
    "                count = issue_matrix[i][j]\n",
    "                if count > 0:\n",
    "                    text = ax4.text(j, i, str(count), ha=\"center\", va=\"center\", \n",
    "                                   color=\"white\" if count > 2 else \"black\", fontweight='bold')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax4, shrink=0.8)\n",
    "        cbar.set_label('Number of Issues Detected')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No Issues Detected\\nAcross All Scenarios', \n",
    "                ha='center', va='center', transform=ax4.transAxes, \n",
    "                fontsize=14, fontweight='bold')\n",
    "        ax4.set_title('Training Issues Detection Matrix', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Comprehensive Training Diagnostics Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'training_diagnostics.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_training_diagnostics(scenario_results)\n",
    "\n",
    "# Print detailed analysis summary\n",
    "print(f\"\\nüìã Training Diagnostics Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for scenario_name, results in scenario_results.items():\n",
    "    print(f\"\\nüéØ {scenario_name.replace('_', ' ').title()}:\")\n",
    "    print(f\"   Final health score: {results['final_health']:.1f}/100\")\n",
    "    print(f\"   Loss reduction: {results['loss_reduction']:.1f}%\")\n",
    "    print(f\"   Issues detected: {len(results['issues'])}\")\n",
    "    \n",
    "    # Show unique issue types\n",
    "    unique_issues = set(issue['type'] for issue in results['issues'])\n",
    "    if unique_issues:\n",
    "        print(f\"   Issue types: {', '.join(unique_issues)}\")\n",
    "    \n",
    "    # Show most recent issues\n",
    "    if results['issues']:\n",
    "        latest_issue = results['issues'][-1]\n",
    "        print(f\"   Latest issue: {latest_issue['message']}\")\n",
    "        print(f\"   Recommendation: {latest_issue['recommendation']}\")\n",
    "\n",
    "print(f\"\\nüí° Training Stability Insights:\")\n",
    "stability_insights = [\n",
    "    \"‚Ä¢ Monitoring gradient norms prevents exploding/vanishing gradient issues\",\n",
    "    \"‚Ä¢ Health scoring provides early warning of training problems\",\n",
    "    \"‚Ä¢ Automated issue detection saves debugging time and effort\",\n",
    "    \"‚Ä¢ Gradient clipping serves as effective safety net for unstable training\",\n",
    "    \"‚Ä¢ Loss smoothness tracking identifies oscillatory training behavior\",\n",
    "    \"‚Ä¢ Layer-wise statistics help pinpoint problematic network components\",\n",
    "    \"‚Ä¢ Comprehensive logging enables post-hoc training analysis\"\n",
    "]\n",
    "\n",
    "for insight in stability_insights:\n",
    "    print(f\"  {insight}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training stability and debugging analysis complete!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Comprehensive Training Pipeline Integration\n",
    "\n",
    "### 7.1 Advanced Training Framework\n",
    "\n",
    "```python\n",
    "print(\"=== 7.1 Comprehensive Advanced Training Pipeline ===\\n\")\n",
    "\n",
    "class AdvancedTrainer:\n",
    "    \"\"\"Production-ready training pipeline integrating all advanced techniques.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize components\n",
    "        self._setup_optimizer()\n",
    "        self._setup_scheduler()\n",
    "        self._setup_loss_function()\n",
    "        self._setup_regularization()\n",
    "        self._setup_monitoring()\n",
    "        self._setup_augmentation()\n",
    "        \n",
    "        # Training state\n",
    "        self.epoch = 0\n",
    "        self.global_step = 0\n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_model_state = None\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_acc': [],\n",
    "            'learning_rates': [],\n",
    "            'health_scores': []\n",
    "        }\n",
    "        \n",
    "        print(f\"üöÄ Advanced Trainer Initialized:\")\n",
    "        print(f\"   Model: {type(self.model).__name__}\")\n",
    "        print(f\"   Optimizer: {type(self.optimizer).__name__}\")\n",
    "        print(f\"   Scheduler: {type(self.scheduler).__name__ if self.scheduler else 'None'}\")\n",
    "        print(f\"   Loss Function: {type(self.criterion).__name__}\")\n",
    "        print(f\"   Regularization: {list(self.config.get('regularization', {}).keys())}\")\n",
    "        print(f\"   Monitoring: Enabled with issue detection\")\n",
    "    \n",
    "    def _setup_optimizer(self):\n",
    "        \"\"\"Initialize optimizer based on configuration.\"\"\"\n",
    "        opt_config = self.config.get('optimizer', {'type': 'adamw', 'lr': 0.001})\n",
    "        \n",
    "        if opt_config['type'].lower() == 'adamw':\n",
    "            self.optimizer = optim.AdamW(\n",
    "                self.model.parameters(),\n",
    "                lr=opt_config['lr'],\n",
    "                weight_decay=opt_config.get('weight_decay', 0.01),\n",
    "                betas=opt_config.get('betas', (0.9, 0.999))\n",
    "            )\n",
    "        elif opt_config['type'].lower() == 'lamb':\n",
    "            self.optimizer = LAMB(\n",
    "                self.model.parameters(),\n",
    "                lr=opt_config['lr'],\n",
    "                weight_decay=opt_config.get('weight_decay', 0.01)\n",
    "            )\n",
    "        elif opt_config['type'].lower() == 'sam':\n",
    "            base_opt = optim.SGD if opt_config.get('base', 'sgd') == 'sgd' else optim.Adam\n",
    "            self.optimizer = SAM(\n",
    "                self.model.parameters(),\n",
    "                base_optimizer=base_opt,\n",
    "                lr=opt_config['lr'],\n",
    "                rho=opt_config.get('rho', 0.05)\n",
    "            )\n",
    "        elif opt_config['type'].lower() == 'sgd':\n",
    "            self.optimizer = optim.SGD(\n",
    "                self.model.parameters(),\n",
    "                lr=opt_config['lr'],\n",
    "                momentum=opt_config.get('momentum', 0.9),\n",
    "                weight_decay=opt_config.get('weight_decay', 1e-4)\n",
    "            )\n",
    "        else:\n",
    "            self.optimizer = optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr=opt_config['lr'],\n",
    "                weight_decay=opt_config.get('weight_decay', 1e-4)\n",
    "            )\n",
    "    \n",
    "    def _setup_scheduler(self):\n",
    "        \"\"\"Initialize learning rate scheduler.\"\"\"\n",
    "        sched_config = self.config.get('scheduler', None)\n",
    "        \n",
    "        if not sched_config:\n",
    "            self.scheduler = None\n",
    "            return\n",
    "        \n",
    "        sched_type = sched_config['type'].lower()\n",
    "        \n",
    "        if sched_type == 'cosine':\n",
    "            self.scheduler = CosineAnnealingLR(\n",
    "                self.optimizer,\n",
    "                T_max=sched_config.get('T_max', 100),\n",
    "                eta_min=sched_config.get('eta_min', 0)\n",
    "            )\n",
    "        elif sched_type == 'onecycle':\n",
    "            self.scheduler = OneCycleLR(\n",
    "                self.optimizer,\n",
    "                max_lr=sched_config.get('max_lr', 0.1),\n",
    "                total_steps=sched_config.get('total_steps', 1000)\n",
    "            )\n",
    "        elif sched_type == 'step':\n",
    "            self.scheduler = StepLR(\n",
    "                self.optimizer,\n",
    "                step_size=sched_config.get('step_size', 30),\n",
    "                gamma=sched_config.get('gamma', 0.1)\n",
    "            )\n",
    "        elif sched_type == 'warmup':\n",
    "            self.scheduler = WarmupScheduler(\n",
    "                self.optimizer,\n",
    "                warmup_epochs=sched_config.get('warmup_epochs', 10),\n",
    "                base_lr=sched_config.get('base_lr', 1e-6),\n",
    "                target_lr=self.optimizer.param_groups[0]['lr']\n",
    "            )\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "    \n",
    "    def _setup_loss_function(self):\n",
    "        \"\"\"Initialize loss function with advanced options.\"\"\"\n",
    "        loss_config = self.config.get('loss', {'type': 'crossentropy'})\n",
    "        \n",
    "        if loss_config['type'] == 'crossentropy':\n",
    "            if loss_config.get('label_smoothing', 0) > 0:\n",
    "                self.criterion = LabelSmoothing(\n",
    "                    num_classes=loss_config.get('num_classes', 10),\n",
    "                    smoothing=loss_config['label_smoothing']\n",
    "                )\n",
    "            else:\n",
    "                self.criterion = nn.CrossEntropyLoss()\n",
    "        elif loss_config['type'] == 'mse':\n",
    "            self.criterion = nn.MSELoss()\n",
    "        else:\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def _setup_regularization(self):\n",
    "        \"\"\"Initialize regularization techniques.\"\"\"\n",
    "        reg_config = self.config.get('regularization', {})\n",
    "        \n",
    "        # Data mixing augmentations\n",
    "        self.mixup = None\n",
    "        self.cutmix = None\n",
    "        \n",
    "        if reg_config.get('mixup', False):\n",
    "            self.mixup = MixUp(alpha=reg_config.get('mixup_alpha', 0.2))\n",
    "        \n",
    "        if reg_config.get('cutmix', False):\n",
    "            self.cutmix = CutMix(alpha=reg_config.get('cutmix_alpha', 1.0))\n",
    "        \n",
    "        # Gradient clipping\n",
    "        self.grad_clip = reg_config.get('grad_clip', 0)\n",
    "    \n",
    "    def _setup_monitoring(self):\n",
    "        \"\"\"Initialize training monitoring.\"\"\"\n",
    "        self.monitor = TrainingMonitor(self.model)\n",
    "        self.enable_monitoring = self.config.get('monitoring', {}).get('enabled', True)\n",
    "        self.monitoring_frequency = self.config.get('monitoring', {}).get('frequency', 10)\n",
    "    \n",
    "    def _setup_augmentation(self):\n",
    "        \"\"\"Setup advanced data augmentation.\"\"\"\n",
    "        aug_config = self.config.get('augmentation', {})\n",
    "        \n",
    "        self.use_randaugment = aug_config.get('randaugment', False)\n",
    "        if self.use_randaugment:\n",
    "            self.randaugment = RandAugment(\n",
    "                n=aug_config.get('randaugment_n', 2),\n",
    "                m=aug_config.get('randaugment_m', 10)\n",
    "            )\n",
    "        \n",
    "        self.use_trivialaugment = aug_config.get('trivialaugment', False)\n",
    "        if self.use_trivialaugment:\n",
    "            self.trivialaugment = TrivialAugmentWide()\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch with all advanced techniques.\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_metrics = {\n",
    "            'loss': 0.0,\n",
    "            'correct': 0,\n",
    "            'total': 0,\n",
    "            'batches': 0\n",
    "        }\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Apply data augmentation\n",
    "            inputs, targets = self._apply_augmentation(inputs, targets)\n",
    "            \n",
    "            # Forward pass\n",
    "            if isinstance(self.optimizer, SAM):\n",
    "                # SAM requires two forward passes\n",
    "                outputs = self._sam_forward_pass(inputs, targets)\n",
    "            else:\n",
    "                outputs = self._standard_forward_pass(inputs, targets)\n",
    "            \n",
    "            batch_time = time.time() - start_time\n",
    "            \n",
    "            # Update metrics\n",
    "            if not self._is_mixed_targets(targets):\n",
    "                _, predicted = outputs.max(1)\n",
    "                epoch_metrics['total'] += targets.size(0)\n",
    "                epoch_metrics['correct'] += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            epoch_metrics['batches'] += 1\n",
    "            self.global_step += 1\n",
    "            \n",
    "            # Monitoring\n",
    "            if self.enable_monitoring and batch_idx % self.monitoring_frequency == 0:\n",
    "                current_loss = outputs  # Loss is returned from forward pass methods\n",
    "                self.monitor.log_iteration(current_loss, self.optimizer, batch_time, self.epoch)\n",
    "                \n",
    "                # Check for training issues\n",
    "                issues = self.monitor.detect_training_issues()\n",
    "                if issues:\n",
    "                    critical_issues = [i for i in issues if i['severity'] == 'critical']\n",
    "                    if critical_issues:\n",
    "                        print(f\"\\nüö® Critical training issues detected at epoch {self.epoch}, batch {batch_idx}:\")\n",
    "                        for issue in critical_issues:\n",
    "                            print(f\"   {issue['message']}\")\n",
    "                            print(f\"   Recommendation: {issue['recommendation']}\")\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        epoch_loss = epoch_metrics['loss'] / epoch_metrics['batches']\n",
    "        epoch_acc = 100. * epoch_metrics['correct'] / max(epoch_metrics['total'], 1)\n",
    "        \n",
    "        # Step scheduler\n",
    "        if self.scheduler and not isinstance(self.scheduler, OneCycleLR):\n",
    "            self.scheduler.step()\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def _apply_augmentation(self, inputs, targets):\n",
    "        \"\"\"Apply configured data augmentation techniques.\"\"\"\n",
    "        # Apply image-level augmentations (would be done in dataloader normally)\n",
    "        # Here we simulate for demonstration\n",
    "        \n",
    "        # Apply data mixing augmentations\n",
    "        mixed = False\n",
    "        if self.mixup and np.random.random() < 0.5:\n",
    "            inputs, targets = self.mixup(inputs, targets)\n",
    "            mixed = True\n",
    "        elif self.cutmix and np.random.random() < 0.5:\n",
    "            inputs, targets = self.cutmix(inputs, targets)\n",
    "            mixed = True\n",
    "        \n",
    "        return inputs, targets\n",
    "    \n",
    "    def _standard_forward_pass(self, inputs, targets):\n",
    "        \"\"\"Standard forward pass with loss computation.\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(inputs)\n",
    "        \n",
    "        # Compute loss based on target type\n",
    "        if self._is_mixed_targets(targets):\n",
    "            targets_a, targets_b, lam = targets\n",
    "            loss = lam * self.criterion(outputs, targets_a) + (1 - lam) * self.criterion(outputs, targets_b)\n",
    "        else:\n",
    "            loss = self.criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Apply gradient clipping\n",
    "        if self.grad_clip > 0:\n",
    "            GradientClipper.clip_grad_norm(self.model.parameters(), max_norm=self.grad_clip)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss  # Return loss for monitoring\n",
    "    \n",
    "    def _sam_forward_pass(self, inputs, targets):\n",
    "        \"\"\"SAM optimizer forward pass requiring two passes.\"\"\"\n",
    "        # First forward pass (ascent step)\n",
    "        outputs = self.model(inputs)\n",
    "        if self._is_mixed_targets(targets):\n",
    "            targets_a, targets_b, lam = targets\n",
    "            loss = lam * self.criterion(outputs, targets_a) + (1 - lam) * self.criterion(outputs, targets_b)\n",
    "        else:\n",
    "            loss = self.criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.first_step(zero_grad=True)\n",
    "        \n",
    "        # Second forward pass (descent step)\n",
    "        outputs = self.model(inputs)\n",
    "        if self._is_mixed_targets(targets):\n",
    "            targets_a, targets_b, lam = targets\n",
    "            loss = lam * self.criterion(outputs, targets_a) + (1 - lam) * self.criterion(outputs, targets_b)\n",
    "        else:\n",
    "            loss = self.criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.second_step(zero_grad=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _is_mixed_targets(self, targets):\n",
    "        \"\"\"Check if targets are from mixing augmentation.\"\"\"\n",
    "        return isinstance(targets, tuple) and len(targets) == 3\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validate the model on validation set.\"\"\"\n",
    "        self.model.eval()\n",
    "        val_metrics = {\n",
    "            'loss': 0.0,\n",
    "            'correct': 0,\n",
    "            'total': 0,\n",
    "            'batches': 0\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.val_loader:\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                val_metrics['loss'] += loss.item()\n",
    "                val_metrics['batches'] += 1\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                val_metrics['total'] += targets.size(0)\n",
    "                val_metrics['correct'] += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        val_loss = val_metrics['loss'] / val_metrics['batches']\n",
    "        val_acc = 100. * val_metrics['correct'] / val_metrics['total']\n",
    "        \n",
    "        # Update best model\n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            self.best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "        \n",
    "        return val_loss, val_acc\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"Complete training loop with comprehensive logging.\"\"\"\n",
    "        print(f\"\\nüöÄ Starting Advanced Training for {num_epochs} epochs\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.epoch = epoch\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_acc = self.validate()\n",
    "            \n",
    "            # Update training history\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['train_acc'].append(train_acc)\n",
    "            self.training_history['val_acc'].append(val_acc)\n",
    "            self.training_history['learning_rates'].append(self.optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # Calculate health score\n",
    "            if self.enable_monitoring:\n",
    "                health_score = self.monitor.get_training_health_score()\n",
    "                self.training_history['health_scores'].append(health_score)\n",
    "            else:\n",
    "                health_score = 0\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start\n",
    "            \n",
    "            # Progress reporting\n",
    "            if epoch % max(1, num_epochs // 10) == 0 or epoch == num_epochs - 1:\n",
    "                print(f\"Epoch {epoch:3d}/{num_epochs}: \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "                      f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | \"\n",
    "                      f\"LR: {self.optimizer.param_groups[0]['lr']:.2e} | \"\n",
    "                      f\"Health: {health_score:.0f}/100 | \"\n",
    "                      f\"Time: {epoch_time:.1f}s\")\n",
    "                \n",
    "                # Check for critical issues\n",
    "                if self.enable_monitoring:\n",
    "                    recent_issues = self.monitor.detect_training_issues()\n",
    "                    critical_issues = [i for i in recent_issues if i['severity'] == 'critical']\n",
    "                    if critical_issues:\n",
    "                        print(f\"   üö® Critical issues: {len(critical_issues)}\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Final training report\n",
    "        self._generate_final_report(total_time)\n",
    "        \n",
    "        return self.training_history\n",
    "    \n",
    "    def _generate_final_report(self, total_time):\n",
    "        \"\"\"Generate comprehensive final training report.\"\"\"\n",
    "        print(f\"\\nüìä TRAINING COMPLETE - FINAL REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Basic metrics\n",
    "        final_train_loss = self.training_history['train_loss'][-1]\n",
    "        final_val_loss = self.training_history['val_loss'][-1]\n",
    "        final_train_acc = self.training_history['train_acc'][-1]\n",
    "        final_val_acc = self.training_history['val_acc'][-1]\n",
    "        \n",
    "        print(f\"üéØ Final Performance:\")\n",
    "        print(f\"   Training Loss: {final_train_loss:.4f}\")\n",
    "        print(f\"   Validation Loss: {final_val_loss:.4f}\")\n",
    "        print(f\"   Training Accuracy: {final_train_acc:.2f}%\")\n",
    "        print(f\"   Validation Accuracy: {final_val_acc:.2f}%\")\n",
    "        print(f\"   Best Validation Accuracy: {self.best_val_acc:.2f}%\")\n",
    "        \n",
    "        # Training dynamics\n",
    "        if len(self.training_history['train_loss']) > 1:\n",
    "            loss_improvement = (self.training_history['train_loss'][0] - final_train_loss) / self.training_history['train_loss'][0] * 100\n",
    "            print(f\"   Loss Improvement: {loss_improvement:.1f}%\")\n",
    "        \n",
    "        # Generalization analysis\n",
    "        generalization_gap = final_train_acc - final_val_acc\n",
    "        print(f\"   Generalization Gap: {generalization_gap:.2f}%\")\n",
    "        \n",
    "        if generalization_gap > 10:\n",
    "            print(f\"   ‚ö†Ô∏è Large generalization gap suggests overfitting\")\n",
    "        elif generalization_gap < 2:\n",
    "            print(f\"   ‚úÖ Good generalization\")\n",
    "        \n",
    "        # Performance metrics\n",
    "        print(f\"\\n‚ö° Training Efficiency:\")\n",
    "        print(f\"   Total Training Time: {total_time:.1f}s ({total_time/60:.1f}m)\")\n",
    "        print(f\"   Time per Epoch: {total_time/self.epoch:.1f}s\")\n",
    "        \n",
    "        # Health analysis\n",
    "        if self.training_history['health_scores']:\n",
    "            avg_health = np.mean(self.training_history['health_scores'])\n",
    "            final_health = self.training_history['health_scores'][-1]\n",
    "            print(f\"   Average Health Score: {avg_health:.1f}/100\")\n",
    "            print(f\"   Final Health Score: {final_health:.1f}/100\")\n",
    "        \n",
    "        # Generate monitoring report if available\n",
    "        if self.enable_monitoring:\n",
    "            print(f\"\\nüìã Training Monitoring Report:\")\n",
    "            monitor_report = self.monitor.generate_training_report()\n",
    "            print(monitor_report)\n",
    "\n",
    "print(\"‚úÖ Advanced training pipeline implemented!\")\n",
    "print(\"   ‚Ä¢ Comprehensive optimizer and scheduler support\")\n",
    "print(\"   ‚Ä¢ Integrated regularization and augmentation techniques\")\n",
    "print(\"   ‚Ä¢ Real-time training monitoring and issue detection\")\n",
    "print(\"   ‚Ä¢ Production-ready training framework\")\n",
    "```\n",
    "\n",
    "### 7.2 Training Pipeline Demonstration\n",
    "\n",
    "```python\n",
    "def create_training_demonstration():\n",
    "    \"\"\"Demonstrate the advanced training pipeline with different configurations.\"\"\"\n",
    "    \n",
    "    print(\"üéì Advanced Training Pipeline Demonstration:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create synthetic dataset for demonstration\n",
    "    class SyntheticDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, num_samples=1000, input_size=32, num_classes=10):\n",
    "            self.num_samples = num_samples\n",
    "            self.data = torch.randn(num_samples, 3, input_size, input_size)\n",
    "            self.targets = torch.randint(0, num_classes, (num_samples,))\n",
    "        \n",
    "        def __len__(self):\n",
    "            return self.num_samples\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx], self.targets[idx]\n",
    "    \n",
    "    # Create test model\n",
    "    class DemoNet(nn.Module):\n",
    "        def __init__(self, num_classes=10):\n",
    "            super().__init__()\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(32, 64, 3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.AdaptiveAvgPool2d(1)\n",
    "            )\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(64, num_classes)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = SyntheticDataset(num_samples=800)\n",
    "    val_dataset = SyntheticDataset(num_samples=200)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Define different training configurations\n",
    "    configurations = {\n",
    "        'basic_training': {\n",
    "            'optimizer': {'type': 'adam', 'lr': 0.001},\n",
    "            'scheduler': None,\n",
    "            'loss': {'type': 'crossentropy'},\n",
    "            'regularization': {},\n",
    "            'monitoring': {'enabled': True, 'frequency': 10}\n",
    "        },\n",
    "        'advanced_training': {\n",
    "            'optimizer': {'type': 'adamw', 'lr': 0.001, 'weight_decay': 0.01},\n",
    "            'scheduler': {'type': 'cosine', 'T_max': 20},\n",
    "            'loss': {'type': 'crossentropy', 'label_smoothing': 0.1, 'num_classes': 10},\n",
    "            'regularization': {\n",
    "                'mixup': True, 'mixup_alpha': 0.2,\n",
    "                'grad_clip': 1.0\n",
    "            },\n",
    "            'augmentation': {\n",
    "                'randaugment': True, 'randaugment_n': 2, 'randaugment_m': 10\n",
    "            },\n",
    "            'monitoring': {'enabled': True, 'frequency': 5}\n",
    "        },\n",
    "        'research_training': {\n",
    "            'optimizer': {'type': 'sam', 'base': 'sgd', 'lr': 0.1, 'rho': 0.05},\n",
    "            'scheduler': {'type': 'onecycle', 'max_lr': 0.1, 'total_steps': 100},\n",
    "            'loss': {'type': 'crossentropy', 'label_smoothing': 0.1, 'num_classes': 10},\n",
    "            'regularization': {\n",
    "                'mixup': True, 'mixup_alpha': 0.4,\n",
    "                'cutmix': True, 'cutmix_alpha': 1.0,\n",
    "                'grad_clip': 0.5\n",
    "            },\n",
    "            'monitoring': {'enabled': True, 'frequency': 3}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Run training experiments\n",
    "    experiment_results = {}\n",
    "    num_epochs = 10  # Short demo\n",
    "    \n",
    "    for config_name, config in configurations.items():\n",
    "        print(f\"\\nüß™ Running Experiment: {config_name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create model and trainer\n",
    "        model = DemoNet(num_classes=10)\n",
    "        trainer = AdvancedTrainer(model, train_loader, val_loader, config)\n",
    "        \n",
    "        # Train model\n",
    "        try:\n",
    "            training_history = trainer.train(num_epochs)\n",
    "            \n",
    "            # Store results\n",
    "            experiment_results[config_name] = {\n",
    "                'trainer': trainer,\n",
    "                'history': training_history,\n",
    "                'final_val_acc': training_history['val_acc'][-1],\n",
    "                'best_val_acc': trainer.best_val_acc,\n",
    "                'total_params': count_parameters(model)\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Experiment completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Experiment failed: {str(e)}\")\n",
    "            experiment_results[config_name] = {'error': str(e)}\n",
    "    \n",
    "    return experiment_results, configurations\n",
    "\n",
    "# Run training demonstration\n",
    "experiment_results, configurations = create_training_demonstration()\n",
    "```\n",
    "\n",
    "### 7.3 Training Results Analysis and Visualization\n",
    "\n",
    "```python\n",
    "def analyze_training_experiments(experiment_results, configurations):\n",
    "    \"\"\"Analyze and visualize training experiment results.\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìà Training Experiments Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Filter successful experiments\n",
    "    successful_experiments = {name: results for name, results in experiment_results.items() \n",
    "                            if 'error' not in results}\n",
    "    \n",
    "    if not successful_experiments:\n",
    "        print(\"‚ùå No successful experiments to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"Successful experiments: {len(successful_experiments)}\")\n",
    "    print(f\"\\n{'Configuration':<20} {'Final Val Acc':<15} {'Best Val Acc':<15} {'Parameters':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for name, results in successful_experiments.items():\n",
    "        final_acc = results['final_val_acc']\n",
    "        best_acc = results['best_val_acc']\n",
    "        params = results['total_params']\n",
    "        \n",
    "        print(f\"{name:<20} {final_acc:<15.2f}% {best_acc:<15.2f}% {params:<12,}\")\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Plot 1: Training loss curves\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    for name, results in successful_experiments.items():\n",
    "        history = results['history']\n",
    "        epochs = range(len(history['train_loss']))\n",
    "        ax1.plot(epochs, history['train_loss'], linewidth=2.5, label=f\"{name} (Train)\", alpha=0.8)\n",
    "        ax1.plot(epochs, history['val_loss'], linewidth=2.5, linestyle='--', label=f\"{name} (Val)\", alpha=0.8)\n",
    "    \n",
    "    ax1.set_title('Training and Validation Loss', fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Accuracy curves\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    for name, results in successful_experiments.items():\n",
    "        history = results['history']\n",
    "        epochs = range(len(history['train_acc']))\n",
    "        ax2.plot(epochs, history['train_acc'], linewidth=2.5, label=f\"{name} (Train)\", alpha=0.8)\n",
    "        ax2.plot(epochs, history['val_acc'], linewidth=2.5, linestyle='--', label=f\"{name} (Val)\", alpha=0.8)\n",
    "    \n",
    "    ax2.set_title('Training and Validation Accuracy', fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Learning rate schedules\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    for name, results in successful_experiments.items():\n",
    "        history = results['history']\n",
    "        if 'learning_rates' in history:\n",
    "            epochs = range(len(history['learning_rates']))\n",
    "            ax3.plot(epochs, history['learning_rates'], linewidth=3, label=name, alpha=0.8)\n",
    "    \n",
    "    ax3.set_title('Learning Rate Schedules', fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_yscale('log')\n",
    "    \n",
    "    # Plot 4: Health scores\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    for name, results in successful_experiments.items():\n",
    "        history = results['history']\n",
    "        if 'health_scores' in history and history['health_scores']:\n",
    "            epochs = range(len(history['health_scores']))\n",
    "            ax4.plot(epochs, history['health_scores'], linewidth=3, label=name, alpha=0.8)\n",
    "    \n",
    "    ax4.set_title('Training Health Scores', fontweight='bold')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Health Score (0-100)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.axhline(y=80, color='green', linestyle='--', alpha=0.5, label='Excellent')\n",
    "    ax4.axhline(y=60, color='orange', linestyle='--', alpha=0.5, label='Good')\n",
    "    \n",
    "    # Plot 5: Final performance comparison\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    exp_names = list(successful_experiments.keys())\n",
    "    final_accs = [successful_experiments[name]['final_val_acc'] for name in exp_names]\n",
    "    best_accs = [successful_experiments[name]['best_val_acc'] for name in exp_names]\n",
    "    \n",
    "    x = np.arange(len(exp_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax5.bar(x - width/2, final_accs, width, label='Final Val Acc', alpha=0.8, color='lightblue')\n",
    "    bars2 = ax5.bar(x + width/2, best_accs, width, label='Best Val Acc', alpha=0.8, color='lightgreen')\n",
    "    \n",
    "    ax5.set_xlabel('Configuration')\n",
    "    ax5.set_ylabel('Accuracy (%)')\n",
    "    ax5.set_title('Final Performance Comparison', fontweight='bold')\n",
    "    ax5.set_xticks(x)\n",
    "    ax5.set_xticklabels([name.replace('_', '\\n') for name in exp_names], fontsize=9)\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax5.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                     f'{height:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Plot 6: Configuration complexity analysis\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    \n",
    "    # Calculate complexity scores for each configuration\n",
    "    complexity_scores = []\n",
    "    effectiveness_scores = []\n",
    "    \n",
    "    for name in exp_names:\n",
    "        config = configurations[name]\n",
    "        results = successful_experiments[name]\n",
    "        \n",
    "        # Complexity based on features used\n",
    "        complexity = 0\n",
    "        if config.get('scheduler'): complexity += 2\n",
    "        if config.get('regularization'): complexity += len(config['regularization'])\n",
    "        if config.get('augmentation'): complexity += len(config['augmentation'])\n",
    "        if config.get('loss', {}).get('label_smoothing'): complexity += 1\n",
    "        \n",
    "        complexity_scores.append(complexity)\n",
    "        effectiveness_scores.append(results['best_val_acc'])\n",
    "    \n",
    "    colors = ['blue', 'orange', 'green'][:len(exp_names)]\n",
    "    scatter = ax6.scatter(complexity_scores, effectiveness_scores, s=200, alpha=0.7, c=colors)\n",
    "    \n",
    "    for i, name in enumerate(exp_names):\n",
    "        ax6.annotate(name.replace('_', '\\n'), (complexity_scores[i], effectiveness_scores[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax6.set_xlabel('Configuration Complexity')\n",
    "    ax6.set_ylabel('Best Validation Accuracy (%)')\n",
    "    ax6.set_title('Complexity vs Performance Trade-off', fontweight='bold')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 7: Training dynamics comparison\n",
    "    ax7 = fig.add_subplot(gs[2, :])\n",
    "    \n",
    "    # Create subplot for each experiment\n",
    "    n_experiments = len(successful_experiments)\n",
    "    if n_experiments > 0:\n",
    "        for i, (name, results) in enumerate(successful_experiments.items()):\n",
    "            history = results['history']\n",
    "            \n",
    "            # Calculate generalization gap over time\n",
    "            if len(history['train_acc']) == len(history['val_acc']):\n",
    "                gen_gaps = [t - v for t, v in zip(history['train_acc'], history['val_acc'])]\n",
    "                epochs = range(len(gen_gaps))\n",
    "                \n",
    "                ax7.plot(epochs, gen_gaps, linewidth=3, label=f\"{name} Gen Gap\", alpha=0.8)\n",
    "        \n",
    "        ax7.set_title('Generalization Gap Evolution', fontweight='bold')\n",
    "        ax7.set_xlabel('Epoch')\n",
    "        ax7.set_ylabel('Train Acc - Val Acc (%)')\n",
    "        ax7.legend()\n",
    "        ax7.grid(True, alpha=0.3)\n",
    "        ax7.axhline(y=0, color='red', linestyle='-', alpha=0.5, label='Perfect Generalization')\n",
    "        ax7.axhline(y=5, color='orange', linestyle='--', alpha=0.5, label='Acceptable Gap')\n",
    "        ax7.axhline(y=10, color='red', linestyle='--', alpha=0.5, label='Overfitting Risk')\n",
    "    \n",
    "    plt.suptitle('Comprehensive Training Pipeline Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.savefig(os.path.join(results_dir, 'training_pipeline_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Run comprehensive analysis\n",
    "analyze_training_experiments(experiment_results, configurations)\n",
    "\n",
    "print(f\"\\nüí° Advanced Training Pipeline Insights:\")\n",
    "pipeline_insights = [\n",
    "    \"‚Ä¢ Configuration complexity doesn't always correlate with better performance\",\n",
    "    \"‚Ä¢ Advanced optimizers (SAM, AdamW) often provide better generalization\",\n",
    "    \"‚Ä¢ Learning rate scheduling is crucial for optimal convergence\",\n",
    "    \"‚Ä¢ Regularization techniques work synergistically when combined properly\",\n",
    "    \"‚Ä¢ Health monitoring enables early detection of training issues\",\n",
    "    \"‚Ä¢ Data augmentation provides consistent improvements across architectures\",\n",
    "    \"‚Ä¢ Pipeline flexibility allows adaptation to different problem domains\"\n",
    "]\n",
    "\n",
    "for insight in pipeline_insights:\n",
    "    print(f\"  {insight}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Advanced training pipeline analysis complete!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Summary and Best Practices\n",
    "\n",
    "### 8.1 Comprehensive Training Techniques Summary\n",
    "\n",
    "```python\n",
    "print(\"=== 8.1 Advanced Training Techniques - Comprehensive Summary ===\\n\")\n",
    "\n",
    "def create_training_summary():\n",
    "    \"\"\"Generate comprehensive summary of all training techniques covered.\"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'analysis_timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'techniques_implemented': {},\n",
    "        'key_innovations': [],\n",
    "        'performance_insights': {},\n",
    "        'best_practices': [],\n",
    "        'selection_guidelines': {}\n",
    "    }\n",
    "    \n",
    "    # Categorize implemented techniques\n",
    "    summary['techniques_implemented'] = {\n",
    "        'Regularization': [\n",
    "            'DropBlock2D for spatial regularization',\n",
    "            'StochasticDepth for training efficiency',\n",
    "            'MixUp for linear sample mixing',\n",
    "            'CutMix for regional replacement',\n",
    "            'LabelSmoothing for confidence calibration'\n",
    "        ],\n",
    "        'Optimization': [\n",
    "            'AdamW with decoupled weight decay',\n",
    "            'LAMB for large batch training',\n",
    "            'SAM for sharpness-aware minimization',\n",
    "            'Advanced gradient clipping strategies'\n",
    "        ],\n",
    "        'Learning Rate Scheduling': [\n",
    "            'WarmupScheduler for stable initialization',\n",
    "            'CosineAnnealingWarmRestarts (SGDR)',\n",
    "            'OneCycleLR for super-convergence',\n",
    "            'PolynomialLR for controlled decay'\n",
    "        ],\n",
    "        'Data Augmentation': [\n",
    "            'RandAugment with automated policies',\n",
    "            'TrivialAugmentWide for simplicity',\n",
    "            'Random Erasing for occlusion robustness',\n",
    "            'Grid Shuffle for spatial reasoning'\n",
    "        ],\n",
    "        'Training Monitoring': [\n",
    "            'Comprehensive metrics tracking',\n",
    "            'Automated issue detection',\n",
    "            'Training health scoring',\n",
    "            'Real-time debugging capabilities'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Key innovations and principles\n",
    "    summary['key_innovations'] = [\n",
    "        \"Decoupled weight decay in AdamW for better generalization\",\n",
    "        \"Sharpness-aware minimization for flatter loss landscapes\", \n",
    "        \"Automated augmentation policy learning\",\n",
    "        \"Real-time training health monitoring and issue detection\",\n",
    "        \"Integrated pipeline combining multiple advanced techniques\",\n",
    "        \"Adaptive gradient clipping strategies\",\n",
    "        \"Multi-level regularization stacking\"\n",
    "    ]\n",
    "    \n",
    "    # Best practices learned\n",
    "    summary['best_practices'] = [\n",
    "        \"Always use learning rate warmup for large models\",\n",
    "        \"Combine multiple regularization techniques synergistically\", \n",
    "        \"Monitor gradient norms to prevent training instabilities\",\n",
    "        \"Use appropriate learning rate scheduling for your training budget\",\n",
    "        \"Apply data augmentation suited to your domain\",\n",
    "        \"Implement comprehensive training monitoring for debugging\",\n",
    "        \"Choose optimizers based on your specific constraints\",\n",
    "        \"Balance model complexity with training techniques complexity\"\n",
    "    ]\n",
    "    \n",
    "    # Selection guidelines\n",
    "    summary['selection_guidelines'] = {\n",
    "        'Small Datasets': {\n",
    "            'regularization': 'Heavy (MixUp, CutMix, strong augmentation)',\n",
    "            'optimizer': 'AdamW or SGD with momentum',\n",
    "            'scheduling': 'Cosine annealing or OneCycle',\n",
    "            'monitoring': 'Essential for overfitting detection'\n",
    "        },\n",
    "        'Large Datasets': {\n",
    "            'regularization': 'Moderate (dropout, batch norm)',\n",
    "            'optimizer': 'AdamW or LAMB for distributed training',\n",
    "            'scheduling': 'Linear decay or cosine annealing',\n",
    "            'monitoring': 'Focus on convergence and efficiency'\n",
    "        },\n",
    "        'Research Settings': {\n",
    "            'regularization': 'Experimental combinations',\n",
    "            'optimizer': 'SAM for better generalization',\n",
    "            'scheduling': 'Advanced schedules (SGDR, OneCycle)',\n",
    "            'monitoring': 'Comprehensive analysis and debugging'\n",
    "        },\n",
    "        'Production Systems': {\n",
    "            'regularization': 'Proven techniques (dropout, batch norm)',\n",
    "            'optimizer': 'AdamW for reliability',\n",
    "            'scheduling': 'Simple and robust (step or cosine)',\n",
    "            'monitoring': 'Health scoring and automated alerts'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate comprehensive summary\n",
    "training_summary = create_training_summary()\n",
    "\n",
    "print(\"üéØ ADVANCED TRAINING TECHNIQUES - FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n‚è∞ Analysis completed: {training_summary['analysis_timestamp']}\")\n",
    "\n",
    "print(f\"\\nüìö Techniques Implemented by Category:\")\n",
    "for category, techniques in training_summary['techniques_implemented'].items():\n",
    "    print(f\"\\n  {category}:\")\n",
    "    for i, technique in enumerate(techniques, 1):\n",
    "        print(f\"    {i}. {technique}\")\n",
    "\n",
    "print(f\"\\nüî¨ Key Innovations Explored:\")\n",
    "for i, innovation in enumerate(training_summary['key_innovations'], 1):\n",
    "    print(f\"  {i}. {innovation}\")\n",
    "\n",
    "print(f\"\\nüí° Best Practices Learned:\")\n",
    "for i, practice in enumerate(training_summary['best_practices'], 1):\n",
    "    print(f\"  {i}. {practice}\")\n",
    "\n",
    "print(f\"\\nüéØ Selection Guidelines by Use Case:\")\n",
    "for use_case, guidelines in training_summary['selection_guidelines'].items():\n",
    "    print(f\"\\n  {use_case}:\")\n",
    "    for aspect, recommendation in guidelines.items():\n",
    "        print(f\"    ‚Ä¢ {aspect.title()}: {recommendation}\")\n",
    "```\n",
    "\n",
    "### 8.2 Advanced Training Checklist and Decision Framework\n",
    "\n",
    "```python\n",
    "def create_training_decision_framework():\n",
    "    \"\"\"Create decision framework for selecting training techniques.\"\"\"\n",
    "    \n",
    "    print(f\"\\nüß≠ Advanced Training Decision Framework:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    decision_tree = {\n",
    "        'Dataset Size': {\n",
    "            'Small (<10K samples)': {\n",
    "                'priority': 'Prevent overfitting',\n",
    "                'techniques': [\n",
    "                    'Heavy data augmentation (RandAugment)',\n",
    "                    'Strong regularization (MixUp, CutMix)',\n",
    "                    'Label smoothing',\n",
    "                    'Early stopping with patience'\n",
    "                ]\n",
    "            },\n",
    "            'Medium (10K-100K)': {\n",
    "                'priority': 'Balance generalization and optimization',\n",
    "                'techniques': [\n",
    "                    'Moderate augmentation',\n",
    "                    'Standard regularization (dropout, batch norm)',\n",
    "                    'Learning rate scheduling',\n",
    "                    'Gradient clipping'\n",
    "                ]\n",
    "            },\n",
    "            'Large (>100K)': {\n",
    "                'priority': 'Optimize training efficiency',\n",
    "                'techniques': [\n",
    "                    'Efficient optimizers (AdamW, LAMB)',\n",
    "                    'Large batch training techniques',\n",
    "                    'Distributed training considerations',\n",
    "                    'Computational efficiency focus'\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        'Model Complexity': {\n",
    "            'Simple (few layers)': {\n",
    "                'focus': 'Maximize learning capacity',\n",
    "                'recommendations': [\n",
    "                    'Higher learning rates',\n",
    "                    'Less aggressive regularization',\n",
    "                    'Focus on data augmentation'\n",
    "                ]\n",
    "            },\n",
    "            'Deep (many layers)': {\n",
    "                'focus': 'Ensure stable training',\n",
    "                'recommendations': [\n",
    "                    'Learning rate warmup',\n",
    "                    'Gradient clipping',\n",
    "                    'Residual connections',\n",
    "                    'Careful initialization'\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        'Training Budget': {\n",
    "            'Limited Time': {\n",
    "                'strategy': 'Fast convergence',\n",
    "                'methods': [\n",
    "                    'OneCycle learning rate',\n",
    "                    'Larger batch sizes',\n",
    "                    'Efficient augmentation (TrivialAugment)',\n",
    "                    'Pre-trained models'\n",
    "                ]\n",
    "            },\n",
    "            'Extensive Time': {\n",
    "                'strategy': 'Optimal performance',\n",
    "                'methods': [\n",
    "                    'Comprehensive hyperparameter search',\n",
    "                    'Advanced scheduling (SGDR)',\n",
    "                    'Multiple regularization techniques',\n",
    "                    'Ensemble methods'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Print decision framework\n",
    "    for main_factor, sub_factors in decision_tree.items():\n",
    "        print(f\"\\nüìã {main_factor}:\")\n",
    "        for sub_factor, details in sub_factors.items():\n",
    "            print(f\"\\n  {sub_factor}:\")\n",
    "            if 'priority' in details:\n",
    "                print(f\"    Priority: {details['priority']}\")\n",
    "            if 'focus' in details:\n",
    "                print(f\"    Focus: {details['focus']}\")\n",
    "            if 'strategy' in details:\n",
    "                print(f\"    Strategy: {details['strategy']}\")\n",
    "            \n",
    "            # Print techniques/recommendations\n",
    "            technique_key = next((key for key in ['techniques', 'recommendations', 'methods'] if key in details), None)\n",
    "            if technique_key:\n",
    "                print(f\"    {technique_key.title()}:\")\n",
    "                for technique in details[technique_key]:\n",
    "                    print(f\"      ‚Ä¢ {technique}\")\n",
    "    \n",
    "    return decision_tree\n",
    "\n",
    "def create_training_checklist():\n",
    "    \"\"\"Create comprehensive training checklist.\"\"\"\n",
    "    \n",
    "    print(f\"\\n‚úÖ Advanced Training Techniques Checklist:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    checklist = {\n",
    "        'Pre-Training Setup': [\n",
    "            '‚ñ° Dataset analysis and preprocessing completed',\n",
    "            '‚ñ° Model architecture appropriate for task',\n",
    "            '‚ñ° Baseline training configuration established',\n",
    "            '‚ñ° Evaluation metrics and validation strategy defined',\n",
    "            '‚ñ° Training monitoring and logging setup'\n",
    "        ],\n",
    "        'Optimization Configuration': [\n",
    "            '‚ñ° Optimizer selection based on task requirements',\n",
    "            '‚ñ° Learning rate range testing completed',\n",
    "            '‚ñ° Weight decay value tuned if using AdamW',\n",
    "            '‚ñ° Gradient clipping threshold set if needed',\n",
    "            '‚ñ° Batch size optimized for hardware and stability'\n",
    "        ],\n",
    "        'Regularization Strategy': [\n",
    "            '‚ñ° Dropout rates tuned for architecture',\n",
    "            '‚ñ° Data augmentation policy selected and tested',\n",
    "            '‚ñ° Label smoothing considered for classification',\n",
    "            '‚ñ° Batch normalization placement optimized',\n",
    "            '‚ñ° Advanced techniques (MixUp, CutMix) evaluated'\n",
    "        ],\n",
    "        'Learning Rate Scheduling': [\n",
    "            '‚ñ° Warmup period configured for stable start',\n",
    "            '‚ñ° Main scheduling strategy selected',\n",
    "            '‚ñ° Schedule parameters tuned for training length',\n",
    "            '‚ñ° Minimum learning rate threshold set',\n",
    "            '‚ñ° Schedule compatibility with optimizer verified'\n",
    "        ],\n",
    "        'Training Monitoring': [\n",
    "            '‚ñ° Loss and accuracy tracking enabled',\n",
    "            '‚ñ° Gradient norm monitoring configured',\n",
    "            '‚ñ° Health scoring system activated',\n",
    "            '‚ñ° Issue detection thresholds calibrated',\n",
    "            '‚ñ° Checkpoint saving strategy implemented'\n",
    "        ],\n",
    "        'Validation and Testing': [\n",
    "            '‚ñ° Validation frequency optimized',\n",
    "            '‚ñ° Early stopping criteria defined',\n",
    "            '‚ñ° Best model saving logic implemented',\n",
    "            '‚ñ° Test set evaluation reserved for final assessment',\n",
    "            '‚ñ° Generalization gap monitoring active'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Print checklist\n",
    "    for category, items in checklist.items():\n",
    "        print(f\"\\nüìù {category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  {item}\")\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "# Generate decision framework and checklist\n",
    "decision_framework = create_training_decision_framework()\n",
    "training_checklist = create_training_checklist()\n",
    "\n",
    "print(f\"\\nüéì Graduation Criteria - What You've Mastered:\")\n",
    "graduation_criteria = [\n",
    "    \"‚úÖ Advanced regularization techniques and their synergistic combinations\",\n",
    "    \"‚úÖ Modern optimization algorithms and their appropriate use cases\", \n",
    "    \"‚úÖ Sophisticated learning rate scheduling strategies\",\n",
    "    \"‚úÖ Cutting-edge data augmentation methods and policies\",\n",
    "    \"‚úÖ Comprehensive training monitoring and debugging systems\",\n",
    "    \"‚úÖ Production-ready training pipeline implementation\",\n",
    "    \"‚úÖ Training technique selection based on problem constraints\",\n",
    "    \"‚úÖ Performance analysis and interpretation of training dynamics\",\n",
    "    \"‚úÖ Integration of multiple advanced techniques in unified framework\",\n",
    "    \"‚úÖ Best practices for stable and efficient neural network training\"\n",
    "]\n",
    "\n",
    "for criterion in graduation_criteria:\n",
    "    print(f\"  {criterion}\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps and Advanced Applications:\")\n",
    "next_steps = [\n",
    "    \"üî¨ Apply techniques to your specific domain and datasets\",\n",
    "    \"üìä Experiment with technique combinations for your use cases\", \n",
    "    \"üè≠ Implement production training pipelines with monitoring\",\n",
    "    \"üìö Explore domain-specific training techniques (NLP, Computer Vision)\",\n",
    "    \"üåê Scale training to distributed and multi-GPU settings\",\n",
    "    \"üß™ Contribute to training technique research and development\",\n",
    "    \"üéØ Optimize training for edge deployment and mobile constraints\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"  {step}\")\n",
    "```\n",
    "\n",
    "### 8.3 Final Resource Summary and Documentation\n",
    "\n",
    "```python\n",
    "# Save comprehensive training results and documentation\n",
    "training_artifacts = {\n",
    "    'summary': training_summary,\n",
    "    'decision_framework': decision_framework,\n",
    "    'checklist': training_checklist,\n",
    "    'experiment_results': experiment_results if 'experiment_results' in locals() else {},\n",
    "    'generated_files': []\n",
    "}\n",
    "\n",
    "# List all generated files\n",
    "try:\n",
    "    result_files = [f for f in os.listdir(results_dir) if os.path.isfile(os.path.join(results_dir, f))]\n",
    "    training_artifacts['generated_files'] = result_files\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Save final artifacts\n",
    "import json\n",
    "summary_file = os.path.join(results_dir, 'training_techniques_summary.json')\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(training_artifacts, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nüìÅ Generated Training Resources:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "resource_categories = {\n",
    "    'Analysis Reports': [\n",
    "        'regularization_analysis.png - Comprehensive regularization technique analysis',\n",
    "        'optimizer_comparison.png - Modern optimizer performance comparison',\n",
    "        'lr_schedules_analysis.png - Learning rate scheduling strategies',\n",
    "        'augmentation_analysis.png - Data augmentation effectiveness study',\n",
    "        'training_diagnostics.png - Training stability and debugging analysis',\n",
    "        'training_pipeline_analysis.png - Integrated pipeline performance'\n",
    "    ],\n",
    "    'Implementation Code': [\n",
    "        'Advanced regularization classes (DropBlock, StochasticDepth, etc.)',\n",
    "        'Modern optimizer implementations (AdamW, LAMB, SAM)',\n",
    "        'Learning rate scheduler collection',\n",
    "        'Data augmentation techniques (RandAugment, TrivialAugment)',\n",
    "        'Training monitoring and debugging system',\n",
    "        'Complete advanced training pipeline framework'\n",
    "    ],\n",
    "    'Documentation': [\n",
    "        'training_techniques_summary.json - Complete analysis results',\n",
    "        'Decision framework for technique selection',\n",
    "        'Training checklist and best practices',\n",
    "        'Performance benchmarks and comparisons'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, resources in resource_categories.items():\n",
    "    print(f\"\\nüìã {category}:\")\n",
    "    for resource in resources:\n",
    "        print(f\"   ‚Ä¢ {resource}\")\n",
    "\n",
    "print(f\"\\nüíæ All results saved to: {results_dir}\")\n",
    "print(f\"üìÑ Complete summary: {summary_file}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"üéâ ADVANCED TRAINING TECHNIQUES MASTERY COMPLETE! üéâ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "final_message = \"\"\"\n",
    "üåü Congratulations! You have successfully mastered advanced neural network training techniques.\n",
    "\n",
    "üéØ What You've Achieved:\n",
    "   ‚Ä¢ Deep understanding of modern training methodologies\n",
    "   ‚Ä¢ Practical implementation of cutting-edge techniques\n",
    "   ‚Ä¢ Comprehensive training pipeline development skills\n",
    "   ‚Ä¢ Advanced debugging and monitoring capabilities\n",
    "   ‚Ä¢ Production-ready training system design\n",
    "\n",
    "üöÄ You're Now Ready To:\n",
    "   ‚Ä¢ Train state-of-the-art models with confidence\n",
    "   ‚Ä¢ Debug and optimize complex training scenarios\n",
    "   ‚Ä¢ Design custom training pipelines for specific domains\n",
    "   ‚Ä¢ Contribute to the advancement of training methodologies\n",
    "\n",
    "üí™ Keep pushing the boundaries of what's possible with neural networks!\n",
    "\"\"\"\n",
    "\n",
    "print(final_message)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This comprehensive notebook has taken you through the complete landscape of advanced neural network training techniques. You've mastered the essential skills needed to train robust, high-performance models in real-world scenarios.\n",
    "\n",
    "### üèÜ What You've Accomplished\n",
    "\n",
    "- **Advanced Regularization**: Implemented cutting-edge techniques like DropBlock, StochasticDepth, and advanced data mixing methods\n",
    "- **Modern Optimization**: Mastered AdamW, LAMB, SAM, and their appropriate applications\n",
    "- **Learning Rate Mastery**: Designed sophisticated scheduling strategies including warmup, SGDR, and OneCycle\n",
    "- **Data Augmentation Excellence**: Applied automated augmentation policies and advanced techniques\n",
    "- **Training Stability**: Built comprehensive monitoring and debugging systems\n",
    "- **Production Readiness**: Created integrated training pipelines suitable for real-world deployment\n",
    "\n",
    "### üöÄ Ready for Advanced Applications\n",
    "\n",
    "With this foundation, you're prepared to tackle:\n",
    "- Large-scale distributed training\n",
    "- Domain-specific optimization challenges  \n",
    "- Research-level training methodologies\n",
    "- Production deployment requirements\n",
    "- Custom training system development\n",
    "\n",
    "The journey through advanced training techniques prepares you for the cutting edge of deep learning, where the quality of your training methodology often determines the success of your models.\n",
    "\n",
    "**Keep innovating and pushing the boundaries of what's possible! üåü**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
