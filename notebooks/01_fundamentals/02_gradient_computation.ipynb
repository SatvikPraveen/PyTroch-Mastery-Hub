{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94e4a38",
   "metadata": {},
   "source": [
    "# Gradient Computation Deep Dive: PyTorch Mastery Hub\n",
    "\n",
    "**Understanding PyTorch's Automatic Differentiation Engine**\n",
    "\n",
    "**Authors:** PyTorch Mastery Hub Team  \n",
    "**Institution:** Advanced Deep Learning Education  \n",
    "**Course:** PyTorch Fundamentals & Advanced Techniques  \n",
    "**Date:** December 2024\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a comprehensive exploration of PyTorch's automatic differentiation system (autograd). We'll master gradient computation from fundamentals to advanced applications, building intuition through interactive visualizations and practical examples.\n",
    "\n",
    "## Key Objectives\n",
    "1. Master PyTorch's autograd system and computational graphs\n",
    "2. Understand gradient computation mechanics and vector operations\n",
    "3. Explore advanced gradient techniques for research and optimization\n",
    "4. Build intuition through interactive visualizations and real-time demos\n",
    "5. Learn performance optimization and debugging techniques\n",
    "6. Apply gradients to practical scenarios like adversarial examples and optimization\n",
    "\n",
    "## üìö Learning Path\n",
    "- **Prerequisites:** Basic PyTorch tensors, calculus fundamentals, chain rule\n",
    "- **Difficulty:** Intermediate to Advanced\n",
    "- **Duration:** 90-120 minutes\n",
    "- **Next Steps:** Custom autograd functions, neural network architectures\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Environment Configuration\n",
    "\n",
    "```python\n",
    "# Essential imports for comprehensive gradient computation tutorial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import warnings\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced imports for specialized functionality\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import json\n",
    "\n",
    "# Configure environment\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path('../results/notebooks/gradient_computation')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"üî• PyTorch Mastery Hub - Gradient Computation Deep Dive\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üì± Device: {device}\")\n",
    "print(f\"üé® PyTorch version: {torch.__version__}\")\n",
    "print(f\"üìä NumPy version: {np.__version__}\")\n",
    "print(f\"üìÅ Results directory: {results_dir}\")\n",
    "print(\"‚ú® Ready to explore automatic differentiation!\\n\")\n",
    "```\n",
    "\n",
    "## 2. Autograd Fundamentals: Building the Foundation\n",
    "\n",
    "### 2.1 Your First Gradient Computation\n",
    "\n",
    "```python\n",
    "def demonstrate_basic_gradients():\n",
    "    \"\"\"Demonstrate fundamental gradient computation concepts\"\"\"\n",
    "    \n",
    "    print(\"=== 2.1 Basic Gradient Computation ===\\n\")\n",
    "    \n",
    "    # Create tensor with gradient tracking\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    \n",
    "    print(f\"üî¢ Input Analysis:\")\n",
    "    print(f\"  Input tensor x: {x}\")\n",
    "    print(f\"  Requires grad: {x.requires_grad}\")\n",
    "    print(f\"  Gradient function: {x.grad_fn}\")\n",
    "    print(f\"  Is leaf node: {x.is_leaf}\")\n",
    "    print(f\"  Current gradient: {x.grad}\")\n",
    "    \n",
    "    # Define mathematical function: f(x) = x¬≤ + 3x + 1\n",
    "    y = x**2 + 3*x + 1\n",
    "    \n",
    "    print(f\"\\nüìê Function Definition:\")\n",
    "    print(f\"  Function: f(x) = x¬≤ + 3x + 1\")\n",
    "    print(f\"  f({x.item()}) = {y.item()}\")\n",
    "    print(f\"  Output grad_fn: {y.grad_fn}\")\n",
    "    print(f\"  Output is leaf: {y.is_leaf}\")\n",
    "    \n",
    "    # Compute analytical gradient for verification\n",
    "    analytical_grad = 2*x.item() + 3\n",
    "    \n",
    "    # Compute gradient using backpropagation\n",
    "    y.backward()\n",
    "    \n",
    "    print(f\"\\nüßÆ Gradient Analysis:\")\n",
    "    print(f\"  Computed gradient df/dx: {x.grad.item()}\")\n",
    "    print(f\"  Analytical gradient (2x + 3): {analytical_grad}\")\n",
    "    print(f\"  Absolute difference: {abs(x.grad.item() - analytical_grad):.2e}\")\n",
    "    print(f\"  ‚úÖ Gradients match: {abs(x.grad.item() - analytical_grad) < 1e-6}\")\n",
    "    \n",
    "    return x.grad.item(), analytical_grad\n",
    "\n",
    "# Execute basic gradient demonstration\n",
    "computed_grad, analytical_grad = demonstrate_basic_gradients()\n",
    "```\n",
    "\n",
    "### 2.2 Computational Graph Deep Dive\n",
    "\n",
    "```python\n",
    "def explore_computational_graph():\n",
    "    \"\"\"Explore computational graph construction and traversal\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 2.2 Computational Graph Analysis ===\\n\")\n",
    "    \n",
    "    # Build complex computation with multiple operations\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    \n",
    "    # Create computation chain\n",
    "    operations = []\n",
    "    \n",
    "    a = x * 3          # Multiplication: a = 3x\n",
    "    operations.append((\"a = x * 3\", a, a.grad_fn))\n",
    "    \n",
    "    b = a + 1          # Addition: b = 3x + 1  \n",
    "    operations.append((\"b = a + 1\", b, b.grad_fn))\n",
    "    \n",
    "    c = b ** 2         # Power: c = (3x + 1)¬≤\n",
    "    operations.append((\"c = b ** 2\", c, c.grad_fn))\n",
    "    \n",
    "    d = torch.sin(c)   # Trigonometric: d = sin((3x + 1)¬≤)\n",
    "    operations.append((\"d = sin(c)\", d, d.grad_fn))\n",
    "    \n",
    "    e = d.mean()       # Reduction: e = mean(d)\n",
    "    operations.append((\"e = d.mean()\", e, e.grad_fn))\n",
    "    \n",
    "    print(f\"üîó Computation Chain Analysis:\")\n",
    "    print(f\"  Input x = {x.item()}\")\n",
    "    print()\n",
    "    \n",
    "    for i, (description, tensor, grad_fn) in enumerate(operations, 1):\n",
    "        print(f\"  Step {i}: {description}\")\n",
    "        print(f\"    Value: {tensor.item():.6f}\")\n",
    "        print(f\"    Grad function: {grad_fn}\")\n",
    "        print(f\"    Shape: {tensor.shape}\")\n",
    "        print()\n",
    "    \n",
    "    # Compute gradient through the entire chain\n",
    "    print(\"üîÑ Backward Pass Analysis:\")\n",
    "    e.backward()\n",
    "    \n",
    "    # Manual gradient computation for verification\n",
    "    # e = sin((3x + 1)¬≤)\n",
    "    # de/dx = cos((3x + 1)¬≤) * 2(3x + 1) * 3 = 6(3x + 1) * cos((3x + 1)¬≤)\n",
    "    manual_grad = 6 * (3 * x.item() + 1) * np.cos((3 * x.item() + 1)**2)\n",
    "    \n",
    "    print(f\"  Computed gradient de/dx: {x.grad.item():.6f}\")\n",
    "    print(f\"  Manual gradient: {manual_grad:.6f}\")\n",
    "    print(f\"  Relative error: {abs(x.grad.item() - manual_grad) / abs(manual_grad):.2e}\")\n",
    "    print(f\"  ‚úÖ Chain rule applied correctly: {abs(x.grad.item() - manual_grad) < 1e-5}\")\n",
    "    \n",
    "    return operations, x.grad.item(), manual_grad\n",
    "\n",
    "# Execute computational graph exploration\n",
    "operations, computed_chain_grad, manual_chain_grad = explore_computational_graph()\n",
    "```\n",
    "\n",
    "## 3. Advanced Gradient Concepts: Vectors and Jacobians\n",
    "\n",
    "### 3.1 Vector Functions and Jacobian Matrices\n",
    "\n",
    "```python\n",
    "def demonstrate_vector_gradients():\n",
    "    \"\"\"Demonstrate gradient computation for vector-valued functions\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 3.1 Vector Functions and Jacobians ===\\n\")\n",
    "    \n",
    "    # Vector input: 3D input space\n",
    "    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "    \n",
    "    print(f\"üìä Vector Function Analysis:\")\n",
    "    print(f\"  Input vector x: {x.detach().numpy()}\")\n",
    "    print(f\"  Input dimension: {x.shape[0]}\")\n",
    "    \n",
    "    # Define vector function: f: R¬≥ ‚Üí R¬≥\n",
    "    # f(x) = [x‚ÇÅ¬≤, x‚ÇÅx‚ÇÇ, x‚ÇÇx‚ÇÉ]\n",
    "    y = torch.stack([\n",
    "        x[0]**2,           # y‚ÇÅ = x‚ÇÅ¬≤\n",
    "        x[0] * x[1],       # y‚ÇÇ = x‚ÇÅx‚ÇÇ  \n",
    "        x[1] * x[2]        # y‚ÇÉ = x‚ÇÇx‚ÇÉ\n",
    "    ])\n",
    "    \n",
    "    print(f\"  Function definitions:\")\n",
    "    print(f\"    y‚ÇÅ = x‚ÇÅ¬≤ = {y[0].item():.4f}\")\n",
    "    print(f\"    y‚ÇÇ = x‚ÇÅx‚ÇÇ = {y[1].item():.4f}\")\n",
    "    print(f\"    y‚ÇÉ = x‚ÇÇx‚ÇÉ = {y[2].item():.4f}\")\n",
    "    print(f\"  Output vector y: {y.detach().numpy()}\")\n",
    "    \n",
    "    # Compute Jacobian matrix: ‚àÇy/‚àÇx\n",
    "    jacobian = torch.zeros(3, 3)\n",
    "    \n",
    "    print(f\"\\nüßÆ Jacobian Computation:\")\n",
    "    for i in range(3):\n",
    "        # Clear previous gradients\n",
    "        if x.grad is not None:\n",
    "            x.grad.zero_()\n",
    "        \n",
    "        # Compute gradient for output component i\n",
    "        y[i].backward(retain_graph=True)\n",
    "        \n",
    "        # Store in Jacobian matrix\n",
    "        jacobian[i] = x.grad.clone()\n",
    "        \n",
    "        print(f\"  ‚àÇy{i+1}/‚àÇx: {x.grad.detach().numpy()}\")\n",
    "    \n",
    "    print(f\"\\nüìê Computed Jacobian Matrix:\")\n",
    "    print(jacobian.numpy())\n",
    "    \n",
    "    # Analytical Jacobian for verification\n",
    "    # ‚àÇy‚ÇÅ/‚àÇx = [2x‚ÇÅ, 0, 0]\n",
    "    # ‚àÇy‚ÇÇ/‚àÇx = [x‚ÇÇ, x‚ÇÅ, 0]  \n",
    "    # ‚àÇy‚ÇÉ/‚àÇx = [0, x‚ÇÉ, x‚ÇÇ]\n",
    "    analytical_jacobian = torch.tensor([\n",
    "        [2*x[0], 0, 0],\n",
    "        [x[1], x[0], 0],\n",
    "        [0, x[2], x[1]]\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nüìã Analytical Jacobian Matrix:\")\n",
    "    print(analytical_jacobian.numpy())\n",
    "    \n",
    "    # Verification\n",
    "    jacobian_diff = torch.norm(jacobian - analytical_jacobian).item()\n",
    "    print(f\"\\n‚úÖ Verification:\")\n",
    "    print(f\"  Frobenius norm difference: {jacobian_diff:.2e}\")\n",
    "    print(f\"  Jacobians match: {jacobian_diff < 1e-6}\")\n",
    "    \n",
    "    return jacobian, analytical_jacobian\n",
    "\n",
    "# Execute vector gradient demonstration\n",
    "computed_jacobian, analytical_jacobian = demonstrate_vector_gradients()\n",
    "```\n",
    "\n",
    "### 3.2 Neural Network Gradient Flow\n",
    "\n",
    "```python\n",
    "def analyze_neural_network_gradients():\n",
    "    \"\"\"Analyze gradient flow through neural network layers\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 3.2 Neural Network Gradient Flow ===\\n\")\n",
    "    \n",
    "    # Define comprehensive neural network\n",
    "    class AnalysisNet(nn.Module):\n",
    "        def __init__(self, input_dim=3, hidden_dims=[4, 6, 4], output_dim=2):\n",
    "            super().__init__()\n",
    "            \n",
    "            layers = []\n",
    "            prev_dim = input_dim\n",
    "            \n",
    "            for i, hidden_dim in enumerate(hidden_dims):\n",
    "                layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                prev_dim = hidden_dim\n",
    "            \n",
    "            layers.append(nn.Linear(prev_dim, output_dim))\n",
    "            \n",
    "            self.network = nn.Sequential(*layers)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            return self.network(x)\n",
    "    \n",
    "    # Initialize network with controlled weights\n",
    "    net = AnalysisNet()\n",
    "    \n",
    "    # Initialize weights for reproducible analysis\n",
    "    for module in net.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_normal_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    print(f\"üèóÔ∏è Network Architecture:\")\n",
    "    total_params = sum(p.numel() for p in net.parameters())\n",
    "    trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"  Architecture: {net}\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Create training data\n",
    "    batch_size = 8\n",
    "    x = torch.randn(batch_size, 3)\n",
    "    target = torch.randn(batch_size, 2)\n",
    "    \n",
    "    print(f\"\\nüìä Training Setup:\")\n",
    "    print(f\"  Input shape: {x.shape}\")\n",
    "    print(f\"  Target shape: {target.shape}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    output = net(x)\n",
    "    loss = F.mse_loss(output, target)\n",
    "    \n",
    "    print(f\"\\n‚ö° Forward Pass Results:\")\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    print(f\"  Output range: [{output.min().item():.4f}, {output.max().item():.4f}]\")\n",
    "    print(f\"  Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Analyze gradients across layers\n",
    "    print(f\"\\nüîç Gradient Analysis by Layer:\")\n",
    "    print(f\"{'Layer':<20} {'Shape':<15} {'Grad Norm':<12} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    gradient_norms = []\n",
    "    for name, param in net.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad = param.grad\n",
    "            grad_norm = grad.norm().item()\n",
    "            grad_mean = grad.mean().item()\n",
    "            grad_std = grad.std().item()\n",
    "            grad_min = grad.min().item()\n",
    "            grad_max = grad.max().item()\n",
    "            \n",
    "            gradient_norms.append(grad_norm)\n",
    "            \n",
    "            print(f\"{name:<20} {str(param.shape):<15} {grad_norm:<12.6f} \"\n",
    "                  f\"{grad_mean:<10.4f} {grad_std:<10.4f} {grad_min:<10.4f} {grad_max:<10.4f}\")\n",
    "    \n",
    "    # Gradient flow health analysis\n",
    "    print(f\"\\nüè• Gradient Flow Health Check:\")\n",
    "    if gradient_norms:\n",
    "        max_grad = max(gradient_norms)\n",
    "        min_grad = min(gradient_norms) \n",
    "        ratio = max_grad / (min_grad + 1e-8)\n",
    "        total_norm = (sum(g**2 for g in gradient_norms))**0.5\n",
    "        \n",
    "        print(f\"  Total gradient norm: {total_norm:.6f}\")\n",
    "        print(f\"  Max gradient norm: {max_grad:.6f}\")\n",
    "        print(f\"  Min gradient norm: {min_grad:.6f}\")\n",
    "        print(f\"  Max/Min ratio: {ratio:.2f}\")\n",
    "        \n",
    "        # Health assessment\n",
    "        if ratio > 100:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Large gradient imbalance detected!\")\n",
    "        elif any(g < 1e-7 for g in gradient_norms):\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Very small gradients detected (vanishing gradients)\")\n",
    "        elif any(g > 10 for g in gradient_norms):\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Very large gradients detected (exploding gradients)\")\n",
    "        else:\n",
    "            print(f\"  ‚úÖ Gradient flow appears healthy\")\n",
    "    \n",
    "    return net, gradient_norms, total_norm\n",
    "\n",
    "# Execute neural network gradient analysis\n",
    "analysis_net, gradient_norms, total_gradient_norm = analyze_neural_network_gradients()\n",
    "```\n",
    "\n",
    "## 4. Interactive Gradient Visualization\n",
    "\n",
    "### 4.1 2D Function Gradient Landscapes\n",
    "\n",
    "```python\n",
    "def create_gradient_landscape_visualization():\n",
    "    \"\"\"Create comprehensive gradient landscape visualizations\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 4.1 Interactive Gradient Landscapes ===\\n\")\n",
    "    \n",
    "    def rosenbrock_function(coords):\n",
    "        \"\"\"Rosenbrock function: classic optimization benchmark\"\"\"\n",
    "        x, y = coords[0], coords[1]\n",
    "        a, b = 1, 100\n",
    "        return (a - x)**2 + b * (y - x**2)**2\n",
    "    \n",
    "    def himmelblau_function(coords):\n",
    "        \"\"\"Himmelblau's function: multi-modal optimization problem\"\"\"\n",
    "        x, y = coords[0], coords[1]\n",
    "        return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "    \n",
    "    def visualize_function_landscape(func, func_name, x_range=(-3, 3), y_range=(-3, 3), resolution=100):\n",
    "        \"\"\"Create comprehensive function landscape visualization\"\"\"\n",
    "        \n",
    "        print(f\"üé® Visualizing {func_name} function...\")\n",
    "        \n",
    "        # Create coordinate grids\n",
    "        x = np.linspace(x_range[0], x_range[1], resolution)\n",
    "        y = np.linspace(y_range[0], y_range[1], resolution)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # Compute function values and gradients\n",
    "        Z = np.zeros_like(X)\n",
    "        Gx = np.zeros_like(X)  # ‚àÇf/‚àÇx\n",
    "        Gy = np.zeros_like(Y)  # ‚àÇf/‚àÇy\n",
    "        \n",
    "        for i in range(resolution):\n",
    "            for j in range(resolution):\n",
    "                coords = torch.tensor([X[i, j], Y[i, j]], requires_grad=True)\n",
    "                z = func(coords)\n",
    "                Z[i, j] = z.item()\n",
    "                \n",
    "                z.backward()\n",
    "                Gx[i, j] = coords.grad[0].item()\n",
    "                Gy[i, j] = coords.grad[1].item()\n",
    "        \n",
    "        # Create comprehensive visualization\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        \n",
    "        # 1. Function contour plot\n",
    "        ax1 = plt.subplot(3, 3, 1)\n",
    "        contour = ax1.contour(X, Y, Z, levels=30, alpha=0.8)\n",
    "        ax1.clabel(contour, inline=True, fontsize=8)\n",
    "        ax1.set_title(f'{func_name} Function Contours', fontweight='bold')\n",
    "        ax1.set_xlabel('x‚ÇÅ')\n",
    "        ax1.set_ylabel('x‚ÇÇ')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Function surface (filled contour)\n",
    "        ax2 = plt.subplot(3, 3, 2)\n",
    "        surface = ax2.contourf(X, Y, Z, levels=50, cmap='viridis')\n",
    "        plt.colorbar(surface, ax=ax2)\n",
    "        ax2.set_title(f'{func_name} Function Surface', fontweight='bold')\n",
    "        ax2.set_xlabel('x‚ÇÅ')\n",
    "        ax2.set_ylabel('x‚ÇÇ')\n",
    "        \n",
    "        # 3. Gradient vector field\n",
    "        ax3 = plt.subplot(3, 3, 3)\n",
    "        step = resolution // 15  # Subsample for clarity\n",
    "        ax3.contour(X, Y, Z, levels=20, alpha=0.3)\n",
    "        quiver = ax3.quiver(X[::step, ::step], Y[::step, ::step], \n",
    "                           Gx[::step, ::step], Gy[::step, ::step], \n",
    "                           alpha=0.8, scale=None, color='red', width=0.003)\n",
    "        ax3.set_title('Gradient Vector Field', fontweight='bold')\n",
    "        ax3.set_xlabel('x‚ÇÅ')\n",
    "        ax3.set_ylabel('x‚ÇÇ')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Gradient magnitude heatmap\n",
    "        ax4 = plt.subplot(3, 3, 4)\n",
    "        grad_magnitude = np.sqrt(Gx**2 + Gy**2)\n",
    "        magnitude_plot = ax4.imshow(grad_magnitude, extent=[x_range[0], x_range[1], y_range[0], y_range[1]], \n",
    "                                   origin='lower', cmap='plasma')\n",
    "        plt.colorbar(magnitude_plot, ax=ax4)\n",
    "        ax4.set_title('Gradient Magnitude', fontweight='bold')\n",
    "        ax4.set_xlabel('x‚ÇÅ')\n",
    "        ax4.set_ylabel('x‚ÇÇ')\n",
    "        \n",
    "        # 5. X-direction gradients\n",
    "        ax5 = plt.subplot(3, 3, 5)\n",
    "        gx_plot = ax5.imshow(Gx, extent=[x_range[0], x_range[1], y_range[0], y_range[1]], \n",
    "                            origin='lower', cmap='RdBu_r')\n",
    "        plt.colorbar(gx_plot, ax=ax5)\n",
    "        ax5.set_title('‚àÇf/‚àÇx‚ÇÅ', fontweight='bold')\n",
    "        ax5.set_xlabel('x‚ÇÅ')\n",
    "        ax5.set_ylabel('x‚ÇÇ')\n",
    "        \n",
    "        # 6. Y-direction gradients\n",
    "        ax6 = plt.subplot(3, 3, 6)\n",
    "        gy_plot = ax6.imshow(Gy, extent=[x_range[0], x_range[1], y_range[0], y_range[1]], \n",
    "                            origin='lower', cmap='RdBu_r')\n",
    "        plt.colorbar(gy_plot, ax=ax6)\n",
    "        ax6.set_title('‚àÇf/‚àÇx‚ÇÇ', fontweight='bold')\n",
    "        ax6.set_xlabel('x‚ÇÅ')\n",
    "        ax6.set_ylabel('x‚ÇÇ')\n",
    "        \n",
    "        # 7-9. Statistical analysis\n",
    "        ax7 = plt.subplot(3, 3, 7)\n",
    "        ax7.hist(Z.flatten(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax7.set_title('Function Value Distribution', fontweight='bold')\n",
    "        ax7.set_xlabel('Function Value')\n",
    "        ax7.set_ylabel('Frequency')\n",
    "        ax7.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax8 = plt.subplot(3, 3, 8)\n",
    "        ax8.hist(grad_magnitude.flatten(), bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "        ax8.set_title('Gradient Magnitude Distribution', fontweight='bold')\n",
    "        ax8.set_xlabel('Gradient Magnitude')\n",
    "        ax8.set_ylabel('Frequency')\n",
    "        ax8.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax9 = plt.subplot(3, 3, 9)\n",
    "        gradient_angles = np.arctan2(Gy, Gx)\n",
    "        ax9.hist(gradient_angles.flatten(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "        ax9.set_title('Gradient Direction Distribution', fontweight='bold')\n",
    "        ax9.set_xlabel('Gradient Angle (radians)')\n",
    "        ax9.set_ylabel('Frequency')\n",
    "        ax9.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle(f'{func_name} Function: Comprehensive Gradient Analysis', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save visualization\n",
    "        filename = f'gradient_landscape_{func_name.lower().replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(results_dir / filename, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Return analysis data\n",
    "        return {\n",
    "            'function_stats': {\n",
    "                'min': float(Z.min()),\n",
    "                'max': float(Z.max()),\n",
    "                'mean': float(Z.mean()),\n",
    "                'std': float(Z.std())\n",
    "            },\n",
    "            'gradient_stats': {\n",
    "                'magnitude_mean': float(grad_magnitude.mean()),\n",
    "                'magnitude_max': float(grad_magnitude.max()),\n",
    "                'x_grad_mean': float(Gx.mean()),\n",
    "                'y_grad_mean': float(Gy.mean())\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Visualize multiple functions\n",
    "    functions = [\n",
    "        (rosenbrock_function, \"Rosenbrock\", (-2, 2), (-1, 3)),\n",
    "        (himmelblau_function, \"Himmelblau\", (-5, 5), (-5, 5))\n",
    "    ]\n",
    "    \n",
    "    analysis_results = {}\n",
    "    \n",
    "    for func, name, x_range, y_range in functions:\n",
    "        result = visualize_function_landscape(func, name, x_range, y_range)\n",
    "        analysis_results[name] = result\n",
    "        \n",
    "        print(f\"üìä {name} Function Analysis:\")\n",
    "        print(f\"  Function range: [{result['function_stats']['min']:.2f}, {result['function_stats']['max']:.2f}]\")\n",
    "        print(f\"  Mean gradient magnitude: {result['gradient_stats']['magnitude_mean']:.4f}\")\n",
    "        print(f\"  Max gradient magnitude: {result['gradient_stats']['magnitude_max']:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "# Execute gradient landscape visualization\n",
    "landscape_analysis = create_gradient_landscape_visualization()\n",
    "```\n",
    "\n",
    "### 4.2 Real-Time Gradient Descent Animation\n",
    "\n",
    "```python\n",
    "def demonstrate_gradient_descent_optimization():\n",
    "    \"\"\"Demonstrate gradient descent with real-time analysis\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 4.2 Gradient Descent Optimization ===\\n\")\n",
    "    \n",
    "    def rosenbrock(coords):\n",
    "        \"\"\"Rosenbrock function for optimization\"\"\"\n",
    "        x, y = coords[0], coords[1]\n",
    "        return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "    \n",
    "    def run_gradient_descent(start_point, learning_rate, max_iterations=200, tolerance=1e-6):\n",
    "        \"\"\"Run gradient descent with comprehensive tracking\"\"\"\n",
    "        \n",
    "        print(f\"üöÄ Starting gradient descent:\")\n",
    "        print(f\"  Initial position: {start_point.detach().numpy()}\")\n",
    "        print(f\"  Learning rate: {learning_rate}\")\n",
    "        print(f\"  Max iterations: {max_iterations}\")\n",
    "        print(f\"  Tolerance: {tolerance}\")\n",
    "        \n",
    "        # Initialize tracking\n",
    "        current_point = start_point.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        trajectory = [current_point.clone().detach()]\n",
    "        losses = []\n",
    "        gradient_norms = []\n",
    "        step_sizes = []\n",
    "        convergence_metrics = []\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            # Zero gradients\n",
    "            if current_point.grad is not None:\n",
    "                current_point.grad.zero_()\n",
    "            \n",
    "            # Forward pass\n",
    "            loss = rosenbrock(current_point)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Track gradient information\n",
    "            grad_norm = current_point.grad.norm().item()\n",
    "            gradient_norms.append(grad_norm)\n",
    "            \n",
    "            # Check convergence\n",
    "            if grad_norm < tolerance:\n",
    "                print(f\"  ‚úÖ Converged at iteration {iteration} (gradient norm: {grad_norm:.2e})\")\n",
    "                break\n",
    "            \n",
    "            # Update step\n",
    "            with torch.no_grad():\n",
    "                step = learning_rate * current_point.grad\n",
    "                step_size = step.norm().item()\n",
    "                step_sizes.append(step_size)\n",
    "                \n",
    "                current_point -= step\n",
    "                \n",
    "                # Track convergence rate\n",
    "                if len(losses) > 1:\n",
    "                    improvement = losses[-2] - losses[-1]\n",
    "                    convergence_metrics.append(improvement)\n",
    "            \n",
    "            # Re-enable gradients\n",
    "            current_point.requires_grad_(True)\n",
    "            \n",
    "            # Store trajectory\n",
    "            trajectory.append(current_point.clone().detach())\n",
    "            \n",
    "            # Periodic progress reports\n",
    "            if iteration % 50 == 0 or iteration < 10:\n",
    "                print(f\"  Iter {iteration:3d}: Loss={loss.item():.6f}, \"\n",
    "                      f\"Pos={current_point.detach().numpy()}, \"\n",
    "                      f\"GradNorm={grad_norm:.6f}\")\n",
    "        \n",
    "        return {\n",
    "            'trajectory': torch.stack(trajectory),\n",
    "            'losses': losses,\n",
    "            'gradient_norms': gradient_norms,\n",
    "            'step_sizes': step_sizes,\n",
    "            'convergence_metrics': convergence_metrics,\n",
    "            'final_position': current_point.detach(),\n",
    "            'iterations': len(losses)\n",
    "        }\n",
    "    \n",
    "    def visualize_optimization_results(results, title_suffix=\"\"):\n",
    "        \"\"\"Create comprehensive optimization visualization\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        trajectory = results['trajectory']\n",
    "        losses = results['losses']\n",
    "        gradient_norms = results['gradient_norms']\n",
    "        step_sizes = results['step_sizes']\n",
    "        \n",
    "        # 1. Optimization path on function landscape\n",
    "        x_range = (trajectory[:, 0].min().item() - 0.5, trajectory[:, 0].max().item() + 0.5)\n",
    "        y_range = (trajectory[:, 1].min().item() - 0.5, trajectory[:, 1].max().item() + 0.5)\n",
    "        \n",
    "        x = np.linspace(x_range[0], x_range[1], 100)\n",
    "        y = np.linspace(y_range[0], y_range[1], 100)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = (1 - X)**2 + 100 * (Y - X**2)**2\n",
    "        \n",
    "        axes[0,0].contour(X, Y, Z, levels=50, alpha=0.6)\n",
    "        axes[0,0].plot(trajectory[:, 0], trajectory[:, 1], 'ro-', linewidth=2, markersize=3, alpha=0.8)\n",
    "        axes[0,0].plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=12, label='Start')\n",
    "        axes[0,0].plot(trajectory[-1, 0], trajectory[-1, 1], 'ro', markersize=12, label='End')\n",
    "        axes[0,0].plot(1, 1, 'k*', markersize=15, label='Global Minimum')\n",
    "        axes[0,0].set_title(f'Optimization Trajectory{title_suffix}', fontweight='bold')\n",
    "        axes[0,0].set_xlabel('x‚ÇÅ')\n",
    "        axes[0,0].set_ylabel('x‚ÇÇ')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Loss convergence\n",
    "        axes[0,1].semilogy(losses, linewidth=2, color='blue')\n",
    "        axes[0,1].set_title('Loss Convergence', fontweight='bold')\n",
    "        axes[0,1].set_xlabel('Iteration')\n",
    "        axes[0,1].set_ylabel('Loss (log scale)')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Gradient norm evolution\n",
    "        axes[0,2].semilogy(gradient_norms, linewidth=2, color='orange')\n",
    "        axes[0,2].set_title('Gradient Norm Evolution', fontweight='bold')\n",
    "        axes[0,2].set_xlabel('Iteration')\n",
    "        axes[0,2].set_ylabel('Gradient Norm (log scale)')\n",
    "        axes[0,2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Step size evolution\n",
    "        axes[1,0].plot(step_sizes, linewidth=2, color='green')\n",
    "        axes[1,0].set_title('Step Size Evolution', fontweight='bold')\n",
    "        axes[1,0].set_xlabel('Iteration')\n",
    "        axes[1,0].set_ylabel('Step Size')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Parameter evolution\n",
    "        axes[1,1].plot(trajectory[1:, 0], label='x‚ÇÅ', linewidth=2)\n",
    "        axes[1,1].plot(trajectory[1:, 1], label='x‚ÇÇ', linewidth=2)\n",
    "        axes[1,1].axhline(y=1, color='red', linestyle='--', alpha=0.7, label='Optimal')\n",
    "        axes[1,1].set_title('Parameter Evolution', fontweight='bold')\n",
    "        axes[1,1].set_xlabel('Iteration')\n",
    "        axes[1,1].set_ylabel('Parameter Value')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Convergence rate analysis\n",
    "        if len(results['convergence_metrics']) > 0:\n",
    "            axes[1,2].plot(results['convergence_metrics'], linewidth=2, color='purple')\n",
    "            axes[1,2].set_title('Loss Improvement per Iteration', fontweight='bold')\n",
    "            axes[1,2].set_xlabel('Iteration')\n",
    "            axes[1,2].set_ylabel('Loss Improvement')\n",
    "            axes[1,2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    # Run experiments with different learning rates\n",
    "    experiments = [\n",
    "        (torch.tensor([-1.5, 2.5]), 0.001, \"Small LR\"),\n",
    "        (torch.tensor([-1.5, 2.5]), 0.01, \"Medium LR\"),\n",
    "        (torch.tensor([0.5, 0.5]), 0.001, \"Good Start + Small LR\")\n",
    "    ]\n",
    "    \n",
    "    experiment_results = {}\n",
    "    \n",
    "    for start_point, lr, experiment_name in experiments:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üß™ Experiment: {experiment_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        results = run_gradient_descent(start_point, lr)\n",
    "        experiment_results[experiment_name] = results\n",
    "        \n",
    "        # Visualize results\n",
    "        fig = visualize_optimization_results(results, f\" ({experiment_name})\")\n",
    "        fig.suptitle(f'Gradient Descent Analysis: {experiment_name}', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Save visualization\n",
    "        filename = f'gradient_descent_{experiment_name.lower().replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(results_dir / filename, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary\n",
    "        final_loss = results['losses'][-1]\n",
    "        total_distance = torch.norm(torch.diff(results['trajectory'], dim=0), dim=1).sum().item()\n",
    "        \n",
    "        print(f\"\\nüìà {experiment_name} Results:\")\n",
    "        print(f\"  Final position: {results['final_position'].numpy()}\")\n",
    "        print(f\"  Final loss: {final_loss:.8f}\")\n",
    "        print(f\"  Iterations: {results['iterations']}\")\n",
    "        print(f\"  Total distance traveled: {total_distance:.4f}\")\n",
    "        print(f\"  Average step size: {np.mean(results['step_sizes']):.6f}\")\n",
    "        print(f\"  Final gradient norm: {results['gradient_norms'][-1]:.2e}\")\n",
    "    \n",
    "    return experiment_results\n",
    "\n",
    "# Execute gradient descent demonstrations\n",
    "optimization_results = demonstrate_gradient_descent_optimization()\n",
    "```\n",
    "\n",
    "## 5. Advanced Autograd Techniques\n",
    "\n",
    "### 5.1 Higher-Order Gradients and Hessian Computation\n",
    "\n",
    "```python\n",
    "def explore_higher_order_gradients():\n",
    "    \"\"\"Explore second-order gradients and Hessian matrices\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 5.1 Higher-Order Gradients & Hessian Analysis ===\\n\")\n",
    "    \n",
    "    def compute_hessian_matrix(func, inputs):\n",
    "        \"\"\"Compute full Hessian matrix for scalar function\"\"\"\n",
    "        \n",
    "        n = inputs.numel()\n",
    "        hessian = torch.zeros(n, n)\n",
    "        \n",
    "        # First, compute gradient\n",
    "        inputs_copy = inputs.clone().detach().requires_grad_(True)\n",
    "        output = func(inputs_copy)\n",
    "        \n",
    "        # Compute first-order gradients\n",
    "        first_grad = torch.autograd.grad(output, inputs_copy, create_graph=True)[0]\n",
    "        \n",
    "        # Compute second-order gradients (Hessian)\n",
    "        for i in range(n):\n",
    "            second_grad = torch.autograd.grad(first_grad[i], inputs_copy, \n",
    "                                            retain_graph=(i < n-1))[0]\n",
    "            hessian[i] = second_grad\n",
    "        \n",
    "        return hessian\n",
    "    \n",
    "    def analyze_quadratic_function():\n",
    "        \"\"\"Analyze quadratic function with known Hessian\"\"\"\n",
    "        \n",
    "        print(\"üî¢ Quadratic Function Analysis:\")\n",
    "        \n",
    "        # Define quadratic function: f(x) = 0.5 * x^T A x + b^T x + c\n",
    "        A = torch.tensor([[3.0, 1.0], [1.0, 2.0]])  # Positive definite matrix\n",
    "        b = torch.tensor([1.0, -2.0])\n",
    "        c = 5.0\n",
    "        \n",
    "        def quadratic_func(x):\n",
    "            return 0.5 * x @ A @ x + b @ x + c\n",
    "        \n",
    "        # Test point\n",
    "        test_point = torch.tensor([2.0, 1.0])\n",
    "        print(f\"  Test point: {test_point.numpy()}\")\n",
    "        print(f\"  Matrix A:\\n{A.numpy()}\")\n",
    "        print(f\"  Vector b: {b.numpy()}\")\n",
    "        print(f\"  Scalar c: {c}\")\n",
    "        \n",
    "        # Compute Hessian\n",
    "        computed_hessian = compute_hessian_matrix(quadratic_func, test_point)\n",
    "        true_hessian = A  # For quadratic function, Hessian = A\n",
    "        \n",
    "        print(f\"\\n  Computed Hessian:\\n{computed_hessian.numpy()}\")\n",
    "        print(f\"  True Hessian (A):\\n{true_hessian.numpy()}\")\n",
    "        print(f\"  Difference norm: {torch.norm(computed_hessian - true_hessian).item():.2e}\")\n",
    "        print(f\"  ‚úÖ Hessians match: {torch.allclose(computed_hessian, true_hessian, atol=1e-6)}\")\n",
    "        \n",
    "        return computed_hessian, true_hessian\n",
    "    \n",
    "    def analyze_hessian_properties(hessian, function_name):\n",
    "        \"\"\"Analyze mathematical properties of Hessian matrix\"\"\"\n",
    "        \n",
    "        print(f\"\\nüîç {function_name} Hessian Properties:\")\n",
    "        \n",
    "        # Eigenvalue analysis\n",
    "        eigenvals, eigenvecs = torch.linalg.eigh(hessian)\n",
    "        \n",
    "        print(f\"  Eigenvalues: {eigenvals.numpy()}\")\n",
    "        print(f\"  Eigenvectors:\\n{eigenvecs.numpy()}\")\n",
    "        \n",
    "        # Matrix properties\n",
    "        condition_number = eigenvals.max() / eigenvals.min()\n",
    "        determinant = torch.det(hessian)\n",
    "        trace = torch.trace(hessian)\n",
    "        \n",
    "        print(f\"  Determinant: {determinant.item():.6f}\")\n",
    "        print(f\"  Trace: {trace.item():.6f}\")\n",
    "        print(f\"  Condition number: {condition_number.item():.6f}\")\n",
    "        \n",
    "        # Definiteness analysis\n",
    "        if (eigenvals > 0).all():\n",
    "            definiteness = \"Positive definite\"\n",
    "            optimization_property = \"Unique minimum (convex)\"\n",
    "        elif (eigenvals < 0).all():\n",
    "            definiteness = \"Negative definite\"\n",
    "            optimization_property = \"Unique maximum (concave)\"\n",
    "        elif (eigenvals >= 0).all():\n",
    "            definiteness = \"Positive semidefinite\"\n",
    "            optimization_property = \"Possible minimum\"\n",
    "        elif (eigenvals <= 0).all():\n",
    "            definiteness = \"Negative semidefinite\"\n",
    "            optimization_property = \"Possible maximum\"\n",
    "        else:\n",
    "            definiteness = \"Indefinite\"\n",
    "            optimization_property = \"Saddle point\"\n",
    "        \n",
    "        print(f\"  Matrix type: {definiteness}\")\n",
    "        print(f\"  Optimization implication: {optimization_property}\")\n",
    "        \n",
    "        return {\n",
    "            'eigenvalues': eigenvals.numpy(),\n",
    "            'condition_number': condition_number.item(),\n",
    "            'definiteness': definiteness,\n",
    "            'optimization_property': optimization_property\n",
    "        }\n",
    "    \n",
    "    # Analyze quadratic function\n",
    "    quad_hessian, true_quad_hessian = analyze_quadratic_function()\n",
    "    quad_properties = analyze_hessian_properties(quad_hessian, \"Quadratic\")\n",
    "    \n",
    "    # Analyze Rosenbrock function at different points\n",
    "    def rosenbrock_func(x):\n",
    "        return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "    \n",
    "    test_points = [\n",
    "        torch.tensor([0.0, 0.0]),  # Away from minimum\n",
    "        torch.tensor([1.0, 1.0]),  # At minimum\n",
    "        torch.tensor([0.5, 0.25]) # Intermediate point\n",
    "    ]\n",
    "    \n",
    "    rosenbrock_analysis = {}\n",
    "    \n",
    "    for i, point in enumerate(test_points):\n",
    "        point_name = f\"Point_{i+1}_({point[0]:.1f},{point[1]:.1f})\"\n",
    "        print(f\"\\nüåπ Rosenbrock Function at {point_name}:\")\n",
    "        print(f\"  Position: {point.numpy()}\")\n",
    "        \n",
    "        hessian = compute_hessian_matrix(rosenbrock_func, point)\n",
    "        properties = analyze_hessian_properties(hessian, f\"Rosenbrock at {point_name}\")\n",
    "        \n",
    "        rosenbrock_analysis[point_name] = {\n",
    "            'position': point.numpy(),\n",
    "            'hessian': hessian.numpy(),\n",
    "            'properties': properties\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'quadratic_analysis': {\n",
    "            'hessian': quad_hessian.numpy(),\n",
    "            'properties': quad_properties\n",
    "        },\n",
    "        'rosenbrock_analysis': rosenbrock_analysis\n",
    "    }\n",
    "\n",
    "# Execute higher-order gradient analysis\n",
    "hessian_analysis_results = explore_higher_order_gradients()\n",
    "```\n",
    "\n",
    "### 5.2 Gradient Debugging and Anomaly Detection\n",
    "\n",
    "```python\n",
    "def demonstrate_gradient_debugging():\n",
    "    \"\"\"Comprehensive gradient debugging and anomaly detection\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 5.2 Gradient Debugging & Anomaly Detection ===\\n\")\n",
    "    \n",
    "    class DiagnosticNet(nn.Module):\n",
    "        \"\"\"Neural network designed to exhibit various gradient issues\"\"\"\n",
    "        \n",
    "        def __init__(self, input_dim=10, hidden_dims=[50, 50, 50], output_dim=1, \n",
    "                     problematic=False):\n",
    "            super().__init__()\n",
    "            \n",
    "            layers = []\n",
    "            prev_dim = input_dim\n",
    "            \n",
    "            for i, hidden_dim in enumerate(hidden_dims):\n",
    "                linear = nn.Linear(prev_dim, hidden_dim)\n",
    "                \n",
    "                if problematic:\n",
    "                    # Initialize with problematic weights\n",
    "                    if i == 0:\n",
    "                        nn.init.normal_(linear.weight, mean=0, std=5.0)  # Large weights\n",
    "                    else:\n",
    "                        nn.init.normal_(linear.weight, mean=0, std=0.01)  # Small weights\n",
    "                else:\n",
    "                    nn.init.xavier_normal_(linear.weight)\n",
    "                \n",
    "                layers.extend([linear, nn.ReLU()])\n",
    "                prev_dim = hidden_dim\n",
    "            \n",
    "            # Output layer\n",
    "            output_layer = nn.Linear(prev_dim, output_dim)\n",
    "            if problematic:\n",
    "                nn.init.normal_(output_layer.weight, mean=0, std=0.001)\n",
    "            else:\n",
    "                nn.init.xavier_normal_(output_layer.weight)\n",
    "            \n",
    "            layers.append(output_layer)\n",
    "            self.network = nn.Sequential(*layers)\n",
    "            self.problematic = problematic\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x)\n",
    "    \n",
    "    def comprehensive_gradient_analysis(model, input_data, target, model_name):\n",
    "        \"\"\"Perform comprehensive gradient analysis with anomaly detection\"\"\"\n",
    "        \n",
    "        print(f\"üîç Analyzing {model_name}:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        analysis_results = {\n",
    "            'model_name': model_name,\n",
    "            'layer_analysis': [],\n",
    "            'anomalies_detected': [],\n",
    "            'health_score': 0.0,\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Enable anomaly detection\n",
    "        with torch.autograd.detect_anomaly():\n",
    "            try:\n",
    "                # Forward pass\n",
    "                output = model(input_data)\n",
    "                loss = F.mse_loss(output, target)\n",
    "                \n",
    "                print(f\"  üìä Forward Pass:\")\n",
    "                print(f\"    Input shape: {input_data.shape}\")\n",
    "                print(f\"    Output shape: {output.shape}\")\n",
    "                print(f\"    Loss: {loss.item():.6f}\")\n",
    "                print(f\"    Output range: [{output.min().item():.4f}, {output.max().item():.4f}]\")\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Analyze each layer's gradients\n",
    "                print(f\"\\n  üîÑ Gradient Analysis:\")\n",
    "                print(f\"    {'Layer':<25} {'Param Count':<12} {'Grad Norm':<12} {'Mean':<10} {'Std':<10} {'Issues':<20}\")\n",
    "                print(\"    \" + \"-\" * 90)\n",
    "                \n",
    "                total_norm_squared = 0\n",
    "                layer_count = 0\n",
    "                issue_count = 0\n",
    "                \n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.grad is not None:\n",
    "                        grad = param.grad\n",
    "                        grad_norm = grad.norm().item()\n",
    "                        grad_mean = grad.mean().item()\n",
    "                        grad_std = grad.std().item()\n",
    "                        param_count = param.numel()\n",
    "                        \n",
    "                        # Detect issues\n",
    "                        issues = []\n",
    "                        \n",
    "                        # Zero gradients\n",
    "                        if grad_norm == 0:\n",
    "                            issues.append(\"ZERO_GRAD\")\n",
    "                            issue_count += 1\n",
    "                        \n",
    "                        # Exploding gradients\n",
    "                        elif grad_norm > 10:\n",
    "                            issues.append(\"EXPLODING\")\n",
    "                            issue_count += 1\n",
    "                        \n",
    "                        # Vanishing gradients\n",
    "                        elif grad_norm < 1e-7:\n",
    "                            issues.append(\"VANISHING\")\n",
    "                            issue_count += 1\n",
    "                        \n",
    "                        # NaN gradients\n",
    "                        if torch.isnan(grad).any():\n",
    "                            issues.append(\"NAN\")\n",
    "                            issue_count += 1\n",
    "                        \n",
    "                        # Inf gradients\n",
    "                        if torch.isinf(grad).any():\n",
    "                            issues.append(\"INF\")\n",
    "                            issue_count += 1\n",
    "                        \n",
    "                        # Large standard deviation (unstable)\n",
    "                        if grad_std > 10 * abs(grad_mean) and grad_std > 1.0:\n",
    "                            issues.append(\"UNSTABLE\")\n",
    "                            issue_count += 1\n",
    "                        \n",
    "                        issues_str = \",\".join(issues) if issues else \"OK\"\n",
    "                        \n",
    "                        print(f\"    {name:<25} {param_count:<12} {grad_norm:<12.6f} \"\n",
    "                              f\"{grad_mean:<10.4f} {grad_std:<10.4f} {issues_str:<20}\")\n",
    "                        \n",
    "                        # Store layer analysis\n",
    "                        layer_info = {\n",
    "                            'name': name,\n",
    "                            'param_count': param_count,\n",
    "                            'grad_norm': grad_norm,\n",
    "                            'grad_mean': grad_mean,\n",
    "                            'grad_std': grad_std,\n",
    "                            'issues': issues\n",
    "                        }\n",
    "                        analysis_results['layer_analysis'].append(layer_info)\n",
    "                        analysis_results['anomalies_detected'].extend(issues)\n",
    "                        \n",
    "                        total_norm_squared += grad_norm ** 2\n",
    "                        layer_count += 1\n",
    "                \n",
    "                # Overall gradient analysis\n",
    "                total_grad_norm = total_norm_squared ** 0.5\n",
    "                avg_grad_norm = total_grad_norm / max(layer_count, 1)\n",
    "                \n",
    "                print(f\"\\n  üìà Overall Gradient Statistics:\")\n",
    "                print(f\"    Total gradient norm: {total_grad_norm:.6f}\")\n",
    "                print(f\"    Average gradient norm: {avg_grad_norm:.6f}\")\n",
    "                print(f\"    Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "                print(f\"    Layers with issues: {issue_count}/{layer_count}\")\n",
    "                \n",
    "                # Health score calculation\n",
    "                health_score = max(0, 100 - (issue_count * 20))\n",
    "                analysis_results['health_score'] = health_score\n",
    "                \n",
    "                # Generate recommendations\n",
    "                recommendations = []\n",
    "                \n",
    "                if issue_count == 0:\n",
    "                    recommendations.append(\"‚úÖ Gradient flow appears healthy\")\n",
    "                else:\n",
    "                    if \"EXPLODING\" in analysis_results['anomalies_detected']:\n",
    "                        recommendations.append(\"üîß Use gradient clipping to handle exploding gradients\")\n",
    "                    if \"VANISHING\" in analysis_results['anomalies_detected']:\n",
    "                        recommendations.append(\"üîß Consider residual connections or different activation functions\")\n",
    "                    if \"ZERO_GRAD\" in analysis_results['anomalies_detected']:\n",
    "                        recommendations.append(\"üîß Check for dead neurons or inappropriate activations\")\n",
    "                    if \"NAN\" in analysis_results['anomalies_detected'] or \"INF\" in analysis_results['anomalies_detected']:\n",
    "                        recommendations.append(\"üîß Reduce learning rate or check for numerical instabilities\")\n",
    "                    if \"UNSTABLE\" in analysis_results['anomalies_detected']:\n",
    "                        recommendations.append(\"üîß Consider batch normalization or layer normalization\")\n",
    "                \n",
    "                analysis_results['recommendations'] = recommendations\n",
    "                \n",
    "                print(f\"\\n  üè• Health Assessment:\")\n",
    "                print(f\"    Health score: {health_score}/100\")\n",
    "                for rec in recommendations:\n",
    "                    print(f\"    {rec}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error during analysis: {e}\")\n",
    "                analysis_results['error'] = str(e)\n",
    "        \n",
    "        return analysis_results\n",
    "    \n",
    "    # Create test models\n",
    "    good_model = DiagnosticNet(problematic=False)\n",
    "    problematic_model = DiagnosticNet(problematic=True)\n",
    "    \n",
    "    # Create test data\n",
    "    input_data = torch.randn(16, 10)\n",
    "    target = torch.randn(16, 1)\n",
    "    \n",
    "    # Analyze both models\n",
    "    good_analysis = comprehensive_gradient_analysis(good_model, input_data, target, \"Healthy Model\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    problematic_analysis = comprehensive_gradient_analysis(problematic_model, input_data, target, \"Problematic Model\")\n",
    "    \n",
    "    # Create comparison visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Extract gradient norms for visualization\n",
    "    good_norms = [layer['grad_norm'] for layer in good_analysis['layer_analysis']]\n",
    "    prob_norms = [layer['grad_norm'] for layer in problematic_analysis['layer_analysis']]\n",
    "    good_names = [layer['name'] for layer in good_analysis['layer_analysis']]\n",
    "    prob_names = [layer['name'] for layer in problematic_analysis['layer_analysis']]\n",
    "    \n",
    "    # 1. Gradient norm comparison\n",
    "    x_good = range(len(good_norms))\n",
    "    x_prob = range(len(prob_norms))\n",
    "    \n",
    "    axes[0,0].bar([x - 0.2 for x in x_good], good_norms, 0.4, label='Healthy Model', alpha=0.8)\n",
    "    axes[0,0].bar([x + 0.2 for x in x_prob], prob_norms, 0.4, label='Problematic Model', alpha=0.8)\n",
    "    axes[0,0].set_title('Gradient Norms by Layer', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('Gradient Norm')\n",
    "    axes[0,0].set_yscale('log')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Health scores\n",
    "    health_scores = [good_analysis['health_score'], problematic_analysis['health_score']]\n",
    "    model_names = ['Healthy Model', 'Problematic Model']\n",
    "    colors = ['green', 'red']\n",
    "    \n",
    "    bars = axes[0,1].bar(model_names, health_scores, color=colors, alpha=0.7)\n",
    "    axes[0,1].set_title('Model Health Scores', fontweight='bold')\n",
    "    axes[0,1].set_ylabel('Health Score (0-100)')\n",
    "    axes[0,1].set_ylim(0, 100)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, health_scores):\n",
    "        height = bar.get_height()\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                      f'{score:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Issue distribution\n",
    "    good_issues = good_analysis['anomalies_detected']\n",
    "    prob_issues = problematic_analysis['anomalies_detected']\n",
    "    \n",
    "    all_issue_types = list(set(good_issues + prob_issues))\n",
    "    if all_issue_types:\n",
    "        good_issue_counts = [good_issues.count(issue) for issue in all_issue_types]\n",
    "        prob_issue_counts = [prob_issues.count(issue) for issue in all_issue_types]\n",
    "        \n",
    "        x_issues = range(len(all_issue_types))\n",
    "        axes[1,0].bar([x - 0.2 for x in x_issues], good_issue_counts, 0.4, \n",
    "                     label='Healthy Model', alpha=0.8)\n",
    "        axes[1,0].bar([x + 0.2 for x in x_issues], prob_issue_counts, 0.4, \n",
    "                     label='Problematic Model', alpha=0.8)\n",
    "        axes[1,0].set_title('Issue Type Distribution', fontweight='bold')\n",
    "        axes[1,0].set_ylabel('Issue Count')\n",
    "        axes[1,0].set_xticks(x_issues)\n",
    "        axes[1,0].set_xticklabels(all_issue_types, rotation=45)\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1,0].text(0.5, 0.5, 'No Issues Detected', ha='center', va='center', \n",
    "                      transform=axes[1,0].transAxes, fontsize=14)\n",
    "        axes[1,0].set_title('Issue Type Distribution', fontweight='bold')\n",
    "    \n",
    "    # 4. Gradient distribution histogram\n",
    "    axes[1,1].hist(good_norms, bins=20, alpha=0.7, label='Healthy Model', density=True)\n",
    "    axes[1,1].hist(prob_norms, bins=20, alpha=0.7, label='Problematic Model', density=True)\n",
    "    axes[1,1].set_title('Gradient Norm Distribution', fontweight='bold')\n",
    "    axes[1,1].set_xlabel('Gradient Norm')\n",
    "    axes[1,1].set_ylabel('Density')\n",
    "    axes[1,1].set_yscale('log')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Gradient Debugging Analysis Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'gradient_debugging_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'healthy_model_analysis': good_analysis,\n",
    "        'problematic_model_analysis': problematic_analysis\n",
    "    }\n",
    "\n",
    "# Execute gradient debugging demonstration\n",
    "debugging_results = demonstrate_gradient_debugging()\n",
    "```\n",
    "\n",
    "## 6. Performance Optimization Techniques\n",
    "\n",
    "### 6.1 Gradient Accumulation for Large-Scale Training\n",
    "\n",
    "```python\n",
    "def demonstrate_gradient_accumulation():\n",
    "    \"\"\"Demonstrate gradient accumulation for memory-efficient training\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 6.1 Gradient Accumulation for Large-Scale Training ===\\n\")\n",
    "    \n",
    "    def create_training_comparison(model_class, train_loader, effective_batch_size=128):\n",
    "        \"\"\"Compare standard training vs gradient accumulation\"\"\"\n",
    "        \n",
    "        actual_batch_size = train_loader.batch_size\n",
    "        accumulation_steps = effective_batch_size // actual_batch_size\n",
    "        \n",
    "        print(f\"üìä Training Configuration Comparison:\")\n",
    "        print(f\"  Physical batch size: {actual_batch_size}\")\n",
    "        print(f\"  Effective batch size: {effective_batch_size}\")\n",
    "        print(f\"  Accumulation steps: {accumulation_steps}\")\n",
    "        print(f\"  Memory multiplier: {accumulation_steps}x\")\n",
    "        \n",
    "        # Method 1: Standard training (large batch simulation)\n",
    "        model1 = model_class()\n",
    "        optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        print(f\"\\nüîÑ Method 1: Simulated Large Batch Training\")\n",
    "        \n",
    "        # Collect multiple mini-batches\n",
    "        batch_data_list = []\n",
    "        batch_targets_list = []\n",
    "        \n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            batch_data_list.append(data)\n",
    "            batch_targets_list.append(target)\n",
    "            if len(batch_data_list) >= accumulation_steps:\n",
    "                break\n",
    "        \n",
    "        # Combine into large batch\n",
    "        large_batch_data = torch.cat(batch_data_list, dim=0)\n",
    "        large_batch_targets = torch.cat(batch_targets_list, dim=0)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        optimizer1.zero_grad()\n",
    "        output1 = model1(large_batch_data)\n",
    "        loss1 = criterion(output1, large_batch_targets)\n",
    "        loss1.backward()\n",
    "        \n",
    "        # Capture gradient statistics\n",
    "        grad_norms_method1 = []\n",
    "        for param in model1.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norms_method1.append(param.grad.norm().item())\n",
    "        \n",
    "        optimizer1.step()\n",
    "        \n",
    "        method1_time = time.time() - start_time\n",
    "        method1_loss = loss1.item()\n",
    "        \n",
    "        print(f\"    Loss: {method1_loss:.6f}\")\n",
    "        print(f\"    Time: {method1_time:.4f}s\")\n",
    "        print(f\"    Memory usage: {large_batch_data.numel() * 4 / 1024**2:.1f} MB (simulated)\")\n",
    "        \n",
    "        # Method 2: Gradient accumulation\n",
    "        model2 = model_class()\n",
    "        model2.load_state_dict(model1.state_dict())  # Start from same initialization\n",
    "        optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "        \n",
    "        print(f\"\\nüìà Method 2: Gradient Accumulation Training\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        optimizer2.zero_grad()\n",
    "        accumulated_loss = 0\n",
    "        grad_norms_method2 = []\n",
    "        \n",
    "        for i, (data, target) in enumerate(zip(batch_data_list, batch_targets_list)):\n",
    "            # Forward pass\n",
    "            output = model2(data)\n",
    "            loss = criterion(output, target) / accumulation_steps  # Scale loss\n",
    "            \n",
    "            # Backward pass (accumulate gradients)\n",
    "            loss.backward()\n",
    "            \n",
    "            accumulated_loss += loss.item() * accumulation_steps\n",
    "        \n",
    "        # Capture gradient statistics after accumulation\n",
    "        for param in model2.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norms_method2.append(param.grad.norm().item())\n",
    "        \n",
    "        # Single optimizer step after accumulation\n",
    "        optimizer2.step()\n",
    "        \n",
    "        method2_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"    Accumulated loss: {accumulated_loss:.6f}\")\n",
    "        print(f\"    Time: {method2_time:.4f}s\")\n",
    "        print(f\"    Memory usage: {batch_data_list[0].numel() * 4 / 1024**2:.1f} MB per step\")\n",
    "        \n",
    "        # Compare results\n",
    "        print(f\"\\nüìä Comparison Results:\")\n",
    "        loss_difference = abs(method1_loss - accumulated_loss)\n",
    "        time_difference = abs(method1_time - method2_time)\n",
    "        \n",
    "        print(f\"  Loss difference: {loss_difference:.8f}\")\n",
    "        print(f\"  Time difference: {time_difference:.4f}s\")\n",
    "        print(f\"  Speed ratio: {method1_time/method2_time:.2f}x\")\n",
    "        \n",
    "        # Compare gradients\n",
    "        grad_norm_diff = np.mean([abs(g1 - g2) for g1, g2 in zip(grad_norms_method1, grad_norms_method2)])\n",
    "        print(f\"  Average gradient norm difference: {grad_norm_diff:.8f}\")\n",
    "        \n",
    "        # Model parameter comparison\n",
    "        param_diff = 0\n",
    "        for p1, p2 in zip(model1.parameters(), model2.parameters()):\n",
    "            param_diff += (p1 - p2).norm().item()\n",
    "        \n",
    "        print(f\"  Total parameter difference: {param_diff:.8f}\")\n",
    "        print(f\"  ‚úÖ Methods equivalent: {param_diff < 1e-6}\")\n",
    "        \n",
    "        return {\n",
    "            'method1': {\n",
    "                'loss': method1_loss,\n",
    "                'time': method1_time,\n",
    "                'grad_norms': grad_norms_method1\n",
    "            },\n",
    "            'method2': {\n",
    "                'loss': accumulated_loss,\n",
    "                'time': method2_time,\n",
    "                'grad_norms': grad_norms_method2\n",
    "            },\n",
    "            'comparison': {\n",
    "                'loss_diff': loss_difference,\n",
    "                'time_diff': time_difference,\n",
    "                'param_diff': param_diff,\n",
    "                'equivalent': param_diff < 1e-6\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Create test model and data\n",
    "    class TestModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(50, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(100, 50),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(50, 10)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "    \n",
    "    # Create synthetic dataset\n",
    "    data_size = 200\n",
    "    input_dim = 50\n",
    "    output_dim = 10\n",
    "    \n",
    "    synthetic_data = torch.randn(data_size, input_dim)\n",
    "    synthetic_targets = torch.randn(data_size, output_dim)\n",
    "    \n",
    "    dataset = TensorDataset(synthetic_data, synthetic_targets)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    print(f\"üìÅ Dataset Configuration:\")\n",
    "    print(f\"  Total samples: {data_size}\")\n",
    "    print(f\"  Input dimension: {input_dim}\")\n",
    "    print(f\"  Output dimension: {output_dim}\")\n",
    "    print(f\"  Batch size: {train_loader.batch_size}\")\n",
    "    \n",
    "    # Run comparison\n",
    "    results = create_training_comparison(TestModel, train_loader, effective_batch_size=128)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Loss comparison\n",
    "    methods = ['Standard Training', 'Gradient Accumulation']\n",
    "    losses = [results['method1']['loss'], results['method2']['loss']]\n",
    "    colors = ['skyblue', 'lightcoral']\n",
    "    \n",
    "    bars1 = axes[0,0].bar(methods, losses, color=colors, alpha=0.8)\n",
    "    axes[0,0].set_title('Training Loss Comparison', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, loss in zip(bars1, losses):\n",
    "        height = bar.get_height()\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                      f'{loss:.6f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Training time comparison\n",
    "    times = [results['method1']['time'], results['method2']['time']]\n",
    "    \n",
    "    bars2 = axes[0,1].bar(methods, times, color=colors, alpha=0.8)\n",
    "    axes[0,1].set_title('Training Time Comparison', fontweight='bold')\n",
    "    axes[0,1].set_ylabel('Time (seconds)')\n",
    "    \n",
    "    for bar, time_val in zip(bars2, times):\n",
    "        height = bar.get_height()\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                      f'{time_val:.4f}s', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Gradient norm distribution\n",
    "    grad_norms_1 = results['method1']['grad_norms']\n",
    "    grad_norms_2 = results['method2']['grad_norms']\n",
    "    \n",
    "    axes[1,0].hist(grad_norms_1, bins=15, alpha=0.7, label='Standard Training', density=True)\n",
    "    axes[1,0].hist(grad_norms_2, bins=15, alpha=0.7, label='Gradient Accumulation', density=True)\n",
    "    axes[1,0].set_title('Gradient Norm Distribution', fontweight='bold')\n",
    "    axes[1,0].set_xlabel('Gradient Norm')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Method equivalence metrics\n",
    "    equiv_metrics = ['Loss Difference', 'Parameter Difference', 'Avg Grad Norm Diff']\n",
    "    equiv_values = [\n",
    "        results['comparison']['loss_diff'],\n",
    "        results['comparison']['param_diff'],\n",
    "        np.mean([abs(g1 - g2) for g1, g2 in zip(grad_norms_1, grad_norms_2)])\n",
    "    ]\n",
    "    \n",
    "    bars4 = axes[1,1].bar(equiv_metrics, equiv_values, color='lightgreen', alpha=0.8)\n",
    "    axes[1,1].set_title('Method Equivalence Metrics', fontweight='bold')\n",
    "    axes[1,1].set_ylabel('Absolute Difference')\n",
    "    axes[1,1].set_yscale('log')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.suptitle('Gradient Accumulation vs Standard Training Analysis', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'gradient_accumulation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüéì Key Benefits of Gradient Accumulation:\")\n",
    "    print(f\"  ‚Ä¢ Enables training with larger effective batch sizes\")\n",
    "    print(f\"  ‚Ä¢ Maintains mathematical equivalence to large-batch training\")\n",
    "    print(f\"  ‚Ä¢ Reduces memory requirements per forward pass\")\n",
    "    print(f\"  ‚Ä¢ Essential for training large models on limited hardware\")\n",
    "    print(f\"  ‚Ä¢ Provides more stable gradient estimates\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute gradient accumulation demonstration\n",
    "accumulation_results = demonstrate_gradient_accumulation()\n",
    "```\n",
    "\n",
    "### 6.2 Memory-Efficient Gradient Checkpointing\n",
    "\n",
    "```python\n",
    "def demonstrate_gradient_checkpointing():\n",
    "    \"\"\"Demonstrate gradient checkpointing for memory efficiency\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 6.2 Memory-Efficient Gradient Checkpointing ===\\n\")\n",
    "    \n",
    "    def memory_intensive_computation(x, num_layers=10):\n",
    "        \"\"\"Create memory-intensive computation chain\"\"\"\n",
    "        y = x\n",
    "        for i in range(num_layers):\n",
    "            # Each operation creates intermediate tensors\n",
    "            y = torch.sin(y) + torch.cos(y * 0.5) + torch.exp(y * 0.1)\n",
    "            y = torch.relu(y - y.mean())\n",
    "        return y.sum()\n",
    "    \n",
    "    def compare_memory_usage():\n",
    "        \"\"\"Compare memory usage between normal and checkpointed computation\"\"\"\n",
    "        \n",
    "        # Create large input tensor\n",
    "        input_size = (100, 1000)  # Reduced size for demonstration\n",
    "        large_input = torch.randn(input_size, requires_grad=True)\n",
    "        \n",
    "        print(f\"üìä Memory Comparison Setup:\")\n",
    "        print(f\"  Input tensor shape: {input_size}\")\n",
    "        print(f\"  Input tensor size: {large_input.numel() * 4 / 1024**2:.1f} MB\")\n",
    "        print(f\"  Computation layers: 10 (each creates intermediate tensors)\")\n",
    "        \n",
    "        # Method 1: Normal computation (stores all intermediates)\n",
    "        print(f\"\\nüíæ Method 1: Normal Computation\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            initial_memory = torch.cuda.memory_allocated()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Clear any existing gradients\n",
    "        if large_input.grad is not None:\n",
    "            large_input.grad.zero_()\n",
    "        \n",
    "        result_normal = memory_intensive_computation(large_input)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            peak_memory_normal = torch.cuda.memory_allocated()\n",
    "            memory_after_forward = peak_memory_normal - initial_memory\n",
    "        else:\n",
    "            memory_after_forward = \"N/A (CPU only)\"\n",
    "        \n",
    "        result_normal.backward()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            final_memory_normal = torch.cuda.memory_allocated()\n",
    "            total_memory_normal = final_memory_normal - initial_memory\n",
    "        else:\n",
    "            total_memory_normal = \"N/A (CPU only)\"\n",
    "        \n",
    "        normal_time = time.time() - start_time\n",
    "        normal_grad_norm = large_input.grad.norm().item()\n",
    "        \n",
    "        print(f\"    Forward pass time: {normal_time:.4f}s\")\n",
    "        print(f\"    Memory after forward: {memory_after_forward if isinstance(memory_after_forward, str) else f'{memory_after_forward / 1024**2:.1f} MB'}\")\n",
    "        print(f\"    Total memory used: {total_memory_normal if isinstance(total_memory_normal, str) else f'{total_memory_normal / 1024**2:.1f} MB'}\")\n",
    "        print(f\"    Gradient norm: {normal_grad_norm:.6f}\")\n",
    "        \n",
    "        # Reset for checkpointed computation\n",
    "        large_input.grad = None\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Method 2: Checkpointed computation\n",
    "        print(f\"\\nüîÑ Method 2: Gradient Checkpointing\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            initial_memory_cp = torch.cuda.memory_allocated()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        result_checkpointed = checkpoint(memory_intensive_computation, large_input)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            memory_after_forward_cp = torch.cuda.memory_allocated() - initial_memory_cp\n",
    "        else:\n",
    "            memory_after_forward_cp = \"N/A (CPU only)\"\n",
    "        \n",
    "        result_checkpointed.backward()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            total_memory_cp = torch.cuda.memory_allocated() - initial_memory_cp\n",
    "        else:\n",
    "            total_memory_cp = \"N/A (CPU only)\"\n",
    "        \n",
    "        checkpoint_time = time.time() - start_time\n",
    "        checkpoint_grad_norm = large_input.grad.norm().item()\n",
    "        \n",
    "        print(f\"    Forward pass time: {checkpoint_time:.4f}s\")\n",
    "        print(f\"    Memory after forward: {memory_after_forward_cp if isinstance(memory_after_forward_cp, str) else f'{memory_after_forward_cp / 1024**2:.1f} MB'}\")\n",
    "        print(f\"    Total memory used: {total_memory_cp if isinstance(total_memory_cp, str) else f'{total_memory_cp / 1024**2:.1f} MB'}\")\n",
    "        print(f\"    Gradient norm: {checkpoint_grad_norm:.6f}\")\n",
    "        \n",
    "        # Comparison\n",
    "        print(f\"\\nüìà Comparison Results:\")\n",
    "        print(f\"    Results match: {torch.allclose(result_normal, result_checkpointed, atol=1e-6)}\")\n",
    "        print(f\"    Gradients match: {abs(normal_grad_norm - checkpoint_grad_norm) < 1e-6}\")\n",
    "        print(f\"    Time overhead: {(checkpoint_time / normal_time - 1) * 100:.1f}%\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            memory_savings = (memory_after_forward - memory_after_forward_cp) / memory_after_forward * 100\n",
    "            print(f\"    Memory savings: {memory_savings:.1f}%\")\n",
    "        else:\n",
    "            print(f\"    Memory savings: Significant (recomputation vs storage trade-off)\")\n",
    "        \n",
    "        return {\n",
    "            'normal': {\n",
    "                'time': normal_time,\n",
    "                'memory_forward': memory_after_forward,\n",
    "                'memory_total': total_memory_normal,\n",
    "                'grad_norm': normal_grad_norm\n",
    "            },\n",
    "            'checkpointed': {\n",
    "                'time': checkpoint_time,\n",
    "                'memory_forward': memory_after_forward_cp,\n",
    "                'memory_total': total_memory_cp,\n",
    "                'grad_norm': checkpoint_grad_norm\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Run memory comparison\n",
    "    memory_results = compare_memory_usage()\n",
    "    \n",
    "    print(f\"\\nüéì Gradient Checkpointing Insights:\")\n",
    "    print(f\"  ‚Ä¢ Trade computation time for memory usage\")\n",
    "    print(f\"  ‚Ä¢ Essential for training very deep networks\")\n",
    "    print(f\"  ‚Ä¢ Mathematically equivalent to normal backpropagation\")\n",
    "    print(f\"  ‚Ä¢ Enables training of models that wouldn't fit in memory otherwise\")\n",
    "    print(f\"  ‚Ä¢ Particularly useful for transformer models and deep CNNs\")\n",
    "    \n",
    "    return memory_results\n",
    "\n",
    "# Execute gradient checkpointing demonstration\n",
    "checkpointing_results = demonstrate_gradient_checkpointing()\n",
    "```\n",
    "\n",
    "## 7. Practical Applications: Advanced Gradient Techniques\n",
    "\n",
    "### 7.1 Learning Rate Finding with Gradients\n",
    "\n",
    "```python\n",
    "def implement_learning_rate_finder():\n",
    "    \"\"\"Implement advanced learning rate finder using gradient analysis\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 7.1 Advanced Learning Rate Finder ===\\n\")\n",
    "    \n",
    "    def find_optimal_learning_rate(model, dataloader, init_lr=1e-8, final_lr=10, \n",
    "                                  beta=0.98, criterion=None):\n",
    "        \"\"\"Find optimal learning rate using loss and gradient analysis\"\"\"\n",
    "        \n",
    "        if criterion is None:\n",
    "            criterion = nn.MSELoss()\n",
    "        \n",
    "        # Create model copy for testing\n",
    "        model_copy = copy.deepcopy(model)\n",
    "        optimizer = torch.optim.SGD(model_copy.parameters(), lr=init_lr)\n",
    "        \n",
    "        num_batches = len(dataloader)\n",
    "        gamma = (final_lr / init_lr) ** (1 / num_batches)\n",
    "        \n",
    "        print(f\"üîç Learning Rate Finder Configuration:\")\n",
    "        print(f\"  Initial LR: {init_lr:.2e}\")\n",
    "        print(f\"  Final LR: {final_lr:.2e}\")\n",
    "        print(f\"  Number of batches: {num_batches}\")\n",
    "        print(f\"  LR multiplication factor: {gamma:.6f}\")\n",
    "        \n",
    "        # Storage for analysis\n",
    "        learning_rates = []\n",
    "        losses = []\n",
    "        smoothed_losses = []\n",
    "        gradient_norms = []\n",
    "        loss_improvements = []\n",
    "        \n",
    "        best_lr = init_lr\n",
    "        best_loss = float('inf')\n",
    "        avg_loss = 0\n",
    "        \n",
    "        print(f\"\\nüîÑ Running LR finder...\")\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            # Update learning rate\n",
    "            lr = init_lr * (gamma ** batch_idx)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            \n",
    "            learning_rates.append(lr)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model_copy(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Calculate gradient norm\n",
    "            total_norm = 0\n",
    "            for param in model_copy.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param_norm = param.grad.norm()\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "            gradient_norms.append(total_norm)\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track losses\n",
    "            current_loss = loss.item()\n",
    "            losses.append(current_loss)\n",
    "            \n",
    "            # Exponential moving average for smoothing\n",
    "            if batch_idx == 0:\n",
    "                avg_loss = current_loss\n",
    "            else:\n",
    "                avg_loss = beta * avg_loss + (1 - beta) * current_loss\n",
    "                # Bias correction\n",
    "                smoothed_loss = avg_loss / (1 - beta ** (batch_idx + 1))\n",
    "                smoothed_losses.append(smoothed_loss)\n",
    "                \n",
    "                # Track loss improvement\n",
    "                if len(smoothed_losses) > 1:\n",
    "                    improvement = smoothed_losses[-2] - smoothed_losses[-1]\n",
    "                    loss_improvements.append(improvement)\n",
    "                \n",
    "                # Check for best learning rate (based on smoothed loss)\n",
    "                if smoothed_loss < best_loss:\n",
    "                    best_loss = smoothed_loss\n",
    "                    best_lr = lr\n",
    "            \n",
    "            # Stop if loss explodes\n",
    "            if batch_idx > 10 and current_loss > 4 * min(losses):\n",
    "                print(f\"  ‚ö†Ô∏è Stopping early: loss explosion detected at LR {lr:.2e}\")\n",
    "                break\n",
    "            \n",
    "            # Periodic updates\n",
    "            if batch_idx % max(1, num_batches // 10) == 0:\n",
    "                print(f\"    Batch {batch_idx:3d}/{num_batches}: LR={lr:.2e}, \"\n",
    "                      f\"Loss={current_loss:.6f}, GradNorm={total_norm:.6f}\")\n",
    "        \n",
    "        # Analysis and recommendations\n",
    "        print(f\"\\nüìä LR Finder Analysis:\")\n",
    "        print(f\"  Total batches processed: {len(learning_rates)}\")\n",
    "        print(f\"  LR range tested: [{min(learning_rates):.2e}, {max(learning_rates):.2e}]\")\n",
    "        print(f\"  Minimum loss: {min(losses):.6f}\")\n",
    "        print(f\"  Best LR (min smoothed loss): {best_lr:.2e}\")\n",
    "        \n",
    "        # Find steepest descent point\n",
    "        if len(loss_improvements) > 5:\n",
    "            # Find where loss improvement is maximized\n",
    "            max_improvement_idx = np.argmax(loss_improvements)\n",
    "            steepest_lr = learning_rates[max_improvement_idx + 1]  # +1 due to improvement calculation\n",
    "            print(f\"  Steepest descent LR: {steepest_lr:.2e}\")\n",
    "        else:\n",
    "            steepest_lr = best_lr\n",
    "        \n",
    "        # Find gradient-based recommendation\n",
    "        if len(gradient_norms) > 5:\n",
    "            # Smooth gradient norms\n",
    "            grad_smooth = np.convolve(gradient_norms, np.ones(5)/5, mode='valid')\n",
    "            grad_lr_range = learning_rates[2:-2]  # Account for convolution size\n",
    "            \n",
    "            # Find LR where gradients are stable but not too small\n",
    "            stable_grad_mask = (grad_smooth > np.percentile(grad_smooth, 10)) & \\\n",
    "                              (grad_smooth < np.percentile(grad_smooth, 90))\n",
    "            if np.any(stable_grad_mask):\n",
    "                stable_indices = np.where(stable_grad_mask)[0]\n",
    "                gradient_recommended_lr = grad_lr_range[stable_indices[len(stable_indices)//2]]\n",
    "                print(f\"  Gradient-stable LR: {gradient_recommended_lr:.2e}\")\n",
    "            else:\n",
    "                gradient_recommended_lr = best_lr\n",
    "        else:\n",
    "            gradient_recommended_lr = best_lr\n",
    "        \n",
    "        return {\n",
    "            'learning_rates': learning_rates,\n",
    "            'losses': losses,\n",
    "            'smoothed_losses': smoothed_losses,\n",
    "            'gradient_norms': gradient_norms,\n",
    "            'loss_improvements': loss_improvements,\n",
    "            'recommendations': {\n",
    "                'best_lr': best_lr,\n",
    "                'steepest_lr': steepest_lr,\n",
    "                'gradient_stable_lr': gradient_recommended_lr\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def visualize_lr_finder_results(results):\n",
    "        \"\"\"Create comprehensive LR finder visualization\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        lrs = results['learning_rates']\n",
    "        losses = results['losses']\n",
    "        smoothed_losses = results['smoothed_losses']\n",
    "        grad_norms = results['gradient_norms']\n",
    "        \n",
    "        # 1. Loss vs Learning Rate\n",
    "        axes[0,0].semilogx(lrs, losses, alpha=0.6, label='Raw Loss')\n",
    "        if len(smoothed_losses) > 0:\n",
    "            axes[0,0].semilogx(lrs[1:len(smoothed_losses)+1], smoothed_losses, \n",
    "                              linewidth=2, label='Smoothed Loss')\n",
    "        \n",
    "        # Mark recommendations\n",
    "        recs = results['recommendations']\n",
    "        axes[0,0].axvline(recs['best_lr'], color='red', linestyle='--', \n",
    "                         label=f'Best LR: {recs[\"best_lr\"]:.2e}')\n",
    "        axes[0,0].axvline(recs['steepest_lr'], color='orange', linestyle='--', \n",
    "                         label=f'Steepest: {recs[\"steepest_lr\"]:.2e}')\n",
    "        \n",
    "        axes[0,0].set_title('Loss vs Learning Rate', fontweight='bold')\n",
    "        axes[0,0].set_xlabel('Learning Rate')\n",
    "        axes[0,0].set_ylabel('Loss')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Gradient Norm vs Learning Rate\n",
    "        axes[0,1].semilogx(lrs, grad_norms, linewidth=2, color='green')\n",
    "        axes[0,1].axvline(recs['gradient_stable_lr'], color='purple', linestyle='--', \n",
    "                         label=f'Grad Stable: {recs[\"gradient_stable_lr\"]:.2e}')\n",
    "        axes[0,1].set_title('Gradient Norm vs Learning Rate', fontweight='bold')\n",
    "        axes[0,1].set_xlabel('Learning Rate')\n",
    "        axes[0,1].set_ylabel('Gradient Norm')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Loss Improvement Rate\n",
    "        if len(results['loss_improvements']) > 0:\n",
    "            improvement_lrs = lrs[2:len(results['loss_improvements'])+2]\n",
    "            axes[0,2].semilogx(improvement_lrs, results['loss_improvements'], \n",
    "                              linewidth=2, color='orange')\n",
    "            axes[0,2].set_title('Loss Improvement Rate', fontweight='bold')\n",
    "            axes[0,2].set_xlabel('Learning Rate')\n",
    "            axes[0,2].set_ylabel('Loss Improvement')\n",
    "            axes[0,2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Learning Rate Schedule\n",
    "        axes[1,0].plot(lrs, linewidth=2)\n",
    "        axes[1,0].set_title('Learning Rate Schedule', fontweight='bold')\n",
    "        axes[1,0].set_xlabel('Batch')\n",
    "        axes[1,0].set_ylabel('Learning Rate')\n",
    "        axes[1,0].set_yscale('log')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Loss Distribution\n",
    "        axes[1,1].hist(losses, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[1,1].set_title('Loss Distribution', fontweight='bold')\n",
    "        axes[1,1].set_xlabel('Loss')\n",
    "        axes[1,1].set_ylabel('Frequency')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Gradient Norm Distribution\n",
    "        axes[1,2].hist(grad_norms, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        axes[1,2].set_title('Gradient Norm Distribution', fontweight='bold')\n",
    "        axes[1,2].set_xlabel('Gradient Norm')\n",
    "        axes[1,2].set_ylabel('Frequency')\n",
    "        axes[1,2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Learning Rate Finder: Comprehensive Analysis', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(results_dir / 'learning_rate_finder_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Create test model and data\n",
    "    class LRTestModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(20, 50),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(50, 30),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(30, 5)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "    \n",
    "    # Create synthetic dataset\n",
    "    test_data = torch.randn(200, 20)\n",
    "    test_targets = torch.randn(200, 5)\n",
    "    test_dataset = TensorDataset(test_data, test_targets)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "    model = LRTestModel()\n",
    "    \n",
    "    print(f\"üß™ LR Finder Test Setup:\")\n",
    "    print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  Dataset size: {len(test_dataset)}\")\n",
    "    print(f\"  Batch size: {test_loader.batch_size}\")\n",
    "    \n",
    "    # Run LR finder\n",
    "    lr_results = find_optimal_learning_rate(model, test_loader)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_lr_finder_results(lr_results)\n",
    "    \n",
    "    # Print recommendations\n",
    "    print(f\"\\nüéØ Learning Rate Recommendations:\")\n",
    "    recs = lr_results['recommendations']\n",
    "    print(f\"  Best LR (minimum loss): {recs['best_lr']:.2e}\")\n",
    "    print(f\"  Steepest descent LR: {recs['steepest_lr']:.2e}\")\n",
    "    print(f\"  Gradient-stable LR: {recs['gradient_stable_lr']:.2e}\")\n",
    "    print(f\"\\nüí° Usage Suggestions:\")\n",
    "    print(f\"  ‚Ä¢ Start training with: {recs['steepest_lr']:.2e}\")\n",
    "    print(f\"  ‚Ä¢ Use for fine-tuning: {recs['best_lr']:.2e}\")\n",
    "    print(f\"  ‚Ä¢ Conservative choice: {recs['gradient_stable_lr']:.2e}\")\n",
    "    \n",
    "    return lr_results\n",
    "\n",
    "# Execute learning rate finder\n",
    "lr_finder_results = implement_learning_rate_finder()\n",
    "```\n",
    "\n",
    "### 7.2 Adversarial Examples with Gradients\n",
    "\n",
    "```python\n",
    "def demonstrate_adversarial_examples():\n",
    "    \"\"\"Demonstrate gradient-based adversarial example generation\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 7.2 Adversarial Examples with Gradient-Based Attacks ===\\n\")\n",
    "    \n",
    "    def fast_gradient_sign_method(model, image, target, epsilon=0.1):\n",
    "        \"\"\"Implement Fast Gradient Sign Method (FGSM)\"\"\"\n",
    "        \n",
    "        model.eval()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Ensure image requires gradients\n",
    "        image_copy = image.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(image_copy)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Get original prediction\n",
    "        original_pred = output.argmax(dim=1)\n",
    "        original_confidence = torch.softmax(output, dim=1).max()\n",
    "        \n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Generate adversarial example\n",
    "        data_grad = image_copy.grad.data\n",
    "        sign_data_grad = data_grad.sign()\n",
    "        perturbed_image = image_copy + epsilon * sign_data_grad\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "        \n",
    "        # Test adversarial example\n",
    "        with torch.no_grad():\n",
    "            perturbed_output = model(perturbed_image)\n",
    "            perturbed_pred = perturbed_output.argmax(dim=1)\n",
    "            perturbed_confidence = torch.softmax(perturbed_output, dim=1).max()\n",
    "        \n",
    "        return {\n",
    "            'original_image': image.detach(),\n",
    "            'perturbed_image': perturbed_image.detach(),\n",
    "            'perturbation': (perturbed_image - image).detach(),\n",
    "            'original_pred': original_pred.item(),\n",
    "            'perturbed_pred': perturbed_pred.item(),\n",
    "            'original_confidence': original_confidence.item(),\n",
    "            'perturbed_confidence': perturbed_confidence.item(),\n",
    "            'attack_success': original_pred.item() != perturbed_pred.item()\n",
    "        }\n",
    "    \n",
    "    def projected_gradient_descent(model, image, target, epsilon=0.1, \n",
    "                                  alpha=0.01, num_iter=10):\n",
    "        \"\"\"Implement Projected Gradient Descent (PGD) attack\"\"\"\n",
    "        \n",
    "        model.eval()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize with small random perturbation\n",
    "        perturbed_image = image.clone().detach()\n",
    "        perturbed_image += torch.empty_like(perturbed_image).uniform_(-epsilon, epsilon)\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "        \n",
    "        original_image = image.clone().detach()\n",
    "        \n",
    "        attack_trajectory = []\n",
    "        \n",
    "        for iteration in range(num_iter):\n",
    "            perturbed_image.requires_grad_(True)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(perturbed_image)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # PGD update\n",
    "            with torch.no_grad():\n",
    "                data_grad = perturbed_image.grad.data\n",
    "                sign_data_grad = data_grad.sign()\n",
    "                \n",
    "                # Take step in direction of gradient\n",
    "                perturbed_image = perturbed_image + alpha * sign_data_grad\n",
    "                \n",
    "                # Project back to epsilon ball\n",
    "                eta = perturbed_image - original_image\n",
    "                eta = torch.clamp(eta, -epsilon, epsilon)\n",
    "                perturbed_image = original_image + eta\n",
    "                \n",
    "                # Ensure valid pixel range\n",
    "                perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "                \n",
    "                # Track progress\n",
    "                pred = output.argmax(dim=1).item()\n",
    "                confidence = torch.softmax(output, dim=1).max().item()\n",
    "                perturbation_norm = eta.norm().item()\n",
    "                \n",
    "                attack_trajectory.append({\n",
    "                    'iteration': iteration,\n",
    "                    'prediction': pred,\n",
    "                    'confidence': confidence,\n",
    "                    'perturbation_norm': perturbation_norm\n",
    "                })\n",
    "        \n",
    "        # Final evaluation\n",
    "        with torch.no_grad():\n",
    "            final_output = model(perturbed_image)\n",
    "            final_pred = final_output.argmax(dim=1)\n",
    "            final_confidence = torch.softmax(final_output, dim=1).max()\n",
    "        \n",
    "        return {\n",
    "            'original_image': original_image,\n",
    "            'perturbed_image': perturbed_image.detach(),\n",
    "            'perturbation': (perturbed_image - original_image).detach(),\n",
    "            'final_pred': final_pred.item(),\n",
    "            'final_confidence': final_confidence.item(),\n",
    "            'trajectory': attack_trajectory,\n",
    "            'attack_success': len(set(step['prediction'] for step in attack_trajectory)) > 1\n",
    "        }\n",
    "    \n",
    "    def visualize_adversarial_attack(attack_results, attack_name, class_names=None):\n",
    "        \"\"\"Visualize adversarial attack results\"\"\"\n",
    "        \n",
    "        if class_names is None:\n",
    "            class_names = [f\"Class {i}\" for i in range(10)]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        original_img = attack_results['original_image'][0]\n",
    "        perturbed_img = attack_results['perturbed_image'][0]\n",
    "        perturbation = attack_results['perturbation'][0]\n",
    "        \n",
    "        # Handle different image formats\n",
    "        if original_img.dim() == 3 and original_img.shape[0] in [1, 3]:\n",
    "            if original_img.shape[0] == 3:  # RGB\n",
    "                orig_display = original_img.permute(1, 2, 0).numpy()\n",
    "                pert_display = perturbed_img.permute(1, 2, 0).numpy()\n",
    "                diff_display = perturbation.permute(1, 2, 0).numpy()\n",
    "            else:  # Grayscale\n",
    "                orig_display = original_img[0].numpy()\n",
    "                pert_display = perturbed_img[0].numpy()\n",
    "                diff_display = perturbation[0].numpy()\n",
    "        else:\n",
    "            orig_display = original_img.numpy()\n",
    "            pert_display = perturbed_img.numpy()\n",
    "            diff_display = perturbation.numpy()\n",
    "        \n",
    "        # 1. Original image\n",
    "        if len(orig_display.shape) == 2:\n",
    "            axes[0,0].imshow(orig_display, cmap='gray')\n",
    "        else:\n",
    "            axes[0,0].imshow(orig_display)\n",
    "        \n",
    "        if 'original_pred' in attack_results:\n",
    "            orig_class = class_names[attack_results['original_pred']]\n",
    "            orig_conf = attack_results['original_confidence']\n",
    "            axes[0,0].set_title(f'Original\\n{orig_class}\\n(Conf: {orig_conf:.3f})', fontweight='bold')\n",
    "        else:\n",
    "            axes[0,0].set_title('Original Image', fontweight='bold')\n",
    "        axes[0,0].axis('off')\n",
    "        \n",
    "        # 2. Adversarial image\n",
    "        if len(pert_display.shape) == 2:\n",
    "            axes[0,1].imshow(pert_display, cmap='gray')\n",
    "        else:\n",
    "            axes[0,1].imshow(pert_display)\n",
    "        \n",
    "        if 'perturbed_pred' in attack_results:\n",
    "            pert_class = class_names[attack_results['perturbed_pred']]\n",
    "            pert_conf = attack_results['perturbed_confidence']\n",
    "        elif 'final_pred' in attack_results:\n",
    "            pert_class = class_names[attack_results['final_pred']]\n",
    "            pert_conf = attack_results['final_confidence']\n",
    "        else:\n",
    "            pert_class = \"Unknown\"\n",
    "            pert_conf = 0.0\n",
    "        \n",
    "        axes[0,1].set_title(f'Adversarial\\n{pert_class}\\n(Conf: {pert_conf:.3f})', fontweight='bold')\n",
    "        axes[0,1].axis('off')\n",
    "        \n",
    "        # 3. Perturbation (amplified for visibility)\n",
    "        perturbation_amplified = diff_display * 10  # Amplify for visualization\n",
    "        axes[0,2].imshow(perturbation_amplified, cmap='RdBu_r')\n",
    "        axes[0,2].set_title('Perturbation\\n(10x amplified)', fontweight='bold')\n",
    "        axes[0,2].axis('off')\n",
    "        \n",
    "        # 4. Perturbation statistics\n",
    "        perturbation_flat = perturbation.flatten()\n",
    "        axes[1,0].hist(perturbation_flat.numpy(), bins=50, alpha=0.7, color='orange')\n",
    "        axes[1,0].set_title('Perturbation Distribution', fontweight='bold')\n",
    "        axes[1,0].set_xlabel('Perturbation Value')\n",
    "        axes[1,0].set_ylabel('Frequency')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Attack trajectory (if available)\n",
    "        if 'trajectory' in attack_results and attack_results['trajectory']:\n",
    "            trajectory = attack_results['trajectory']\n",
    "            iterations = [step['iteration'] for step in trajectory]\n",
    "            confidences = [step['confidence'] for step in trajectory]\n",
    "            predictions = [step['prediction'] for step in trajectory]\n",
    "            \n",
    "            axes[1,1].plot(iterations, confidences, 'o-', linewidth=2, markersize=6)\n",
    "            axes[1,1].set_title('Confidence Over Iterations', fontweight='bold')\n",
    "            axes[1,1].set_xlabel('Iteration')\n",
    "            axes[1,1].set_ylabel('Prediction Confidence')\n",
    "            axes[1,1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Show prediction changes\n",
    "            for i, (iter_num, pred) in enumerate(zip(iterations, predictions)):\n",
    "                if i == 0 or pred != predictions[i-1]:\n",
    "                    axes[1,1].axvline(iter_num, color='red', linestyle='--', alpha=0.7)\n",
    "                    axes[1,1].text(iter_num, confidences[i], f'‚Üí{class_names[pred][:5]}', \n",
    "                                  rotation=90, fontsize=8)\n",
    "        else:\n",
    "            axes[1,1].text(0.5, 0.5, 'No trajectory data\\n(single-step attack)', \n",
    "                          ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "            axes[1,1].set_title('Attack Trajectory', fontweight='bold')\n",
    "        \n",
    "        # 6. Attack summary\n",
    "        axes[1,2].axis('off')\n",
    "        summary_text = f\"{attack_name} Attack Results\\n\\n\"\n",
    "        \n",
    "        if 'attack_success' in attack_results:\n",
    "            success = attack_results['attack_success']\n",
    "            summary_text += f\"Attack Success: {'‚úÖ Yes' if success else '‚ùå No'}\\n\"\n",
    "        \n",
    "        perturbation_norm = perturbation.norm().item()\n",
    "        perturbation_linf = perturbation.abs().max().item()\n",
    "        \n",
    "        summary_text += f\"L2 Norm: {perturbation_norm:.6f}\\n\"\n",
    "        summary_text += f\"L‚àû Norm: {perturbation_linf:.6f}\\n\"\n",
    "        summary_text += f\"Mean |perturbation|: {perturbation.abs().mean().item():.6f}\\n\"\n",
    "        \n",
    "        axes[1,2].text(0.1, 0.9, summary_text, transform=axes[1,2].transAxes, \n",
    "                      fontsize=12, verticalalignment='top', \n",
    "                      bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "        \n",
    "        plt.suptitle(f'{attack_name} Adversarial Attack Analysis', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(results_dir / f'adversarial_attack_{attack_name.lower().replace(\" \", \"_\")}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Create simple CNN for demonstration\n",
    "    class AdversarialTestCNN(nn.Module):\n",
    "        def __init__(self, num_classes=10):\n",
    "            super().__init__()\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, 3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(32, 64, 3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.AdaptiveAvgPool2d(4)\n",
    "            )\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(64 * 16, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(128, num_classes)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "    \n",
    "    # Create model and synthetic data\n",
    "    model = AdversarialTestCNN(num_classes=10)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create synthetic image (28x28 grayscale, like MNIST)\n",
    "    synthetic_image = torch.rand(1, 1, 28, 28)\n",
    "    true_target = torch.tensor([3])  # Pretend this is class 3\n",
    "    \n",
    "    class_names = [f\"Class_{i}\" for i in range(10)]\n",
    "    \n",
    "    print(f\"üéØ Adversarial Attack Setup:\")\n",
    "    print(f\"  Model: Simple CNN with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    print(f\"  Image shape: {synthetic_image.shape}\")\n",
    "    print(f\"  True class: {class_names[true_target.item()]}\")\n",
    "    \n",
    "    # Get original prediction\n",
    "    with torch.no_grad():\n",
    "        orig_output = model(synthetic_image)\n",
    "        orig_pred = orig_output.argmax(dim=1)\n",
    "        orig_conf = torch.softmax(orig_output, dim=1).max()\n",
    "    \n",
    "    print(f\"  Original prediction: {class_names[orig_pred.item()]} (confidence: {orig_conf:.3f})\")\n",
    "    \n",
    "    # Test FGSM attack\n",
    "    print(f\"\\nüèÉ‚Äç‚ôÇÔ∏è Running FGSM Attack...\")\n",
    "    fgsm_results = fast_gradient_sign_method(model, synthetic_image, true_target, epsilon=0.3)\n",
    "    visualize_adversarial_attack(fgsm_results, \"FGSM\", class_names)\n",
    "    \n",
    "    print(f\"FGSM Results:\")\n",
    "    print(f\"  Original: {class_names[fgsm_results['original_pred']]} ‚Üí Adversarial: {class_names[fgsm_results['perturbed_pred']]}\")\n",
    "    print(f\"  Attack success: {fgsm_results['attack_success']}\")\n",
    "    print(f\"  Perturbation L2 norm: {fgsm_results['perturbation'].norm().item():.6f}\")\n",
    "    \n",
    "    # Test PGD attack\n",
    "    print(f\"\\nüîÑ Running PGD Attack...\")\n",
    "    pgd_results = projected_gradient_descent(model, synthetic_image, true_target, \n",
    "                                           epsilon=0.3, alpha=0.05, num_iter=10)\n",
    "    visualize_adversarial_attack(pgd_results, \"PGD\", class_names)\n",
    "    \n",
    "    print(f\"PGD Results:\")\n",
    "    print(f\"  Final prediction: {class_names[pgd_results['final_pred']]}\")\n",
    "    print(f\"  Attack success: {pgd_results['attack_success']}\")\n",
    "    print(f\"  Perturbation L2 norm: {pgd_results['perturbation'].norm().item():.6f}\")\n",
    "    print(f\"  Iterations: {len(pgd_results['trajectory'])}\")\n",
    "    \n",
    "    print(f\"\\nüõ°Ô∏è Adversarial Examples Key Insights:\")\n",
    "    print(f\"  ‚Ä¢ Small perturbations can fool neural networks\")\n",
    "    print(f\"  ‚Ä¢ Gradients reveal model vulnerabilities\")\n",
    "    print(f\"  ‚Ä¢ FGSM is fast but less effective than PGD\")\n",
    "    print(f\"  ‚Ä¢ PGD is iterative and often more successful\")\n",
    "    print(f\"  ‚Ä¢ Defense mechanisms use similar gradient techniques\")\n",
    "    print(f\"  ‚Ä¢ Important for security and robustness in AI systems\")\n",
    "    \n",
    "    return {\n",
    "        'fgsm_results': fgsm_results,\n",
    "        'pgd_results': pgd_results,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "# Execute adversarial examples demonstration\n",
    "adversarial_results = demonstrate_adversarial_examples()\n",
    "```\n",
    "\n",
    "## 8. Comprehensive Summary and Next Steps\n",
    "\n",
    "```python\n",
    "def generate_comprehensive_summary():\n",
    "    \"\"\"Generate final comprehensive summary of gradient computation mastery\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéì PYTORCH GRADIENT COMPUTATION MASTERY - COMPREHENSIVE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary = {\n",
    "        'completion_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'notebook_sections': 8,\n",
    "        'total_demonstrations': 12,\n",
    "        'concepts_mastered': [],\n",
    "        'practical_skills': [],\n",
    "        'advanced_techniques': [],\n",
    "        'performance_optimizations': [],\n",
    "        'applications_explored': []\n",
    "    }\n",
    "    \n",
    "    # Concepts mastered\n",
    "    concepts_mastered = [\n",
    "        \"‚úÖ Fundamental autograd mechanics and computational graphs\",\n",
    "        \"‚úÖ Vector functions and Jacobian matrix computation\", \n",
    "        \"‚úÖ Neural network gradient flow analysis\",\n",
    "        \"‚úÖ Higher-order gradients and Hessian matrices\",\n",
    "        \"‚úÖ Gradient debugging and anomaly detection\",\n",
    "        \"‚úÖ Interactive gradient visualization techniques\",\n",
    "        \"‚úÖ Real-time optimization trajectory analysis\"\n",
    "    ]\n",
    "    \n",
    "    practical_skills = [\n",
    "        \"üîß Gradient accumulation for large-scale training\",\n",
    "        \"üîß Memory-efficient gradient checkpointing\",\n",
    "        \"üîß Learning rate finding with gradient analysis\",\n",
    "        \"üîß Comprehensive gradient debugging workflows\",\n",
    "        \"üîß Performance profiling and optimization\",\n",
    "        \"üîß Custom autograd function development readiness\"\n",
    "    ]\n",
    "    \n",
    "    advanced_techniques = [\n",
    "        \"üöÄ Fast Gradient Sign Method (FGSM) implementation\",\n",
    "        \"üöÄ Projected Gradient Descent (PGD) attacks\",\n",
    "        \"üöÄ Multi-dimensional gradient landscape visualization\",\n",
    "        \"üöÄ Eigenvalue analysis for optimization insights\",\n",
    "        \"üöÄ Gradient-based hyperparameter optimization\",\n",
    "        \"üöÄ Mixed precision training considerations\"\n",
    "    ]\n",
    "    \n",
    "    performance_optimizations = [\n",
    "        \"‚ö° Gradient checkpointing for memory efficiency\",\n",
    "        \"‚ö° Accumulation techniques for effective large batches\",\n",
    "        \"‚ö° Memory usage profiling and optimization\",\n",
    "        \"‚ö° Computational graph optimization strategies\",\n",
    "        \"‚ö° Efficient second-order derivative computation\"\n",
    "    ]\n",
    "    \n",
    "    applications_explored = [\n",
    "        \"üéØ Adversarial example generation and analysis\",\n",
    "        \"üéØ Learning rate schedule optimization\",\n",
    "        \"üéØ Model vulnerability assessment\",\n",
    "        \"üéØ Training stability analysis\",\n",
    "        \"üéØ Optimization landscape exploration\",\n",
    "        \"üéØ Gradient-based meta-learning foundations\"\n",
    "    ]\n",
    "    \n",
    "    # Display mastery overview\n",
    "    print(f\"\\nüìö CORE CONCEPTS MASTERED:\")\n",
    "    for concept in concepts_mastered:\n",
    "        print(f\"  {concept}\")\n",
    "        \n",
    "    print(f\"\\nüõ†Ô∏è PRACTICAL SKILLS DEVELOPED:\")\n",
    "    for skill in practical_skills:\n",
    "        print(f\"  {skill}\")\n",
    "        \n",
    "    print(f\"\\nüß† ADVANCED TECHNIQUES LEARNED:\")\n",
    "    for technique in advanced_techniques:\n",
    "        print(f\"  {technique}\")\n",
    "        \n",
    "    print(f\"\\n‚ö° PERFORMANCE OPTIMIZATIONS:\")\n",
    "    for optimization in performance_optimizations:\n",
    "        print(f\"  {optimization}\")\n",
    "        \n",
    "    print(f\"\\nüéØ APPLICATIONS EXPLORED:\")\n",
    "    for application in applications_explored:\n",
    "        print(f\"  {application}\")\n",
    "    \n",
    "    # Store in summary\n",
    "    summary['concepts_mastered'] = concepts_mastered\n",
    "    summary['practical_skills'] = practical_skills\n",
    "    summary['advanced_techniques'] = advanced_techniques\n",
    "    summary['performance_optimizations'] = performance_optimizations\n",
    "    summary['applications_explored'] = applications_explored\n",
    "    \n",
    "    # Next steps and recommendations\n",
    "    next_steps = [\n",
    "        \"üìì 03_custom_autograd_functions.ipynb - Implement custom autograd operations\",\n",
    "        \"üìì 04_neural_network_architectures.ipynb - Build advanced network architectures\",\n",
    "        \"üìì 05_optimization_algorithms.ipynb - Explore advanced optimizers\",\n",
    "        \"üìì 06_computer_vision_applications.ipynb - Apply to CNN architectures\",\n",
    "        \"üìì 07_sequence_modeling.ipynb - Gradient flow in RNNs and Transformers\",\n",
    "        \"üìì 08_research_applications.ipynb - Meta-learning and neural architecture search\"\n",
    "    ]\n",
    "    \n",
    "    advanced_challenges = [\n",
    "        \"üèÜ Implement MAML (Model-Agnostic Meta-Learning)\",\n",
    "        \"üèÜ Create custom activation functions with proper gradients\",\n",
    "        \"üèÜ Build gradient penalty for improved GAN training\",\n",
    "        \"üèÜ Develop adversarial training defense mechanisms\", \n",
    "        \"üèÜ Implement neural architecture search with gradients\",\n",
    "        \"üèÜ Create curriculum learning with gradient-based difficulty\"\n",
    "    ]\n",
    "    \n",
    "    research_directions = [\n",
    "        \"üî¨ Second-order optimization methods (K-FAC, Natural Gradients)\",\n",
    "        \"üî¨ Gradient-based neural architecture search (GDAS, DARTS)\",\n",
    "        \"üî¨ Meta-learning and few-shot learning applications\",\n",
    "        \"üî¨ Adversarial training and certified defenses\",\n",
    "        \"üî¨ Gradient flow analysis in very deep networks\",\n",
    "        \"üî¨ Quantum-inspired gradient computation methods\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüöÄ IMMEDIATE NEXT STEPS:\")\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\")\n",
    "        \n",
    "    print(f\"\\nüèÜ ADVANCED CHALLENGES TO TACKLE:\")\n",
    "    for challenge in advanced_challenges:\n",
    "        print(f\"  {challenge}\")\n",
    "        \n",
    "    print(f\"\\nüî¨ RESEARCH DIRECTIONS TO EXPLORE:\")\n",
    "    for direction in research_directions:\n",
    "        print(f\"  {direction}\")\n",
    "    \n",
    "    # Key insights and best practices\n",
    "    key_insights = [\n",
    "        \"üí° Always monitor gradient norms during training for stability\",\n",
    "        \"üí° Use gradient clipping for RNN and very deep network training\",\n",
    "        \"üí° Leverage gradient accumulation for memory-constrained environments\",\n",
    "        \"üí° Visualize gradient landscapes to understand optimization challenges\",\n",
    "        \"üí° Implement comprehensive gradient debugging for research projects\",\n",
    "        \"üí° Consider second-order information for advanced optimization\",\n",
    "        \"üí° Apply adversarial techniques for robustness testing\"\n",
    "    ]\n",
    "    \n",
    "    best_practices = [\n",
    "        \"üéØ Enable anomaly detection during development phases\",\n",
    "        \"üéØ Use learning rate finders for new model architectures\",\n",
    "        \"üéØ Profile memory usage with gradient checkpointing\",\n",
    "        \"üéØ Implement gradient-based hyperparameter optimization\",\n",
    "        \"üéØ Document gradient flow patterns for reproducible research\",\n",
    "        \"üéØ Test model robustness with adversarial examples\",\n",
    "        \"üéØ Use mixed precision when available for efficiency\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüí° KEY INSIGHTS FOR PRACTITIONERS:\")\n",
    "    for insight in key_insights:\n",
    "        print(f\"  {insight}\")\n",
    "        \n",
    "    print(f\"\\nüéØ BEST PRACTICES FOR PRODUCTION:\")\n",
    "    for practice in best_practices:\n",
    "        print(f\"  {practice}\")\n",
    "    \n",
    "    # Performance metrics from demonstrations\n",
    "    print(f\"\\nüìä NOTEBOOK PERFORMANCE METRICS:\")\n",
    "    print(f\"  Total functions implemented: 15+\")\n",
    "    print(f\"  Visualizations created: 25+\")\n",
    "    print(f\"  Code examples: 50+\")\n",
    "    print(f\"  Mathematical concepts: 12\")\n",
    "    print(f\"  Optimization techniques: 8\")\n",
    "    print(f\"  Debugging methods: 6\")\n",
    "    \n",
    "    # Save comprehensive summary\n",
    "    summary.update({\n",
    "        'next_steps': next_steps,\n",
    "        'advanced_challenges': advanced_challenges,\n",
    "        'research_directions': research_directions,\n",
    "        'key_insights': key_insights,\n",
    "        'best_practices': best_practices\n",
    "    })\n",
    "    \n",
    "    with open(results_dir / 'comprehensive_gradient_mastery_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Complete mastery summary saved to:\")\n",
    "    print(f\"    {results_dir / 'comprehensive_gradient_mastery_summary.json'}\")\n",
    "    \n",
    "    # List all generated files\n",
    "    print(f\"\\nüìÇ Generated Learning Artifacts:\")\n",
    "    all_files = list(results_dir.glob('*'))\n",
    "    \n",
    "    for file_path in sorted(all_files):\n",
    "        if file_path.is_file():\n",
    "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  üìÑ {file_path.name} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    total_size = sum(f.stat().st_size for f in all_files if f.is_file()) / (1024 * 1024)\n",
    "    print(f\"\\nüìä Total artifacts: {len(all_files)} files ({total_size:.1f} MB)\")\n",
    "    \n",
    "    print(f\"\\nüåü CONGRATULATIONS! You've achieved GRADIENT COMPUTATION MASTERY!\")\n",
    "    print(f\"üéØ You're now ready to tackle advanced PyTorch research and development!\")\n",
    "    print(f\"üöÄ Continue your journey with the next notebook in the PyTorch Mastery Hub series!\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate final comprehensive summary\n",
    "final_summary = generate_comprehensive_summary()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ PYTORCH GRADIENT COMPUTATION DEEP DIVE - COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Final Notes\n",
    "\n",
    "This comprehensive gradient computation notebook has taken you from basic autograd concepts to advanced research-level techniques. You've explored:\n",
    "\n",
    "### üéì **Theoretical Foundations**\n",
    "- Computational graph construction and traversal\n",
    "- Vector calculus and Jacobian matrices\n",
    "- Higher-order derivatives and Hessian analysis\n",
    "- Mathematical optimization principles\n",
    "\n",
    "### üõ†Ô∏è **Practical Implementation Skills**\n",
    "- Interactive gradient visualization techniques\n",
    "- Performance optimization strategies\n",
    "- Memory-efficient training methods\n",
    "- Comprehensive debugging workflows\n",
    "\n",
    "### üöÄ **Advanced Applications**\n",
    "- Adversarial example generation and analysis\n",
    "- Learning rate optimization techniques\n",
    "- Gradient-based hyperparameter tuning\n",
    "- Research-ready optimization tools\n",
    "\n",
    "### üìä **Production-Ready Techniques**\n",
    "- Gradient accumulation for scalable training\n",
    "- Memory profiling and optimization\n",
    "- Robust debugging and monitoring\n",
    "- Performance analysis and optimization\n",
    "\n",
    "**You are now equipped with the knowledge and skills to tackle advanced PyTorch projects, implement custom autograd functions, debug complex training scenarios, and explore cutting-edge research applications.**\n",
    "\n",
    "**Next recommended notebooks:**\n",
    "- Custom Autograd Functions Development\n",
    "- Advanced Neural Network Architectures  \n",
    "- Meta-Learning and Few-Shot Learning\n",
    "- Neural Architecture Search with Gradients\n",
    "\n",
    "**Happy gradient computing! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
