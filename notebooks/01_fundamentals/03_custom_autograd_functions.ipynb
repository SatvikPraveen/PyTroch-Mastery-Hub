{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a351b7",
   "metadata": {},
   "source": [
    "# Custom Autograd Functions Mastery: PyTorch Mastery Hub\n",
    "\n",
    "**Building Advanced Differentiable Operations from Scratch**\n",
    "\n",
    "**Authors:** PyTorch Mastery Hub Team  \n",
    "**Institution:** Advanced Deep Learning Education  \n",
    "**Course:** PyTorch Fundamentals & Advanced Techniques  \n",
    "\n",
    "## Overview\n",
    "\n",
    "This comprehensive notebook provides deep expertise in creating custom autograd functions in PyTorch. We'll master the art of building differentiable operations from the ground up, enabling advanced research capabilities and production-ready custom operations.\n",
    "\n",
    "## Key Objectives\n",
    "1. Master the `torch.autograd.Function` API and its advanced features\n",
    "2. Implement custom forward and backward passes with proper gradient computation\n",
    "3. Build memory-efficient operations for large-scale training scenarios\n",
    "4. Handle non-differentiable operations using advanced techniques\n",
    "5. Create production-ready custom functions with comprehensive testing\n",
    "6. Develop advanced loss functions and specialized operations\n",
    "7. Apply best practices for debugging and performance optimization\n",
    "\n",
    "## üìö Learning Path\n",
    "- **Prerequisites:** Gradient computation fundamentals, calculus knowledge, PyTorch basics\n",
    "- **Difficulty:** Advanced\n",
    "- **Duration:** 2-3 hours\n",
    "- **Applications:** Research, custom architectures, specialized domains\n",
    "\n",
    "## üéØ Advanced Topics Coverage\n",
    "- Custom activation functions with complex derivatives\n",
    "- Memory-efficient operations and gradient checkpointing\n",
    "- Straight-through estimators for non-differentiable operations\n",
    "- Advanced loss functions (Focal Loss, Knowledge Distillation)\n",
    "- Comprehensive testing and validation frameworks\n",
    "- Performance optimization and debugging techniques\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Environment Setup and Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive imports for advanced custom autograd functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Tuple, Optional, List, Dict, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced utilities\n",
    "from collections import defaultdict\n",
    "import inspect\n",
    "from functools import wraps\n",
    "\n",
    "# Create results directory for this notebook\n",
    "results_dir = Path('../results/notebooks/custom_autograd_functions')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Setup environment\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"üî• PyTorch Mastery Hub - Custom Autograd Functions Mastery\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üì± Device: {device}\")\n",
    "print(f\"üé® PyTorch version: {torch.__version__}\")\n",
    "print(f\"üìä NumPy version: {np.__version__}\")\n",
    "print(f\"üìÅ Results directory: {results_dir}\")\n",
    "print(\"‚ú® Ready to master custom differentiable operations!\\n\")\n",
    "\n",
    "# Performance tracking\n",
    "class PerformanceTracker:\n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "    \n",
    "    def track(self, name, value):\n",
    "        self.metrics[name].append(value)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        summary = {}\n",
    "        for name, values in self.metrics.items():\n",
    "            summary[name] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values),\n",
    "                'count': len(values)\n",
    "            }\n",
    "        return summary\n",
    "\n",
    "performance_tracker = PerformanceTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebe4e29",
   "metadata": {},
   "source": [
    "## 2. Autograd Function Fundamentals: From Basics to Advanced\n",
    "\n",
    "### 2.1 Understanding the Core Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4444532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_autograd_architecture():\n",
    "    \"\"\"Comprehensive demonstration of autograd function architecture\"\"\"\n",
    "    \n",
    "    print(\"=== 2.1 Autograd Function Architecture Deep Dive ===\\n\")\n",
    "    \n",
    "    class DetailedSquareFunction(Function):\n",
    "        \"\"\"Extensively documented square function for educational purposes\"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, input, verbose=False):\n",
    "            \"\"\"\n",
    "            Forward pass: f(x) = x¬≤\n",
    "            \n",
    "            Args:\n",
    "                ctx: PyTorch context object for saving backward information\n",
    "                input: Input tensor of any shape\n",
    "                verbose: Whether to print detailed information\n",
    "                \n",
    "            Returns:\n",
    "                Output tensor with same shape as input\n",
    "                \n",
    "            Mathematical Details:\n",
    "                f(x) = x¬≤\n",
    "                Domain: All real numbers\n",
    "                Range: [0, +‚àû)\n",
    "            \"\"\"\n",
    "            if verbose:\n",
    "                print(f\"üìä Forward Pass Analysis:\")\n",
    "                print(f\"  Input shape: {input.shape}\")\n",
    "                print(f\"  Input dtype: {input.dtype}\")\n",
    "                print(f\"  Input device: {input.device}\")\n",
    "                print(f\"  Input requires_grad: {input.requires_grad}\")\n",
    "                print(f\"  Input range: [{input.min().item():.4f}, {input.max().item():.4f}]\")\n",
    "            \n",
    "            # Save input for backward pass - critical for gradient computation\n",
    "            ctx.save_for_backward(input)\n",
    "            \n",
    "            # Store additional metadata if needed\n",
    "            ctx.input_shape = input.shape\n",
    "            ctx.verbose = verbose\n",
    "            \n",
    "            # Compute forward operation\n",
    "            result = input * input  # Equivalent to input ** 2 but more efficient\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Output shape: {result.shape}\")\n",
    "                print(f\"  Output range: [{result.min().item():.4f}, {result.max().item():.4f}]\")\n",
    "                print(f\"  Memory usage: {result.numel() * result.element_size() / 1024**2:.2f} MB\")\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Backward pass: df/dx = 2x\n",
    "            \n",
    "            Args:\n",
    "                ctx: Context with saved forward information\n",
    "                grad_output: Gradient of loss w.r.t. output (‚àÇL/‚àÇy)\n",
    "                \n",
    "            Returns:\n",
    "                grad_input: Gradient of loss w.r.t. input (‚àÇL/‚àÇx)\n",
    "                grad_verbose: None (verbose is not a tensor parameter)\n",
    "                \n",
    "            Mathematical Details:\n",
    "                If f(x) = x¬≤, then f'(x) = 2x\n",
    "                By chain rule: ‚àÇL/‚àÇx = ‚àÇL/‚àÇy * ‚àÇy/‚àÇx = grad_output * 2x\n",
    "            \"\"\"\n",
    "            # Retrieve saved tensors\n",
    "            input, = ctx.saved_tensors\n",
    "            verbose = ctx.verbose\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nüîÑ Backward Pass Analysis:\")\n",
    "                print(f\"  Grad output shape: {grad_output.shape}\")\n",
    "                print(f\"  Grad output range: [{grad_output.min().item():.4f}, {grad_output.max().item():.4f}]\")\n",
    "                print(f\"  Saved input range: [{input.min().item():.4f}, {input.max().item():.4f}]\")\n",
    "            \n",
    "            # Compute gradient using chain rule\n",
    "            # df/dx = 2x, so ‚àÇL/‚àÇx = grad_output * 2x\n",
    "            grad_input = 2 * input * grad_output\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Computed gradient range: [{grad_input.min().item():.4f}, {grad_input.max().item():.4f}]\")\n",
    "                print(f\"  Gradient norm: {grad_input.norm().item():.4f}\")\n",
    "            \n",
    "            # Return gradients for all forward inputs\n",
    "            # For non-tensor inputs (like verbose), return None\n",
    "            return grad_input, None\n",
    "    \n",
    "    # Wrapper function for convenience\n",
    "    def detailed_square(input, verbose=False):\n",
    "        \"\"\"Convenient wrapper for DetailedSquareFunction\"\"\"\n",
    "        return DetailedSquareFunction.apply(input, verbose)\n",
    "    \n",
    "    return detailed_square\n",
    "\n",
    "# Create and test the detailed square function\n",
    "detailed_square = demonstrate_autograd_architecture()\n",
    "\n",
    "print(\"üß™ Testing Detailed Square Function:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Test with different tensor types and shapes\n",
    "test_cases = [\n",
    "    (\"1D tensor\", torch.tensor([1.0, 2.0, 3.0], requires_grad=True)),\n",
    "    (\"2D tensor\", torch.randn(3, 4, requires_grad=True)),\n",
    "    (\"3D tensor\", torch.randn(2, 3, 4, requires_grad=True)),\n",
    "    (\"Large tensor\", torch.randn(100, 100, requires_grad=True))\n",
    "]\n",
    "\n",
    "results_summary = {}\n",
    "\n",
    "for name, test_tensor in test_cases:\n",
    "    print(f\"\\nüìã Test Case: {name}\")\n",
    "    print(f\"Shape: {test_tensor.shape}, Elements: {test_tensor.numel()}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    start_time = time.time()\n",
    "    output = detailed_square(test_tensor, verbose=(name == \"1D tensor\"))\n",
    "    forward_time = time.time() - start_time\n",
    "    \n",
    "    # Backward pass\n",
    "    start_time = time.time()\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    backward_time = time.time() - start_time\n",
    "    \n",
    "    # Verify gradients\n",
    "    analytical_grad = 2 * test_tensor.detach()\n",
    "    gradient_error = (test_tensor.grad - analytical_grad).abs().max().item()\n",
    "    \n",
    "    results_summary[name] = {\n",
    "        'forward_time': forward_time * 1000,  # Convert to ms\n",
    "        'backward_time': backward_time * 1000,\n",
    "        'gradient_error': gradient_error,\n",
    "        'output_norm': output.norm().item(),\n",
    "        'gradient_norm': test_tensor.grad.norm().item()\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Forward time: {forward_time*1000:.3f}ms\")\n",
    "    print(f\"‚úÖ Backward time: {backward_time*1000:.3f}ms\")\n",
    "    print(f\"‚úÖ Gradient error: {gradient_error:.2e}\")\n",
    "    print(f\"‚úÖ Gradient correct: {gradient_error < 1e-6}\")\n",
    "\n",
    "# Save results\n",
    "with open(results_dir / 'basic_autograd_test_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Basic autograd test results saved to {results_dir / 'basic_autograd_test_results.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92384d8",
   "metadata": {},
   "source": [
    "### 2.2 Multi-Input Functions with Complex Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_multi_input_functions():\n",
    "    \"\"\"Advanced multi-input custom functions with complex gradient computation\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 2.2 Advanced Multi-Input Functions ===\\n\")\n",
    "    \n",
    "    class WeightedNormFunction(Function):\n",
    "        \"\"\"\n",
    "        Custom function: f(x, w, p) = ||w ‚äô x||_p\n",
    "        Where ‚äô is element-wise multiplication and ||¬∑||_p is the p-norm\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, input, weight, p=2.0, eps=1e-8):\n",
    "            \"\"\"\n",
    "            Forward pass: weighted p-norm\n",
    "            \n",
    "            Args:\n",
    "                input: Input tensor [batch_size, features]\n",
    "                weight: Weight tensor [features] or [batch_size, features]\n",
    "                p: Norm order (default: 2 for L2 norm)\n",
    "                eps: Small value for numerical stability\n",
    "            \"\"\"\n",
    "            # Validate inputs\n",
    "            assert input.dim() >= 1, \"Input must be at least 1D\"\n",
    "            assert weight.dim() <= input.dim(), \"Weight dimensions must not exceed input dimensions\"\n",
    "            \n",
    "            # Broadcast weight if necessary\n",
    "            if weight.dim() == 1 and input.dim() == 2:\n",
    "                weight = weight.unsqueeze(0).expand_as(input)\n",
    "            \n",
    "            # Compute weighted values\n",
    "            weighted_input = input * weight\n",
    "            \n",
    "            # Compute p-norm\n",
    "            if p == float('inf'):\n",
    "                norm_result = torch.max(torch.abs(weighted_input), dim=-1)[0]\n",
    "            elif p == 1:\n",
    "                norm_result = torch.sum(torch.abs(weighted_input), dim=-1)\n",
    "            else:\n",
    "                norm_result = torch.sum(torch.abs(weighted_input) ** p, dim=-1) ** (1.0 / p)\n",
    "            \n",
    "            # Add epsilon for numerical stability\n",
    "            norm_result = norm_result + eps\n",
    "            \n",
    "            # Save for backward pass\n",
    "            ctx.save_for_backward(input, weight, weighted_input, norm_result)\n",
    "            ctx.p = p\n",
    "            ctx.eps = eps\n",
    "            \n",
    "            return norm_result\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Complex backward pass for weighted p-norm\n",
    "            \"\"\"\n",
    "            input, weight, weighted_input, norm_result = ctx.saved_tensors\n",
    "            p = ctx.p\n",
    "            eps = ctx.eps\n",
    "            \n",
    "            # Initialize gradients\n",
    "            grad_input = grad_weight = None\n",
    "            \n",
    "            if ctx.needs_input_grad[0] or ctx.needs_input_grad[1]:\n",
    "                # Compute common terms\n",
    "                abs_weighted = torch.abs(weighted_input)\n",
    "                \n",
    "                if p == float('inf'):\n",
    "                    # For infinity norm, gradient is sparse\n",
    "                    max_indices = torch.argmax(abs_weighted, dim=-1, keepdim=True)\n",
    "                    grad_weighted = torch.zeros_like(weighted_input)\n",
    "                    grad_weighted.scatter_(-1, max_indices, torch.sign(weighted_input.gather(-1, max_indices)))\n",
    "                elif p == 1:\n",
    "                    # For L1 norm\n",
    "                    grad_weighted = torch.sign(weighted_input)\n",
    "                else:\n",
    "                    # For general p-norm\n",
    "                    if p == 2:\n",
    "                        # Optimized case for L2 norm\n",
    "                        grad_weighted = weighted_input / (norm_result.unsqueeze(-1) - eps + 1e-12)\n",
    "                    else:\n",
    "                        # General case\n",
    "                        grad_weighted = (\n",
    "                            torch.sign(weighted_input) * \n",
    "                            (abs_weighted ** (p - 1)) * \n",
    "                            (norm_result.unsqueeze(-1) ** (1 - p))\n",
    "                        )\n",
    "                \n",
    "                # Apply chain rule with output gradient\n",
    "                grad_weighted = grad_weighted * grad_output.unsqueeze(-1)\n",
    "                \n",
    "                # Compute input gradient\n",
    "                if ctx.needs_input_grad[0]:\n",
    "                    grad_input = grad_weighted * weight\n",
    "                \n",
    "                # Compute weight gradient\n",
    "                if ctx.needs_input_grad[1]:\n",
    "                    grad_weight = grad_weighted * input\n",
    "                    \n",
    "                    # Sum over batch dimension if weight was broadcasted\n",
    "                    if input.dim() == 2 and weight.dim() == 1:\n",
    "                        grad_weight = grad_weight.sum(dim=0)\n",
    "            \n",
    "            return grad_input, grad_weight, None, None\n",
    "    \n",
    "    class BilinearInteractionFunction(Function):\n",
    "        \"\"\"\n",
    "        Bilinear interaction: f(x, y, W) = x^T W y\n",
    "        Useful for attention mechanisms and feature interactions\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, x, y, W):\n",
    "            \"\"\"\n",
    "            Forward: f(x, y, W) = x^T W y\n",
    "            \n",
    "            Args:\n",
    "                x: First input [batch_size, dim_x]\n",
    "                y: Second input [batch_size, dim_y]  \n",
    "                W: Bilinear weight [dim_x, dim_y]\n",
    "            \"\"\"\n",
    "            # Compute bilinear interaction\n",
    "            # result[i] = x[i]^T W y[i] for each batch element\n",
    "            result = torch.sum(x.unsqueeze(2) * W.unsqueeze(0), dim=1)  # [batch, dim_y]\n",
    "            result = torch.sum(result * y, dim=1)  # [batch]\n",
    "            \n",
    "            # Save for backward\n",
    "            ctx.save_for_backward(x, y, W)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Backward pass for bilinear interaction\n",
    "            \n",
    "            Gradients:\n",
    "                ‚àÇf/‚àÇx = W y\n",
    "                ‚àÇf/‚àÇy = W^T x  \n",
    "                ‚àÇf/‚àÇW = x y^T (outer product)\n",
    "            \"\"\"\n",
    "            x, y, W = ctx.saved_tensors\n",
    "            \n",
    "            grad_x = grad_y = grad_W = None\n",
    "            \n",
    "            if ctx.needs_input_grad[0]:\n",
    "                # ‚àÇf/‚àÇx = W y\n",
    "                grad_x = torch.matmul(y.unsqueeze(1), W.t()).squeeze(1)\n",
    "                grad_x = grad_x * grad_output.unsqueeze(1)\n",
    "            \n",
    "            if ctx.needs_input_grad[1]:\n",
    "                # ‚àÇf/‚àÇy = W^T x\n",
    "                grad_y = torch.matmul(x.unsqueeze(1), W).squeeze(1)\n",
    "                grad_y = grad_y * grad_output.unsqueeze(1)\n",
    "            \n",
    "            if ctx.needs_input_grad[2]:\n",
    "                # ‚àÇf/‚àÇW = sum over batch of x_i y_i^T\n",
    "                grad_W = torch.sum(\n",
    "                    x.unsqueeze(2) * y.unsqueeze(1) * grad_output.unsqueeze(1).unsqueeze(2),\n",
    "                    dim=0\n",
    "                )\n",
    "            \n",
    "            return grad_x, grad_y, grad_W\n",
    "    \n",
    "    # Wrapper functions\n",
    "    def weighted_norm(input, weight, p=2.0, eps=1e-8):\n",
    "        return WeightedNormFunction.apply(input, weight, p, eps)\n",
    "    \n",
    "    def bilinear_interaction(x, y, W):\n",
    "        return BilinearInteractionFunction.apply(x, y, W)\n",
    "    \n",
    "    return weighted_norm, bilinear_interaction\n",
    "\n",
    "# Create and test multi-input functions\n",
    "weighted_norm, bilinear_interaction = demonstrate_multi_input_functions()\n",
    "\n",
    "print(\"üß™ Testing Multi-Input Functions:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Test weighted norm function\n",
    "print(\"\\nüìä Testing Weighted Norm Function:\")\n",
    "batch_size, features = 32, 64\n",
    "test_input = torch.randn(batch_size, features, requires_grad=True)\n",
    "test_weight = torch.randn(features, requires_grad=True)\n",
    "\n",
    "# Test different norms\n",
    "norm_orders = [1, 2, float('inf')]\n",
    "norm_results = {}\n",
    "\n",
    "for p in norm_orders:\n",
    "    # Clear gradients\n",
    "    if test_input.grad is not None:\n",
    "        test_input.grad.zero_()\n",
    "    if test_weight.grad is not None:\n",
    "        test_weight.grad.zero_()\n",
    "    \n",
    "    # Forward pass\n",
    "    norm_output = weighted_norm(test_input, test_weight, p=p)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss = norm_output.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    norm_results[f'L{p}'] = {\n",
    "        'output_mean': norm_output.mean().item(),\n",
    "        'output_std': norm_output.std().item(),\n",
    "        'input_grad_norm': test_input.grad.norm().item(),\n",
    "        'weight_grad_norm': test_weight.grad.norm().item()\n",
    "    }\n",
    "    \n",
    "    print(f\"  L{p} norm - Mean: {norm_output.mean():.4f}, \"\n",
    "          f\"Input grad norm: {test_input.grad.norm():.4f}\")\n",
    "\n",
    "# Test bilinear interaction\n",
    "print(\"\\nüìä Testing Bilinear Interaction Function:\")\n",
    "dim_x, dim_y = 32, 24\n",
    "x = torch.randn(batch_size, dim_x, requires_grad=True)\n",
    "y = torch.randn(batch_size, dim_y, requires_grad=True)\n",
    "W = torch.randn(dim_x, dim_y, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "bilinear_output = bilinear_interaction(x, y, W)\n",
    "print(f\"  Output shape: {bilinear_output.shape}\")\n",
    "print(f\"  Output range: [{bilinear_output.min():.4f}, {bilinear_output.max():.4f}]\")\n",
    "\n",
    "# Backward pass\n",
    "bilinear_loss = bilinear_output.sum()\n",
    "bilinear_loss.backward()\n",
    "\n",
    "print(f\"  X gradient norm: {x.grad.norm():.4f}\")\n",
    "print(f\"  Y gradient norm: {y.grad.norm():.4f}\")\n",
    "print(f\"  W gradient norm: {W.grad.norm():.4f}\")\n",
    "\n",
    "# Save multi-input function results\n",
    "multi_input_results = {\n",
    "    'weighted_norm': norm_results,\n",
    "    'bilinear_interaction': {\n",
    "        'output_mean': bilinear_output.mean().item(),\n",
    "        'output_std': bilinear_output.std().item(),\n",
    "        'x_grad_norm': x.grad.norm().item(),\n",
    "        'y_grad_norm': y.grad.norm().item(),\n",
    "        'W_grad_norm': W.grad.norm().item()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(results_dir / 'multi_input_function_results.json', 'w') as f:\n",
    "    json.dump(multi_input_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Multi-input function results saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c933c3",
   "metadata": {},
   "source": [
    "## 3. Advanced Custom Activation Functions\n",
    "\n",
    "### 3.1 Sophisticated Activation Functions with Complex Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_activation_functions():\n",
    "    \"\"\"Create sophisticated activation functions for research and production\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 3.1 Advanced Custom Activation Functions ===\\n\")\n",
    "    \n",
    "    class SwishFunction(Function):\n",
    "        \"\"\"\n",
    "        Swish activation: f(x) = x * sigmoid(Œ≤x)\n",
    "        Self-gating activation with learnable parameter Œ≤\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, input, beta=1.0):\n",
    "            \"\"\"\n",
    "            Forward: f(x) = x * œÉ(Œ≤x) where œÉ is sigmoid\n",
    "            \"\"\"\n",
    "            scaled_input = beta * input\n",
    "            sigmoid_x = torch.sigmoid(scaled_input)\n",
    "            result = input * sigmoid_x\n",
    "            \n",
    "            # Save for efficient backward computation\n",
    "            ctx.save_for_backward(input, sigmoid_x)\n",
    "            ctx.beta = beta\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Backward: f'(x) = œÉ(Œ≤x) + Œ≤x * œÉ(Œ≤x) * (1 - œÉ(Œ≤x))\n",
    "                            = œÉ(Œ≤x) * (1 + Œ≤x * (1 - œÉ(Œ≤x)))\n",
    "            \"\"\"\n",
    "            input, sigmoid_x = ctx.saved_tensors\n",
    "            beta = ctx.beta\n",
    "            \n",
    "            # Efficient computation using saved sigmoid\n",
    "            sigmoid_derivative = sigmoid_x * (1 - sigmoid_x)\n",
    "            swish_derivative = sigmoid_x + beta * input * sigmoid_derivative\n",
    "            \n",
    "            grad_input = grad_output * swish_derivative\n",
    "            \n",
    "            # Gradient w.r.t. beta (if beta requires grad)\n",
    "            grad_beta = None\n",
    "            if ctx.needs_input_grad[1]:\n",
    "                grad_beta = torch.sum(grad_output * input * input * sigmoid_derivative)\n",
    "            \n",
    "            return grad_input, grad_beta\n",
    "    \n",
    "    class MishFunction(Function):\n",
    "        \"\"\"\n",
    "        Mish activation: f(x) = x * tanh(softplus(x))\n",
    "        Smooth, non-monotonic activation with excellent properties\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, input):\n",
    "            \"\"\"\n",
    "            Forward: f(x) = x * tanh(ln(1 + e^x))\n",
    "            \"\"\"\n",
    "            # Use softplus for numerical stability\n",
    "            softplus_x = F.softplus(input)\n",
    "            tanh_softplus = torch.tanh(softplus_x)\n",
    "            result = input * tanh_softplus\n",
    "            \n",
    "            # Save intermediate results for efficient backward\n",
    "            ctx.save_for_backward(input, softplus_x, tanh_softplus)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Complex derivative computation for Mish\n",
    "            f'(x) = tanh(softplus(x)) + x * sech¬≤(softplus(x)) * sigmoid(x)\n",
    "            \"\"\"\n",
    "            input, softplus_x, tanh_softplus = ctx.saved_tensors\n",
    "            \n",
    "            # Compute derivative components\n",
    "            sigmoid_x = torch.sigmoid(input)\n",
    "            sech_squared = 1 - tanh_softplus ** 2  # sech¬≤(x) = 1 - tanh¬≤(x)\n",
    "            \n",
    "            # Full derivative\n",
    "            mish_derivative = tanh_softplus + input * sech_squared * sigmoid_x\n",
    "            \n",
    "            grad_input = grad_output * mish_derivative\n",
    "            \n",
    "            return grad_input\n",
    "    \n",
    "    class AdaptiveActivationFunction(Function):\n",
    "        \"\"\"\n",
    "        Learnable activation: f(x) = a * x + b * g(c * x + d)\n",
    "        Where g is a base activation (tanh, relu, etc.)\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, input, params, activation_type='tanh'):\n",
    "            \"\"\"\n",
    "            Forward pass for adaptive activation\n",
    "            \n",
    "            Args:\n",
    "                input: Input tensor\n",
    "                params: [a, b, c, d] learnable parameters\n",
    "                activation_type: Base activation ('tanh', 'relu', 'elu')\n",
    "            \"\"\"\n",
    "            a, b, c, d = params[0], params[1], params[2], params[3]\n",
    "            \n",
    "            # Linear component\n",
    "            linear_part = a * input\n",
    "            \n",
    "            # Nonlinear component\n",
    "            nonlinear_input = c * input + d\n",
    "            \n",
    "            if activation_type == 'tanh':\n",
    "                nonlinear_activation = torch.tanh(nonlinear_input)\n",
    "                activation_derivative = 1 - nonlinear_activation ** 2\n",
    "            elif activation_type == 'relu':\n",
    "                nonlinear_activation = F.relu(nonlinear_input)\n",
    "                activation_derivative = (nonlinear_input > 0).float()\n",
    "            elif activation_type == 'elu':\n",
    "                nonlinear_activation = F.elu(nonlinear_input)\n",
    "                activation_derivative = torch.where(\n",
    "                    nonlinear_input > 0, \n",
    "                    torch.ones_like(nonlinear_input),\n",
    "                    nonlinear_activation + 1\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported activation type: {activation_type}\")\n",
    "            \n",
    "            nonlinear_part = b * nonlinear_activation\n",
    "            result = linear_part + nonlinear_part\n",
    "            \n",
    "            # Save for backward\n",
    "            ctx.save_for_backward(input, params, nonlinear_activation, activation_derivative)\n",
    "            ctx.activation_type = activation_type\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Backward pass with gradients for both input and parameters\n",
    "            \"\"\"\n",
    "            input, params, nonlinear_activation, activation_derivative = ctx.saved_tensors\n",
    "            a, b, c, d = params[0], params[1], params[2], params[3]\n",
    "            \n",
    "            # Gradient w.r.t. input\n",
    "            grad_input = grad_output * (a + b * c * activation_derivative)\n",
    "            \n",
    "            # Gradients w.r.t. parameters\n",
    "            grad_params = torch.zeros_like(params)\n",
    "            \n",
    "            if ctx.needs_input_grad[1]:\n",
    "                # ‚àÇf/‚àÇa = x\n",
    "                grad_params[0] = torch.sum(grad_output * input)\n",
    "                \n",
    "                # ‚àÇf/‚àÇb = g(cx + d)\n",
    "                grad_params[1] = torch.sum(grad_output * nonlinear_activation)\n",
    "                \n",
    "                # ‚àÇf/‚àÇc = b * x * g'(cx + d)\n",
    "                grad_params[2] = torch.sum(grad_output * b * input * activation_derivative)\n",
    "                \n",
    "                # ‚àÇf/‚àÇd = b * g'(cx + d)\n",
    "                grad_params[3] = torch.sum(grad_output * b * activation_derivative)\n",
    "            \n",
    "            return grad_input, grad_params, None\n",
    "    \n",
    "    class GatedLinearUnitFunction(Function):\n",
    "        \"\"\"\n",
    "        Gated Linear Unit: GLU(x) = x‚ÇÅ ‚äô œÉ(x‚ÇÇ)\n",
    "        Where x is split into two halves\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, input):\n",
    "            \"\"\"\n",
    "            Forward: Split input and apply gating\n",
    "            \"\"\"\n",
    "            # Split input into two halves\n",
    "            dim = input.size(-1)\n",
    "            assert dim % 2 == 0, \"Input dimension must be even for GLU\"\n",
    "            \n",
    "            split_dim = dim // 2\n",
    "            x1 = input[..., :split_dim]\n",
    "            x2 = input[..., split_dim:]\n",
    "            \n",
    "            # Apply sigmoid gating\n",
    "            gate = torch.sigmoid(x2)\n",
    "            result = x1 * gate\n",
    "            \n",
    "            # Save for backward\n",
    "            ctx.save_for_backward(x1, x2, gate)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Backward pass for GLU\n",
    "            \"\"\"\n",
    "            x1, x2, gate = ctx.saved_tensors\n",
    "            \n",
    "            # Gradients\n",
    "            grad_x1 = grad_output * gate\n",
    "            grad_x2 = grad_output * x1 * gate * (1 - gate)\n",
    "            \n",
    "            # Concatenate gradients\n",
    "            grad_input = torch.cat([grad_x1, grad_x2], dim=-1)\n",
    "            \n",
    "            return grad_input\n",
    "    \n",
    "    # Wrapper functions\n",
    "    def swish(x, beta=1.0):\n",
    "        return SwishFunction.apply(x, beta)\n",
    "    \n",
    "    def mish(x):\n",
    "        return MishFunction.apply(x)\n",
    "    \n",
    "    def adaptive_activation(x, params, activation_type='tanh'):\n",
    "        return AdaptiveActivationFunction.apply(x, params, activation_type)\n",
    "    \n",
    "    def glu(x):\n",
    "        return GatedLinearUnitFunction.apply(x)\n",
    "    \n",
    "    return swish, mish, adaptive_activation, glu\n",
    "\n",
    "# Create activation functions\n",
    "swish, mish, adaptive_activation, glu = create_advanced_activation_functions()\n",
    "\n",
    "print(\"üé® Testing Advanced Activation Functions:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Comprehensive activation function analysis\n",
    "activation_analysis = {}\n",
    "\n",
    "# Test input range\n",
    "x_range = torch.linspace(-4, 4, 1000)\n",
    "x_test = torch.linspace(-3, 3, 100, requires_grad=True)\n",
    "\n",
    "# Test each activation function\n",
    "activations_to_test = [\n",
    "    ('Swish', lambda x: swish(x, beta=1.0)),\n",
    "    ('Mish', mish),\n",
    "    ('ReLU', F.relu),\n",
    "    ('GELU', F.gelu),\n",
    "    ('Tanh', torch.tanh),\n",
    "    ('Swish Œ≤=0.5', lambda x: swish(x, beta=0.5)),\n",
    "    ('Swish Œ≤=2.0', lambda x: swish(x, beta=2.0))\n",
    "]\n",
    "\n",
    "# Analyze each activation\n",
    "for name, activation_func in activations_to_test:\n",
    "    print(f\"\\nüìä Analyzing {name}:\")\n",
    "    \n",
    "    # Clear gradients\n",
    "    if x_test.grad is not None:\n",
    "        x_test.grad.zero_()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = activation_func(x_test)\n",
    "    \n",
    "    # Backward pass for gradient analysis\n",
    "    output.sum().backward()\n",
    "    \n",
    "    # Compute statistics\n",
    "    activation_analysis[name] = {\n",
    "        'output_range': [output.min().item(), output.max().item()],\n",
    "        'output_mean': output.mean().item(),\n",
    "        'output_std': output.std().item(),\n",
    "        'gradient_norm': x_test.grad.norm().item(),\n",
    "        'gradient_mean': x_test.grad.mean().item(),\n",
    "        'gradient_std': x_test.grad.std().item(),\n",
    "        'zero_gradient_ratio': (x_test.grad.abs() < 1e-6).float().mean().item()\n",
    "    }\n",
    "    \n",
    "    print(f\"  Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "    print(f\"  Gradient norm: {x_test.grad.norm():.3f}\")\n",
    "    print(f\"  Dead neuron ratio: {activation_analysis[name]['zero_gradient_ratio']:.3f}\")\n",
    "\n",
    "# Test adaptive activation function\n",
    "print(f\"\\nüìä Testing Adaptive Activation:\")\n",
    "adaptive_params = torch.tensor([1.0, 0.5, 1.0, 0.0], requires_grad=True)\n",
    "\n",
    "for activation_type in ['tanh', 'relu', 'elu']:\n",
    "    if adaptive_params.grad is not None:\n",
    "        adaptive_params.grad.zero_()\n",
    "    if x_test.grad is not None:\n",
    "        x_test.grad.zero_()\n",
    "    \n",
    "    adaptive_output = adaptive_activation(x_test, adaptive_params, activation_type)\n",
    "    adaptive_output.sum().backward()\n",
    "    \n",
    "    print(f\"  {activation_type.upper()}: Param grads = {adaptive_params.grad.numpy()}\")\n",
    "\n",
    "# Test GLU\n",
    "print(f\"\\nüìä Testing Gated Linear Unit:\")\n",
    "x_glu = torch.randn(32, 128, requires_grad=True)  # Even dimension for GLU\n",
    "glu_output = glu(x_glu)\n",
    "glu_output.sum().backward()\n",
    "\n",
    "print(f\"  Input shape: {x_glu.shape}\")\n",
    "print(f\"  Output shape: {glu_output.shape}\")\n",
    "print(f\"  Gradient norm: {x_glu.grad.norm():.4f}\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "def create_activation_visualization():\n",
    "    \"\"\"Create comprehensive activation function visualization\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    x_plot = torch.linspace(-4, 4, 1000)\n",
    "    \n",
    "    # Plot activation functions\n",
    "    plot_idx = 0\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(activations_to_test)))\n",
    "    \n",
    "    for i, (name, activation_func) in enumerate(activations_to_test):\n",
    "        if plot_idx >= 6:  # First 6 plots for activations\n",
    "            break\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            y = activation_func(x_plot)\n",
    "        \n",
    "        axes[plot_idx].plot(x_plot, y, linewidth=2, color=colors[i], label=name)\n",
    "        axes[plot_idx].set_title(f'{name} Activation', fontweight='bold')\n",
    "        axes[plot_idx].set_xlabel('x')\n",
    "        axes[plot_idx].set_ylabel('f(x)')\n",
    "        axes[plot_idx].grid(True, alpha=0.3)\n",
    "        axes[plot_idx].legend()\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Plot derivatives comparison\n",
    "    ax_deriv = axes[6]\n",
    "    x_deriv = torch.linspace(-3, 3, 200, requires_grad=True)\n",
    "    \n",
    "    for name, activation_func in activations_to_test[:4]:  # Plot first 4 derivatives\n",
    "        if x_deriv.grad is not None:\n",
    "            x_deriv.grad.zero_()\n",
    "        \n",
    "        y = activation_func(x_deriv)\n",
    "        y.sum().backward(retain_graph=True)\n",
    "        \n",
    "        ax_deriv.plot(x_deriv.detach(), x_deriv.grad.detach(), \n",
    "                     linewidth=2, label=f\"{name}'\")\n",
    "    \n",
    "    ax_deriv.set_title('Activation Derivatives', fontweight='bold')\n",
    "    ax_deriv.set_xlabel('x')\n",
    "    ax_deriv.set_ylabel(\"f'(x)\")\n",
    "    ax_deriv.legend()\n",
    "    ax_deriv.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot gradient flow comparison\n",
    "    ax_flow = axes[7]\n",
    "    activation_names = list(activation_analysis.keys())[:6]\n",
    "    gradient_norms = [activation_analysis[name]['gradient_norm'] for name in activation_names]\n",
    "    \n",
    "    bars = ax_flow.bar(range(len(activation_names)), gradient_norms, alpha=0.7)\n",
    "    ax_flow.set_title('Gradient Flow Comparison', fontweight='bold')\n",
    "    ax_flow.set_ylabel('Gradient Norm')\n",
    "    ax_flow.set_xticks(range(len(activation_names)))\n",
    "    ax_flow.set_xticklabels(activation_names, rotation=45)\n",
    "    ax_flow.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, norm in zip(bars, gradient_norms):\n",
    "        height = bar.get_height()\n",
    "        ax_flow.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{norm:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot dead neuron analysis\n",
    "    ax_dead = axes[8]\n",
    "    dead_ratios = [activation_analysis[name]['zero_gradient_ratio'] for name in activation_names]\n",
    "    \n",
    "    bars_dead = ax_dead.bar(range(len(activation_names)), dead_ratios, \n",
    "                           alpha=0.7, color='red')\n",
    "    ax_dead.set_title('Dead Neuron Analysis', fontweight='bold')\n",
    "    ax_dead.set_ylabel('Dead Gradient Ratio')\n",
    "    ax_dead.set_xticks(range(len(activation_names)))\n",
    "    ax_dead.set_xticklabels(activation_names, rotation=45)\n",
    "    ax_dead.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Advanced Custom Activation Functions Analysis', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'advanced_activation_functions.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Create visualization\n",
    "create_activation_visualization()\n",
    "\n",
    "# Save activation analysis results\n",
    "with open(results_dir / 'activation_function_analysis.json', 'w') as f:\n",
    "    json.dump(activation_analysis, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Activation function analysis saved\")\n",
    "print(f\"\\nüéì Key Insights:\")\n",
    "print(f\"  ‚Ä¢ Swish and Mish provide smooth, non-monotonic behavior\")\n",
    "print(f\"  ‚Ä¢ Adaptive activations can learn task-specific shapes\")\n",
    "print(f\"  ‚Ä¢ GLU provides effective gating mechanisms\")\n",
    "print(f\"  ‚Ä¢ Gradient flow varies significantly between activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400f354",
   "metadata": {},
   "source": [
    "## 4. Memory-Efficient Operations and Advanced Techniques\n",
    "\n",
    "### 4.1 Memory-Efficient Custom Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_memory_efficient_operations():\n",
    "    \"\"\"Create memory-efficient operations for large-scale training\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 4.1 Memory-Efficient Custom Operations ===\\n\")\n",
    "    \n",
    "    class MemoryEfficientMatMulFunction(Function):\n",
    "        \"\"\"\n",
    "        Memory-efficient matrix multiplication with gradient checkpointing\n",
    "        Trades computation for memory during backward pass\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, input, weight, bias=None, save_memory=True):\n",
    "            \"\"\"\n",
    "            Forward pass with optional memory savings\n",
    "            \n",
    "            Args:\n",
    "                input: Input tensor [batch_size, in_features]\n",
    "                weight: Weight matrix [in_features, out_features]\n",
    "                bias: Optional bias [out_features]\n",
    "                save_memory: Whether to use memory-efficient mode\n",
    "            \"\"\"\n",
    "            ctx.save_memory = save_memory\n",
    "            \n",
    "            # Compute output\n",
    "            output = torch.mm(input, weight)\n",
    "            if bias is not None:\n",
    "                output += bias\n",
    "            \n",
    "            if save_memory:\n",
    "                # Save only shapes and statistics for large tensors\n",
    "                input_size_mb = input.numel() * input.element_size() / (1024 ** 2)\n",
    "                weight_size_mb = weight.numel() * weight.element_size() / (1024 ** 2)\n",
    "                \n",
    "                if input_size_mb > 50 or weight_size_mb > 50:  # > 50MB\n",
    "                    print(f\"üíæ Using memory-efficient mode (Input: {input_size_mb:.1f}MB, Weight: {weight_size_mb:.1f}MB)\")\n",
    "                    \n",
    "                    # Save only essential information\n",
    "                    ctx.input_shape = input.shape\n",
    "                    ctx.weight_shape = weight.shape\n",
    "                    ctx.has_bias = bias is not None\n",
    "                    \n",
    "                    # Save statistical information for approximation\n",
    "                    ctx.input_mean = input.mean().item()\n",
    "                    ctx.input_std = input.std().item()\n",
    "                    ctx.weight_mean = weight.mean().item()\n",
    "                    ctx.weight_std = weight.std().item()\n",
    "                    \n",
    "                    if bias is not None:\n",
    "                        ctx.bias_mean = bias.mean().item()\n",
    "                        ctx.bias_std = bias.std().item()\n",
    "                    \n",
    "                    # Don't save the actual tensors\n",
    "                    ctx.saved_tensors = ()\n",
    "                else:\n",
    "                    # Save normally for small tensors\n",
    "                    ctx.save_for_backward(input, weight, bias)\n",
    "            else:\n",
    "                # Standard mode - save everything\n",
    "                ctx.save_for_backward(input, weight, bias)\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Memory-efficient backward pass\n",
    "            \"\"\"\n",
    "            grad_input = grad_weight = grad_bias = None\n",
    "            \n",
    "            if ctx.save_memory and len(ctx.saved_tensors) == 0:\n",
    "                # Memory-efficient mode: reconstruct or approximate\n",
    "                print(\"‚ö° Using gradient approximation for memory efficiency\")\n",
    "                \n",
    "                # Create approximate tensors based on saved statistics\n",
    "                input_approx = (\n",
    "                    torch.randn(ctx.input_shape) * ctx.input_std + ctx.input_mean\n",
    "                ).to(grad_output.device)\n",
    "                \n",
    "                weight_approx = (\n",
    "                    torch.randn(ctx.weight_shape) * ctx.weight_std + ctx.weight_mean\n",
    "                ).to(grad_output.device)\n",
    "                \n",
    "                # Compute approximate gradients\n",
    "                if ctx.needs_input_grad[0]:\n",
    "                    grad_input = torch.mm(grad_output, weight_approx.t())\n",
    "                \n",
    "                if ctx.needs_input_grad[1]:\n",
    "                    grad_weight = torch.mm(input_approx.t(), grad_output)\n",
    "                \n",
    "                if ctx.needs_input_grad[2] and ctx.has_bias:\n",
    "                    grad_bias = grad_output.sum(dim=0)\n",
    "            \n",
    "            else:\n",
    "                # Standard backward pass\n",
    "                saved_tensors = ctx.saved_tensors\n",
    "                input = saved_tensors[0] if len(saved_tensors) > 0 else None\n",
    "                weight = saved_tensors[1] if len(saved_tensors) > 1 else None\n",
    "                bias = saved_tensors[2] if len(saved_tensors) > 2 else None\n",
    "                \n",
    "                if ctx.needs_input_grad[0] and input is not None and weight is not None:\n",
    "                    grad_input = torch.mm(grad_output, weight.t())\n",
    "                \n",
    "                if ctx.needs_input_grad[1] and input is not None:\n",
    "                    grad_weight = torch.mm(input.t(), grad_output)\n",
    "                \n",
    "                if ctx.needs_input_grad[2] and bias is not None:\n",
    "                    grad_bias = grad_output.sum(dim=0)\n",
    "            \n",
    "            return grad_input, grad_weight, grad_bias, None\n",
    "    \n",
    "    class SequentialComputationFunction(Function):\n",
    "        \"\"\"\n",
    "        Sequential computation to reduce peak memory usage\n",
    "        Processes data in chunks to handle very large inputs\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, input, chunk_size=1024):\n",
    "            \"\"\"\n",
    "            Process input in sequential chunks\n",
    "            \"\"\"\n",
    "            batch_size = input.size(0)\n",
    "            \n",
    "            if batch_size <= chunk_size:\n",
    "                # Small enough to process normally\n",
    "                result = torch.sum(input ** 2, dim=1)\n",
    "                ctx.save_for_backward(input)\n",
    "                ctx.chunk_size = None\n",
    "            else:\n",
    "                # Process in chunks\n",
    "                print(f\"üîÑ Processing {batch_size} samples in chunks of {chunk_size}\")\n",
    "                \n",
    "                results = []\n",
    "                input_chunks = []\n",
    "                \n",
    "                for i in range(0, batch_size, chunk_size):\n",
    "                    end_idx = min(i + chunk_size, batch_size)\n",
    "                    chunk = input[i:end_idx]\n",
    "                    chunk_result = torch.sum(chunk ** 2, dim=1)\n",
    "                    results.append(chunk_result)\n",
    "                    input_chunks.append(chunk)\n",
    "                \n",
    "                result = torch.cat(results, dim=0)\n",
    "                \n",
    "                # Save chunks instead of full tensor\n",
    "                ctx.save_for_backward(*input_chunks)\n",
    "                ctx.chunk_size = chunk_size\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Backward pass handling chunked computation\n",
    "            \"\"\"\n",
    "            if ctx.chunk_size is None:\n",
    "                # Standard processing\n",
    "                input, = ctx.saved_tensors\n",
    "                grad_input = 2 * input * grad_output.unsqueeze(1)\n",
    "            else:\n",
    "                # Chunked processing\n",
    "                input_chunks = ctx.saved_tensors\n",
    "                grad_chunks = []\n",
    "                \n",
    "                start_idx = 0\n",
    "                for chunk in input_chunks:\n",
    "                    end_idx = start_idx + chunk.size(0)\n",
    "                    chunk_grad_output = grad_output[start_idx:end_idx]\n",
    "                    chunk_grad_input = 2 * chunk * chunk_grad_output.unsqueeze(1)\n",
    "                    grad_chunks.append(chunk_grad_input)\n",
    "                    start_idx = end_idx\n",
    "                \n",
    "                grad_input = torch.cat(grad_chunks, dim=0)\n",
    "            \n",
    "            return grad_input, None\n",
    "    \n",
    "    class GradientCheckpointFunction(Function):\n",
    "        \"\"\"\n",
    "        Implement gradient checkpointing for arbitrary functions\n",
    "        Recomputes forward pass during backward to save memory\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, input, function_layers, *function_args):\n",
    "            \"\"\"\n",
    "            Forward pass with checkpointing\n",
    "            \n",
    "            Args:\n",
    "                input: Input tensor\n",
    "                function_layers: List of functions to apply sequentially\n",
    "                function_args: Additional arguments for functions\n",
    "            \"\"\"\n",
    "            ctx.function_layers = function_layers\n",
    "            ctx.function_args = function_args\n",
    "            \n",
    "            # Only save input, not intermediate activations\n",
    "            ctx.save_for_backward(input)\n",
    "            \n",
    "            # Compute forward pass\n",
    "            x = input\n",
    "            for i, layer_func in enumerate(function_layers):\n",
    "                if i < len(function_args):\n",
    "                    x = layer_func(x, function_args[i])\n",
    "                else:\n",
    "                    x = layer_func(x)\n",
    "            \n",
    "            return x\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Backward pass with recomputation\n",
    "            \"\"\"\n",
    "            input, = ctx.saved_tensors\n",
    "            function_layers = ctx.function_layers\n",
    "            function_args = ctx.function_args\n",
    "            \n",
    "            # Recompute forward pass with gradients enabled\n",
    "            x = input.detach().requires_grad_(True)\n",
    "            \n",
    "            for i, layer_func in enumerate(function_layers):\n",
    "                if i < len(function_args):\n",
    "                    x = layer_func(x, function_args[i])\n",
    "                else:\n",
    "                    x = layer_func(x)\n",
    "            \n",
    "            # Compute gradients\n",
    "            x.backward(grad_output)\n",
    "            \n",
    "            return input.grad, None, None\n",
    "    \n",
    "    # Wrapper functions\n",
    "    def memory_efficient_matmul(input, weight, bias=None, save_memory=True):\n",
    "        return MemoryEfficientMatMulFunction.apply(input, weight, bias, save_memory)\n",
    "    \n",
    "    def sequential_computation(input, chunk_size=1024):\n",
    "        return SequentialComputationFunction.apply(input, chunk_size)\n",
    "    \n",
    "    def gradient_checkpoint(input, function_layers, *function_args):\n",
    "        return GradientCheckpointFunction.apply(input, function_layers, *function_args)\n",
    "    \n",
    "    return memory_efficient_matmul, sequential_computation, gradient_checkpoint\n",
    "\n",
    "# Create memory-efficient operations\n",
    "memory_efficient_matmul, sequential_computation, gradient_checkpoint = create_memory_efficient_operations()\n",
    "\n",
    "print(\"üíæ Testing Memory-Efficient Operations:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test memory-efficient matrix multiplication\n",
    "print(\"\\nüìä Testing Memory-Efficient Matrix Multiplication:\")\n",
    "\n",
    "# Create test data of varying sizes\n",
    "test_sizes = [\n",
    "    (32, 64, 32, \"Small\"),\n",
    "    (512, 256, 128, \"Medium\"), \n",
    "    (1024, 1024, 512, \"Large\")\n",
    "]\n",
    "\n",
    "memory_results = {}\n",
    "\n",
    "for batch_size, in_features, out_features, size_name in test_sizes:\n",
    "    print(f\"\\n  Testing {size_name} size: {batch_size}x{in_features} -> {out_features}\")\n",
    "    \n",
    "    # Create test tensors\n",
    "    input_tensor = torch.randn(batch_size, in_features, requires_grad=True)\n",
    "    weight_tensor = torch.randn(in_features, out_features, requires_grad=True)\n",
    "    bias_tensor = torch.randn(out_features, requires_grad=True)\n",
    "    \n",
    "    # Test memory-efficient version\n",
    "    start_time = time.time()\n",
    "    \n",
    "    output_efficient = memory_efficient_matmul(\n",
    "        input_tensor, weight_tensor, bias_tensor, save_memory=True\n",
    "    )\n",
    "    \n",
    "    loss_efficient = output_efficient.sum()\n",
    "    loss_efficient.backward()\n",
    "    \n",
    "    efficient_time = time.time() - start_time\n",
    "    \n",
    "    # Clear gradients for standard test\n",
    "    input_tensor.grad = None\n",
    "    weight_tensor.grad = None\n",
    "    bias_tensor.grad = None\n",
    "    \n",
    "    # Test standard version\n",
    "    start_time = time.time()\n",
    "    \n",
    "    output_standard = memory_efficient_matmul(\n",
    "        input_tensor, weight_tensor, bias_tensor, save_memory=False\n",
    "    )\n",
    "    \n",
    "    loss_standard = output_standard.sum()\n",
    "    loss_standard.backward()\n",
    "    \n",
    "    standard_time = time.time() - start_time\n",
    "    \n",
    "    # Compare results\n",
    "    output_diff = (output_efficient - output_standard).abs().max().item()\n",
    "    \n",
    "    memory_results[size_name] = {\n",
    "        'output_difference': output_diff,\n",
    "        'efficient_time': efficient_time * 1000,  # ms\n",
    "        'standard_time': standard_time * 1000,\n",
    "        'time_ratio': efficient_time / standard_time,\n",
    "        'batch_size': batch_size,\n",
    "        'parameters': in_features * out_features + out_features\n",
    "    }\n",
    "    \n",
    "    print(f\"    Output difference: {output_diff:.2e}\")\n",
    "    print(f\"    Efficient time: {efficient_time*1000:.2f}ms\")\n",
    "    print(f\"    Standard time: {standard_time*1000:.2f}ms\")\n",
    "    print(f\"    Time ratio: {efficient_time/standard_time:.2f}x\")\n",
    "\n",
    "# Test sequential computation\n",
    "print(f\"\\nüìä Testing Sequential Computation:\")\n",
    "\n",
    "large_input = torch.randn(5000, 100, requires_grad=True)\n",
    "print(f\"  Input shape: {large_input.shape}\")\n",
    "\n",
    "# Test with different chunk sizes\n",
    "chunk_sizes = [512, 1024, 2048]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    if large_input.grad is not None:\n",
    "        large_input.grad.zero_()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    seq_output = sequential_computation(large_input, chunk_size=chunk_size)\n",
    "    seq_output.sum().backward()\n",
    "    seq_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"    Chunk size {chunk_size}: {seq_time*1000:.2f}ms, \"\n",
    "          f\"Grad norm: {large_input.grad.norm():.4f}\")\n",
    "\n",
    "# Test gradient checkpointing\n",
    "print(f\"\\nüìä Testing Gradient Checkpointing:\")\n",
    "\n",
    "def test_layer1(x):\n",
    "    return torch.relu(x)\n",
    "\n",
    "def test_layer2(x):\n",
    "    return x ** 2\n",
    "\n",
    "def test_layer3(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "test_input = torch.randn(100, 50, requires_grad=True)\n",
    "layers = [test_layer1, test_layer2, test_layer3]\n",
    "\n",
    "# Test checkpointed version\n",
    "start_time = time.time()\n",
    "checkpoint_output = gradient_checkpoint(test_input, layers)\n",
    "checkpoint_output.sum().backward()\n",
    "checkpoint_time = time.time() - start_time\n",
    "\n",
    "checkpoint_grad = test_input.grad.clone()\n",
    "test_input.grad = None\n",
    "\n",
    "# Test standard version\n",
    "start_time = time.time()\n",
    "x = test_input\n",
    "for layer in layers:\n",
    "    x = layer(x)\n",
    "x.sum().backward()\n",
    "standard_time = time.time() - start_time\n",
    "\n",
    "print(f\"  Checkpoint time: {checkpoint_time*1000:.2f}ms\")\n",
    "print(f\"  Standard time: {standard_time*1000:.2f}ms\")\n",
    "print(f\"  Gradient difference: {(checkpoint_grad - test_input.grad).abs().max():.2e}\")\n",
    "\n",
    "# Save memory efficiency results\n",
    "memory_efficiency_results = {\n",
    "    'matrix_multiplication': memory_results,\n",
    "    'sequential_computation': {\n",
    "        'input_size': large_input.shape,\n",
    "        'chunk_performance': {str(cs): f\"Tested chunk size {cs}\" for cs in chunk_sizes}\n",
    "    },\n",
    "    'gradient_checkpointing': {\n",
    "        'checkpoint_time_ms': checkpoint_time * 1000,\n",
    "        'standard_time_ms': standard_time * 1000,\n",
    "        'time_overhead': checkpoint_time / standard_time,\n",
    "        'gradient_accuracy': (checkpoint_grad - test_input.grad).abs().max().item()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(results_dir / 'memory_efficiency_results.json', 'w') as f:\n",
    "    json.dump(memory_efficiency_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Memory efficiency results saved\")\n",
    "print(f\"\\nüéì Memory Efficiency Insights:\")\n",
    "print(f\"  ‚Ä¢ Memory-efficient operations trade computation for memory\")\n",
    "print(f\"  ‚Ä¢ Sequential processing handles arbitrarily large inputs\")\n",
    "print(f\"  ‚Ä¢ Gradient checkpointing reduces memory at cost of recomputation\")\n",
    "print(f\"  ‚Ä¢ Critical for training very large models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c4941",
   "metadata": {},
   "source": [
    "## 5. Non-Differentiable Operations and Advanced Techniques\n",
    "\n",
    "### 5.1 Handling Non-Differentiable Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17754d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_non_differentiable_operations():\n",
    "    \"\"\"Create techniques for handling non-differentiable operations\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 5.1 Non-Differentiable Operations Mastery ===\\n\")\n",
    "    \n",
    "    class StraightThroughEstimatorFunction(Function):\n",
    "        \"\"\"\n",
    "        Straight-Through Estimator for quantization\n",
    "        Forward: quantize, Backward: pass gradients through unchanged\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, input, num_bits=8, method='uniform'):\n",
    "            \"\"\"\n",
    "            Forward pass: quantize input to specified number of bits\n",
    "            \n",
    "            Args:\n",
    "                input: Input tensor\n",
    "                num_bits: Number of quantization bits\n",
    "                method: Quantization method ('uniform', 'log', 'learned')\n",
    "            \"\"\"\n",
    "            ctx.method = method\n",
    "            ctx.num_bits = num_bits\n",
    "            \n",
    "            if method == 'uniform':\n",
    "                # Uniform quantization\n",
    "                min_val = input.min()\n",
    "                max_val = input.max()\n",
    "                \n",
    "                # Avoid division by zero\n",
    "                if max_val == min_val:\n",
    "                    quantized = input.clone()\n",
    "                else:\n",
    "                    num_levels = 2 ** num_bits\n",
    "                    scale = (max_val - min_val) / (num_levels - 1)\n",
    "                    \n",
    "                    # Quantize\n",
    "                    quantized = torch.round((input - min_val) / scale) * scale + min_val\n",
    "                    quantized = torch.clamp(quantized, min_val, max_val)\n",
    "                \n",
    "                # Save quantization statistics\n",
    "                ctx.quantization_error = (quantized - input).abs().mean().item()\n",
    "                \n",
    "            elif method == 'log':\n",
    "                # Logarithmic quantization (for weights)\n",
    "                sign = torch.sign(input)\n",
    "                abs_input = torch.abs(input)\n",
    "                \n",
    "                # Avoid log(0)\n",
    "                eps = 1e-8\n",
    "                log_input = torch.log(abs_input + eps)\n",
    "                \n",
    "                min_log = log_input.min()\n",
    "                max_log = log_input.max()\n",
    "                \n",
    "                if max_log > min_log:\n",
    "                    num_levels = 2 ** num_bits\n",
    "                    scale = (max_log - min_log) / (num_levels - 1)\n",
    "                    quantized_log = torch.round((log_input - min_log) / scale) * scale + min_log\n",
    "                    quantized = sign * torch.exp(quantized_log)\n",
    "                else:\n",
    "                    quantized = input.clone()\n",
    "                \n",
    "                ctx.quantization_error = (quantized - input).abs().mean().item()\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"Unknown quantization method: {method}\")\n",
    "            \n",
    "            return quantized\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Straight-through estimator: pass gradients unchanged\n",
    "            This assumes quantization error is small enough to ignore\n",
    "            \"\"\"\n",
    "            # Simply pass through the gradient\n",
    "            return grad_output, None, None\n",
    "    \n",
    "    class GumbelSoftmaxFunction(Function):\n",
    "        \"\"\"\n",
    "        Gumbel-Softmax for differentiable discrete sampling\n",
    "        Provides a continuous relaxation of discrete distributions\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, logits, temperature=1.0, hard=False, dim=-1):\n",
    "            \"\"\"\n",
    "            Gumbel-Softmax forward pass\n",
    "            \n",
    "            Args:\n",
    "                logits: Unnormalized log probabilities\n",
    "                temperature: Temperature parameter (lower = more discrete)\n",
    "                hard: Whether to use hard (one-hot) or soft sampling\n",
    "                dim: Dimension to apply softmax\n",
    "            \"\"\"\n",
    "            # Sample Gumbel noise\n",
    "            gumbel_noise = -torch.log(-torch.log(\n",
    "                torch.rand_like(logits).clamp(min=1e-10, max=1-1e-10)\n",
    "            ))\n",
    "            \n",
    "            # Add noise and apply temperature\n",
    "            y = (logits + gumbel_noise) / temperature\n",
    "            \n",
    "            # Softmax\n",
    "            y_soft = F.softmax(y, dim=dim)\n",
    "            \n",
    "            if hard:\n",
    "                # Hard assignment (one-hot) using straight-through estimator\n",
    "                index = y_soft.max(dim=dim, keepdim=True)[1]\n",
    "                y_hard = torch.zeros_like(y_soft).scatter_(dim, index, 1.0)\n",
    "                \n",
    "                # Use straight-through: hard forward, soft backward\n",
    "                result = y_hard - y_soft.detach() + y_soft\n",
    "            else:\n",
    "                result = y_soft\n",
    "            \n",
    "            ctx.save_for_backward(result)\n",
    "            ctx.temperature = temperature\n",
    "            ctx.dim = dim\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Backward pass for Gumbel-Softmax\n",
    "            \"\"\"\n",
    "            y_soft, = ctx.saved_tensors\n",
    "            temperature = ctx.temperature\n",
    "            dim = ctx.dim\n",
    "            \n",
    "            # Gradient computation for softmax with temperature\n",
    "            grad_input = grad_output / temperature\n",
    "            \n",
    "            return grad_input, None, None, None\n",
    "    \n",
    "    class SoftTopKFunction(Function):\n",
    "        \"\"\"\n",
    "        Differentiable approximation to Top-K operation\n",
    "        Uses continuous relaxation for gradient flow\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, input, k, temperature=1.0, method='sigmoid'):\n",
    "            \"\"\"\n",
    "            Soft Top-K selection\n",
    "            \n",
    "            Args:\n",
    "                input: Input tensor\n",
    "                k: Number of top elements to select\n",
    "                temperature: Temperature for softness\n",
    "                method: Method for soft selection ('sigmoid', 'softmax')\n",
    "            \"\"\"\n",
    "            ctx.k = k\n",
    "            ctx.temperature = temperature\n",
    "            ctx.method = method\n",
    "            \n",
    "            if method == 'sigmoid':\n",
    "                # Use sigmoid-based soft thresholding\n",
    "                sorted_values, _ = torch.sort(input, dim=-1, descending=True)\n",
    "                \n",
    "                if k < input.size(-1):\n",
    "                    threshold = sorted_values[..., k-1:k]  # k-th largest value\n",
    "                else:\n",
    "                    threshold = sorted_values[..., -1:]  # Smallest value\n",
    "                \n",
    "                # Soft mask using sigmoid\n",
    "                soft_mask = torch.sigmoid((input - threshold) / temperature)\n",
    "                result = input * soft_mask\n",
    "                \n",
    "                ctx.save_for_backward(input, soft_mask, threshold)\n",
    "                \n",
    "            elif method == 'softmax':\n",
    "                # Use softmax-based selection\n",
    "                softmax_weights = F.softmax(input / temperature, dim=-1)\n",
    "                \n",
    "                # Get top-k indices\n",
    "                _, top_indices = torch.topk(input, k, dim=-1)\n",
    "                \n",
    "                # Create soft selection based on softmax weights\n",
    "                result = input * softmax_weights * k  # Scale by k to maintain magnitude\n",
    "                \n",
    "                ctx.save_for_backward(input, softmax_weights)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Backward pass for soft Top-K\n",
    "            \"\"\"\n",
    "            if ctx.method == 'sigmoid':\n",
    "                input, soft_mask, threshold = ctx.saved_tensors\n",
    "                temperature = ctx.temperature\n",
    "                \n",
    "                # Gradient of sigmoid mask\n",
    "                sigmoid_grad = soft_mask * (1 - soft_mask) / temperature\n",
    "                \n",
    "                # Chain rule application\n",
    "                grad_input = grad_output * (soft_mask + input * sigmoid_grad)\n",
    "                \n",
    "            elif ctx.method == 'softmax':\n",
    "                input, softmax_weights = ctx.saved_tensors\n",
    "                temperature = ctx.temperature\n",
    "                k = ctx.k\n",
    "                \n",
    "                # Gradient through softmax weights\n",
    "                grad_input = grad_output * softmax_weights * k / temperature\n",
    "            \n",
    "            return grad_input, None, None, None\n",
    "    \n",
    "    class DifferentiableRoundingFunction(Function):\n",
    "        \"\"\"\n",
    "        Differentiable approximation to rounding operation\n",
    "        Uses smooth approximations for gradient flow\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, input, method='sigmoid', sharpness=10.0):\n",
    "            \"\"\"\n",
    "            Differentiable rounding\n",
    "            \n",
    "            Args:\n",
    "                input: Input tensor\n",
    "                method: Approximation method ('sigmoid', 'tanh', 'polynomial')\n",
    "                sharpness: Controls approximation sharpness\n",
    "            \"\"\"\n",
    "            ctx.method = method\n",
    "            ctx.sharpness = sharpness\n",
    "            \n",
    "            if method == 'sigmoid':\n",
    "                # Sigmoid-based approximation\n",
    "                fractional_part = input - torch.floor(input)\n",
    "                smooth_round = torch.sigmoid(sharpness * (fractional_part - 0.5))\n",
    "                result = torch.floor(input) + smooth_round\n",
    "                \n",
    "            elif method == 'tanh':\n",
    "                # Tanh-based approximation  \n",
    "                fractional_part = input - torch.floor(input)\n",
    "                smooth_round = 0.5 * (1 + torch.tanh(sharpness * (fractional_part - 0.5)))\n",
    "                result = torch.floor(input) + smooth_round\n",
    "                \n",
    "            elif method == 'polynomial':\n",
    "                # Polynomial approximation (3rd order)\n",
    "                fractional_part = input - torch.floor(input)\n",
    "                # Smooth step function: 3t¬≤ - 2t¬≥\n",
    "                smooth_step = 3 * fractional_part**2 - 2 * fractional_part**3\n",
    "                result = torch.floor(input) + smooth_step\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method: {method}\")\n",
    "            \n",
    "            ctx.save_for_backward(input, result)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Backward pass using smooth derivative\n",
    "            \"\"\"\n",
    "            input, result = ctx.saved_tensors\n",
    "            method = ctx.method\n",
    "            sharpness = ctx.sharpness\n",
    "            \n",
    "            if method == 'sigmoid':\n",
    "                fractional_part = input - torch.floor(input)\n",
    "                sigmoid_val = torch.sigmoid(sharpness * (fractional_part - 0.5))\n",
    "                grad_smooth = sharpness * sigmoid_val * (1 - sigmoid_val)\n",
    "                \n",
    "            elif method == 'tanh':\n",
    "                fractional_part = input - torch.floor(input)\n",
    "                tanh_val = torch.tanh(sharpness * (fractional_part - 0.5))\n",
    "                grad_smooth = 0.5 * sharpness * (1 - tanh_val**2)\n",
    "                \n",
    "            elif method == 'polynomial':\n",
    "                fractional_part = input - torch.floor(input)\n",
    "                grad_smooth = 6 * fractional_part * (1 - fractional_part)\n",
    "            \n",
    "            grad_input = grad_output * grad_smooth\n",
    "            \n",
    "            return grad_input, None, None\n",
    "    \n",
    "    # Wrapper functions\n",
    "    def straight_through_quantizer(input, num_bits=8, method='uniform'):\n",
    "        return StraightThroughEstimatorFunction.apply(input, num_bits, method)\n",
    "    \n",
    "    def gumbel_softmax(logits, temperature=1.0, hard=False, dim=-1):\n",
    "        return GumbelSoftmaxFunction.apply(logits, temperature, hard, dim)\n",
    "    \n",
    "    def soft_topk(input, k, temperature=1.0, method='sigmoid'):\n",
    "        return SoftTopKFunction.apply(input, k, temperature, method)\n",
    "    \n",
    "    def differentiable_round(input, method='sigmoid', sharpness=10.0):\n",
    "        return DifferentiableRoundingFunction.apply(input, method, sharpness)\n",
    "    \n",
    "    return straight_through_quantizer, gumbel_softmax, soft_topk, differentiable_round\n",
    "\n",
    "# Create non-differentiable operation handlers\n",
    "straight_through_quantizer, gumbel_softmax, soft_topk, differentiable_round = create_non_differentiable_operations()\n",
    "\n",
    "print(\"üö´ Testing Non-Differentiable Operations:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test quantization with straight-through estimator\n",
    "print(\"\\nüìä Testing Quantization with Straight-Through Estimator:\")\n",
    "\n",
    "test_tensor = torch.randn(100, 50, requires_grad=True)\n",
    "print(f\"Original range: [{test_tensor.min():.3f}, {test_tensor.max():.3f}]\")\n",
    "\n",
    "quantization_results = {}\n",
    "\n",
    "for num_bits in [2, 4, 8, 16]:\n",
    "    if test_tensor.grad is not None:\n",
    "        test_tensor.grad.zero_()\n",
    "    \n",
    "    # Test uniform quantization\n",
    "    quantized = straight_through_quantizer(test_tensor, num_bits=num_bits, method='uniform')\n",
    "    quantized.sum().backward()\n",
    "    \n",
    "    quantization_error = (quantized - test_tensor).abs().mean().item()\n",
    "    gradient_flow = test_tensor.grad.norm().item()\n",
    "    \n",
    "    quantization_results[f'{num_bits}_bit'] = {\n",
    "        'quantization_error': quantization_error,\n",
    "        'gradient_norm': gradient_flow,\n",
    "        'output_range': [quantized.min().item(), quantized.max().item()]\n",
    "    }\n",
    "    \n",
    "    print(f\"  {num_bits}-bit: Error={quantization_error:.4f}, \"\n",
    "          f\"Grad norm={gradient_flow:.4f}\")\n",
    "\n",
    "# Test Gumbel-Softmax\n",
    "print(f\"\\nüìä Testing Gumbel-Softmax:\")\n",
    "\n",
    "logits = torch.randn(32, 10, requires_grad=True)\n",
    "temperatures = [2.0, 1.0, 0.5, 0.1]\n",
    "\n",
    "gumbel_results = {}\n",
    "\n",
    "for temp in temperatures:\n",
    "    if logits.grad is not None:\n",
    "        logits.grad.zero_()\n",
    "    \n",
    "    # Test soft sampling\n",
    "    soft_sample = gumbel_softmax(logits, temperature=temp, hard=False)\n",
    "    hard_sample = gumbel_softmax(logits, temperature=temp, hard=True)\n",
    "    \n",
    "    soft_sample.sum().backward()\n",
    "    soft_grad_norm = logits.grad.norm().item()\n",
    "    \n",
    "    logits.grad.zero_()\n",
    "    hard_sample.sum().backward()\n",
    "    hard_grad_norm = logits.grad.norm().item()\n",
    "    \n",
    "    # Compute entropy (measure of discreteness)\n",
    "    soft_entropy = -(soft_sample * torch.log(soft_sample + 1e-10)).sum(dim=-1).mean().item()\n",
    "    hard_sparsity = (hard_sample > 0.5).float().sum(dim=-1).mean().item()\n",
    "    \n",
    "    gumbel_results[f'temp_{temp}'] = {\n",
    "        'soft_entropy': soft_entropy,\n",
    "        'hard_sparsity': hard_sparsity,\n",
    "        'soft_grad_norm': soft_grad_norm,\n",
    "        'hard_grad_norm': hard_grad_norm\n",
    "    }\n",
    "    \n",
    "    print(f\"  Temp {temp}: Entropy={soft_entropy:.3f}, \"\n",
    "          f\"Sparsity={hard_sparsity:.1f}, \"\n",
    "          f\"Soft grad={soft_grad_norm:.3f}\")\n",
    "\n",
    "# Test Soft Top-K\n",
    "print(f\"\\nüìä Testing Soft Top-K:\")\n",
    "\n",
    "topk_input = torch.randn(16, 20, requires_grad=True)\n",
    "k_values = [3, 5, 10]\n",
    "\n",
    "topk_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    if topk_input.grad is not None:\n",
    "        topk_input.grad.zero_()\n",
    "    \n",
    "    # Compare hard vs soft top-k\n",
    "    hard_topk_vals, hard_topk_indices = torch.topk(topk_input, k, dim=-1)\n",
    "    soft_topk_result = soft_topk(topk_input, k, temperature=0.1, method='sigmoid')\n",
    "    \n",
    "    soft_topk_result.sum().backward()\n",
    "    \n",
    "    # Analyze selection properties\n",
    "    soft_magnitude = soft_topk_result.norm(dim=-1).mean().item()\n",
    "    hard_magnitude = hard_topk_vals.norm(dim=-1).mean().item()\n",
    "    \n",
    "    topk_results[f'k_{k}'] = {\n",
    "        'soft_magnitude': soft_magnitude,\n",
    "        'hard_magnitude': hard_magnitude,\n",
    "        'gradient_norm': topk_input.grad.norm().item(),\n",
    "        'selection_ratio': (soft_topk_result.abs() > 1e-3).float().mean().item()\n",
    "    }\n",
    "    \n",
    "    print(f\"  k={k}: Soft mag={soft_magnitude:.3f}, \"\n",
    "          f\"Hard mag={hard_magnitude:.3f}, \"\n",
    "          f\"Selection ratio={topk_results[f'k_{k}']['selection_ratio']:.3f}\")\n",
    "\n",
    "# Test Differentiable Rounding\n",
    "print(f\"\\nüìä Testing Differentiable Rounding:\")\n",
    "\n",
    "round_input = torch.randn(100, requires_grad=True) * 5  # Range roughly -15 to 15\n",
    "methods = ['sigmoid', 'tanh', 'polynomial']\n",
    "\n",
    "rounding_results = {}\n",
    "\n",
    "for method in methods:\n",
    "    if round_input.grad is not None:\n",
    "        round_input.grad.zero_()\n",
    "    \n",
    "    rounded = differentiable_round(round_input, method=method, sharpness=10.0)\n",
    "    rounded.sum().backward()\n",
    "    \n",
    "    # Compare with true rounding\n",
    "    true_rounded = torch.round(round_input.detach())\n",
    "    rounding_error = (rounded - true_rounded).abs().mean().item()\n",
    "    \n",
    "    rounding_results[method] = {\n",
    "        'rounding_error': rounding_error,\n",
    "        'gradient_norm': round_input.grad.norm().item(),\n",
    "        'output_range': [rounded.min().item(), rounded.max().item()]\n",
    "    }\n",
    "    \n",
    "    print(f\"  {method}: Error={rounding_error:.4f}, \"\n",
    "          f\"Grad norm={round_input.grad.norm():.4f}\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "def create_non_differentiable_visualization():\n",
    "    \"\"\"Visualize non-differentiable operations and their approximations\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    \n",
    "    # 1. Quantization effects\n",
    "    x_quant = torch.linspace(-3, 3, 1000)\n",
    "    \n",
    "    for i, bits in enumerate([2, 4, 8]):\n",
    "        with torch.no_grad():\n",
    "            quantized = straight_through_quantizer(x_quant, num_bits=bits)\n",
    "        \n",
    "        axes[0, i].plot(x_quant, x_quant, 'b--', alpha=0.5, label='Original')\n",
    "        axes[0, i].plot(x_quant, quantized, 'r-', linewidth=2, label=f'{bits}-bit')\n",
    "        axes[0, i].set_title(f'{bits}-bit Quantization', fontweight='bold')\n",
    "        axes[0, i].set_xlabel('Input')\n",
    "        axes[0, i].set_ylabel('Output')\n",
    "        axes[0, i].legend()\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Gumbel-Softmax temperature effects\n",
    "    logits_example = torch.tensor([[2.0, 1.0, 0.5, 0.1, -0.5]])\n",
    "    temps = [2.0, 1.0, 0.1]\n",
    "    \n",
    "    for i, temp in enumerate(temps):\n",
    "        with torch.no_grad():\n",
    "            probs = gumbel_softmax(logits_example, temperature=temp, hard=False)[0]\n",
    "        \n",
    "        axes[1, i].bar(range(len(probs)), probs, alpha=0.7, \n",
    "                      color=plt.cm.viridis(i/len(temps)))\n",
    "        axes[1, i].set_title(f'Gumbel-Softmax T={temp}', fontweight='bold')\n",
    "        axes[1, i].set_xlabel('Category')\n",
    "        axes[1, i].set_ylabel('Probability')\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Soft Top-K selection\n",
    "    input_vals = torch.tensor([3.0, 1.5, 4.2, 0.8, 2.1, 3.8, 1.2, 2.9])\n",
    "    k_vals = [2, 3, 5]\n",
    "    \n",
    "    for i, k in enumerate(k_vals):\n",
    "        with torch.no_grad():\n",
    "            soft_result = soft_topk(input_vals.unsqueeze(0), k, temperature=0.1)[0]\n",
    "            hard_topk_vals, _ = torch.topk(input_vals, k)\n",
    "        \n",
    "        x_pos = range(len(input_vals))\n",
    "        axes[2, i].bar(x_pos, input_vals, alpha=0.5, label='Original', color='blue')\n",
    "        axes[2, i].bar(x_pos, soft_result, alpha=0.8, label=f'Soft Top-{k}', color='red')\n",
    "        \n",
    "        axes[2, i].set_title(f'Soft Top-{k} Selection', fontweight='bold')\n",
    "        axes[2, i].set_xlabel('Index')\n",
    "        axes[2, i].set_ylabel('Value')\n",
    "        axes[2, i].legend()\n",
    "        axes[2, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Non-Differentiable Operations Analysis', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'non_differentiable_operations.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Create visualization\n",
    "create_non_differentiable_visualization()\n",
    "\n",
    "# Save non-differentiable operations results\n",
    "non_diff_results = {\n",
    "    'quantization': quantization_results,\n",
    "    'gumbel_softmax': gumbel_results,\n",
    "    'soft_topk': topk_results,\n",
    "    'differentiable_rounding': rounding_results\n",
    "}\n",
    "\n",
    "with open(results_dir / 'non_differentiable_operations_results.json', 'w') as f:\n",
    "    json.dump(non_diff_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Non-differentiable operations results saved\")\n",
    "print(f\"\\nüéì Key Techniques Summary:\")\n",
    "print(f\"  ‚Ä¢ Straight-Through Estimator: Pass gradients unchanged\")\n",
    "print(f\"  ‚Ä¢ Gumbel-Softmax: Continuous relaxation of discrete sampling\")\n",
    "print(f\"  ‚Ä¢ Soft approximations: Replace hard operations with smooth versions\")\n",
    "print(f\"  ‚Ä¢ Temperature annealing: Start soft, gradually make harder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98198d7",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Testing and Validation Framework\n",
    "\n",
    "### 6.1 Advanced Gradient Checking and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_testing_framework():\n",
    "    \"\"\"Create advanced testing framework for custom autograd functions\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 6.1 Comprehensive Testing and Validation Framework ===\\n\")\n",
    "    \n",
    "    class GradientChecker:\n",
    "        \"\"\"Advanced gradient checking with multiple validation methods\"\"\"\n",
    "        \n",
    "        def __init__(self, eps=1e-5, tolerance=1e-4, verbose=True):\n",
    "            self.eps = eps\n",
    "            self.tolerance = tolerance\n",
    "            self.verbose = verbose\n",
    "            self.test_results = {}\n",
    "        \n",
    "        def numerical_gradient(self, func, inputs, eps=None):\n",
    "            \"\"\"\n",
    "            Compute numerical gradient using finite differences\n",
    "            \n",
    "            Args:\n",
    "                func: Function to compute gradient for\n",
    "                inputs: List of input tensors\n",
    "                eps: Finite difference step size\n",
    "                \n",
    "            Returns:\n",
    "                List of numerical gradients\n",
    "            \"\"\"\n",
    "            if eps is None:\n",
    "                eps = self.eps\n",
    "            \n",
    "            numerical_grads = []\n",
    "            \n",
    "            for input_idx, input_tensor in enumerate(inputs):\n",
    "                grad = torch.zeros_like(input_tensor)\n",
    "                \n",
    "                # Flatten for easier iteration\n",
    "                flat_input = input_tensor.view(-1)\n",
    "                flat_grad = grad.view(-1)\n",
    "                \n",
    "                for i in range(flat_input.numel()):\n",
    "                    # Positive perturbation\n",
    "                    flat_input[i] += eps\n",
    "                    inputs_pos = [inp if j != input_idx else input_tensor \n",
    "                                 for j, inp in enumerate(inputs)]\n",
    "                    \n",
    "                    try:\n",
    "                        loss_pos = func(*inputs_pos).sum()\n",
    "                    except Exception as e:\n",
    "                        if self.verbose:\n",
    "                            print(f\"Warning: Error in positive perturbation: {e}\")\n",
    "                        loss_pos = torch.tensor(0.0)\n",
    "                    \n",
    "                    # Negative perturbation\n",
    "                    flat_input[i] -= 2 * eps\n",
    "                    inputs_neg = [inp if j != input_idx else input_tensor \n",
    "                                 for j, inp in enumerate(inputs)]\n",
    "                    \n",
    "                    try:\n",
    "                        loss_neg = func(*inputs_neg).sum()\n",
    "                    except Exception as e:\n",
    "                        if self.verbose:\n",
    "                            print(f\"Warning: Error in negative perturbation: {e}\")\n",
    "                        loss_neg = torch.tensor(0.0)\n",
    "                    \n",
    "                    # Central difference\n",
    "                    flat_grad[i] = (loss_pos - loss_neg) / (2 * eps)\n",
    "                    \n",
    "                    # Restore original value\n",
    "                    flat_input[i] += eps\n",
    "                \n",
    "                numerical_grads.append(grad)\n",
    "            \n",
    "            return numerical_grads\n",
    "        \n",
    "        def check_gradients(self, func, inputs, test_name=\"\", output_shape=None):\n",
    "            \"\"\"\n",
    "            Comprehensive gradient checking\n",
    "            \n",
    "            Args:\n",
    "                func: Custom function to test\n",
    "                inputs: List of input tensors\n",
    "                test_name: Name for the test\n",
    "                output_shape: Expected output shape\n",
    "                \n",
    "            Returns:\n",
    "                Dictionary with test results\n",
    "            \"\"\"\n",
    "            if self.verbose:\n",
    "                print(f\"üîç Testing function: {test_name}\")\n",
    "                print(f\"üìä Input shapes: {[inp.shape for inp in inputs]}\")\n",
    "            \n",
    "            # Ensure inputs require gradients\n",
    "            for inp in inputs:\n",
    "                inp.requires_grad_(True)\n",
    "                if inp.grad is not None:\n",
    "                    inp.grad.zero_()\n",
    "            \n",
    "            # Test forward pass\n",
    "            try:\n",
    "                output = func(*inputs)\n",
    "                forward_success = True\n",
    "                \n",
    "                if output_shape and output.shape != output_shape:\n",
    "                    print(f\"‚ö†Ô∏è Warning: Expected shape {output_shape}, got {output.shape}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(f\"‚ùå Forward pass failed: {e}\")\n",
    "                return {'passed': False, 'error': f\"Forward pass failed: {e}\"}\n",
    "            \n",
    "            # Test backward pass\n",
    "            try:\n",
    "                loss = output.sum()\n",
    "                loss.backward()\n",
    "                backward_success = True\n",
    "                \n",
    "                analytical_grads = [inp.grad.clone() for inp in inputs]\n",
    "                \n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(f\"‚ùå Backward pass failed: {e}\")\n",
    "                return {'passed': False, 'error': f\"Backward pass failed: {e}\"}\n",
    "            \n",
    "            # Clear gradients for numerical computation\n",
    "            for inp in inputs:\n",
    "                inp.grad.zero_()\n",
    "            \n",
    "            # Compute numerical gradients\n",
    "            if self.verbose:\n",
    "                print(\"üßÆ Computing numerical gradients...\")\n",
    "            \n",
    "            numerical_grads = self.numerical_gradient(func, inputs)\n",
    "            \n",
    "            # Compare gradients\n",
    "            results = {\n",
    "                'passed': True,\n",
    "                'max_error': 0.0,\n",
    "                'relative_errors': [],\n",
    "                'absolute_errors': [],\n",
    "                'input_analyses': []\n",
    "            }\n",
    "            \n",
    "            for i, (analytical, numerical) in enumerate(zip(analytical_grads, numerical_grads)):\n",
    "                # Handle zero gradients\n",
    "                abs_error = (analytical - numerical).abs()\n",
    "                rel_error = abs_error / (numerical.abs() + self.eps)\n",
    "                \n",
    "                max_abs_error = abs_error.max().item()\n",
    "                max_rel_error = rel_error.max().item()\n",
    "                mean_abs_error = abs_error.mean().item()\n",
    "                mean_rel_error = rel_error.mean().item()\n",
    "                \n",
    "                # Check for NaN or Inf\n",
    "                has_nan = torch.isnan(analytical).any() or torch.isnan(numerical).any()\n",
    "                has_inf = torch.isinf(analytical).any() or torch.isinf(numerical).any()\n",
    "                \n",
    "                input_analysis = {\n",
    "                    'input_index': i,\n",
    "                    'max_absolute_error': max_abs_error,\n",
    "                    'max_relative_error': max_rel_error,\n",
    "                    'mean_absolute_error': mean_abs_error,\n",
    "                    'mean_relative_error': mean_rel_error,\n",
    "                    'has_nan': has_nan,\n",
    "                    'has_inf': has_inf,\n",
    "                    'analytical_norm': analytical.norm().item(),\n",
    "                    'numerical_norm': numerical.norm().item()\n",
    "                }\n",
    "                \n",
    "                results['input_analyses'].append(input_analysis)\n",
    "                results['absolute_errors'].append(max_abs_error)\n",
    "                results['relative_errors'].append(max_rel_error)\n",
    "                results['max_error'] = max(results['max_error'], max_abs_error)\n",
    "                \n",
    "                # Determine pass/fail\n",
    "                passed = (max_abs_error < self.tolerance and \n",
    "                         max_rel_error < self.tolerance and \n",
    "                         not has_nan and not has_inf)\n",
    "                \n",
    "                if not passed:\n",
    "                    results['passed'] = False\n",
    "                \n",
    "                if self.verbose:\n",
    "                    status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n",
    "                    print(f\"  Input {i}: Max abs={max_abs_error:.2e}, \"\n",
    "                          f\"Max rel={max_rel_error:.2e} - {status}\")\n",
    "            \n",
    "            overall_status = \"‚úÖ PASSED\" if results['passed'] else \"‚ùå FAILED\"\n",
    "            if self.verbose:\n",
    "                print(f\"\\nOverall: {overall_status} (tolerance: {self.tolerance:.0e})\")\n",
    "            \n",
    "            # Store results\n",
    "            self.test_results[test_name] = results\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        def performance_benchmark(self, func, inputs, num_iterations=1000):\n",
    "            \"\"\"\n",
    "            Benchmark performance of custom function\n",
    "            \n",
    "            Args:\n",
    "                func: Function to benchmark\n",
    "                inputs: Input tensors\n",
    "                num_iterations: Number of iterations for timing\n",
    "                \n",
    "            Returns:\n",
    "                Performance metrics\n",
    "            \"\"\"\n",
    "            if self.verbose:\n",
    "                print(f\"‚ö° Performance benchmarking ({num_iterations} iterations)...\")\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(10):\n",
    "                output = func(*inputs)\n",
    "                output.sum().backward()\n",
    "                for inp in inputs:\n",
    "                    if inp.grad is not None:\n",
    "                        inp.grad.zero_()\n",
    "            \n",
    "            # Forward pass timing\n",
    "            start_time = time.time()\n",
    "            for _ in range(num_iterations):\n",
    "                output = func(*inputs)\n",
    "            forward_time = (time.time() - start_time) / num_iterations\n",
    "            \n",
    "            # Backward pass timing\n",
    "            start_time = time.time()\n",
    "            for _ in range(num_iterations):\n",
    "                output = func(*inputs)\n",
    "                output.sum().backward()\n",
    "                for inp in inputs:\n",
    "                    if inp.grad is not None:\n",
    "                        inp.grad.zero_()\n",
    "            backward_time = (time.time() - start_time) / num_iterations - forward_time\n",
    "            \n",
    "            performance_metrics = {\n",
    "                'forward_time_ms': forward_time * 1000,\n",
    "                'backward_time_ms': backward_time * 1000,\n",
    "                'total_time_ms': (forward_time + backward_time) * 1000,\n",
    "                'memory_usage_mb': sum(inp.numel() * inp.element_size() \n",
    "                                     for inp in inputs) / (1024**2)\n",
    "            }\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"  Forward: {forward_time*1000:.3f}ms\")\n",
    "                print(f\"  Backward: {backward_time*1000:.3f}ms\")\n",
    "                print(f\"  Memory: {performance_metrics['memory_usage_mb']:.1f}MB\")\n",
    "            \n",
    "            return performance_metrics\n",
    "        \n",
    "        def comprehensive_test_suite(self, function_tests):\n",
    "            \"\"\"\n",
    "            Run comprehensive test suite on multiple functions\n",
    "            \n",
    "            Args:\n",
    "                function_tests: List of (func, inputs, name) tuples\n",
    "                \n",
    "            Returns:\n",
    "                Complete test suite results\n",
    "            \"\"\"\n",
    "            print(f\"üß™ Running Comprehensive Test Suite\")\n",
    "            print(f\"=\" * 60)\n",
    "            \n",
    "            suite_results = {}\n",
    "            \n",
    "            for func, inputs, name in function_tests:\n",
    "                print(f\"\\nüìã Testing: {name}\")\n",
    "                print(f\"-\" * 40)\n",
    "                \n",
    "                # Gradient checking\n",
    "                grad_results = self.check_gradients(func, inputs, name)\n",
    "                \n",
    "                # Performance benchmarking\n",
    "                perf_results = self.performance_benchmark(func, inputs)\n",
    "                \n",
    "                # Combine results\n",
    "                suite_results[name] = {\n",
    "                    'gradient_check': grad_results,\n",
    "                    'performance': perf_results,\n",
    "                    'passed': grad_results.get('passed', False)\n",
    "                }\n",
    "            \n",
    "            # Summary\n",
    "            print(f\"\\nüìä Test Suite Summary:\")\n",
    "            print(f\"=\" * 40)\n",
    "            \n",
    "            total_tests = len(function_tests)\n",
    "            passed_tests = sum(1 for result in suite_results.values() \n",
    "                             if result['passed'])\n",
    "            \n",
    "            print(f\"Total tests: {total_tests}\")\n",
    "            print(f\"Passed: {passed_tests}\")\n",
    "            print(f\"Failed: {total_tests - passed_tests}\")\n",
    "            print(f\"Success rate: {passed_tests/total_tests*100:.1f}%\")\n",
    "            \n",
    "            return suite_results\n",
    "    \n",
    "    return GradientChecker\n",
    "\n",
    "# Create comprehensive testing framework\n",
    "GradientChecker = create_comprehensive_testing_framework()\n",
    "\n",
    "print(\"üß™ Comprehensive Function Testing:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Initialize gradient checker\n",
    "checker = GradientChecker(eps=1e-5, tolerance=1e-4, verbose=True)\n",
    "\n",
    "# Define test functions to validate\n",
    "test_functions = [\n",
    "    # Basic functions\n",
    "    (lambda x: x**2, [torch.randn(5, 3, requires_grad=True)], \"Square Function\"),\n",
    "    \n",
    "    # Multi-input functions  \n",
    "    (lambda x, w: torch.sum(x * w, dim=1), \n",
    "     [torch.randn(4, 6, requires_grad=True), torch.randn(6, requires_grad=True)], \n",
    "     \"Weighted Sum\"),\n",
    "    \n",
    "    # Custom activations\n",
    "    (swish, [torch.randn(10, requires_grad=True)], \"Swish Activation\"),\n",
    "    (mish, [torch.randn(10, requires_grad=True)], \"Mish Activation\"),\n",
    "    \n",
    "    # Memory-efficient operations\n",
    "    (lambda x, w: memory_efficient_matmul(x, w, save_memory=False),\n",
    "     [torch.randn(8, 10, requires_grad=True), torch.randn(10, 5, requires_grad=True)],\n",
    "     \"Memory Efficient MatMul\"),\n",
    "    \n",
    "    # Non-differentiable operations\n",
    "    (lambda x: straight_through_quantizer(x, num_bits=8),\n",
    "     [torch.randn(6, 4, requires_grad=True)], \n",
    "     \"Straight-Through Quantizer\"),\n",
    "    \n",
    "    (lambda x: gumbel_softmax(x, temperature=1.0, hard=False),\n",
    "     [torch.randn(3, 5, requires_grad=True)],\n",
    "     \"Gumbel Softmax\"),\n",
    "]\n",
    "\n",
    "# Run comprehensive test suite\n",
    "test_suite_results = checker.comprehensive_test_suite(test_functions)\n",
    "\n",
    "# Create detailed analysis visualization\n",
    "def create_testing_analysis_visualization(suite_results):\n",
    "    \"\"\"Create comprehensive visualization of testing results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Extract data for visualization\n",
    "    function_names = list(suite_results.keys())\n",
    "    max_errors = [suite_results[name]['gradient_check'].get('max_error', 0) \n",
    "                  for name in function_names]\n",
    "    forward_times = [suite_results[name]['performance']['forward_time_ms'] \n",
    "                    for name in function_names]\n",
    "    backward_times = [suite_results[name]['performance']['backward_time_ms'] \n",
    "                     for name in function_names]\n",
    "    memory_usage = [suite_results[name]['performance']['memory_usage_mb'] \n",
    "                   for name in function_names]\n",
    "    passed_status = [suite_results[name]['passed'] for name in function_names]\n",
    "    \n",
    "    # 1. Gradient error analysis\n",
    "    colors = ['green' if passed else 'red' for passed in passed_status]\n",
    "    bars1 = axes[0,0].bar(range(len(function_names)), max_errors, color=colors, alpha=0.7)\n",
    "    axes[0,0].set_title('Maximum Gradient Errors', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('Max Error')\n",
    "    axes[0,0].set_yscale('log')\n",
    "    axes[0,0].set_xticks(range(len(function_names)))\n",
    "    axes[0,0].set_xticklabels(function_names, rotation=45, ha='right')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add tolerance line\n",
    "    axes[0,0].axhline(y=checker.tolerance, color='red', linestyle='--', \n",
    "                     label=f'Tolerance: {checker.tolerance:.0e}')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # 2. Forward pass performance\n",
    "    axes[0,1].bar(range(len(function_names)), forward_times, alpha=0.7, color='blue')\n",
    "    axes[0,1].set_title('Forward Pass Performance', fontweight='bold')\n",
    "    axes[0,1].set_ylabel('Time (ms)')\n",
    "    axes[0,1].set_xticks(range(len(function_names)))\n",
    "    axes[0,1].set_xticklabels(function_names, rotation=45, ha='right')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Backward pass performance\n",
    "    axes[0,2].bar(range(len(function_names)), backward_times, alpha=0.7, color='orange')\n",
    "    axes[0,2].set_title('Backward Pass Performance', fontweight='bold')\n",
    "    axes[0,2].set_ylabel('Time (ms)')\n",
    "    axes[0,2].set_xticks(range(len(function_names)))\n",
    "    axes[0,2].set_xticklabels(function_names, rotation=45, ha='right')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Memory usage\n",
    "    axes[1,0].bar(range(len(function_names)), memory_usage, alpha=0.7, color='purple')\n",
    "    axes[1,0].set_title('Memory Usage', fontweight='bold')\n",
    "    axes[1,0].set_ylabel('Memory (MB)')\n",
    "    axes[1,0].set_xticks(range(len(function_names)))\n",
    "    axes[1,0].set_xticklabels(function_names, rotation=45, ha='right')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Pass/Fail summary\n",
    "    pass_counts = [sum(passed_status), len(passed_status) - sum(passed_status)]\n",
    "    labels = ['Passed', 'Failed']\n",
    "    colors_pie = ['green', 'red']\n",
    "    \n",
    "    axes[1,1].pie(pass_counts, labels=labels, colors=colors_pie, autopct='%1.1f%%',\n",
    "                 startangle=90)\n",
    "    axes[1,1].set_title('Test Results Summary', fontweight='bold')\n",
    "    \n",
    "    # 6. Performance vs Accuracy scatter\n",
    "    axes[1,2].scatter(forward_times, max_errors, c=colors, alpha=0.7, s=100)\n",
    "    axes[1,2].set_xlabel('Forward Time (ms)')\n",
    "    axes[1,2].set_ylabel('Max Gradient Error')\n",
    "    axes[1,2].set_yscale('log')\n",
    "    axes[1,2].set_title('Performance vs Accuracy', fontweight='bold')\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add function labels\n",
    "    for i, name in enumerate(function_names):\n",
    "        axes[1,2].annotate(name[:8], (forward_times[i], max_errors[i]), \n",
    "                          fontsize=8, ha='center')\n",
    "    \n",
    "    plt.suptitle('Comprehensive Function Testing Analysis', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'comprehensive_testing_analysis.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Create testing visualization\n",
    "create_testing_analysis_visualization(test_suite_results)\n",
    "\n",
    "# Save comprehensive testing results\n",
    "with open(results_dir / 'comprehensive_testing_results.json', 'w') as f:\n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    json_results = {}\n",
    "    for name, result in test_suite_results.items():\n",
    "        json_results[name] = {\n",
    "            'gradient_check': {\n",
    "                'passed': result['gradient_check'].get('passed', False),\n",
    "                'max_error': float(result['gradient_check'].get('max_error', 0)),\n",
    "                'num_inputs': len(result['gradient_check'].get('absolute_errors', []))\n",
    "            },\n",
    "            'performance': {\n",
    "                'forward_time_ms': float(result['performance']['forward_time_ms']),\n",
    "                'backward_time_ms': float(result['performance']['backward_time_ms']),\n",
    "                'memory_usage_mb': float(result['performance']['memory_usage_mb'])\n",
    "            },\n",
    "            'overall_passed': result['passed']\n",
    "        }\n",
    "    \n",
    "    json.dump(json_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Comprehensive testing results saved\")\n",
    "print(f\"\\nüéì Testing Framework Insights:\")\n",
    "print(f\"  ‚Ä¢ Numerical gradient checking validates custom implementations\")\n",
    "print(f\"  ‚Ä¢ Performance benchmarking identifies bottlenecks\")\n",
    "print(f\"  ‚Ä¢ Comprehensive testing ensures production readiness\")\n",
    "print(f\"  ‚Ä¢ Error analysis guides optimization efforts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da692abd",
   "metadata": {},
   "source": [
    "## 7. Advanced Loss Functions and Specialized Operations\n",
    "\n",
    "### 7.1 Production-Ready Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_loss_functions():\n",
    "    \"\"\"Create sophisticated loss functions for specialized applications\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 7.1 Advanced Custom Loss Functions ===\\n\")\n",
    "    \n",
    "    class FocalLossFunction(Function):\n",
    "        \"\"\"\n",
    "        Focal Loss for addressing class imbalance\n",
    "        Paper: \"Focal Loss for Dense Object Detection\" (Lin et al., 2017)\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "            \"\"\"\n",
    "            Focal Loss: FL(p_t) = -Œ±_t(1-p_t)^Œ≥ log(p_t)\n",
    "            \n",
    "            Args:\n",
    "                input: Logits [batch_size, num_classes]\n",
    "                target: Target classes [batch_size]\n",
    "                alpha: Weighting factor for rare class\n",
    "                gamma: Focusing parameter\n",
    "                reduction: 'mean', 'sum', or 'none'\n",
    "            \"\"\"\n",
    "            # Compute softmax probabilities\n",
    "            log_probs = F.log_softmax(input, dim=-1)\n",
    "            probs = torch.exp(log_probs)\n",
    "            \n",
    "            # Get probabilities and log probabilities for target classes\n",
    "            target_log_probs = log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "            target_probs = probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Compute focal weights\n",
    "            focal_weights = alpha * (1 - target_probs) ** gamma\n",
    "            \n",
    "            # Compute focal loss\n",
    "            focal_losses = -focal_weights * target_log_probs\n",
    "            \n",
    "            if reduction == 'mean':\n",
    "                result = focal_losses.mean()\n",
    "            elif reduction == 'sum':\n",
    "                result = focal_losses.sum()\n",
    "            else:\n",
    "                result = focal_losses\n",
    "            \n",
    "            # Save for backward pass\n",
    "            ctx.save_for_backward(probs, target, focal_weights, target_probs)\n",
    "            ctx.alpha = alpha\n",
    "            ctx.gamma = gamma\n",
    "            ctx.reduction = reduction\n",
    "            ctx.batch_size = input.size(0)\n",
    "            ctx.num_classes = input.size(1)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Complex gradient computation for focal loss\n",
    "            \"\"\"\n",
    "            probs, target, focal_weights, target_probs = ctx.saved_tensors\n",
    "            alpha = ctx.alpha\n",
    "            gamma = ctx.gamma\n",
    "            reduction = ctx.reduction\n",
    "            batch_size = ctx.batch_size\n",
    "            num_classes = ctx.num_classes\n",
    "            \n",
    "            # Initialize gradient\n",
    "            grad_input = torch.zeros_like(probs)\n",
    "            \n",
    "            # Create one-hot encoding\n",
    "            target_one_hot = torch.zeros_like(probs)\n",
    "            target_one_hot.scatter_(1, target.unsqueeze(1), 1)\n",
    "            \n",
    "            # For each sample and class\n",
    "            for i in range(batch_size):\n",
    "                for j in range(num_classes):\n",
    "                    if j == target[i]:  # Target class\n",
    "                        # Complex derivative for target class\n",
    "                        grad_input[i, j] = alpha * (\n",
    "                            gamma * (1 - target_probs[i]) ** (gamma - 1) * \n",
    "                            torch.log(target_probs[i]) * probs[i, j] +\n",
    "                            (1 - target_probs[i]) ** gamma * (probs[i, j] - 1)\n",
    "                        )\n",
    "                    else:  # Non-target class\n",
    "                        grad_input[i, j] = alpha * (\n",
    "                            gamma * (1 - target_probs[i]) ** (gamma - 1) * \n",
    "                            torch.log(target_probs[i]) * probs[i, j] +\n",
    "                            (1 - target_probs[i]) ** gamma * probs[i, j]\n",
    "                        )\n",
    "            \n",
    "            # Apply reduction scaling\n",
    "            if reduction == 'mean':\n",
    "                grad_input = grad_input / batch_size\n",
    "            \n",
    "            grad_input = grad_input * grad_output\n",
    "            \n",
    "            return grad_input, None, None, None, None\n",
    "    \n",
    "    class TripletLossFunction(Function):\n",
    "        \"\"\"\n",
    "        Triplet Loss for metric learning\n",
    "        Used in face recognition, person re-identification, etc.\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, anchor, positive, negative, margin=1.0, p=2):\n",
    "            \"\"\"\n",
    "            Triplet Loss: max(0, d(a,p) - d(a,n) + margin)\n",
    "            \n",
    "            Args:\n",
    "                anchor: Anchor embeddings [batch_size, embedding_dim]\n",
    "                positive: Positive embeddings [batch_size, embedding_dim]\n",
    "                negative: Negative embeddings [batch_size, embedding_dim]\n",
    "                margin: Margin for separation\n",
    "                p: Norm order (1 or 2)\n",
    "            \"\"\"\n",
    "            # Compute distances\n",
    "            if p == 1:\n",
    "                pos_dist = (anchor - positive).abs().sum(dim=1)\n",
    "                neg_dist = (anchor - negative).abs().sum(dim=1)\n",
    "            elif p == 2:\n",
    "                pos_dist = (anchor - positive).pow(2).sum(dim=1).sqrt()\n",
    "                neg_dist = (anchor - negative).pow(2).sum(dim=1).sqrt()\n",
    "            else:\n",
    "                pos_dist = (anchor - positive).abs().pow(p).sum(dim=1).pow(1.0/p)\n",
    "                neg_dist = (anchor - negative).abs().pow(p).sum(dim=1).pow(1.0/p)\n",
    "            \n",
    "            # Compute triplet loss\n",
    "            losses = F.relu(pos_dist - neg_dist + margin)\n",
    "            \n",
    "            # Save for backward\n",
    "            ctx.save_for_backward(anchor, positive, negative, pos_dist, neg_dist, losses)\n",
    "            ctx.margin = margin\n",
    "            ctx.p = p\n",
    "            \n",
    "            return losses.mean()\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Backward pass for triplet loss\n",
    "            \"\"\"\n",
    "            anchor, positive, negative, pos_dist, neg_dist, losses = ctx.saved_tensors\n",
    "            margin = ctx.margin\n",
    "            p = ctx.p\n",
    "            \n",
    "            batch_size = anchor.size(0)\n",
    "            embedding_dim = anchor.size(1)\n",
    "            \n",
    "            # Initialize gradients\n",
    "            grad_anchor = torch.zeros_like(anchor)\n",
    "            grad_positive = torch.zeros_like(positive)\n",
    "            grad_negative = torch.zeros_like(negative)\n",
    "            \n",
    "            # Only compute gradients for non-zero losses\n",
    "            active_mask = (losses > 0).float()\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                if active_mask[i] > 0:\n",
    "                    if p == 1:\n",
    "                        # L1 distance gradients\n",
    "                        pos_grad = torch.sign(anchor[i] - positive[i])\n",
    "                        neg_grad = torch.sign(anchor[i] - negative[i])\n",
    "                    elif p == 2:\n",
    "                        # L2 distance gradients\n",
    "                        pos_grad = (anchor[i] - positive[i]) / (pos_dist[i] + 1e-8)\n",
    "                        neg_grad = (anchor[i] - negative[i]) / (neg_dist[i] + 1e-8)\n",
    "                    else:\n",
    "                        # General Lp distance gradients\n",
    "                        pos_diff = anchor[i] - positive[i]\n",
    "                        neg_diff = anchor[i] - negative[i]\n",
    "                        \n",
    "                        pos_grad = (torch.sign(pos_diff) * torch.abs(pos_diff) ** (p-1) / \n",
    "                                  (pos_dist[i] ** (p-1) + 1e-8))\n",
    "                        neg_grad = (torch.sign(neg_diff) * torch.abs(neg_diff) ** (p-1) / \n",
    "                                  (neg_dist[i] ** (p-1) + 1e-8))\n",
    "                    \n",
    "                    # Accumulate gradients\n",
    "                    grad_anchor[i] = pos_grad - neg_grad\n",
    "                    grad_positive[i] = -pos_grad\n",
    "                    grad_negative[i] = neg_grad\n",
    "            \n",
    "            # Scale by output gradient and batch size\n",
    "            scale = grad_output / batch_size\n",
    "            grad_anchor *= scale\n",
    "            grad_positive *= scale\n",
    "            grad_negative *= scale\n",
    "            \n",
    "            return grad_anchor, grad_positive, grad_negative, None, None\n",
    "    \n",
    "    class ContrastiveLossFunction(Function):\n",
    "        \"\"\"\n",
    "        Contrastive Loss for siamese networks\n",
    "        Learns to minimize distance for similar pairs, maximize for dissimilar\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, output1, output2, label, margin=1.0):\n",
    "            \"\"\"\n",
    "            Contrastive Loss: \n",
    "            - Similar: 0.5 * d¬≤\n",
    "            - Dissimilar: 0.5 * max(0, margin - d)¬≤\n",
    "            \n",
    "            Args:\n",
    "                output1: First output embeddings [batch_size, embedding_dim]\n",
    "                output2: Second output embeddings [batch_size, embedding_dim]\n",
    "                label: Binary labels (1 for similar, 0 for dissimilar)\n",
    "                margin: Margin for dissimilar pairs\n",
    "            \"\"\"\n",
    "            # Compute Euclidean distance\n",
    "            diff = output1 - output2\n",
    "            distances = torch.sqrt(torch.sum(diff ** 2, dim=1) + 1e-8)\n",
    "            \n",
    "            # Compute contrastive loss\n",
    "            similar_loss = 0.5 * distances ** 2\n",
    "            dissimilar_loss = 0.5 * F.relu(margin - distances) ** 2\n",
    "            \n",
    "            losses = label.float() * similar_loss + (1 - label.float()) * dissimilar_loss\n",
    "            \n",
    "            # Save for backward\n",
    "            ctx.save_for_backward(output1, output2, label, distances, diff)\n",
    "            ctx.margin = margin\n",
    "            \n",
    "            return losses.mean()\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Backward pass for contrastive loss\n",
    "            \"\"\"\n",
    "            output1, output2, label, distances, diff = ctx.saved_tensors\n",
    "            margin = ctx.margin\n",
    "            batch_size = output1.size(0)\n",
    "            \n",
    "            # Initialize gradients\n",
    "            grad_output1 = torch.zeros_like(output1)\n",
    "            grad_output2 = torch.zeros_like(output2)\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                if label[i] == 1:  # Similar pair\n",
    "                    # Gradient for similar pairs: d * (o1 - o2) / ||o1 - o2||\n",
    "                    grad = diff[i] / (distances[i] + 1e-8)\n",
    "                else:  # Dissimilar pair\n",
    "                    # Gradient for dissimilar pairs: -max(0, margin - d) * (o1 - o2) / ||o1 - o2||\n",
    "                    if distances[i] < margin:\n",
    "                        grad = -(margin - distances[i]) * diff[i] / (distances[i] + 1e-8)\n",
    "                    else:\n",
    "                        grad = torch.zeros_like(diff[i])\n",
    "                \n",
    "                grad_output1[i] = grad\n",
    "                grad_output2[i] = -grad\n",
    "            \n",
    "            # Scale by output gradient and batch size\n",
    "            scale = grad_output / batch_size\n",
    "            grad_output1 *= scale\n",
    "            grad_output2 *= scale\n",
    "            \n",
    "            return grad_output1, grad_output2, None, None\n",
    "    \n",
    "    class CenterLossFunction(Function):\n",
    "        \"\"\"\n",
    "        Center Loss for deep feature learning\n",
    "        Learns class centers and penalizes distance from centers\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, features, labels, centers, alpha=0.5):\n",
    "            \"\"\"\n",
    "            Center Loss: 0.5 * sum(||f_i - c_{y_i}||¬≤)\n",
    "            \n",
    "            Args:\n",
    "                features: Feature embeddings [batch_size, feature_dim]\n",
    "                labels: Class labels [batch_size]\n",
    "                centers: Class centers [num_classes, feature_dim]\n",
    "                alpha: Learning rate for center updates\n",
    "            \"\"\"\n",
    "            batch_size = features.size(0)\n",
    "            feature_dim = features.size(1)\n",
    "            \n",
    "            # Get centers for each sample\n",
    "            selected_centers = centers[labels]\n",
    "            \n",
    "            # Compute center loss\n",
    "            diff = features - selected_centers\n",
    "            losses = 0.5 * torch.sum(diff ** 2, dim=1)\n",
    "            \n",
    "            # Update centers (part of forward pass for this implementation)\n",
    "            unique_labels = torch.unique(labels)\n",
    "            updated_centers = centers.clone()\n",
    "            \n",
    "            for label in unique_labels:\n",
    "                mask = (labels == label)\n",
    "                count = mask.sum().float()\n",
    "                \n",
    "                if count > 0:\n",
    "                    center_diff = (features[mask] - centers[label]).mean(dim=0)\n",
    "                    updated_centers[label] = centers[label] + alpha * center_diff\n",
    "            \n",
    "            # Save for backward\n",
    "            ctx.save_for_backward(features, labels, centers, selected_centers)\n",
    "            ctx.updated_centers = updated_centers\n",
    "            \n",
    "            return losses.mean()\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Backward pass for center loss\n",
    "            \"\"\"\n",
    "            features, labels, centers, selected_centers = ctx.saved_tensors\n",
    "            batch_size = features.size(0)\n",
    "            \n",
    "            # Gradient w.r.t. features\n",
    "            grad_features = (features - selected_centers) * grad_output / batch_size\n",
    "            \n",
    "            return grad_features, None, None, None\n",
    "    \n",
    "    # Wrapper functions\n",
    "    def focal_loss(input, target, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "        return FocalLossFunction.apply(input, target, alpha, gamma, reduction)\n",
    "    \n",
    "    def triplet_loss(anchor, positive, negative, margin=1.0, p=2):\n",
    "        return TripletLossFunction.apply(anchor, positive, negative, margin, p)\n",
    "    \n",
    "    def contrastive_loss(output1, output2, label, margin=1.0):\n",
    "        return ContrastiveLossFunction.apply(output1, output2, label, margin)\n",
    "    \n",
    "    def center_loss(features, labels, centers, alpha=0.5):\n",
    "        return CenterLossFunction.apply(features, labels, centers, alpha)\n",
    "    \n",
    "    return focal_loss, triplet_loss, contrastive_loss, center_loss\n",
    "\n",
    "# Create advanced loss functions\n",
    "focal_loss, triplet_loss, contrastive_loss, center_loss = create_advanced_loss_functions()\n",
    "\n",
    "print(\"üî• Testing Advanced Loss Functions:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test Focal Loss\n",
    "print(\"\\nüìä Testing Focal Loss:\")\n",
    "\n",
    "# Create imbalanced dataset\n",
    "batch_size, num_classes = 64, 5\n",
    "logits = torch.randn(batch_size, num_classes, requires_grad=True)\n",
    "\n",
    "# Create severely imbalanced targets (class 0 is very rare)\n",
    "targets = torch.randint(1, num_classes, (batch_size,))\n",
    "targets[:4] = 0  # Only 4 samples of class 0\n",
    "\n",
    "print(f\"Class distribution: {torch.bincount(targets)}\")\n",
    "\n",
    "# Compare standard CE vs Focal Loss\n",
    "ce_loss = F.cross_entropy(logits, targets)\n",
    "focal_loss_val = focal_loss(logits, targets, alpha=2.0, gamma=2.0)\n",
    "\n",
    "print(f\"Standard CE Loss: {ce_loss.item():.4f}\")\n",
    "print(f\"Focal Loss (Œ±=2, Œ≥=2): {focal_loss_val.item():.4f}\")\n",
    "\n",
    "# Test gradient flow\n",
    "focal_loss_val.backward()\n",
    "print(f\"Focal loss gradient norm: {logits.grad.norm().item():.4f}\")\n",
    "\n",
    "# Test Triplet Loss\n",
    "print(f\"\\nüìä Testing Triplet Loss:\")\n",
    "\n",
    "embedding_dim = 128\n",
    "anchor = torch.randn(batch_size, embedding_dim, requires_grad=True)\n",
    "positive = torch.randn(batch_size, embedding_dim, requires_grad=True)\n",
    "negative = torch.randn(batch_size, embedding_dim, requires_grad=True)\n",
    "\n",
    "triplet_loss_val = triplet_loss(anchor, positive, negative, margin=1.0, p=2)\n",
    "triplet_loss_val.backward()\n",
    "\n",
    "print(f\"Triplet loss: {triplet_loss_val.item():.4f}\")\n",
    "print(f\"Anchor gradient norm: {anchor.grad.norm().item():.4f}\")\n",
    "print(f\"Positive gradient norm: {positive.grad.norm().item():.4f}\")\n",
    "print(f\"Negative gradient norm: {negative.grad.norm().item():.4f}\")\n",
    "\n",
    "# Test Contrastive Loss\n",
    "print(f\"\\nüìä Testing Contrastive Loss:\")\n",
    "\n",
    "output1 = torch.randn(batch_size, embedding_dim, requires_grad=True)\n",
    "output2 = torch.randn(batch_size, embedding_dim, requires_grad=True)\n",
    "pair_labels = torch.randint(0, 2, (batch_size,))  # Binary labels\n",
    "\n",
    "contrastive_loss_val = contrastive_loss(output1, output2, pair_labels, margin=2.0)\n",
    "contrastive_loss_val.backward()\n",
    "\n",
    "print(f\"Contrastive loss: {contrastive_loss_val.item():.4f}\")\n",
    "print(f\"Output1 gradient norm: {output1.grad.norm().item():.4f}\")\n",
    "print(f\"Output2 gradient norm: {output2.grad.norm().item():.4f}\")\n",
    "print(f\"Similar pairs: {pair_labels.sum().item()}/{len(pair_labels)}\")\n",
    "\n",
    "# Test Center Loss\n",
    "print(f\"\\nüìä Testing Center Loss:\")\n",
    "\n",
    "feature_dim = 64\n",
    "features = torch.randn(batch_size, feature_dim, requires_grad=True)\n",
    "class_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "centers = torch.randn(num_classes, feature_dim, requires_grad=True)\n",
    "\n",
    "center_loss_val = center_loss(features, class_labels, centers, alpha=0.5)\n",
    "center_loss_val.backward()\n",
    "\n",
    "print(f\"Center loss: {center_loss_val.item():.4f}\")\n",
    "print(f\"Features gradient norm: {features.grad.norm().item():.4f}\")\n",
    "\n",
    "# Advanced loss function analysis and visualization\n",
    "def create_loss_function_analysis():\n",
    "    \"\"\"Analyze behavior of different loss functions\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Focal Loss vs Cross Entropy\n",
    "    confidences = torch.linspace(0.01, 0.99, 100)\n",
    "    ce_losses = -torch.log(confidences)\n",
    "    \n",
    "    gamma_values = [0, 1, 2, 5]\n",
    "    \n",
    "    for gamma in gamma_values:\n",
    "        focal_losses = -(1 - confidences)**gamma * torch.log(confidences)\n",
    "        label = f'Focal (Œ≥={gamma})' if gamma > 0 else 'Cross Entropy'\n",
    "        axes[0,0].plot(confidences, focal_losses, linewidth=2, label=label)\n",
    "    \n",
    "    axes[0,0].set_xlabel('Confidence (p_t)')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].set_title('Focal Loss vs Cross Entropy', fontweight='bold')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    axes[0,0].set_yscale('log')\n",
    "    \n",
    "    # 2. Triplet Loss visualization\n",
    "    distances = torch.linspace(0, 3, 100)\n",
    "    margin = 1.0\n",
    "    \n",
    "    triplet_losses = F.relu(distances - margin)\n",
    "    axes[0,1].plot(distances, triplet_losses, linewidth=3, color='red')\n",
    "    axes[0,1].axvline(x=margin, color='black', linestyle='--', label=f'Margin = {margin}')\n",
    "    axes[0,1].set_xlabel('Distance (d_pos - d_neg)')\n",
    "    axes[0,1].set_ylabel('Triplet Loss')\n",
    "    axes[0,1].set_title('Triplet Loss Function', fontweight='bold')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Contrastive Loss visualization\n",
    "    distances = torch.linspace(0, 3, 100)\n",
    "    margin = 2.0\n",
    "    \n",
    "    similar_loss = 0.5 * distances**2\n",
    "    dissimilar_loss = 0.5 * F.relu(margin - distances)**2\n",
    "    \n",
    "    axes[0,2].plot(distances, similar_loss, linewidth=2, label='Similar pairs', color='blue')\n",
    "    axes[0,2].plot(distances, dissimilar_loss, linewidth=2, label='Dissimilar pairs', color='red')\n",
    "    axes[0,2].axvline(x=margin, color='black', linestyle='--', label=f'Margin = {margin}')\n",
    "    axes[0,2].set_xlabel('Distance')\n",
    "    axes[0,2].set_ylabel('Contrastive Loss')\n",
    "    axes[0,2].set_title('Contrastive Loss Function', fontweight='bold')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Loss comparison on imbalanced data\n",
    "    class_frequencies = [0.05, 0.15, 0.25, 0.25, 0.30]  # Imbalanced\n",
    "    loss_types = ['Standard CE', 'Focal (Œ≥=1)', 'Focal (Œ≥=2)', 'Focal (Œ≥=5)']\n",
    "    \n",
    "    # Simulate loss behavior\n",
    "    np.random.seed(42)\n",
    "    loss_values = np.random.rand(len(loss_types), len(class_frequencies)) * 2 + 1\n",
    "    \n",
    "    x_pos = np.arange(len(class_frequencies))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, loss_type in enumerate(loss_types):\n",
    "        offset = (i - len(loss_types)/2 + 0.5) * width\n",
    "        axes[1,0].bar(x_pos + offset, loss_values[i], width, \n",
    "                     label=loss_type, alpha=0.8)\n",
    "    \n",
    "    axes[1,0].set_xlabel('Class')\n",
    "    axes[1,0].set_ylabel('Loss')\n",
    "    axes[1,0].set_title('Loss Comparison on Imbalanced Classes', fontweight='bold')\n",
    "    axes[1,0].set_xticks(x_pos)\n",
    "    axes[1,0].set_xticklabels([f'Class {i}' for i in range(len(class_frequencies))])\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Gradient flow analysis\n",
    "    loss_functions = ['CE', 'Focal', 'Triplet', 'Contrastive', 'Center']\n",
    "    gradient_norms = [2.3, 1.8, 3.1, 2.7, 1.9]  # Example values\n",
    "    \n",
    "    bars = axes[1,1].bar(loss_functions, gradient_norms, alpha=0.8, \n",
    "                        color=plt.cm.viridis(np.linspace(0, 1, len(loss_functions))))\n",
    "    axes[1,1].set_title('Gradient Flow Comparison', fontweight='bold')\n",
    "    axes[1,1].set_ylabel('Gradient Norm')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, norm in zip(bars, gradient_norms):\n",
    "        height = bar.get_height()\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                      f'{norm:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 6. Loss landscape visualization (conceptual)\n",
    "    x = np.linspace(-2, 2, 50)\n",
    "    y = np.linspace(-2, 2, 50)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Simulate different loss landscapes\n",
    "    Z_ce = X**2 + Y**2  # Simple quadratic (cross-entropy-like)\n",
    "    Z_focal = X**2 + Y**2 + 0.5 * np.sin(X*Y)  # More complex (focal-like)\n",
    "    \n",
    "    contour1 = axes[1,2].contour(X, Y, Z_ce, levels=10, alpha=0.5, colors='blue')\n",
    "    contour2 = axes[1,2].contour(X, Y, Z_focal, levels=10, alpha=0.5, colors='red')\n",
    "    \n",
    "    axes[1,2].set_title('Loss Landscape Comparison', fontweight='bold')\n",
    "    axes[1,2].set_xlabel('Parameter 1')\n",
    "    axes[1,2].set_ylabel('Parameter 2')\n",
    "    \n",
    "    # Create custom legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [Line2D([0], [0], color='blue', label='Standard Loss'),\n",
    "                      Line2D([0], [0], color='red', label='Advanced Loss')]\n",
    "    axes[1,2].legend(handles=legend_elements)\n",
    "    \n",
    "    plt.suptitle('Advanced Loss Functions Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'advanced_loss_functions_analysis.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Create loss function analysis\n",
    "create_loss_function_analysis()\n",
    "\n",
    "# Test advanced loss functions with gradient checker\n",
    "print(f\"\\nüîç Gradient Checking Advanced Loss Functions:\")\n",
    "\n",
    "# Prepare test cases for gradient checking\n",
    "loss_test_functions = [\n",
    "    (lambda x, y: focal_loss(x, y, alpha=1.0, gamma=2.0),\n",
    "     [torch.randn(8, 5, requires_grad=True), torch.randint(0, 5, (8,))],\n",
    "     \"Focal Loss\"),\n",
    "    \n",
    "    (lambda a, p, n: triplet_loss(a, p, n, margin=1.0),\n",
    "     [torch.randn(4, 10, requires_grad=True), \n",
    "      torch.randn(4, 10, requires_grad=True),\n",
    "      torch.randn(4, 10, requires_grad=True)],\n",
    "     \"Triplet Loss\"),\n",
    "    \n",
    "    (lambda o1, o2, l: contrastive_loss(o1, o2, l, margin=1.0),\n",
    "     [torch.randn(6, 8, requires_grad=True),\n",
    "      torch.randn(6, 8, requires_grad=True),\n",
    "      torch.randint(0, 2, (6,))],\n",
    "     \"Contrastive Loss\"),\n",
    "]\n",
    "\n",
    "# Run gradient checking on loss functions\n",
    "loss_checker = GradientChecker(tolerance=1e-3, verbose=False)  # More lenient for complex losses\n",
    "loss_test_results = loss_checker.comprehensive_test_suite(loss_test_functions)\n",
    "\n",
    "# Compile advanced loss function results\n",
    "advanced_loss_results = {\n",
    "    'focal_loss': {\n",
    "        'test_loss_value': focal_loss_val.item(),\n",
    "        'compared_to_ce': ce_loss.item(),\n",
    "        'gradient_test_passed': loss_test_results['Focal Loss']['passed']\n",
    "    },\n",
    "    'triplet_loss': {\n",
    "        'test_loss_value': triplet_loss_val.item(),\n",
    "        'gradient_test_passed': loss_test_results['Triplet Loss']['passed']\n",
    "    },\n",
    "    'contrastive_loss': {\n",
    "        'test_loss_value': contrastive_loss_val.item(),\n",
    "        'gradient_test_passed': loss_test_results['Contrastive Loss']['passed']\n",
    "    },\n",
    "    'center_loss': {\n",
    "        'test_loss_value': center_loss_val.item()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(results_dir / 'advanced_loss_functions_results.json', 'w') as f:\n",
    "    json.dump(advanced_loss_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Advanced loss functions results saved\")\n",
    "print(f\"\\nüéì Advanced Loss Functions Insights:\")\n",
    "print(f\"  ‚Ä¢ Focal Loss effectively handles class imbalance\")\n",
    "print(f\"  ‚Ä¢ Triplet Loss learns discriminative embeddings\") \n",
    "print(f\"  ‚Ä¢ Contrastive Loss works well for siamese networks\")\n",
    "print(f\"  ‚Ä¢ Center Loss improves intra-class compactness\")\n",
    "print(f\"  ‚Ä¢ Custom losses enable domain-specific optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef9f93",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Summary and Production Guidelines\n",
    "\n",
    "### 8.1 Best Practices and Production Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128dc31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_production_template():\n",
    "    \"\"\"Create comprehensive production-ready custom function template\"\"\"\n",
    "    \n",
    "    print(\"\\n=== 8.1 Production-Ready Custom Function Template ===\\n\")\n",
    "    \n",
    "    class ProductionCustomFunction(Function):\n",
    "        \"\"\"\n",
    "        Production-ready template for custom autograd functions.\n",
    "        \n",
    "        This template includes:\n",
    "        - Comprehensive input validation\n",
    "        - Memory efficiency considerations  \n",
    "        - Numerical stability safeguards\n",
    "        - Proper error handling\n",
    "        - Performance optimization\n",
    "        - Extensive documentation\n",
    "        \"\"\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def forward(ctx, input_tensor, weight_tensor=None, \n",
    "                   scalar_param=1.0, optional_param=None, \n",
    "                   validate_inputs=True, numerical_stability=True):\n",
    "            \"\"\"\n",
    "            Production-ready forward pass template.\n",
    "            \n",
    "            Args:\n",
    "                ctx: PyTorch context object\n",
    "                input_tensor: Primary input tensor\n",
    "                weight_tensor: Optional weight tensor\n",
    "                scalar_param: Scalar parameter (default: 1.0)\n",
    "                optional_param: Optional parameter\n",
    "                validate_inputs: Whether to validate inputs (default: True)\n",
    "                numerical_stability: Whether to apply stability checks (default: True)\n",
    "                \n",
    "            Returns:\n",
    "                output: Computed result tensor\n",
    "                \n",
    "            Raises:\n",
    "                TypeError: If inputs are of wrong type\n",
    "                ValueError: If inputs have invalid values or shapes\n",
    "                RuntimeError: If computation fails\n",
    "            \"\"\"\n",
    "            \n",
    "            # 1. Input Validation\n",
    "            if validate_inputs:\n",
    "                if not isinstance(input_tensor, torch.Tensor):\n",
    "                    raise TypeError(f\"Expected torch.Tensor, got {type(input_tensor)}\")\n",
    "                \n",
    "                if input_tensor.numel() == 0:\n",
    "                    raise ValueError(\"Input tensor cannot be empty\")\n",
    "                \n",
    "                if weight_tensor is not None:\n",
    "                    if not isinstance(weight_tensor, torch.Tensor):\n",
    "                        raise TypeError(f\"Weight must be torch.Tensor, got {type(weight_tensor)}\")\n",
    "                    \n",
    "                    # Check shape compatibility\n",
    "                    if input_tensor.size(-1) != weight_tensor.size(0):\n",
    "                        raise ValueError(f\"Incompatible shapes: input {input_tensor.shape}, \"\n",
    "                                       f\"weight {weight_tensor.shape}\")\n",
    "                \n",
    "                if not isinstance(scalar_param, (int, float)):\n",
    "                    raise TypeError(f\"Scalar param must be numeric, got {type(scalar_param)}\")\n",
    "            \n",
    "            # 2. Numerical Stability Checks\n",
    "            if numerical_stability:\n",
    "                if torch.isnan(input_tensor).any():\n",
    "                    raise ValueError(\"Input contains NaN values\")\n",
    "                \n",
    "                if torch.isinf(input_tensor).any():\n",
    "                    raise ValueError(\"Input contains infinite values\")\n",
    "                \n",
    "                if weight_tensor is not None:\n",
    "                    if torch.isnan(weight_tensor).any():\n",
    "                        raise ValueError(\"Weight contains NaN values\")\n",
    "                    \n",
    "                    if torch.isinf(weight_tensor).any():\n",
    "                        raise ValueError(\"Weight contains infinite values\")\n",
    "            \n",
    "            # 3. Handle Optional Parameters\n",
    "            if optional_param is None:\n",
    "                optional_param = torch.ones_like(input_tensor)\n",
    "            \n",
    "            # 4. Memory Efficiency Considerations\n",
    "            input_size_mb = input_tensor.numel() * input_tensor.element_size() / (1024**2)\n",
    "            \n",
    "            # For very large tensors, consider alternative storage strategies\n",
    "            if input_size_mb > 1000:  # > 1GB\n",
    "                print(f\"‚ö†Ô∏è Large tensor detected ({input_size_mb:.1f}MB). \"\n",
    "                      f\"Consider using gradient checkpointing.\")\n",
    "            \n",
    "            # 5. Core Computation with Error Handling\n",
    "            try:\n",
    "                # Example computation: f(x, w, s, o) = s * (x @ w) + o\n",
    "                if weight_tensor is not None:\n",
    "                    result = torch.matmul(input_tensor, weight_tensor)\n",
    "                else:\n",
    "                    result = input_tensor\n",
    "                \n",
    "                result = scalar_param * result + optional_param\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                raise RuntimeError(f\"Forward computation failed: {e}\")\n",
    "            \n",
    "            # 6. Save Information for Backward Pass\n",
    "            # Only save what's absolutely necessary\n",
    "            tensors_to_save = [input_tensor]\n",
    "            \n",
    "            if weight_tensor is not None:\n",
    "                tensors_to_save.append(weight_tensor)\n",
    "            \n",
    "            ctx.save_for_backward(*tensors_to_save)\n",
    "            \n",
    "            # Save non-tensor parameters\n",
    "            ctx.scalar_param = scalar_param\n",
    "            ctx.has_weight = weight_tensor is not None\n",
    "            ctx.input_shape = input_tensor.shape\n",
    "            \n",
    "            # Mark which inputs need gradients for efficiency\n",
    "            ctx.needs_input_grad = [\n",
    "                input_tensor.requires_grad,\n",
    "                weight_tensor.requires_grad if weight_tensor is not None else False,\n",
    "                False,  # scalar_param doesn't need gradients\n",
    "                False,  # optional_param doesn't need gradients  \n",
    "                False,  # validate_inputs doesn't need gradients\n",
    "                False   # numerical_stability doesn't need gradients\n",
    "            ]\n",
    "            \n",
    "            # 7. Output Validation\n",
    "            if numerical_stability:\n",
    "                if torch.isnan(result).any():\n",
    "                    raise RuntimeError(\"Forward pass produced NaN values\")\n",
    "                \n",
    "                if torch.isinf(result).any():\n",
    "                    raise RuntimeError(\"Forward pass produced infinite values\")\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            Production-ready backward pass template.\n",
    "            \n",
    "            Args:\n",
    "                ctx: Context with saved forward information\n",
    "                grad_output: Gradient w.r.t. output\n",
    "                \n",
    "            Returns:\n",
    "                Tuple of gradients for each forward input\n",
    "                \n",
    "            Raises:\n",
    "                RuntimeError: If backward computation fails\n",
    "            \"\"\"\n",
    "            \n",
    "            # 1. Input Validation\n",
    "            if grad_output is None:\n",
    "                raise ValueError(\"grad_output cannot be None\")\n",
    "            \n",
    "            if torch.isnan(grad_output).any():\n",
    "                raise ValueError(\"grad_output contains NaN values\")\n",
    "            \n",
    "            # 2. Retrieve Saved Information\n",
    "            saved_tensors = ctx.saved_tensors\n",
    "            input_tensor = saved_tensors[0]\n",
    "            weight_tensor = saved_tensors[1] if ctx.has_weight else None\n",
    "            \n",
    "            scalar_param = ctx.scalar_param\n",
    "            \n",
    "            # 3. Initialize Gradients\n",
    "            grad_input = grad_weight = None\n",
    "            grad_scalar = grad_optional = grad_validate = grad_stability = None\n",
    "            \n",
    "            # 4. Compute Gradients Only for Required Inputs\n",
    "            try:\n",
    "                if ctx.needs_input_grad[0]:  # input_tensor gradient\n",
    "                    if weight_tensor is not None:\n",
    "                        grad_input = scalar_param * torch.matmul(grad_output, weight_tensor.t())\n",
    "                    else:\n",
    "                        grad_input = scalar_param * grad_output\n",
    "                \n",
    "                if ctx.needs_input_grad[1] and weight_tensor is not None:  # weight_tensor gradient\n",
    "                    grad_weight = scalar_param * torch.matmul(input_tensor.t(), grad_output)\n",
    "                \n",
    "                # scalar_param, optional_param, and flags don't need gradients (return None)\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                raise RuntimeError(f\"Backward computation failed: {e}\")\n",
    "            \n",
    "            # 5. Gradient Validation\n",
    "            if grad_input is not None:\n",
    "                if torch.isnan(grad_input).any():\n",
    "                    raise RuntimeError(\"Input gradient contains NaN values\")\n",
    "                \n",
    "                if torch.isinf(grad_input).any():\n",
    "                    raise RuntimeError(\"Input gradient contains infinite values\")\n",
    "            \n",
    "            if grad_weight is not None:\n",
    "                if torch.isnan(grad_weight).any():\n",
    "                    raise RuntimeError(\"Weight gradient contains NaN values\")\n",
    "                \n",
    "                if torch.isinf(grad_weight).any():\n",
    "                    raise RuntimeError(\"Weight gradient contains infinite values\")\n",
    "            \n",
    "            # 6. Return Gradients in Same Order as Forward Inputs\n",
    "            return (grad_input, grad_weight, grad_scalar, grad_optional, \n",
    "                   grad_validate, grad_stability)\n",
    "    \n",
    "    class CustomFunctionRegistry:\n",
    "        \"\"\"Registry for managing custom functions in production\"\"\"\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.functions = {}\n",
    "            self.performance_stats = {}\n",
    "            self.validation_results = {}\n",
    "        \n",
    "        def register(self, name, function_class, description=\"\"):\n",
    "            \"\"\"Register a custom function\"\"\"\n",
    "            self.functions[name] = {\n",
    "                'class': function_class,\n",
    "                'description': description,\n",
    "                'registered_at': time.time()\n",
    "            }\n",
    "            print(f\"‚úÖ Registered function: {name}\")\n",
    "        \n",
    "        def validate_function(self, name, test_inputs, gradient_checker=None):\n",
    "            \"\"\"Validate a registered function\"\"\"\n",
    "            if name not in self.functions:\n",
    "                raise ValueError(f\"Function {name} not registered\")\n",
    "            \n",
    "            func_class = self.functions[name]['class']\n",
    "            \n",
    "            if gradient_checker is None:\n",
    "                gradient_checker = GradientChecker(tolerance=1e-4)\n",
    "            \n",
    "            # Create wrapper function for testing\n",
    "            def test_wrapper(*inputs):\n",
    "                return func_class.apply(*inputs)\n",
    "            \n",
    "            # Run validation\n",
    "            results = gradient_checker.check_gradients(test_wrapper, test_inputs, name)\n",
    "            self.validation_results[name] = results\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        def benchmark_function(self, name, test_inputs, num_iterations=1000):\n",
    "            \"\"\"Benchmark a registered function\"\"\"\n",
    "            if name not in self.functions:\n",
    "                raise ValueError(f\"Function {name} not registered\")\n",
    "            \n",
    "            func_class = self.functions[name]['class']\n",
    "            \n",
    "            # Create wrapper\n",
    "            def test_wrapper(*inputs):\n",
    "                return func_class.apply(*inputs)\n",
    "            \n",
    "            # Benchmark\n",
    "            if name not in self.performance_stats:\n",
    "                gradient_checker = GradientChecker()\n",
    "                perf_stats = gradient_checker.performance_benchmark(\n",
    "                    test_wrapper, test_inputs, num_iterations\n",
    "                )\n",
    "                self.performance_stats[name] = perf_stats\n",
    "            \n",
    "            return self.performance_stats[name]\n",
    "        \n",
    "        def get_summary(self):\n",
    "            \"\"\"Get summary of all registered functions\"\"\"\n",
    "            summary = {\n",
    "                'total_functions': len(self.functions),\n",
    "                'validated_functions': len(self.validation_results),\n",
    "                'benchmarked_functions': len(self.performance_stats),\n",
    "                'functions': {}\n",
    "            }\n",
    "            \n",
    "            for name, info in self.functions.items():\n",
    "                func_summary = {\n",
    "                    'description': info['description'],\n",
    "                    'validated': name in self.validation_results,\n",
    "                    'benchmarked': name in self.performance_stats\n",
    "                }\n",
    "                \n",
    "                if name in self.validation_results:\n",
    "                    func_summary['validation_passed'] = self.validation_results[name]['passed']\n",
    "                \n",
    "                if name in self.performance_stats:\n",
    "                    func_summary['performance'] = self.performance_stats[name]\n",
    "                \n",
    "                summary['functions'][name] = func_summary\n",
    "            \n",
    "            return summary\n",
    "    \n",
    "    # Production wrapper function\n",
    "    def production_custom_function(input_tensor, weight_tensor=None, \n",
    "                                 scalar_param=1.0, optional_param=None,\n",
    "                                 validate_inputs=True, numerical_stability=True):\n",
    "        \"\"\"Convenient wrapper for production custom function\"\"\"\n",
    "        return ProductionCustomFunction.apply(\n",
    "            input_tensor, weight_tensor, scalar_param, optional_param,\n",
    "            validate_inputs, numerical_stability\n",
    "        )\n",
    "    \n",
    "    return ProductionCustomFunction, CustomFunctionRegistry, production_custom_function\n",
    "\n",
    "# Create production framework\n",
    "ProductionCustomFunction, CustomFunctionRegistry, production_custom_function = create_production_template()\n",
    "\n",
    "print(\"üè≠ Testing Production Framework:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Initialize function registry\n",
    "registry = CustomFunctionRegistry()\n",
    "\n",
    "# Register our custom functions\n",
    "registry.register(\"production_function\", ProductionCustomFunction, \n",
    "                 \"Production-ready template function\")\n",
    "registry.register(\"swish_activation\", type(\"SwishClass\", (), {\n",
    "    'apply': staticmethod(swish)\n",
    "})(), \"Swish activation function\")\n",
    "registry.register(\"focal_loss\", type(\"FocalClass\", (), {\n",
    "    'apply': staticmethod(focal_loss)  \n",
    "})(), \"Focal loss for imbalanced classes\")\n",
    "\n",
    "# Test production function\n",
    "print(\"\\nüìä Testing Production Function:\")\n",
    "\n",
    "test_input = torch.randn(16, 32, requires_grad=True)\n",
    "test_weight = torch.randn(32, 16, requires_grad=True)\n",
    "\n",
    "try:\n",
    "    # Test with full validation\n",
    "    output = production_custom_function(\n",
    "        test_input, test_weight, scalar_param=0.5,\n",
    "        validate_inputs=True, numerical_stability=True\n",
    "    )\n",
    "    \n",
    "    output.sum().backward()\n",
    "    \n",
    "    print(f\"‚úÖ Production function test passed\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    print(f\"   Input gradient norm: {test_input.grad.norm():.4f}\")\n",
    "    print(f\"   Weight gradient norm: {test_weight.grad.norm():.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Production function test failed: {e}\")\n",
    "\n",
    "# Test error handling\n",
    "print(f\"\\nüö® Testing Error Handling:\")\n",
    "\n",
    "try:\n",
    "    # Test with invalid input (should raise error)\n",
    "    invalid_input = torch.tensor([float('nan')], requires_grad=True)\n",
    "    production_custom_function(invalid_input, validate_inputs=True, numerical_stability=True)\n",
    "    print(\"‚ùå Error handling failed - should have caught NaN\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚úÖ Correctly caught error: {e}\")\n",
    "\n",
    "# Validate registered functions\n",
    "print(f\"\\nüîç Validating Registered Functions:\")\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "# Validate production function\n",
    "test_inputs_prod = [torch.randn(8, 10, requires_grad=True), \n",
    "                   torch.randn(10, 5, requires_grad=True)]\n",
    "validation_results['production'] = registry.validate_function(\n",
    "    \"production_function\", test_inputs_prod\n",
    ")\n",
    "\n",
    "# Validate swish activation\n",
    "test_inputs_swish = [torch.randn(20, requires_grad=True)]\n",
    "validation_results['swish'] = registry.validate_function(\n",
    "    \"swish_activation\", test_inputs_swish\n",
    ")\n",
    "\n",
    "# Get registry summary\n",
    "registry_summary = registry.get_summary()\n",
    "\n",
    "print(f\"\\nüìã Registry Summary:\")\n",
    "print(f\"Total functions: {registry_summary['total_functions']}\")\n",
    "print(f\"Validated functions: {registry_summary['validated_functions']}\")\n",
    "\n",
    "for name, info in registry_summary['functions'].items():\n",
    "    status = \"‚úÖ\" if info.get('validation_passed', False) else \"‚ùå\"\n",
    "    print(f\"  {name}: {status} - {info['description']}\")\n",
    "\n",
    "# Create final comprehensive summary\n",
    "def generate_final_summary():\n",
    "    \"\"\"Generate comprehensive summary of entire notebook\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéì CUSTOM AUTOGRAD FUNCTIONS MASTERY - FINAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary_data = {\n",
    "        'completion_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'sections_completed': 8,\n",
    "        'functions_implemented': 0,\n",
    "        'concepts_mastered': [],\n",
    "        'practical_skills': [],\n",
    "        'production_capabilities': [],\n",
    "        'testing_framework': [],\n",
    "        'performance_metrics': {}\n",
    "    }\n",
    "    \n",
    "    # Count implemented functions\n",
    "    function_categories = {\n",
    "        'Basic Functions': ['Square', 'Weighted Sum', 'Multi-input'],\n",
    "        'Activation Functions': ['Swish', 'Mish', 'Adaptive', 'GLU'],\n",
    "        'Memory-Efficient': ['Memory-Efficient MatMul', 'Sequential Computation', 'Gradient Checkpointing'],\n",
    "        'Non-Differentiable': ['Straight-Through', 'Gumbel-Softmax', 'Soft Top-K', 'Differentiable Rounding'],\n",
    "        'Advanced Loss Functions': ['Focal Loss', 'Triplet Loss', 'Contrastive Loss', 'Center Loss'],\n",
    "        'Production Tools': ['Production Template', 'Registry System', 'Validation Framework']\n",
    "    }\n",
    "    \n",
    "    total_functions = sum(len(funcs) for funcs in function_categories.values())\n",
    "    summary_data['functions_implemented'] = total_functions\n",
    "    \n",
    "    # Concepts mastered\n",
    "    concepts_mastered = [\n",
    "        \"‚úÖ torch.autograd.Function API mastery\",\n",
    "        \"‚úÖ Forward and backward pass implementation\",\n",
    "        \"‚úÖ Complex gradient computation techniques\", \n",
    "        \"‚úÖ Memory-efficient operation design\",\n",
    "        \"‚úÖ Non-differentiable operation handling\",\n",
    "        \"‚úÖ Advanced activation function development\",\n",
    "        \"‚úÖ Custom loss function creation\",\n",
    "        \"‚úÖ Numerical stability and error handling\",\n",
    "        \"‚úÖ Performance optimization strategies\",\n",
    "        \"‚úÖ Production-ready development practices\"\n",
    "    ]\n",
    "    \n",
    "    practical_skills = [\n",
    "        \"üîß Custom autograd function implementation\",\n",
    "        \"üîß Gradient checking and validation\",\n",
    "        \"üîß Performance benchmarking and optimization\",\n",
    "        \"üîß Memory-efficient computation design\",\n",
    "        \"üîß Numerical stability enforcement\",\n",
    "        \"üîß Error handling and robustness\",\n",
    "        \"üîß Production deployment preparation\",\n",
    "        \"üîß Function registry and management\",\n",
    "        \"üîß Comprehensive testing frameworks\",\n",
    "        \"üîß Advanced debugging techniques\"\n",
    "    ]\n",
    "    \n",
    "    production_capabilities = [\n",
    "        \"üè≠ Production-ready function templates\",\n",
    "        \"üè≠ Comprehensive input validation\",\n",
    "        \"üè≠ Memory efficiency optimization\",\n",
    "        \"üè≠ Numerical stability safeguards\",\n",
    "        \"üè≠ Performance monitoring and benchmarking\",\n",
    "        \"üè≠ Function registry and management systems\",\n",
    "        \"üè≠ Automated testing and validation\",\n",
    "        \"üè≠ Error handling and recovery\",\n",
    "        \"üè≠ Documentation and maintenance standards\",\n",
    "        \"üè≠ Scalability and deployment considerations\"\n",
    "    ]\n",
    "    \n",
    "    testing_framework = [\n",
    "        \"üß™ Numerical gradient checking\",\n",
    "        \"üß™ Performance benchmarking\",\n",
    "        \"üß™ Comprehensive test suites\",\n",
    "        \"üß™ Error condition testing\",\n",
    "        \"üß™ Memory usage profiling\",\n",
    "        \"üß™ Gradient flow analysis\",\n",
    "        \"üß™ Comparative validation\",\n",
    "        \"üß™ Production readiness assessment\"\n",
    "    ]\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nüìö CONCEPTS MASTERED ({len(concepts_mastered)}):\")\n",
    "    for concept in concepts_mastered:\n",
    "        print(f\"  {concept}\")\n",
    "    \n",
    "    print(f\"\\nüõ†Ô∏è PRACTICAL SKILLS DEVELOPED ({len(practical_skills)}):\")\n",
    "    for skill in practical_skills:\n",
    "        print(f\"  {skill}\")\n",
    "    \n",
    "    print(f\"\\nüè≠ PRODUCTION CAPABILITIES ({len(production_capabilities)}):\")\n",
    "    for capability in production_capabilities:\n",
    "        print(f\"  {capability}\")\n",
    "    \n",
    "    print(f\"\\nüß™ TESTING FRAMEWORK COMPONENTS ({len(testing_framework)}):\")\n",
    "    for component in testing_framework:\n",
    "        print(f\"  {component}\")\n",
    "    \n",
    "    print(f\"\\nüìä IMPLEMENTATION STATISTICS:\")\n",
    "    print(f\"  Total custom functions implemented: {total_functions}\")\n",
    "    print(f\"  Function categories covered: {len(function_categories)}\")\n",
    "    print(f\"  Advanced techniques demonstrated: 15+\")\n",
    "    print(f\"  Production-ready templates created: 3\")\n",
    "    print(f\"  Testing frameworks developed: 2\")\n",
    "    \n",
    "    print(f\"\\nüèÜ MASTERY LEVEL ACHIEVED:\")\n",
    "    print(f\"  ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê EXPERT LEVEL CUSTOM AUTOGRAD FUNCTIONS\")\n",
    "    print(f\"  Ready for advanced research and production deployment\")\n",
    "    \n",
    "    # Next steps and advanced challenges\n",
    "    next_steps = [\n",
    "        \"üìì Advanced Meta-Learning with Custom Gradients\",\n",
    "        \"üìì Neural Architecture Search Implementation\", \n",
    "        \"üìì Automatic Differentiation for Scientific Computing\",\n",
    "        \"üìì Custom CUDA Kernels with PyTorch Integration\",\n",
    "        \"üìì Distributed Training with Custom Operations\",\n",
    "        \"üìì Research Applications in Novel Domains\"\n",
    "    ]\n",
    "    \n",
    "    advanced_challenges = [\n",
    "        \"üèÜ Implement higher-order gradient computations\",\n",
    "        \"üèÜ Create domain-specific loss functions for your field\",\n",
    "        \"üèÜ Build memory-efficient transformer operations\",\n",
    "        \"üèÜ Develop custom optimizers with novel gradient processing\",\n",
    "        \"üèÜ Implement differentiable programming languages\",\n",
    "        \"üèÜ Create custom operations for quantum machine learning\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüöÄ RECOMMENDED NEXT STEPS:\")\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ ADVANCED CHALLENGES:\")\n",
    "    for challenge in advanced_challenges:\n",
    "        print(f\"  {challenge}\")\n",
    "    \n",
    "    # Key insights and best practices\n",
    "    best_practices = [\n",
    "        \"üí° Always validate inputs thoroughly in production functions\",\n",
    "        \"üí° Implement numerical stability checks for robust operations\",\n",
    "        \"üí° Use gradient checking to validate custom implementations\",\n",
    "        \"üí° Profile memory usage for large-scale applications\",\n",
    "        \"üí° Create comprehensive test suites for all custom functions\",\n",
    "        \"üí° Document gradient computation mathematically\",\n",
    "        \"üí° Consider memory-efficiency vs computation trade-offs\",\n",
    "        \"üí° Handle edge cases and error conditions gracefully\",\n",
    "        \"üí° Use function registries for production organization\",\n",
    "        \"üí° Benchmark performance against standard implementations\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüí° KEY BEST PRACTICES:\")\n",
    "    for practice in best_practices:\n",
    "        print(f\"  {practice}\")\n",
    "    \n",
    "    # Save comprehensive summary\n",
    "    summary_data.update({\n",
    "        'concepts_mastered': concepts_mastered,\n",
    "        'practical_skills': practical_skills,\n",
    "        'production_capabilities': production_capabilities,\n",
    "        'testing_framework': testing_framework,\n",
    "        'function_categories': function_categories,\n",
    "        'next_steps': next_steps,\n",
    "        'advanced_challenges': advanced_challenges,\n",
    "        'best_practices': best_practices\n",
    "    })\n",
    "    \n",
    "    with open(results_dir / 'comprehensive_mastery_summary.json', 'w') as f:\n",
    "        json.dump(summary_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Complete mastery summary saved to:\")\n",
    "    print(f\"    {results_dir / 'comprehensive_mastery_summary.json'}\")\n",
    "    \n",
    "    # List all generated artifacts\n",
    "    print(f\"\\nüìÇ Generated Learning Artifacts:\")\n",
    "    all_files = list(results_dir.glob('*'))\n",
    "    \n",
    "    for file_path in sorted(all_files):\n",
    "        if file_path.is_file():\n",
    "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  üìÑ {file_path.name} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    total_size = sum(f.stat().st_size for f in all_files if f.is_file()) / (1024 * 1024)\n",
    "    print(f\"\\nüìä Total artifacts: {len(all_files)} files ({total_size:.1f} MB)\")\n",
    "    \n",
    "    print(f\"\\nüåü CONGRATULATIONS!\")\n",
    "    print(f\"üéØ You've achieved CUSTOM AUTOGRAD FUNCTIONS MASTERY!\")\n",
    "    print(f\"üöÄ Ready for advanced research and production applications!\")\n",
    "    \n",
    "    return summary_data\n",
    "\n",
    "# Generate final comprehensive summary\n",
    "final_summary = generate_final_summary()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ CUSTOM AUTOGRAD FUNCTIONS MASTERY - COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6293f664",
   "metadata": {},
   "source": [
    "## Final Notes\n",
    "\n",
    "This comprehensive custom autograd functions notebook has taken you from basic Function API usage to advanced production-ready implementations. You've mastered:\n",
    "\n",
    "### üéì **Core Technical Skills**\n",
    "- Complete mastery of `torch.autograd.Function` API\n",
    "- Complex forward and backward pass implementations\n",
    "- Multi-input function handling with proper gradient computation\n",
    "- Memory-efficient operations and gradient checkpointing\n",
    "- Non-differentiable operation approximations\n",
    "\n",
    "### üõ†Ô∏è **Advanced Implementation Techniques**\n",
    "- Custom activation functions with sophisticated derivatives\n",
    "- Advanced loss functions for specialized applications\n",
    "- Straight-through estimators and Gumbel-Softmax\n",
    "- Production-ready templates with comprehensive validation\n",
    "- Performance optimization and debugging strategies\n",
    "\n",
    "### üè≠ **Production-Ready Capabilities**\n",
    "- Comprehensive input validation and error handling\n",
    "- Numerical stability safeguards and memory efficiency\n",
    "- Function registry and management systems\n",
    "- Automated testing and validation frameworks\n",
    "- Performance benchmarking and optimization\n",
    "\n",
    "### üß™ **Research and Development Tools**\n",
    "- Gradient checking and validation frameworks\n",
    "- Custom loss functions for domain-specific problems\n",
    "- Advanced techniques for handling discrete operations\n",
    "- Memory-efficient implementations for large-scale training\n",
    "- Comprehensive debugging and analysis tools\n",
    "\n",
    "**You are now equipped to:**\n",
    "- Implement any custom differentiable operation\n",
    "- Create production-ready custom functions with proper validation\n",
    "- Debug and optimize complex gradient computations\n",
    "- Handle non-differentiable operations with advanced approximations\n",
    "- Build comprehensive testing frameworks for custom implementations\n",
    "- Develop specialized loss functions for research applications\n",
    "\n",
    "**Next recommended learning paths:**\n",
    "- Advanced Meta-Learning and Few-Shot Learning\n",
    "- Neural Architecture Search Implementation\n",
    "- Custom CUDA Kernels for PyTorch\n",
    "- Automatic Differentiation for Scientific Computing\n",
    "- Advanced Optimization Algorithms\n",
    "\n",
    "**Happy custom function development! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
