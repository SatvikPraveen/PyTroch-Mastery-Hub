{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53e6bed",
   "metadata": {},
   "source": [
    "# Backpropagation Visualization Mastery: Interactive Deep Learning Analysis\n",
    "\n",
    "**PyTorch Mastery Hub - Advanced Visualization Module**\n",
    "\n",
    "**Authors:** PyTorch Mastery Hub Development Team  \n",
    "**Institution:** PyTorch Mastery Hub  \n",
    "**Module:** 02_autograd_backpropagation \n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides comprehensive visualization and analysis of backpropagation algorithms in deep neural networks. We focus on creating interactive visualizations that reveal the inner workings of gradient computation, computational graphs, and optimization dynamics to build deep intuition for neural network training.\n",
    "\n",
    "## Key Objectives\n",
    "1. Build and visualize computational graphs interactively\n",
    "2. Animate gradient flow through neural networks in real-time\n",
    "3. Explore 3D loss landscapes and optimization trajectories\n",
    "4. Analyze activation patterns and weight distributions\n",
    "5. Monitor training dynamics and gradient behavior\n",
    "6. Create diagnostic tools for debugging neural networks\n",
    "\n",
    "## 1. Setup and Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5193828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for advanced visualization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, Arrow\n",
    "from matplotlib.collections import LineCollection\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom utilities\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "try:\n",
    "    from src.utils.device_utils import get_device\n",
    "    from src.visualization.training_viz import TrainingVisualizer\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Custom utilities not found, using fallback implementations\")\n",
    "    \n",
    "    def get_device():\n",
    "        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Environment setup\n",
    "device = get_device()\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enhanced plotting configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path('../results/notebooks/backprop_visualization')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üé® PyTorch Mastery Hub - Backpropagation Visualization Mastery\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"üì± Device: {device}\")\n",
    "print(f\"üé≠ PyTorch version: {torch.__version__}\")\n",
    "print(f\"üìÅ Results directory: {results_dir}\")\n",
    "print(f\"‚ú® Ready to visualize the magic of backpropagation!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d88b4",
   "metadata": {},
   "source": [
    "## 2. Computational Graph Construction and Visualization\n",
    "\n",
    "### 2.1 Interactive Computational Graph Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb9760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationGraphVisualizer:\n",
    "    \"\"\"Advanced computational graph visualization with interactive features.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nodes = {}\n",
    "        self.edges = []\n",
    "        self.node_counter = 0\n",
    "        self.colors = {\n",
    "            'input': '#FF6B6B',      # Red for inputs\n",
    "            'operation': '#4ECDC4',   # Teal for operations  \n",
    "            'parameter': '#45B7D1',   # Blue for parameters\n",
    "            'output': '#96CEB4',      # Green for outputs\n",
    "            'loss': '#FFEAA7',       # Yellow for loss\n",
    "            'gradient': '#DDA0DD'     # Purple for gradients\n",
    "        }\n",
    "        self.analysis_results = {}\n",
    "    \n",
    "    def add_node(self, tensor, node_type='operation', label=None):\n",
    "        \"\"\"Add a node to the computational graph with metadata.\"\"\"\n",
    "        node_id = f\"node_{self.node_counter}\"\n",
    "        self.node_counter += 1\n",
    "        \n",
    "        if label is None:\n",
    "            if hasattr(tensor, 'grad_fn') and tensor.grad_fn is not None:\n",
    "                label = tensor.grad_fn.__class__.__name__\n",
    "            else:\n",
    "                label = 'Tensor'\n",
    "        \n",
    "        self.nodes[node_id] = {\n",
    "            'tensor': tensor,\n",
    "            'type': node_type,\n",
    "            'label': label,\n",
    "            'shape': tuple(tensor.shape),\n",
    "            'requires_grad': tensor.requires_grad,\n",
    "            'grad_fn': tensor.grad_fn,\n",
    "            'memory_usage': tensor.element_size() * tensor.nelement(),\n",
    "            'dtype': str(tensor.dtype)\n",
    "        }\n",
    "        \n",
    "        return node_id\n",
    "    \n",
    "    def add_edge(self, from_node, to_node, operation=None, gradient_flow=None):\n",
    "        \"\"\"Add an edge with optional gradient flow information.\"\"\"\n",
    "        self.edges.append({\n",
    "            'from': from_node,\n",
    "            'to': to_node,\n",
    "            'operation': operation,\n",
    "            'gradient_flow': gradient_flow\n",
    "        })\n",
    "    \n",
    "    def visualize_graph(self, figsize=(16, 12), layout='hierarchical', show_gradients=True):\n",
    "        \"\"\"Create comprehensive computational graph visualization.\"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        # 1. Main graph visualization\n",
    "        self._draw_main_graph(ax1, layout, show_gradients)\n",
    "        \n",
    "        # 2. Node statistics\n",
    "        self._draw_node_statistics(ax2)\n",
    "        \n",
    "        # 3. Memory usage analysis\n",
    "        self._draw_memory_analysis(ax3)\n",
    "        \n",
    "        # 4. Gradient flow analysis\n",
    "        self._draw_gradient_flow_summary(ax4)\n",
    "        \n",
    "        plt.suptitle('Computational Graph Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save visualization\n",
    "        plt.savefig(results_dir / 'computational_graph_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def _draw_main_graph(self, ax, layout, show_gradients):\n",
    "        \"\"\"Draw the main computational graph.\"\"\"\n",
    "        # Create NetworkX graph\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes with attributes\n",
    "        for node_id, node_data in self.nodes.items():\n",
    "            G.add_node(node_id, **node_data)\n",
    "        \n",
    "        # Add edges\n",
    "        for edge in self.edges:\n",
    "            G.add_edge(edge['from'], edge['to'], **edge)\n",
    "        \n",
    "        # Choose layout\n",
    "        if layout == 'hierarchical':\n",
    "            pos = self._hierarchical_layout(G)\n",
    "        else:\n",
    "            pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "        \n",
    "        # Draw edges with gradient flow indicators\n",
    "        for edge in G.edges(data=True):\n",
    "            x1, y1 = pos[edge[0]]\n",
    "            x2, y2 = pos[edge[1]]\n",
    "            \n",
    "            # Edge color based on gradient flow\n",
    "            edge_color = 'red' if show_gradients and edge[2].get('gradient_flow') else 'gray'\n",
    "            edge_width = 3 if edge[2].get('gradient_flow') else 1\n",
    "            \n",
    "            ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                       arrowprops=dict(arrowstyle='->', lw=edge_width, \n",
    "                                     color=edge_color, alpha=0.7))\n",
    "            \n",
    "            # Add operation label\n",
    "            if edge[2].get('operation'):\n",
    "                mid_x, mid_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "                ax.text(mid_x, mid_y, edge[2]['operation'], \n",
    "                       bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8),\n",
    "                       ha='center', va='center', fontsize=8)\n",
    "        \n",
    "        # Draw nodes\n",
    "        for node_id, (x, y) in pos.items():\n",
    "            node_data = self.nodes[node_id]\n",
    "            \n",
    "            # Node size based on tensor size\n",
    "            tensor_size = np.prod(node_data['shape'])\n",
    "            node_size = min(0.15, 0.05 + np.log10(max(1, tensor_size)) * 0.02)\n",
    "            \n",
    "            # Node color based on type\n",
    "            color = self.colors.get(node_data['type'], '#CCCCCC')\n",
    "            \n",
    "            # Draw node\n",
    "            circle = Circle((x, y), node_size, color=color, alpha=0.8, zorder=3)\n",
    "            ax.add_patch(circle)\n",
    "            \n",
    "            # Node label\n",
    "            ax.text(x, y, node_data['label'], ha='center', va='center', \n",
    "                   fontsize=9, fontweight='bold', zorder=4)\n",
    "            \n",
    "            # Shape and gradient info\n",
    "            shape_str = str(node_data['shape'])\n",
    "            grad_str = \"‚àá\" if node_data['requires_grad'] else \"\"\n",
    "            ax.text(x, y-node_size-0.05, f\"{shape_str}\\n{grad_str}\", \n",
    "                   ha='center', va='center', fontsize=7, alpha=0.8, zorder=4)\n",
    "        \n",
    "        # Create legend\n",
    "        legend_elements = [\n",
    "            plt.Circle((0, 0), 0.1, color=color, label=node_type.title())\n",
    "            for node_type, color in self.colors.items()\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(0, 1))\n",
    "        \n",
    "        ax.set_xlim(-1.3, 1.3)\n",
    "        ax.set_ylim(-1.3, 1.3)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "        ax.set_title('Computational Graph Structure', fontweight='bold')\n",
    "    \n",
    "    def _draw_node_statistics(self, ax):\n",
    "        \"\"\"Draw node type distribution and statistics.\"\"\"\n",
    "        node_types = [node['type'] for node in self.nodes.values()]\n",
    "        type_counts = pd.Series(node_types).value_counts()\n",
    "        \n",
    "        # Pie chart of node types\n",
    "        colors = [self.colors.get(node_type, '#CCCCCC') for node_type in type_counts.index]\n",
    "        wedges, texts, autotexts = ax.pie(type_counts.values, labels=type_counts.index, \n",
    "                                         autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "        \n",
    "        # Enhance text\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_color('white')\n",
    "            autotext.set_fontweight('bold')\n",
    "        \n",
    "        ax.set_title('Node Type Distribution', fontweight='bold')\n",
    "    \n",
    "    def _draw_memory_analysis(self, ax):\n",
    "        \"\"\"Draw memory usage analysis.\"\"\"\n",
    "        node_names = []\n",
    "        memory_usage = []\n",
    "        \n",
    "        for node_id, node_data in self.nodes.items():\n",
    "            node_names.append(f\"{node_data['label'][:8]}\")\n",
    "            memory_usage.append(node_data['memory_usage'] / 1024)  # Convert to KB\n",
    "        \n",
    "        if memory_usage:\n",
    "            bars = ax.bar(range(len(memory_usage)), memory_usage, \n",
    "                         color=[self.colors.get(self.nodes[f\"node_{i}\"]['type'], '#CCCCCC') \n",
    "                               for i in range(len(memory_usage))], alpha=0.7)\n",
    "            \n",
    "            ax.set_xlabel('Nodes')\n",
    "            ax.set_ylabel('Memory Usage (KB)')\n",
    "            ax.set_title('Memory Usage by Node', fontweight='bold')\n",
    "            ax.set_xticks(range(len(node_names)))\n",
    "            ax.set_xticklabels(node_names, rotation=45, ha='right')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, mem in zip(bars, memory_usage):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + max(memory_usage)*0.01,\n",
    "                       f'{mem:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    def _draw_gradient_flow_summary(self, ax):\n",
    "        \"\"\"Draw gradient flow summary.\"\"\"\n",
    "        grad_nodes = [node for node in self.nodes.values() if node['requires_grad']]\n",
    "        \n",
    "        if grad_nodes:\n",
    "            # Count nodes by type that require gradients\n",
    "            grad_types = [node['type'] for node in grad_nodes]\n",
    "            grad_counts = pd.Series(grad_types).value_counts()\n",
    "            \n",
    "            ax.bar(grad_counts.index, grad_counts.values, \n",
    "                  color='lightcoral', alpha=0.7)\n",
    "            ax.set_title('Gradient-Enabled Nodes', fontweight='bold')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No gradient-enabled nodes', \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_title('Gradient Flow Analysis', fontweight='bold')\n",
    "    \n",
    "    def _hierarchical_layout(self, G):\n",
    "        \"\"\"Create hierarchical layout for computational graph.\"\"\"\n",
    "        levels = {}\n",
    "        for node in nx.topological_sort(G):\n",
    "            if not list(G.predecessors(node)):\n",
    "                levels[node] = 0\n",
    "            else:\n",
    "                levels[node] = max(levels[pred] for pred in G.predecessors(node)) + 1\n",
    "        \n",
    "        # Group nodes by level\n",
    "        level_groups = defaultdict(list)\n",
    "        for node, level in levels.items():\n",
    "            level_groups[level].append(node)\n",
    "        \n",
    "        # Assign positions\n",
    "        pos = {}\n",
    "        max_level = max(level_groups.keys()) if level_groups else 0\n",
    "        \n",
    "        for level, nodes in level_groups.items():\n",
    "            y = 1 - (level / max(max_level, 1)) * 2  # Top to bottom\n",
    "            \n",
    "            if len(nodes) == 1:\n",
    "                pos[nodes[0]] = (0, y)\n",
    "            else:\n",
    "                x_positions = np.linspace(-0.8, 0.8, len(nodes))\n",
    "                for i, node in enumerate(nodes):\n",
    "                    pos[node] = (x_positions[i], y)\n",
    "        \n",
    "        return pos\n",
    "    \n",
    "    def save_analysis(self):\n",
    "        \"\"\"Save computational graph analysis to JSON.\"\"\"\n",
    "        analysis_data = {\n",
    "            'graph_statistics': {\n",
    "                'total_nodes': len(self.nodes),\n",
    "                'total_edges': len(self.edges),\n",
    "                'gradient_nodes': sum(1 for node in self.nodes.values() if node['requires_grad']),\n",
    "                'total_parameters': sum(np.prod(node['shape']) for node in self.nodes.values() \n",
    "                                      if node['type'] == 'parameter'),\n",
    "                'total_memory_kb': sum(node['memory_usage'] for node in self.nodes.values()) / 1024\n",
    "            },\n",
    "            'node_details': {\n",
    "                node_id: {\n",
    "                    'type': data['type'],\n",
    "                    'label': data['label'],\n",
    "                    'shape': data['shape'],\n",
    "                    'requires_grad': data['requires_grad'],\n",
    "                    'memory_kb': data['memory_usage'] / 1024\n",
    "                }\n",
    "                for node_id, data in self.nodes.items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(results_dir / 'computational_graph_analysis.json', 'w') as f:\n",
    "            json.dump(analysis_data, f, indent=2)\n",
    "        \n",
    "        return analysis_data\n",
    "\n",
    "def demonstrate_computational_graphs():\n",
    "    \"\"\"Demonstrate computational graph visualization with examples.\"\"\"\n",
    "    print(\"üåê Computational Graph Visualization Examples\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Example 1: Simple arithmetic computation\n",
    "    print(\"\\nüìä Example 1: Simple Arithmetic Operations\")\n",
    "    \n",
    "    visualizer1 = ComputationGraphVisualizer()\n",
    "    \n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = torch.tensor(3.0, requires_grad=True)\n",
    "    \n",
    "    # Add input nodes\n",
    "    x_node = visualizer1.add_node(x, 'input', 'x')\n",
    "    y_node = visualizer1.add_node(y, 'input', 'y')\n",
    "    \n",
    "    # Perform operations\n",
    "    a = x * y\n",
    "    a_node = visualizer1.add_node(a, 'operation', 'x*y')\n",
    "    visualizer1.add_edge(x_node, a_node, 'mul')\n",
    "    visualizer1.add_edge(y_node, a_node, 'mul')\n",
    "    \n",
    "    b = a + x\n",
    "    b_node = visualizer1.add_node(b, 'operation', 'a+x')\n",
    "    visualizer1.add_edge(a_node, b_node, 'add')\n",
    "    visualizer1.add_edge(x_node, b_node, 'add')\n",
    "    \n",
    "    c = torch.sin(b)\n",
    "    c_node = visualizer1.add_node(c, 'output', 'sin(b)')\n",
    "    visualizer1.add_edge(b_node, c_node, 'sin')\n",
    "    \n",
    "    # Visualize\n",
    "    fig1 = visualizer1.visualize_graph()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save analysis\n",
    "    analysis1 = visualizer1.save_analysis()\n",
    "    print(f\"Graph Statistics: {analysis1['graph_statistics']}\")\n",
    "    \n",
    "    # Example 2: Neural Network Layer\n",
    "    print(\"\\nüß† Example 2: Neural Network Layer Computation\")\n",
    "    \n",
    "    visualizer2 = ComputationGraphVisualizer()\n",
    "    \n",
    "    # Create layer components\n",
    "    batch_size, input_size, output_size = 3, 4, 2\n",
    "    x_nn = torch.randn(batch_size, input_size, requires_grad=True)\n",
    "    W = torch.randn(input_size, output_size, requires_grad=True)\n",
    "    b = torch.randn(output_size, requires_grad=True)\n",
    "    \n",
    "    # Add nodes\n",
    "    x_node = visualizer2.add_node(x_nn, 'input', 'Input\\nX')\n",
    "    w_node = visualizer2.add_node(W, 'parameter', 'Weights\\nW')\n",
    "    b_node = visualizer2.add_node(b, 'parameter', 'Bias\\nb')\n",
    "    \n",
    "    # Forward pass\n",
    "    matmul_result = torch.mm(x_nn, W)\n",
    "    matmul_node = visualizer2.add_node(matmul_result, 'operation', 'MatMul')\n",
    "    visualizer2.add_edge(x_node, matmul_node, 'input')\n",
    "    visualizer2.add_edge(w_node, matmul_node, 'weight')\n",
    "    \n",
    "    output = matmul_result + b\n",
    "    output_node = visualizer2.add_node(output, 'output', 'Linear\\nOutput')\n",
    "    visualizer2.add_edge(matmul_node, output_node, 'add')\n",
    "    visualizer2.add_edge(b_node, output_node, 'bias')\n",
    "    \n",
    "    # Apply activation\n",
    "    activated = torch.relu(output)\n",
    "    activation_node = visualizer2.add_node(activated, 'operation', 'ReLU')\n",
    "    visualizer2.add_edge(output_node, activation_node, 'activation')\n",
    "    \n",
    "    # Visualize\n",
    "    fig2 = visualizer2.visualize_graph()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save analysis\n",
    "    analysis2 = visualizer2.save_analysis()\n",
    "    print(f\"Neural Network Graph Statistics: {analysis2['graph_statistics']}\")\n",
    "    \n",
    "    return [visualizer1, visualizer2], [analysis1, analysis2]\n",
    "\n",
    "# Run computational graph demonstrations\n",
    "graph_visualizers, graph_analyses = demonstrate_computational_graphs()\n",
    "\n",
    "print(f\"\\nüíæ Computational graph analyses saved to {results_dir}\")\n",
    "print(\"\\nüìä Key Insights:\")\n",
    "print(\"‚Ä¢ Red circles indicate gradient-enabled tensors\")\n",
    "print(\"‚Ä¢ Node size reflects tensor dimensionality\")\n",
    "print(\"‚Ä¢ Edge colors show gradient flow (red = active)\")\n",
    "print(\"‚Ä¢ Memory usage helps identify computational bottlenecks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240bda2a",
   "metadata": {},
   "source": [
    "## 3. Gradient Flow Animation and Analysis\n",
    "\n",
    "### 3.1 Real-Time Gradient Flow Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd47c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientFlowAnimator:\n",
    "    \"\"\"Advanced gradient flow visualization with real-time monitoring.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, input_data, target, loss_fn=None):\n",
    "        self.model = model\n",
    "        self.input_data = input_data\n",
    "        self.target = target\n",
    "        self.loss_fn = loss_fn or nn.MSELoss()\n",
    "        \n",
    "        # Data collection\n",
    "        self.layer_activations = []\n",
    "        self.layer_gradients = []\n",
    "        self.parameter_gradients = []\n",
    "        self.gradient_norms = []\n",
    "        self.activation_stats = {}\n",
    "        \n",
    "        # Hook management\n",
    "        self.hooks = []\n",
    "        self.analysis_results = {}\n",
    "        \n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register comprehensive hooks for monitoring.\"\"\"\n",
    "        \n",
    "        def forward_hook(name):\n",
    "            def hook(module, input, output):\n",
    "                if isinstance(output, torch.Tensor):\n",
    "                    self.layer_activations.append({\n",
    "                        'name': name,\n",
    "                        'output': output.detach().clone(),\n",
    "                        'input': input[0].detach().clone() if input and isinstance(input[0], torch.Tensor) else None,\n",
    "                        'mean': float(output.mean()),\n",
    "                        'std': float(output.std()),\n",
    "                        'min': float(output.min()),\n",
    "                        'max': float(output.max()),\n",
    "                        'sparsity': float((output == 0).float().mean())\n",
    "                    })\n",
    "            return hook\n",
    "        \n",
    "        def backward_hook(name):\n",
    "            def hook(module, grad_input, grad_output):\n",
    "                if grad_output and grad_output[0] is not None:\n",
    "                    grad_tensor = grad_output[0].detach().clone()\n",
    "                    self.layer_gradients.append({\n",
    "                        'name': name,\n",
    "                        'grad_output': grad_tensor,\n",
    "                        'grad_norm': float(grad_tensor.norm()),\n",
    "                        'grad_mean': float(grad_tensor.mean()),\n",
    "                        'grad_std': float(grad_tensor.std())\n",
    "                    })\n",
    "            return hook\n",
    "        \n",
    "        # Register hooks for all modules\n",
    "        for name, module in self.model.named_modules():\n",
    "            if len(list(module.children())) == 0:  # Leaf modules only\n",
    "                self.hooks.append(module.register_forward_hook(forward_hook(name)))\n",
    "                self.hooks.append(module.register_backward_hook(backward_hook(name)))\n",
    "    \n",
    "    def capture_gradient_flow(self):\n",
    "        \"\"\"Perform forward and backward pass with comprehensive monitoring.\"\"\"\n",
    "        # Clear previous data\n",
    "        self.layer_activations.clear()\n",
    "        self.layer_gradients.clear()\n",
    "        self.parameter_gradients.clear()\n",
    "        \n",
    "        # Forward pass\n",
    "        self.model.zero_grad()\n",
    "        self.model.train()\n",
    "        \n",
    "        output = self.model(self.input_data)\n",
    "        loss = self.loss_fn(output, self.target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Collect parameter gradients\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                self.parameter_gradients.append({\n",
    "                    'name': name,\n",
    "                    'grad': param.grad.detach().clone(),\n",
    "                    'param': param.detach().clone(),\n",
    "                    'grad_norm': float(param.grad.norm()),\n",
    "                    'param_norm': float(param.norm()),\n",
    "                    'grad_to_param_ratio': float(param.grad.norm() / (param.norm() + 1e-8))\n",
    "                })\n",
    "        \n",
    "        return float(loss.item()), output.detach().clone()\n",
    "    \n",
    "    def create_comprehensive_analysis(self, figsize=(20, 16)):\n",
    "        \"\"\"Create comprehensive gradient flow analysis dashboard.\"\"\"\n",
    "        # Capture data\n",
    "        loss_value, model_output = self.capture_gradient_flow()\n",
    "        \n",
    "        # Create dashboard\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # 1. Network Architecture Overview\n",
    "        ax1 = fig.add_subplot(gs[0, :2])\n",
    "        self._plot_network_architecture(ax1)\n",
    "        \n",
    "        # 2. Real-time Loss and Output\n",
    "        ax2 = fig.add_subplot(gs[0, 2:])\n",
    "        self._plot_model_output_analysis(ax2, model_output, loss_value)\n",
    "        \n",
    "        # 3. Activation Flow Analysis\n",
    "        ax3 = fig.add_subplot(gs[1, :2])\n",
    "        self._plot_activation_analysis(ax3)\n",
    "        \n",
    "        # 4. Gradient Magnitude Distribution\n",
    "        ax4 = fig.add_subplot(gs[1, 2:])\n",
    "        self._plot_gradient_magnitude_analysis(ax4)\n",
    "        \n",
    "        # 5. Parameter Gradient Analysis\n",
    "        ax5 = fig.add_subplot(gs[2, :2])\n",
    "        self._plot_parameter_gradient_analysis(ax5)\n",
    "        \n",
    "        # 6. Gradient Flow Direction\n",
    "        ax6 = fig.add_subplot(gs[2, 2:])\n",
    "        self._plot_gradient_flow_direction(ax6)\n",
    "        \n",
    "        # 7. Activation Statistics Heatmap\n",
    "        ax7 = fig.add_subplot(gs[3, :2])\n",
    "        self._plot_activation_statistics_heatmap(ax7)\n",
    "        \n",
    "        # 8. Gradient Health Diagnostics\n",
    "        ax8 = fig.add_subplot(gs[3, 2:])\n",
    "        self._plot_gradient_health_diagnostics(ax8)\n",
    "        \n",
    "        plt.suptitle(f'Gradient Flow Analysis Dashboard (Loss: {loss_value:.6f})', \n",
    "                    fontsize=18, fontweight='bold', y=0.98)\n",
    "        \n",
    "        # Save comprehensive analysis\n",
    "        plt.savefig(results_dir / 'gradient_flow_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return fig, self._compile_analysis_results(loss_value)\n",
    "    \n",
    "    def _plot_network_architecture(self, ax):\n",
    "        \"\"\"Plot enhanced network architecture diagram.\"\"\"\n",
    "        layer_info = []\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                layer_info.append(f\"Linear\\n{module.in_features}‚Üí{module.out_features}\")\n",
    "            elif isinstance(module, (nn.ReLU, nn.Sigmoid, nn.Tanh, nn.LeakyReLU)):\n",
    "                layer_info.append(module.__class__.__name__)\n",
    "            elif isinstance(module, nn.Dropout):\n",
    "                layer_info.append(f\"Dropout\\n(p={module.p})\")\n",
    "        \n",
    "        if layer_info:\n",
    "            x_positions = np.linspace(0.1, 0.9, len(layer_info))\n",
    "            \n",
    "            for i, (x_pos, layer_name) in enumerate(zip(x_positions, layer_info)):\n",
    "                # Draw layer representation\n",
    "                height = 0.6\n",
    "                rect = FancyBboxPatch(\n",
    "                    (x_pos - 0.04, 0.2), 0.08, height,\n",
    "                    boxstyle=\"round,pad=0.01\",\n",
    "                    facecolor='lightblue' if 'Linear' in layer_name else 'lightgreen',\n",
    "                    edgecolor='navy', alpha=0.7, linewidth=2\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add layer label\n",
    "                ax.text(x_pos, 0.1, layer_name, ha='center', va='center', \n",
    "                       fontsize=9, fontweight='bold')\n",
    "                \n",
    "                # Draw connections\n",
    "                if i < len(layer_info) - 1:\n",
    "                    ax.arrow(x_pos + 0.04, 0.5, x_positions[i+1] - x_pos - 0.08, 0,\n",
    "                            head_width=0.03, head_length=0.02, fc='darkblue', ec='darkblue')\n",
    "        \n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title('Neural Network Architecture', fontweight='bold', fontsize=14)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    def _plot_model_output_analysis(self, ax, model_output, loss_value):\n",
    "        \"\"\"Plot model output and loss analysis.\"\"\"\n",
    "        if model_output.dim() == 2:\n",
    "            # For batch outputs, show distribution\n",
    "            output_flat = model_output.flatten().cpu().numpy()\n",
    "            target_flat = self.target.flatten().cpu().numpy()\n",
    "            \n",
    "            # Create scatter plot of predictions vs targets\n",
    "            ax.scatter(target_flat, output_flat, alpha=0.6, s=50)\n",
    "            \n",
    "            # Perfect prediction line\n",
    "            min_val = min(target_flat.min(), output_flat.min())\n",
    "            max_val = max(target_flat.max(), output_flat.max())\n",
    "            ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)\n",
    "            \n",
    "            ax.set_xlabel('Target Values')\n",
    "            ax.set_ylabel('Predicted Values')\n",
    "            ax.set_title(f'Predictions vs Targets\\nLoss: {loss_value:.6f}', fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Calculate R¬≤\n",
    "            correlation = np.corrcoef(target_flat, output_flat)[0, 1]\n",
    "            ax.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "                   transform=ax.transAxes, fontsize=10, \n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f'Loss: {loss_value:.6f}\\nOutput shape: {model_output.shape}', \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_title('Model Output Summary', fontweight='bold')\n",
    "    \n",
    "    def _plot_activation_analysis(self, ax):\n",
    "        \"\"\"Plot activation flow and statistics.\"\"\"\n",
    "        if not self.layer_activations:\n",
    "            ax.text(0.5, 0.5, 'No activation data captured', ha='center', va='center')\n",
    "            ax.set_title('Activation Analysis')\n",
    "            return\n",
    "        \n",
    "        layer_names = [act['name'] if act['name'] else f\"Layer_{i}\" for i, act in enumerate(self.layer_activations)]\n",
    "        activation_means = [act['mean'] for act in self.layer_activations]\n",
    "        activation_stds = [act['std'] for act in self.layer_activations]\n",
    "        sparsity = [act['sparsity'] for act in self.layer_activations]\n",
    "        \n",
    "        x = np.arange(len(layer_names))\n",
    "        width = 0.25\n",
    "        \n",
    "        # Multiple bar plot\n",
    "        bars1 = ax.bar(x - width, activation_means, width, label='Mean', alpha=0.8, color='skyblue')\n",
    "        bars2 = ax.bar(x, activation_stds, width, label='Std Dev', alpha=0.8, color='lightcoral')\n",
    "        bars3 = ax.bar(x + width, sparsity, width, label='Sparsity', alpha=0.8, color='lightgreen')\n",
    "        \n",
    "        ax.set_xlabel('Layers')\n",
    "        ax.set_ylabel('Activation Statistics')\n",
    "        ax.set_title('Layer-wise Activation Analysis', fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([name[:8] + '...' if len(name) > 8 else name for name in layer_names], \n",
    "                          rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bars in [bars1, bars2, bars3]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    def _plot_gradient_magnitude_analysis(self, ax):\n",
    "        \"\"\"Plot gradient magnitude analysis.\"\"\"\n",
    "        if not self.parameter_gradients:\n",
    "            ax.text(0.5, 0.5, 'No gradient data available', ha='center', va='center')\n",
    "            ax.set_title('Gradient Magnitude Analysis')\n",
    "            return\n",
    "        \n",
    "        param_names = [pg['name'].split('.')[-1][:10] for pg in self.parameter_gradients]\n",
    "        grad_norms = [pg['grad_norm'] for pg in self.parameter_gradients]\n",
    "        param_norms = [pg['param_norm'] for pg in self.parameter_gradients]\n",
    "        \n",
    "        x = np.arange(len(param_names))\n",
    "        \n",
    "        # Dual y-axis plot\n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        bars1 = ax.bar(x - 0.2, grad_norms, 0.4, label='Gradient Norm', \n",
    "                      alpha=0.8, color='red')\n",
    "        bars2 = ax2.bar(x + 0.2, param_norms, 0.4, label='Parameter Norm', \n",
    "                       alpha=0.8, color='blue')\n",
    "        \n",
    "        ax.set_xlabel('Parameters')\n",
    "        ax.set_ylabel('Gradient Norm', color='red')\n",
    "        ax2.set_ylabel('Parameter Norm', color='blue')\n",
    "        ax.set_title('Gradient vs Parameter Magnitudes', fontweight='bold')\n",
    "        \n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(param_names, rotation=45, ha='right')\n",
    "        ax.set_yscale('log')\n",
    "        ax2.set_yscale('log')\n",
    "        \n",
    "        # Combined legend\n",
    "        lines1, labels1 = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    def _plot_parameter_gradient_analysis(self, ax):\n",
    "        \"\"\"Plot parameter gradient detailed analysis.\"\"\"\n",
    "        if not self.parameter_gradients:\n",
    "            ax.text(0.5, 0.5, 'No parameter gradients', ha='center', va='center')\n",
    "            ax.set_title('Parameter Gradient Analysis')\n",
    "            return\n",
    "        \n",
    "        # Calculate gradient-to-parameter ratios\n",
    "        ratios = [pg['grad_to_param_ratio'] for pg in self.parameter_gradients]\n",
    "        param_names = [pg['name'].split('.')[-1][:8] for pg in self.parameter_gradients]\n",
    "        \n",
    "        # Create scatter plot\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(ratios)))\n",
    "        scatter = ax.scatter(range(len(ratios)), ratios, c=colors, s=100, alpha=0.7)\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(ratios) > 1:\n",
    "            z = np.polyfit(range(len(ratios)), ratios, 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax.plot(range(len(ratios)), p(range(len(ratios))), \"r--\", alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Parameter Index')\n",
    "        ax.set_ylabel('Gradient/Parameter Ratio')\n",
    "        ax.set_title('Gradient-to-Parameter Ratio Analysis', fontweight='bold')\n",
    "        ax.set_xticks(range(len(param_names)))\n",
    "        ax.set_xticklabels(param_names, rotation=45, ha='right')\n",
    "        ax.set_yscale('log')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add threshold lines\n",
    "        ax.axhline(y=0.01, color='orange', linestyle='--', alpha=0.7, label='Conservative threshold')\n",
    "        ax.axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='Aggressive threshold')\n",
    "        ax.legend()\n",
    "    \n",
    "    def _plot_gradient_flow_direction(self, ax):\n",
    "        \"\"\"Plot gradient flow direction through the network.\"\"\"\n",
    "        if not self.layer_gradients:\n",
    "            ax.text(0.5, 0.5, 'No layer gradients captured', ha='center', va='center')\n",
    "            ax.set_title('Gradient Flow Direction')\n",
    "            return\n",
    "        \n",
    "        # Reverse order for backpropagation flow\n",
    "        layer_names = [lg['name'] if lg['name'] else f\"Layer_{i}\" \n",
    "                      for i, lg in enumerate(reversed(self.layer_gradients))]\n",
    "        grad_norms = [lg['grad_norm'] for lg in reversed(self.layer_gradients)]\n",
    "        \n",
    "        # Create flow visualization\n",
    "        x_pos = range(len(layer_names))\n",
    "        \n",
    "        # Plot gradient flow as connected line with markers\n",
    "        ax.plot(x_pos, grad_norms, 'o-', linewidth=3, markersize=8, \n",
    "               color='red', alpha=0.7, label='Gradient Flow')\n",
    "        \n",
    "        # Add directional arrows\n",
    "        for i in range(len(x_pos) - 1):\n",
    "            ax.annotate('', xy=(x_pos[i+1], grad_norms[i+1]), \n",
    "                       xytext=(x_pos[i], grad_norms[i]),\n",
    "                       arrowprops=dict(arrowstyle='->', lw=2, color='darkred', alpha=0.6))\n",
    "        \n",
    "        ax.set_xlabel('Network Layers (Output ‚Üí Input)')\n",
    "        ax.set_ylabel('Gradient Norm (Log Scale)')\n",
    "        ax.set_title('Backpropagation Flow Analysis', fontweight='bold')\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels([name[:10] for name in layer_names], rotation=45, ha='right')\n",
    "        ax.set_yscale('log')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "        \n",
    "        # Highlight potential vanishing/exploding gradients\n",
    "        if grad_norms:\n",
    "            max_grad = max(grad_norms)\n",
    "            min_grad = min(grad_norms)\n",
    "            if max_grad / min_grad > 100:\n",
    "                ax.text(0.02, 0.98, 'Warning: Large gradient variation detected!', \n",
    "                       transform=ax.transAxes, fontsize=10, color='red',\n",
    "                       bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
    "    \n",
    "    def _plot_activation_statistics_heatmap(self, ax):\n",
    "        \"\"\"Plot activation statistics as heatmap.\"\"\"\n",
    "        if not self.layer_activations:\n",
    "            ax.text(0.5, 0.5, 'No activation statistics', ha='center', va='center')\n",
    "            ax.set_title('Activation Statistics Heatmap')\n",
    "            return\n",
    "        \n",
    "        # Prepare data for heatmap\n",
    "        layer_names = [act['name'][:10] if act['name'] else f\"L_{i}\" \n",
    "                      for i, act in enumerate(self.layer_activations)]\n",
    "        \n",
    "        statistics = ['mean', 'std', 'min', 'max', 'sparsity']\n",
    "        data_matrix = []\n",
    "        \n",
    "        for stat in statistics:\n",
    "            row = [act[stat] for act in self.layer_activations]\n",
    "            data_matrix.append(row)\n",
    "        \n",
    "        # Create heatmap\n",
    "        im = ax.imshow(data_matrix, cmap='viridis', aspect='auto')\n",
    "        \n",
    "        # Set ticks and labels\n",
    "        ax.set_xticks(range(len(layer_names)))\n",
    "        ax.set_xticklabels(layer_names, rotation=45, ha='right')\n",
    "        ax.set_yticks(range(len(statistics)))\n",
    "        ax.set_yticklabels(statistics)\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(len(statistics)):\n",
    "            for j in range(len(layer_names)):\n",
    "                text = ax.text(j, i, f'{data_matrix[i][j]:.2f}',\n",
    "                             ha=\"center\", va=\"center\", color=\"white\", fontsize=8)\n",
    "        \n",
    "        ax.set_title('Activation Statistics Heatmap', fontweight='bold')\n",
    "    \n",
    "    def _plot_gradient_health_diagnostics(self, ax):\n",
    "        \"\"\"Plot gradient health diagnostics.\"\"\"\n",
    "        if not self.parameter_gradients:\n",
    "            ax.text(0.5, 0.5, 'No gradient data for diagnostics', ha='center', va='center')\n",
    "            ax.set_title('Gradient Health Diagnostics')\n",
    "            return\n",
    "        \n",
    "        # Calculate health metrics\n",
    "        grad_norms = [pg['grad_norm'] for pg in self.parameter_gradients]\n",
    "        \n",
    "        # Gradient health categories\n",
    "        vanishing_threshold = 1e-6\n",
    "        exploding_threshold = 1.0\n",
    "        \n",
    "        healthy = sum(1 for g in grad_norms if vanishing_threshold < g < exploding_threshold)\n",
    "        vanishing = sum(1 for g in grad_norms if g <= vanishing_threshold)\n",
    "        exploding = sum(1 for g in grad_norms if g >= exploding_threshold)\n",
    "        \n",
    "        # Create pie chart\n",
    "        sizes = [healthy, vanishing, exploding]\n",
    "        labels = [f'Healthy\\n({healthy})', f'Vanishing\\n({vanishing})', f'Exploding\\n({exploding})']\n",
    "        colors = ['green', 'orange', 'red']\n",
    "        \n",
    "        # Only include non-zero categories\n",
    "        non_zero_mask = [s > 0 for s in sizes]\n",
    "        sizes = [s for s, mask in zip(sizes, non_zero_mask) if mask]\n",
    "        labels = [l for l, mask in zip(labels, non_zero_mask) if mask]\n",
    "        colors = [c for c, mask in zip(colors, non_zero_mask) if mask]\n",
    "        \n",
    "        if sizes:\n",
    "            wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors, \n",
    "                                             autopct='%1.1f%%', startangle=90)\n",
    "            \n",
    "            for autotext in autotexts:\n",
    "                autotext.set_color('white')\n",
    "                autotext.set_fontweight('bold')\n",
    "        \n",
    "        ax.set_title('Gradient Health Status', fontweight='bold')\n",
    "        \n",
    "        # Add summary text\n",
    "        total_params = len(grad_norms)\n",
    "        health_score = (healthy / total_params * 100) if total_params > 0 else 0\n",
    "        \n",
    "        ax.text(0.02, 0.02, f'Health Score: {health_score:.1f}%', \n",
    "               transform=ax.transAxes, fontsize=12, fontweight='bold',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    def _compile_analysis_results(self, loss_value):\n",
    "        \"\"\"Compile comprehensive analysis results.\"\"\"\n",
    "        results = {\n",
    "            'loss': loss_value,\n",
    "            'network_statistics': {\n",
    "                'total_parameters': sum(p.numel() for p in self.model.parameters()),\n",
    "                'trainable_parameters': sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "                'total_layers': len(list(self.model.modules())) - 1,  # Exclude the model itself\n",
    "            },\n",
    "            'activation_analysis': {},\n",
    "            'gradient_analysis': {},\n",
    "            'health_metrics': {}\n",
    "        }\n",
    "        \n",
    "        # Activation analysis\n",
    "        if self.layer_activations:\n",
    "            results['activation_analysis'] = {\n",
    "                'layer_count': len(self.layer_activations),\n",
    "                'mean_activation': np.mean([act['mean'] for act in self.layer_activations]),\n",
    "                'mean_sparsity': np.mean([act['sparsity'] for act in self.layer_activations]),\n",
    "                'activation_range': {\n",
    "                    'min': min([act['min'] for act in self.layer_activations]),\n",
    "                    'max': max([act['max'] for act in self.layer_activations])\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Gradient analysis\n",
    "        if self.parameter_gradients:\n",
    "            grad_norms = [pg['grad_norm'] for pg in self.parameter_gradients]\n",
    "            results['gradient_analysis'] = {\n",
    "                'parameter_count': len(self.parameter_gradients),\n",
    "                'mean_grad_norm': float(np.mean(grad_norms)),\n",
    "                'std_grad_norm': float(np.std(grad_norms)),\n",
    "                'min_grad_norm': float(np.min(grad_norms)),\n",
    "                'max_grad_norm': float(np.max(grad_norms)),\n",
    "                'grad_norm_ratio': float(np.max(grad_norms) / (np.min(grad_norms) + 1e-8))\n",
    "            }\n",
    "            \n",
    "            # Health metrics\n",
    "            vanishing_count = sum(1 for g in grad_norms if g < 1e-6)\n",
    "            exploding_count = sum(1 for g in grad_norms if g > 1.0)\n",
    "            healthy_count = len(grad_norms) - vanishing_count - exploding_count\n",
    "            \n",
    "            results['health_metrics'] = {\n",
    "                'healthy_gradients': healthy_count,\n",
    "                'vanishing_gradients': vanishing_count,\n",
    "                'exploding_gradients': exploding_count,\n",
    "                'health_score': healthy_count / len(grad_norms) * 100 if grad_norms else 0\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_analysis(self, results):\n",
    "        \"\"\"Save comprehensive gradient flow analysis.\"\"\"\n",
    "        with open(results_dir / 'gradient_flow_analysis.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Gradient flow analysis saved to {results_dir / 'gradient_flow_analysis.json'}\")\n",
    "        return results\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove hooks to prevent memory leaks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "\n",
    "def demonstrate_gradient_flow_analysis():\n",
    "    \"\"\"Demonstrate gradient flow analysis with different network architectures.\"\"\"\n",
    "    print(\"\\nüåä Gradient Flow Analysis Demonstrations\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Example 1: Healthy Network\n",
    "    print(\"\\n‚úÖ Example 1: Healthy Network Architecture\")\n",
    "    \n",
    "    class HealthyNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(10, 20),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(20, 15),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(15, 5),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(5, 1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "    \n",
    "    # Create healthy network analysis\n",
    "    healthy_net = HealthyNetwork()\n",
    "    healthy_input = torch.randn(5, 10)\n",
    "    healthy_target = torch.randn(5, 1)\n",
    "    \n",
    "    healthy_analyzer = GradientFlowAnimator(healthy_net, healthy_input, healthy_target)\n",
    "    healthy_fig, healthy_results = healthy_analyzer.create_comprehensive_analysis()\n",
    "    healthy_analyzer.save_analysis(healthy_results)\n",
    "    healthy_analyzer.cleanup()\n",
    "    \n",
    "    print(f\"Healthy Network Health Score: {healthy_results['health_metrics']['health_score']:.1f}%\")\n",
    "    \n",
    "    # Example 2: Problematic Network (Deep with potential issues)\n",
    "    print(\"\\n‚ö†Ô∏è Example 2: Problematic Deep Network\")\n",
    "    \n",
    "    class ProblematicNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            # Very deep network with sigmoid activations (prone to vanishing gradients)\n",
    "            input_size = 20\n",
    "            for i in range(8):\n",
    "                layers.extend([\n",
    "                    nn.Linear(input_size, input_size),\n",
    "                    nn.Sigmoid()  # Sigmoid can cause vanishing gradients\n",
    "                ])\n",
    "            layers.append(nn.Linear(input_size, 1))\n",
    "            self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "    \n",
    "    # Create problematic network analysis\n",
    "    problem_net = ProblematicNetwork()\n",
    "    problem_input = torch.randn(3, 20)\n",
    "    problem_target = torch.randn(3, 1)\n",
    "    \n",
    "    problem_analyzer = GradientFlowAnimator(problem_net, problem_input, problem_target)\n",
    "    problem_fig, problem_results = problem_analyzer.create_comprehensive_analysis()\n",
    "    problem_analyzer.save_analysis(problem_results)\n",
    "    problem_analyzer.cleanup()\n",
    "    \n",
    "    print(f\"Problematic Network Health Score: {problem_results['health_metrics']['health_score']:.1f}%\")\n",
    "    \n",
    "    # Comparison analysis\n",
    "    print(\"\\nüìä Comparative Analysis:\")\n",
    "    print(f\"Healthy Network:\")\n",
    "    print(f\"  - Health Score: {healthy_results['health_metrics']['health_score']:.1f}%\")\n",
    "    print(f\"  - Mean Gradient Norm: {healthy_results['gradient_analysis']['mean_grad_norm']:.2e}\")\n",
    "    print(f\"  - Gradient Range: {healthy_results['gradient_analysis']['grad_norm_ratio']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nProblematic Network:\")\n",
    "    print(f\"  - Health Score: {problem_results['health_metrics']['health_score']:.1f}%\")\n",
    "    print(f\"  - Mean Gradient Norm: {problem_results['gradient_analysis']['mean_grad_norm']:.2e}\")\n",
    "    print(f\"  - Gradient Range: {problem_results['gradient_analysis']['grad_norm_ratio']:.2f}\")\n",
    "    \n",
    "    return [healthy_analyzer, problem_analyzer], [healthy_results, problem_results]\n",
    "\n",
    "# Run gradient flow demonstrations\n",
    "flow_analyzers, flow_results = demonstrate_gradient_flow_analysis()\n",
    "\n",
    "print(f\"\\nüí° Key Gradient Flow Insights:\")\n",
    "print(\"‚Ä¢ Activation statistics reveal signal strength through layers\")\n",
    "print(\"‚Ä¢ Gradient magnitude analysis shows learning capacity\")\n",
    "print(\"‚Ä¢ Health diagnostics identify vanishing/exploding gradient problems\")\n",
    "print(\"‚Ä¢ Flow direction visualization shows backpropagation efficiency\")\n",
    "print(\"‚Ä¢ Real-time monitoring enables proactive training adjustments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d91070",
   "metadata": {},
   "source": [
    "## 4. 3D Loss Landscape Exploration\n",
    "\n",
    "### 4.1 Interactive Loss Landscape Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20c61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLandscapeVisualizer:\n",
    "    \"\"\"Advanced 3D loss landscape visualization with optimization path tracking.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, data_loader, loss_fn):\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "        self.loss_fn = loss_fn\n",
    "        self.original_params = self._get_parameters()\n",
    "        self.optimization_history = []\n",
    "        self.landscape_data = {}\n",
    "    \n",
    "    def _get_parameters(self):\n",
    "        \"\"\"Get flattened model parameters.\"\"\"\n",
    "        return torch.cat([p.flatten() for p in self.model.parameters() if p.requires_grad])\n",
    "    \n",
    "    def _set_parameters(self, params):\n",
    "        \"\"\"Set model parameters from flattened vector.\"\"\"\n",
    "        offset = 0\n",
    "        for p in self.model.parameters():\n",
    "            if p.requires_grad:\n",
    "                num_params = p.numel()\n",
    "                p.data = params[offset:offset+num_params].view(p.shape)\n",
    "                offset += num_params\n",
    "    \n",
    "    def _compute_loss(self, params=None):\n",
    "        \"\"\"Compute loss for given parameters.\"\"\"\n",
    "        if params is not None:\n",
    "            self._set_parameters(params)\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_target in self.data_loader:\n",
    "                output = self.model(batch_data)\n",
    "                loss = self.loss_fn(output, batch_target)\n",
    "                total_loss += loss.item() * batch_data.size(0)\n",
    "                total_samples += batch_data.size(0)\n",
    "        \n",
    "        return total_loss / total_samples if total_samples > 0 else 0.0\n",
    "    \n",
    "    def generate_random_directions(self, num_directions=2):\n",
    "        \"\"\"Generate random orthonormal directions for landscape exploration.\"\"\"\n",
    "        param_size = len(self.original_params)\n",
    "        \n",
    "        # Generate random directions\n",
    "        directions = []\n",
    "        for i in range(num_directions):\n",
    "            direction = torch.randn(param_size)\n",
    "            \n",
    "            # Orthogonalize against previous directions\n",
    "            for prev_dir in directions:\n",
    "                direction = direction - (direction @ prev_dir) * prev_dir\n",
    "            \n",
    "            # Normalize\n",
    "            direction = direction / direction.norm()\n",
    "            directions.append(direction)\n",
    "        \n",
    "        return directions\n",
    "    \n",
    "    def create_2d_landscape_slice(self, direction1=None, direction2=None, \n",
    "                                 alpha_range=(-1, 1), beta_range=(-1, 1), resolution=30):\n",
    "        \"\"\"Create 2D slice of loss landscape with enhanced analysis.\"\"\"\n",
    "        \n",
    "        # Generate directions if not provided\n",
    "        if direction1 is None or direction2 is None:\n",
    "            directions = self.generate_random_directions(2)\n",
    "            direction1 = directions[0] if direction1 is None else direction1\n",
    "            direction2 = directions[1] if direction2 is None else direction2\n",
    "        \n",
    "        alphas = np.linspace(alpha_range[0], alpha_range[1], resolution)\n",
    "        betas = np.linspace(beta_range[0], beta_range[1], resolution)\n",
    "        \n",
    "        losses = np.zeros((resolution, resolution))\n",
    "        gradient_norms = np.zeros((resolution, resolution))\n",
    "        \n",
    "        print(f\"üèîÔ∏è Computing {resolution}x{resolution} loss landscape...\")\n",
    "        \n",
    "        # Store original state\n",
    "        original_loss = self._compute_loss()\n",
    "        \n",
    "        for i, alpha in enumerate(alphas):\n",
    "            for j, beta in enumerate(betas):\n",
    "                # Perturb parameters\n",
    "                perturbed_params = self.original_params + alpha * direction1 + beta * direction2\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self._compute_loss(perturbed_params)\n",
    "                losses[i, j] = loss\n",
    "                \n",
    "                # Compute gradient norm at this point (optional, expensive)\n",
    "                if resolution <= 20:  # Only for small grids\n",
    "                    self._set_parameters(perturbed_params)\n",
    "                    self.model.zero_grad()\n",
    "                    \n",
    "                    total_grad_norm = 0\n",
    "                    sample_count = 0\n",
    "                    \n",
    "                    for batch_data, batch_target in self.data_loader:\n",
    "                        if sample_count >= 1:  # Limit to first batch for speed\n",
    "                            break\n",
    "                        output = self.model(batch_data)\n",
    "                        batch_loss = self.loss_fn(output, batch_target)\n",
    "                        batch_loss.backward()\n",
    "                        \n",
    "                        grad_norm = torch.cat([p.grad.flatten() for p in self.model.parameters() \n",
    "                                             if p.grad is not None]).norm()\n",
    "                        total_grad_norm += grad_norm.item()\n",
    "                        sample_count += 1\n",
    "                    \n",
    "                    gradient_norms[i, j] = total_grad_norm / sample_count if sample_count > 0 else 0\n",
    "            \n",
    "            if (i + 1) % max(1, resolution // 10) == 0:\n",
    "                print(f\"  Progress: {(i+1)/resolution*100:.1f}%\")\n",
    "        \n",
    "        # Restore original parameters\n",
    "        self._set_parameters(self.original_params)\n",
    "        \n",
    "        # Store landscape data\n",
    "        self.landscape_data = {\n",
    "            'alphas': alphas,\n",
    "            'betas': betas,\n",
    "            'losses': losses,\n",
    "            'gradient_norms': gradient_norms,\n",
    "            'direction1': direction1,\n",
    "            'direction2': direction2,\n",
    "            'original_loss': original_loss,\n",
    "            'min_loss': float(np.min(losses)),\n",
    "            'max_loss': float(np.max(losses)),\n",
    "            'current_position': (0, 0)  # Origin represents current parameters\n",
    "        }\n",
    "        \n",
    "        return alphas, betas, losses, gradient_norms\n",
    "    \n",
    "    def create_comprehensive_landscape_analysis(self, figsize=(20, 16)):\n",
    "        \"\"\"Create comprehensive loss landscape analysis dashboard.\"\"\"\n",
    "        \n",
    "        # Generate landscape data\n",
    "        alphas, betas, losses, gradient_norms = self.create_2d_landscape_slice(resolution=25)\n",
    "        \n",
    "        # Create dashboard\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3, height_ratios=[1.2, 1, 1])\n",
    "        \n",
    "        # 1. 3D Surface Plot (large, top row)\n",
    "        ax1 = fig.add_subplot(gs[0, :2], projection='3d')\n",
    "        self._plot_3d_surface(ax1, alphas, betas, losses)\n",
    "        \n",
    "        # 2. 2D Contour Plot with Current Position\n",
    "        ax2 = fig.add_subplot(gs[0, 2:])\n",
    "        self._plot_contour_with_analysis(ax2, alphas, betas, losses)\n",
    "        \n",
    "        # 3. Loss Cross-Sections\n",
    "        ax3 = fig.add_subplot(gs[1, :2])\n",
    "        self._plot_loss_cross_sections(ax3, alphas, betas, losses)\n",
    "        \n",
    "        # 4. Gradient Magnitude Heatmap\n",
    "        ax4 = fig.add_subplot(gs[1, 2:])\n",
    "        self._plot_gradient_magnitude_heatmap(ax4, alphas, betas, gradient_norms)\n",
    "        \n",
    "        # 5. Loss Statistics\n",
    "        ax5 = fig.add_subplot(gs[2, 0])\n",
    "        self._plot_loss_statistics(ax5, losses)\n",
    "        \n",
    "        # 6. Curvature Analysis\n",
    "        ax6 = fig.add_subplot(gs[2, 1])\n",
    "        self._plot_curvature_analysis(ax6, alphas, betas, losses)\n",
    "        \n",
    "        # 7. Optimization Difficulty Assessment\n",
    "        ax7 = fig.add_subplot(gs[2, 2])\n",
    "        self._plot_optimization_difficulty(ax7, losses, gradient_norms)\n",
    "        \n",
    "        # 8. Landscape Summary\n",
    "        ax8 = fig.add_subplot(gs[2, 3])\n",
    "        self._plot_landscape_summary(ax8)\n",
    "        \n",
    "        plt.suptitle('Loss Landscape Comprehensive Analysis', fontsize=18, fontweight='bold')\n",
    "        \n",
    "        # Save analysis\n",
    "        plt.savefig(results_dir / 'loss_landscape_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return fig, self.landscape_data\n",
    "    \n",
    "    def _plot_3d_surface(self, ax, alphas, betas, losses):\n",
    "        \"\"\"Plot 3D loss surface with enhanced visualization.\"\"\"\n",
    "        A, B = np.meshgrid(alphas, betas)\n",
    "        \n",
    "        # Create surface plot with custom colormap\n",
    "        surface = ax.plot_surface(A, B, losses.T, cmap='viridis', alpha=0.9,\n",
    "                                 linewidth=0.5, antialiased=True, edgecolor='none')\n",
    "        \n",
    "        # Mark current position\n",
    "        current_loss = self.landscape_data['original_loss']\n",
    "        ax.scatter([0], [0], [current_loss], color='red', s=100, alpha=1.0, \n",
    "                  label=f'Current Position\\n(Loss: {current_loss:.4f})')\n",
    "        \n",
    "        # Mark global minimum in the slice\n",
    "        min_idx = np.unravel_index(np.argmin(losses), losses.shape)\n",
    "        min_alpha = alphas[min_idx[0]]\n",
    "        min_beta = betas[min_idx[1]]\n",
    "        min_loss = losses[min_idx]\n",
    "        ax.scatter([min_alpha], [min_beta], [min_loss], color='lime', s=100, alpha=1.0,\n",
    "                  label=f'Local Minimum\\n(Loss: {min_loss:.4f})')\n",
    "        \n",
    "        ax.set_xlabel('Direction 1 (Œ±)', fontweight='bold')\n",
    "        ax.set_ylabel('Direction 2 (Œ≤)', fontweight='bold')\n",
    "        ax.set_zlabel('Loss', fontweight='bold')\n",
    "        ax.set_title('3D Loss Landscape Surface', fontweight='bold', fontsize=14)\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add colorbar\n",
    "        fig = ax.figure\n",
    "        cbar = fig.colorbar(surface, ax=ax, shrink=0.5, aspect=5)\n",
    "        cbar.set_label('Loss Value', fontweight='bold')\n",
    "    \n",
    "    def _plot_contour_with_analysis(self, ax, alphas, betas, losses):\n",
    "        \"\"\"Plot contour map with detailed analysis.\"\"\"\n",
    "        A, B = np.meshgrid(alphas, betas)\n",
    "        \n",
    "        # Create contour plot\n",
    "        levels = np.logspace(np.log10(np.min(losses)), np.log10(np.max(losses)), 15)\n",
    "        contour = ax.contour(A, B, losses.T, levels=levels, alpha=0.8, colors='black', linewidths=0.5)\n",
    "        contourf = ax.contourf(A, B, losses.T, levels=levels, cmap='viridis', alpha=0.7)\n",
    "        \n",
    "        # Add contour labels\n",
    "        ax.clabel(contour, inline=True, fontsize=8, fmt='%.3f')\n",
    "        \n",
    "        # Mark important points\n",
    "        ax.plot(0, 0, 'ro', markersize=12, label='Current Position', markeredgecolor='darkred')\n",
    "        \n",
    "        # Mark local minima\n",
    "        min_idx = np.unravel_index(np.argmin(losses), losses.shape)\n",
    "        min_alpha = alphas[min_idx[0]]\n",
    "        min_beta = betas[min_idx[1]]\n",
    "        ax.plot(min_alpha, min_beta, 's', color='lime', markersize=10, \n",
    "               label='Local Minimum', markeredgecolor='darkgreen')\n",
    "        \n",
    "        # Add gradient descent path simulation\n",
    "        self._add_gradient_descent_simulation(ax, alphas, betas, losses)\n",
    "        \n",
    "        ax.set_xlabel('Direction 1 (Œ±)', fontweight='bold')\n",
    "        ax.set_ylabel('Direction 2 (Œ≤)', fontweight='bold')\n",
    "        ax.set_title('Loss Contours with Analysis', fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(contourf, ax=ax, label='Loss Value')\n",
    "    \n",
    "    def _add_gradient_descent_simulation(self, ax, alphas, betas, losses):\n",
    "        \"\"\"Add simulated gradient descent path to contour plot.\"\"\"\n",
    "        # Simple gradient descent simulation\n",
    "        A, B = np.meshgrid(alphas, betas)\n",
    "        \n",
    "        # Compute gradients using finite differences\n",
    "        dy, dx = np.gradient(losses.T)\n",
    "        \n",
    "        # Normalize gradients\n",
    "        grad_magnitude = np.sqrt(dx**2 + dy**2)\n",
    "        dx_norm = dx / (grad_magnitude + 1e-8)\n",
    "        dy_norm = dy / (grad_magnitude + 1e-8)\n",
    "        \n",
    "        # Sample points for gradient vectors\n",
    "        step = max(1, len(alphas) // 8)\n",
    "        X_sample = A[::step, ::step]\n",
    "        Y_sample = B[::step, ::step]\n",
    "        U_sample = -dx_norm[::step, ::step]  # Negative for descent\n",
    "        V_sample = -dy_norm[::step, ::step]\n",
    "        \n",
    "        # Add gradient vectors\n",
    "        ax.quiver(X_sample, Y_sample, U_sample, V_sample, \n",
    "                 alpha=0.6, scale=20, width=0.003, color='white')\n",
    "        \n",
    "        # Simulate a gradient descent path from current position\n",
    "        path_alpha, path_beta = [0], [0]\n",
    "        learning_rate = 0.1\n",
    "        \n",
    "        for step in range(20):\n",
    "            current_alpha, current_beta = path_alpha[-1], path_beta[-1]\n",
    "            \n",
    "            # Find nearest grid point\n",
    "            alpha_idx = np.argmin(np.abs(alphas - current_alpha))\n",
    "            beta_idx = np.argmin(np.abs(betas - current_beta))\n",
    "            \n",
    "            if 0 <= alpha_idx < len(alphas)-1 and 0 <= beta_idx < len(betas)-1:\n",
    "                # Compute gradient at current position\n",
    "                grad_alpha = (losses[alpha_idx+1, beta_idx] - losses[alpha_idx, beta_idx]) / (alphas[1] - alphas[0])\n",
    "                grad_beta = (losses[alpha_idx, beta_idx+1] - losses[alpha_idx, beta_idx]) / (betas[1] - betas[0])\n",
    "                \n",
    "                # Update position\n",
    "                new_alpha = current_alpha - learning_rate * grad_alpha\n",
    "                new_beta = current_beta - learning_rate * grad_beta\n",
    "                \n",
    "                # Keep within bounds\n",
    "                new_alpha = np.clip(new_alpha, alphas[0], alphas[-1])\n",
    "                new_beta = np.clip(new_beta, betas[0], betas[-1])\n",
    "                \n",
    "                path_alpha.append(new_alpha)\n",
    "                path_beta.append(new_beta)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Plot gradient descent path\n",
    "        ax.plot(path_alpha, path_beta, 'y-', linewidth=3, alpha=0.8, \n",
    "               label='Simulated GD Path')\n",
    "        ax.plot(path_alpha[-1], path_beta[-1], 'yo', markersize=8, \n",
    "               markeredgecolor='orange')\n",
    "    \n",
    "    def _plot_loss_cross_sections(self, ax, alphas, betas, losses):\n",
    "        \"\"\"Plot loss cross-sections along principal directions.\"\"\"\n",
    "        # Cross-section along direction 1 (beta=0)\n",
    "        center_beta_idx = len(betas) // 2\n",
    "        loss_alpha = losses[:, center_beta_idx]\n",
    "        \n",
    "        # Cross-section along direction 2 (alpha=0)\n",
    "        center_alpha_idx = len(alphas) // 2\n",
    "        loss_beta = losses[center_alpha_idx, :]\n",
    "        \n",
    "        # Plot both cross-sections\n",
    "        ax.plot(alphas, loss_alpha, 'b-', linewidth=3, label='Direction 1 (Œ±)', marker='o', markersize=4)\n",
    "        ax.plot(betas, loss_beta, 'r-', linewidth=3, label='Direction 2 (Œ≤)', marker='s', markersize=4)\n",
    "        \n",
    "        # Mark current position\n",
    "        current_loss = self.landscape_data['original_loss']\n",
    "        ax.axvline(x=0, color='gray', linestyle='--', alpha=0.7, linewidth=2)\n",
    "        ax.axhline(y=current_loss, color='gray', linestyle='--', alpha=0.7, linewidth=2,\n",
    "                  label=f'Current Loss: {current_loss:.4f}')\n",
    "        \n",
    "        # Mark minima in each direction\n",
    "        min_alpha_idx = np.argmin(loss_alpha)\n",
    "        min_beta_idx = np.argmin(loss_beta)\n",
    "        \n",
    "        ax.plot(alphas[min_alpha_idx], loss_alpha[min_alpha_idx], 'bo', markersize=10, \n",
    "               markerfacecolor='lightblue', markeredgecolor='darkblue')\n",
    "        ax.plot(betas[min_beta_idx], loss_beta[min_beta_idx], 'ro', markersize=10,\n",
    "               markerfacecolor='lightcoral', markeredgecolor='darkred')\n",
    "        \n",
    "        ax.set_xlabel('Perturbation Amount', fontweight='bold')\n",
    "        ax.set_ylabel('Loss', fontweight='bold')\n",
    "        ax.set_title('Loss Cross-Sections Along Principal Directions', fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add curvature information\n",
    "        alpha_curvature = np.mean(np.diff(loss_alpha, 2))\n",
    "        beta_curvature = np.mean(np.diff(loss_beta, 2))\n",
    "        \n",
    "        ax.text(0.02, 0.98, f'Curvature Œ±: {alpha_curvature:.3f}\\nCurvature Œ≤: {beta_curvature:.3f}',\n",
    "               transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    def _plot_gradient_magnitude_heatmap(self, ax, alphas, betas, gradient_norms):\n",
    "        \"\"\"Plot gradient magnitude heatmap.\"\"\"\n",
    "        if np.any(gradient_norms > 0):\n",
    "            A, B = np.meshgrid(alphas, betas)\n",
    "            \n",
    "            # Create heatmap\n",
    "            heatmap = ax.imshow(gradient_norms.T, extent=[alphas[0], alphas[-1], betas[0], betas[-1]], \n",
    "                               origin='lower', cmap='plasma', aspect='auto')\n",
    "            \n",
    "            # Add contours\n",
    "            contour = ax.contour(A, B, gradient_norms.T, levels=8, colors='white', alpha=0.5, linewidths=1)\n",
    "            ax.clabel(contour, inline=True, fontsize=8, fmt='%.2f')\n",
    "            \n",
    "            # Mark current position\n",
    "            ax.plot(0, 0, 'wo', markersize=10, markeredgecolor='black', linewidth=2,\n",
    "                   label='Current Position')\n",
    "            \n",
    "            plt.colorbar(heatmap, ax=ax, label='Gradient Magnitude')\n",
    "            ax.set_xlabel('Direction 1 (Œ±)', fontweight='bold')\n",
    "            ax.set_ylabel('Direction 2 (Œ≤)', fontweight='bold')\n",
    "            ax.set_title('Gradient Magnitude Landscape', fontweight='bold')\n",
    "            ax.legend()\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Gradient data not available\\n(Enable for small landscapes)', \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_title('Gradient Magnitude Analysis', fontweight='bold')\n",
    "    \n",
    "    def _plot_loss_statistics(self, ax, losses):\n",
    "        \"\"\"Plot loss distribution statistics.\"\"\"\n",
    "        loss_flat = losses.flatten()\n",
    "        \n",
    "        # Create histogram\n",
    "        n, bins, patches = ax.hist(loss_flat, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        \n",
    "        # Add statistics\n",
    "        mean_loss = np.mean(loss_flat)\n",
    "        median_loss = np.median(loss_flat)\n",
    "        std_loss = np.std(loss_flat)\n",
    "        \n",
    "        ax.axvline(mean_loss, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_loss:.4f}')\n",
    "        ax.axvline(median_loss, color='orange', linestyle='--', linewidth=2, label=f'Median: {median_loss:.4f}')\n",
    "        ax.axvline(self.landscape_data['original_loss'], color='green', linestyle='-', linewidth=3,\n",
    "                  label=f'Current: {self.landscape_data[\"original_loss\"]:.4f}')\n",
    "        \n",
    "        ax.set_xlabel('Loss Value', fontweight='bold')\n",
    "        ax.set_ylabel('Frequency', fontweight='bold')\n",
    "        ax.set_title('Loss Distribution', fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add percentile information\n",
    "        percentiles = [10, 25, 75, 90]\n",
    "        perc_values = np.percentile(loss_flat, percentiles)\n",
    "        \n",
    "        info_text = f'Std: {std_loss:.4f}\\n'\n",
    "        for p, v in zip(percentiles, perc_values):\n",
    "            info_text += f'P{p}: {v:.4f}\\n'\n",
    "        \n",
    "        ax.text(0.98, 0.98, info_text, transform=ax.transAxes, fontsize=9,\n",
    "               verticalalignment='top', horizontalalignment='right',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    def _plot_curvature_analysis(self, ax, alphas, betas, losses):\n",
    "        \"\"\"Plot curvature analysis of the loss landscape.\"\"\"\n",
    "        # Compute second derivatives (curvature)\n",
    "        d2_alpha = np.diff(losses, 2, axis=0)\n",
    "        d2_beta = np.diff(losses, 2, axis=1)\n",
    "        \n",
    "        # Mixed derivative\n",
    "        d_alpha = np.diff(losses, 1, axis=0)\n",
    "        d2_alpha_beta = np.diff(d_alpha, 1, axis=1)\n",
    "        \n",
    "        # Average curvatures\n",
    "        curvature_stats = {\n",
    "            'Alpha direction': np.mean(d2_alpha),\n",
    "            'Beta direction': np.mean(d2_beta),\n",
    "            'Mixed (Œ±,Œ≤)': np.mean(d2_alpha_beta),\n",
    "            'Total curvature': np.mean(np.abs(d2_alpha)) + np.mean(np.abs(d2_beta))\n",
    "        }\n",
    "        \n",
    "        # Create bar plot\n",
    "        names = list(curvature_stats.keys())\n",
    "        values = list(curvature_stats.values())\n",
    "        colors = ['blue', 'red', 'green', 'purple']\n",
    "        \n",
    "        bars = ax.bar(names, values, color=colors, alpha=0.7)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01 * max(np.abs(values)),\n",
    "                   f'{value:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        ax.set_ylabel('Curvature Value', fontweight='bold')\n",
    "        ax.set_title('Landscape Curvature Analysis', fontweight='bold')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Add interpretation\n",
    "        avg_curvature = np.mean([abs(v) for v in values[:3]])\n",
    "        if avg_curvature > 0.1:\n",
    "            interpretation = \"High curvature\\n(Challenging optimization)\"\n",
    "        elif avg_curvature > 0.01:\n",
    "            interpretation = \"Moderate curvature\\n(Standard optimization)\"\n",
    "        else:\n",
    "            interpretation = \"Low curvature\\n(Smooth optimization)\"\n",
    "        \n",
    "        ax.text(0.98, 0.02, interpretation, transform=ax.transAxes, fontsize=10,\n",
    "               verticalalignment='bottom', horizontalalignment='right',\n",
    "               bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
    "    \n",
    "    def _plot_optimization_difficulty(self, ax, losses, gradient_norms):\n",
    "        \"\"\"Plot optimization difficulty assessment.\"\"\"\n",
    "        # Calculate various difficulty metrics\n",
    "        loss_range = np.max(losses) - np.min(losses)\n",
    "        loss_variance = np.var(losses)\n",
    "        \n",
    "        # Local minima detection (simplified)\n",
    "        local_minima_count = 0\n",
    "        flat_losses = losses.flatten()\n",
    "        for i in range(1, len(flat_losses) - 1):\n",
    "            if flat_losses[i] < flat_losses[i-1] and flat_losses[i] < flat_losses[i+1]:\n",
    "                local_minima_count += 1\n",
    "        \n",
    "        # Gradient information if available\n",
    "        if np.any(gradient_norms > 0):\n",
    "            avg_grad_norm = np.mean(gradient_norms[gradient_norms > 0])\n",
    "            grad_variance = np.var(gradient_norms[gradient_norms > 0])\n",
    "        else:\n",
    "            avg_grad_norm = 0\n",
    "            grad_variance = 0\n",
    "        \n",
    "        # Create difficulty metrics\n",
    "        difficulty_metrics = {\n",
    "            'Loss Range': loss_range,\n",
    "            'Loss Variance': loss_variance,\n",
    "            'Local Minima': local_minima_count,\n",
    "            'Avg Gradient': avg_grad_norm,\n",
    "            'Grad Variance': grad_variance\n",
    "        }\n",
    "        \n",
    "        # Normalize metrics for visualization\n",
    "        max_values = {\n",
    "            'Loss Range': loss_range,\n",
    "            'Loss Variance': max(loss_variance, 1),\n",
    "            'Local Minima': max(local_minima_count, 1),\n",
    "            'Avg Gradient': max(avg_grad_norm, 1),\n",
    "            'Grad Variance': max(grad_variance, 1)\n",
    "        }\n",
    "        \n",
    "        normalized_metrics = {k: v / max_values[k] for k, v in difficulty_metrics.items()}\n",
    "        \n",
    "        # Create radar chart\n",
    "        angles = np.linspace(0, 2 * np.pi, len(normalized_metrics), endpoint=False)\n",
    "        values = list(normalized_metrics.values())\n",
    "        \n",
    "        # Close the plot\n",
    "        angles = np.concatenate((angles, [angles[0]]))\n",
    "        values = np.concatenate((values, [values[0]]))\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, color='red', alpha=0.8)\n",
    "        ax.fill(angles, values, alpha=0.25, color='red')\n",
    "        \n",
    "        # Add labels\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(list(normalized_metrics.keys()), fontsize=10)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title('Optimization Difficulty Assessment', fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add difficulty score\n",
    "        difficulty_score = np.mean(values[:-1])\n",
    "        if difficulty_score > 0.7:\n",
    "            difficulty_level = \"Very Hard\"\n",
    "            color = 'red'\n",
    "        elif difficulty_score > 0.5:\n",
    "            difficulty_level = \"Hard\"\n",
    "            color = 'orange'\n",
    "        elif difficulty_score > 0.3:\n",
    "            difficulty_level = \"Moderate\"\n",
    "            color = 'yellow'\n",
    "        else:\n",
    "            difficulty_level = \"Easy\"\n",
    "            color = 'green'\n",
    "        \n",
    "        ax.text(0.02, 0.98, f'Difficulty: {difficulty_level}\\nScore: {difficulty_score:.2f}',\n",
    "               transform=ax.transAxes, fontsize=12, fontweight='bold',\n",
    "               verticalalignment='top', color=color,\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    def _plot_landscape_summary(self, ax):\n",
    "        \"\"\"Plot comprehensive landscape summary.\"\"\"\n",
    "        summary_data = self.landscape_data\n",
    "        \n",
    "        # Create summary text\n",
    "        summary_text = f\"\"\"\n",
    "üèîÔ∏è LANDSCAPE SUMMARY\n",
    "\n",
    "üìä Loss Statistics:\n",
    "  Current Loss: {summary_data['original_loss']:.4f}\n",
    "  Minimum Loss: {summary_data['min_loss']:.4f}\n",
    "  Maximum Loss: {summary_data['max_loss']:.4f}\n",
    "  Loss Range: {summary_data['max_loss'] - summary_data['min_loss']:.4f}\n",
    "\n",
    "üìà Optimization Insights:\n",
    "  ‚Ä¢ Loss landscape explored in 2D slice\n",
    "  ‚Ä¢ Current position marked in red\n",
    "  ‚Ä¢ Gradient flow arrows shown\n",
    "  ‚Ä¢ Local minima identified\n",
    "\n",
    "üéØ Recommendations:\n",
    "  ‚Ä¢ Monitor gradient magnitudes\n",
    "  ‚Ä¢ Consider adaptive learning rates\n",
    "  ‚Ä¢ Watch for vanishing gradients\n",
    "  ‚Ä¢ Use momentum for escaping valleys\n",
    "        \"\"\"\n",
    "        \n",
    "        ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=10,\n",
    "               verticalalignment='top', horizontalalignment='left',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "        \n",
    "        ax.set_title('Analysis Summary', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    def track_optimization_path(self, optimizer, num_steps=50, save_interval=5):\n",
    "        \"\"\"Track optimization path on the loss landscape.\"\"\"\n",
    "        print(f\"üöÄ Tracking optimization path for {num_steps} steps...\")\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.optimization_history = []\n",
    "        param_history = []\n",
    "        loss_history = []\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            # Store current state\n",
    "            current_params = self._get_parameters().clone()\n",
    "            param_history.append(current_params)\n",
    "            \n",
    "            # Training step\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch_data, batch_target in self.data_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(batch_data)\n",
    "                loss = self.loss_fn(output, batch_target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(self.data_loader)\n",
    "            loss_history.append(avg_loss)\n",
    "            \n",
    "            # Save state periodically\n",
    "            if step % save_interval == 0:\n",
    "                self.optimization_history.append({\n",
    "                    'step': step,\n",
    "                    'loss': avg_loss,\n",
    "                    'parameters': current_params.clone()\n",
    "                })\n",
    "                print(f\"  Step {step}: Loss = {avg_loss:.6f}\")\n",
    "        \n",
    "        return param_history, loss_history\n",
    "    \n",
    "    def visualize_optimization_path(self, param_history, loss_history, figsize=(16, 12)):\n",
    "        \"\"\"Visualize the optimization path on the loss landscape.\"\"\"\n",
    "        if not hasattr(self, 'landscape_data') or not self.landscape_data:\n",
    "            print(\"‚ö†Ô∏è No landscape data available. Generate landscape first.\")\n",
    "            return None\n",
    "        \n",
    "        # Project parameter history onto the 2D landscape directions\n",
    "        direction1 = self.landscape_data['direction1']\n",
    "        direction2 = self.landscape_data['direction2']\n",
    "        \n",
    "        path_alphas = []\n",
    "        path_betas = []\n",
    "        \n",
    "        for params in param_history:\n",
    "            param_diff = params - self.original_params\n",
    "            alpha = torch.dot(param_diff, direction1).item()\n",
    "            beta = torch.dot(param_diff, direction2).item()\n",
    "            path_alphas.append(alpha)\n",
    "            path_betas.append(beta)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        # 1. Optimization path on contour plot\n",
    "        alphas = self.landscape_data['alphas']\n",
    "        betas = self.landscape_data['betas']\n",
    "        losses = self.landscape_data['losses']\n",
    "        \n",
    "        A, B = np.meshgrid(alphas, betas)\n",
    "        contour = ax1.contour(A, B, losses.T, levels=15, alpha=0.6)\n",
    "        ax1.clabel(contour, inline=True, fontsize=8)\n",
    "        \n",
    "        # Plot optimization path\n",
    "        path_colors = plt.cm.viridis(np.linspace(0, 1, len(path_alphas)))\n",
    "        \n",
    "        for i in range(len(path_alphas) - 1):\n",
    "            ax1.plot([path_alphas[i], path_alphas[i+1]], [path_betas[i], path_betas[i+1]], \n",
    "                    color=path_colors[i], linewidth=2, alpha=0.8)\n",
    "        \n",
    "        # Mark start and end\n",
    "        ax1.plot(path_alphas[0], path_betas[0], 'go', markersize=12, label='Start')\n",
    "        ax1.plot(path_alphas[-1], path_betas[-1], 'ro', markersize=12, label='End')\n",
    "        \n",
    "        ax1.set_xlabel('Direction 1 (Œ±)')\n",
    "        ax1.set_ylabel('Direction 2 (Œ≤)')\n",
    "        ax1.set_title('Optimization Path on Loss Landscape')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Loss over time\n",
    "        ax2.plot(loss_history, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "        ax2.set_xlabel('Optimization Step')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.set_title('Loss Convergence')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add exponential fit\n",
    "        if len(loss_history) > 10:\n",
    "            steps = np.arange(len(loss_history))\n",
    "            try:\n",
    "                # Fit exponential decay\n",
    "                from scipy.optimize import curve_fit\n",
    "                def exp_decay(x, a, b, c):\n",
    "                    return a * np.exp(-b * x) + c\n",
    "                \n",
    "                popt, _ = curve_fit(exp_decay, steps, loss_history, maxfev=1000)\n",
    "                ax2.plot(steps, exp_decay(steps, *popt), 'r--', alpha=0.8, \n",
    "                        label=f'Exp fit: {popt[0]:.3f}*exp(-{popt[1]:.3f}*x)+{popt[2]:.3f}')\n",
    "                ax2.legend()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # 3. Parameter space trajectory\n",
    "        param_norms = [torch.norm(params - self.original_params).item() for params in param_history]\n",
    "        \n",
    "        ax3.plot(param_norms, 'g-', linewidth=2, marker='s', markersize=4)\n",
    "        ax3.set_xlabel('Optimization Step')\n",
    "        ax3.set_ylabel('Parameter Distance from Start')\n",
    "        ax3.set_title('Parameter Space Movement')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Learning rate analysis\n",
    "        if len(loss_history) > 1:\n",
    "            loss_changes = np.diff(loss_history)\n",
    "            param_changes = np.diff(param_norms)\n",
    "            \n",
    "            ax4.scatter(param_changes[:-1], loss_changes[:-1], alpha=0.6, s=30)\n",
    "            ax4.set_xlabel('Parameter Change')\n",
    "            ax4.set_ylabel('Loss Change')\n",
    "            ax4.set_title('Parameter vs Loss Changes')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add trend line\n",
    "            if len(param_changes) > 2:\n",
    "                z = np.polyfit(param_changes[:-1], loss_changes[:-1], 1)\n",
    "                p = np.poly1d(z)\n",
    "                x_trend = np.linspace(min(param_changes), max(param_changes), 100)\n",
    "                ax4.plot(x_trend, p(x_trend), \"r--\", alpha=0.8)\n",
    "        \n",
    "        plt.suptitle('Optimization Path Analysis', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save visualization\n",
    "        plt.savefig(results_dir / 'optimization_path_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def save_landscape_analysis(self):\n",
    "        \"\"\"Save comprehensive landscape analysis to JSON.\"\"\"\n",
    "        # Prepare serializable data\n",
    "        analysis_data = {\n",
    "            'landscape_statistics': {\n",
    "                'original_loss': float(self.landscape_data['original_loss']),\n",
    "                'min_loss': float(self.landscape_data['min_loss']),\n",
    "                'max_loss': float(self.landscape_data['max_loss']),\n",
    "                'loss_range': float(self.landscape_data['max_loss'] - self.landscape_data['min_loss']),\n",
    "                'landscape_shape': self.landscape_data['losses'].shape\n",
    "            },\n",
    "            'optimization_metrics': {\n",
    "                'parameter_count': len(self.original_params),\n",
    "                'directions_explored': 2,\n",
    "                'resolution': len(self.landscape_data['alphas'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add optimization history if available\n",
    "        if self.optimization_history:\n",
    "            analysis_data['optimization_history'] = [\n",
    "                {\n",
    "                    'step': entry['step'],\n",
    "                    'loss': float(entry['loss'])\n",
    "                }\n",
    "                for entry in self.optimization_history\n",
    "            ]\n",
    "        \n",
    "        with open(results_dir / 'loss_landscape_analysis.json', 'w') as f:\n",
    "            json.dump(analysis_data, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Loss landscape analysis saved to {results_dir / 'loss_landscape_analysis.json'}\")\n",
    "        return analysis_data\n",
    "\n",
    "def demonstrate_loss_landscape_analysis():\n",
    "    \"\"\"Demonstrate loss landscape visualization with different scenarios.\"\"\"\n",
    "    print(\"\\nüèîÔ∏è Loss Landscape Analysis Demonstrations\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create sample dataset\n",
    "    torch.manual_seed(42)\n",
    "    X = torch.randn(100, 8)\n",
    "    y = torch.sum(X[:, :4], dim=1, keepdim=True) + 0.1 * torch.randn(100, 1)\n",
    "    dataset = TensorDataset(X, y)\n",
    "    data_loader = DataLoader(dataset, batch_size=20, shuffle=True)\n",
    "    \n",
    "    # Example 1: Simple Linear Model\n",
    "    print(\"\\nüìà Example 1: Simple Linear Model Landscape\")\n",
    "    \n",
    "    class SimpleLinearModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.linear = nn.Linear(8, 1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.linear(x)\n",
    "    \n",
    "    simple_model = SimpleLinearModel()\n",
    "    simple_visualizer = LossLandscapeVisualizer(simple_model, data_loader, nn.MSELoss())\n",
    "    \n",
    "    # Generate landscape analysis\n",
    "    simple_fig, simple_landscape = simple_visualizer.create_comprehensive_landscape_analysis()\n",
    "    simple_analysis = simple_visualizer.save_landscape_analysis()\n",
    "    \n",
    "    print(f\"Simple Model Landscape:\")\n",
    "    print(f\"  - Loss range: {simple_landscape['max_loss'] - simple_landscape['min_loss']:.4f}\")\n",
    "    print(f\"  - Current loss: {simple_landscape['original_loss']:.4f}\")\n",
    "    \n",
    "    # Example 2: Deep Nonlinear Model\n",
    "    print(\"\\nüß† Example 2: Deep Nonlinear Model Landscape\")\n",
    "    \n",
    "    class DeepNonlinearModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(8, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 12),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(12, 8),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(8, 1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "    \n",
    "    deep_model = DeepNonlinearModel()\n",
    "    deep_visualizer = LossLandscapeVisualizer(deep_model, data_loader, nn.MSELoss())\n",
    "    \n",
    "    # Generate landscape analysis\n",
    "    deep_fig, deep_landscape = deep_visualizer.create_comprehensive_landscape_analysis()\n",
    "    deep_analysis = deep_visualizer.save_landscape_analysis()\n",
    "    \n",
    "    print(f\"Deep Model Landscape:\")\n",
    "    print(f\"  - Loss range: {deep_landscape['max_loss'] - deep_landscape['min_loss']:.4f}\")\n",
    "    print(f\"  - Current loss: {deep_landscape['original_loss']:.4f}\")\n",
    "    \n",
    "    # Example 3: Optimization Path Tracking\n",
    "    print(\"\\nüöÄ Example 3: Optimization Path Tracking\")\n",
    "    \n",
    "    # Train the deep model and track optimization path\n",
    "    optimizer = torch.optim.SGD(deep_model.parameters(), lr=0.01, momentum=0.9)\n",
    "    param_hist, loss_hist = deep_visualizer.track_optimization_path(optimizer, num_steps=30)\n",
    "    \n",
    "    # Visualize optimization path\n",
    "    path_fig = deep_visualizer.visualize_optimization_path(param_hist, loss_hist)\n",
    "    \n",
    "    print(f\"Optimization tracking:\")\n",
    "    print(f\"  - Initial loss: {loss_hist[0]:.6f}\")\n",
    "    print(f\"  - Final loss: {loss_hist[-1]:.6f}\")\n",
    "    print(f\"  - Improvement: {loss_hist[0] - loss_hist[-1]:.6f}\")\n",
    "    \n",
    "    return [simple_visualizer, deep_visualizer], [simple_analysis, deep_analysis]\n",
    "\n",
    "# Run loss landscape demonstrations\n",
    "landscape_visualizers, landscape_analyses = demonstrate_loss_landscape_analysis()\n",
    "\n",
    "print(f\"\\nüí° Key Loss Landscape Insights:\")\n",
    "print(\"‚Ä¢ 3D surfaces reveal optimization complexity\")\n",
    "print(\"‚Ä¢ Contour plots show gradient descent paths\")\n",
    "print(\"‚Ä¢ Curvature analysis predicts optimization difficulty\")\n",
    "print(\"‚Ä¢ Real-time tracking enables adaptive strategies\")\n",
    "print(\"‚Ä¢ Cross-sections reveal directional sensitivities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729fba8b",
   "metadata": {},
   "source": [
    "## 5. Advanced Visualization Techniques\n",
    "\n",
    "### 5.1 Activation Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationAnalyzer:\n",
    "    \"\"\"Comprehensive activation pattern analysis and visualization.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.activations = {}\n",
    "        self.activation_stats = {}\n",
    "        self.hooks = []\n",
    "        self.layer_info = {}\n",
    "        \n",
    "        self._register_activation_hooks()\n",
    "        self._analyze_model_architecture()\n",
    "    \n",
    "    def _register_activation_hooks(self):\n",
    "        \"\"\"Register hooks to capture activations from all layers.\"\"\"\n",
    "        \n",
    "        def get_activation_hook(name):\n",
    "            def hook(module, input, output):\n",
    "                if isinstance(output, torch.Tensor):\n",
    "                    activation_data = {\n",
    "                        'tensor': output.detach().clone(),\n",
    "                        'mean': float(output.mean()),\n",
    "                        'std': float(output.std()),\n",
    "                        'min': float(output.min()),\n",
    "                        'max': float(output.max()),\n",
    "                        'shape': tuple(output.shape),\n",
    "                        'sparsity': float((output == 0).float().mean()),\n",
    "                        'saturation': self._compute_saturation(output, module)\n",
    "                    }\n",
    "                    self.activations[name] = activation_data\n",
    "            return hook\n",
    "        \n",
    "        # Register hooks for all layers\n",
    "        for name, module in self.model.named_modules():\n",
    "            if len(list(module.children())) == 0:  # Leaf modules only\n",
    "                hook = module.register_forward_hook(get_activation_hook(name))\n",
    "                self.hooks.append(hook)\n",
    "                \n",
    "                # Store layer information\n",
    "                self.layer_info[name] = {\n",
    "                    'type': module.__class__.__name__,\n",
    "                    'parameters': sum(p.numel() for p in module.parameters()),\n",
    "                    'trainable': sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "                }\n",
    "    \n",
    "    def _analyze_model_architecture(self):\n",
    "        \"\"\"Analyze model architecture for visualization context.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        \n",
    "        self.model_info = {\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'total_layers': len(self.layer_info),\n",
    "            'model_size_mb': total_params * 4 / (1024 * 1024)  # Assuming float32\n",
    "        }\n",
    "    \n",
    "    def _compute_saturation(self, tensor, module):\n",
    "        \"\"\"Compute activation saturation for different activation functions.\"\"\"\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            # For ReLU, saturation is the fraction of zero activations\n",
    "            return float((tensor == 0).float().mean())\n",
    "        elif isinstance(module, nn.Sigmoid):\n",
    "            # For Sigmoid, saturation is fraction near 0 or 1\n",
    "            threshold = 0.01\n",
    "            saturated = ((tensor < threshold) | (tensor > 1 - threshold)).float()\n",
    "            return float(saturated.mean())\n",
    "        elif isinstance(module, nn.Tanh):\n",
    "            # For Tanh, saturation is fraction near -1 or 1\n",
    "            threshold = 0.02\n",
    "            saturated = ((tensor < -1 + threshold) | (tensor > 1 - threshold)).float()\n",
    "            return float(saturated.mean())\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    def capture_activations(self, input_data):\n",
    "        \"\"\"Capture activations for given input data.\"\"\"\n",
    "        self.activations.clear()\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_data)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def create_comprehensive_activation_analysis(self, input_data, figsize=(20, 16)):\n",
    "        \"\"\"Create comprehensive activation analysis dashboard.\"\"\"\n",
    "        \n",
    "        # Capture activations\n",
    "        model_output = self.capture_activations(input_data)\n",
    "        \n",
    "        # Create dashboard\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        gs = fig.add_gridspec(4, 4, hspace=0.4, wspace=0.3)\n",
    "        \n",
    "        # 1. Activation Flow Overview\n",
    "        ax1 = fig.add_subplot(gs[0, :2])\n",
    "        self._plot_activation_flow_overview(ax1)\n",
    "        \n",
    "        # 2. Activation Statistics Summary\n",
    "        ax2 = fig.add_subplot(gs[0, 2:])\n",
    "        self._plot_activation_statistics_summary(ax2)\n",
    "        \n",
    "        # 3. Layer-wise Activation Distributions\n",
    "        ax3 = fig.add_subplot(gs[1, :2])\n",
    "        self._plot_activation_distributions(ax3)\n",
    "        \n",
    "        # 4. Activation Saturation Analysis\n",
    "        ax4 = fig.add_subplot(gs[1, 2:])\n",
    "        self._plot_saturation_analysis(ax4)\n",
    "        \n",
    "        # 5. Activation Correlation Matrix\n",
    "        ax5 = fig.add_subplot(gs[2, :2])\n",
    "        self._plot_activation_correlation_matrix(ax5)\n",
    "        \n",
    "        # 6. Dead Neuron Detection\n",
    "        ax6 = fig.add_subplot(gs[2, 2:])\n",
    "        self._plot_dead_neuron_analysis(ax6)\n",
    "        \n",
    "        # 7. Activation Range Analysis\n",
    "        ax7 = fig.add_subplot(gs[3, :2])\n",
    "        self._plot_activation_range_analysis(ax7)\n",
    "        \n",
    "        # 8. Model Architecture Summary\n",
    "        ax8 = fig.add_subplot(gs[3, 2:])\n",
    "        self._plot_model_architecture_summary(ax8)\n",
    "        \n",
    "        plt.suptitle('Comprehensive Activation Pattern Analysis', fontsize=18, fontweight='bold')\n",
    "        \n",
    "        # Save analysis\n",
    "        plt.savefig(results_dir / 'activation_pattern_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return fig, self._compile_activation_analysis()\n",
    "    \n",
    "    def _plot_activation_flow_overview(self, ax):\n",
    "        \"\"\"Plot overview of activation flow through the network.\"\"\"\n",
    "        if not self.activations:\n",
    "            ax.text(0.5, 0.5, 'No activation data', ha='center', va='center')\n",
    "            return\n",
    "        \n",
    "        layer_names = list(self.activations.keys())\n",
    "        activation_means = [self.activations[name]['mean'] for name in layer_names]\n",
    "        activation_stds = [self.activations[name]['std'] for name in layer_names]\n",
    "        \n",
    "        x = np.arange(len(layer_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        # Create grouped bar chart\n",
    "        bars1 = ax.bar(x - width/2, activation_means, width, label='Mean Activation', \n",
    "                      alpha=0.8, color='skyblue')\n",
    "        bars2 = ax.bar(x + width/2, activation_stds, width, label='Std Deviation', \n",
    "                      alpha=0.8, color='lightcoral')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01 * max(activation_means + activation_stds),\n",
    "                       f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        ax.set_xlabel('Layers')\n",
    "        ax.set_ylabel('Activation Values')\n",
    "        ax.set_title('Activation Flow Through Network', fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([name[:10] + '...' if len(name) > 10 else name for name in layer_names], \n",
    "                          rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    def _plot_activation_statistics_summary(self, ax):\n",
    "        \"\"\"Plot summary of activation statistics.\"\"\"\n",
    "        if not self.activations:\n",
    "            ax.text(0.5, 0.5, 'No activation statistics', ha='center', va='center')\n",
    "            return\n",
    "        \n",
    "        # Collect statistics\n",
    "        stats_data = {\n",
    "            'Mean': [act['mean'] for act in self.activations.values()],\n",
    "            'Std': [act['std'] for act in self.activations.values()],\n",
    "            'Min': [act['min'] for act in self.activations.values()],\n",
    "            'Max': [act['max'] for act in self.activations.values()],\n",
    "            'Sparsity': [act['sparsity'] for act in self.activations.values()]\n",
    "        }\n",
    "        \n",
    "        # Create box plots\n",
    "        bp = ax.boxplot([stats_data[key] for key in stats_data.keys()], \n",
    "                       labels=list(stats_data.keys()), patch_artist=True)\n",
    "        \n",
    "        # Color the boxes\n",
    "        colors = ['lightblue', 'lightcoral', 'lightgreen', 'gold', 'plum']\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax.set_ylabel('Values')\n",
    "        ax.set_title('Activation Statistics Distribution', fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add median values as text\n",
    "        for i, (key, values) in enumerate(stats_data.items()):\n",
    "            median_val = np.median(values)\n",
    "            ax.text(i + 1, median_val, f'{median_val:.3f}', ha='center', va='bottom',\n",
    "                   fontweight='bold', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    def _plot_activation_distributions(self, ax):\n",
    "        \"\"\"Plot activation value distributions for each layer.\"\"\"\n",
    "        if not self.activations:\n",
    "            ax.text(0.5, 0.5, 'No activation data', ha='center', va='center')\n",
    "            return\n",
    "        \n",
    "        # Sample a few representative layers\n",
    "        layer_names = list(self.activations.keys())\n",
    "        sample_layers = layer_names[::max(1, len(layer_names)//4)][:4]\n",
    "        \n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(sample_layers)))\n",
    "        \n",
    "        for i, (layer_name, color) in enumerate(zip(sample_layers, colors)):\n",
    "            activation_tensor = self.activations[layer_name]['tensor']\n",
    "            flat_activations = activation_tensor.flatten().cpu().numpy()\n",
    "            \n",
    "            # Sample for visualization if too large\n",
    "            if len(flat_activations) > 10000:\n",
    "                flat_activations = np.random.choice(flat_activations, 10000, replace=False)\n",
    "            \n",
    "            ax.hist(flat_activations, bins=50, alpha=0.6, label=layer_name[:15], \n",
    "                   color=color, density=True)\n",
    "        \n",
    "        ax.set_xlabel('Activation Values')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title('Activation Value Distributions by Layer', fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    def _plot_saturation_analysis(self, ax):\n",
    "        \"\"\"Plot activation saturation analysis.\"\"\"\n",
    "        if not self.activations:\n",
    "            ax.text(0.5, 0.5, 'No saturation data', ha='center', va='center')\n",
    "            return\n",
    "        \n",
    "        layer_names = list(self.activations.keys())\n",
    "        saturations = [self.activations[name]['saturation'] for name in layer_names]\n",
    "        layer_types = [self.layer_info[name]['type'] for name in layer_names]\n",
    "        \n",
    "        # Color by layer type\n",
    "        type_colors = {'Linear': 'blue', 'ReLU': 'red', 'Sigmoid': 'green', \n",
    "                      'Tanh': 'orange', 'Conv2d': 'purple'}\n",
    "        colors = [type_colors.get(layer_type, 'gray') for layer_type in layer_types]\n",
    "        \n",
    "        bars = ax.bar(range(len(saturations)), saturations, color=colors, alpha=0.7)\n",
    "        \n",
    "        # Add threshold line\n",
    "        ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.8, \n",
    "                  label='High Saturation Threshold')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, saturation in zip(bars, saturations):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{saturation:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        ax.set_xlabel('Layers')\n",
    "        ax.set_ylabel('Saturation Level')\n",
    "        ax.set_title('Activation Saturation Analysis', fontweight='bold')\n",
    "        ax.set_xticks(range(len(layer_names)))\n",
    "        ax.set_xticklabels([f\"{name[:8]}\\n({self.layer_info[name]['type']})\" \n",
    "                           for name in layer_names], rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add warning for high saturation\n",
    "        high_saturation_layers = [name for name, sat in zip(layer_names, saturations) if sat > 0.5]\n",
    "        if high_saturation_layers:\n",
    "            ax.text(0.02, 0.98, f'‚ö†Ô∏è High saturation in:\\n{\", \".join(high_saturation_layers[:3])}', \n",
    "                   transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
    "    \n",
    "    def _plot_activation_correlation_matrix(self, ax):\n",
    "        \"\"\"Plot correlation matrix of layer activations.\"\"\"\n",
    "        if len(self.activations) < 2:\n",
    "            ax.text(0.5, 0.5, 'Need at least 2 layers for correlation', ha='center', va='center')\n",
    "            return\n",
    "        \n",
    "        # Compute correlation matrix of layer means\n",
    "        layer_names = list(self.activations.keys())\n",
    "        activation_means = np.array([self.activations[name]['mean'] for name in layer_names])\n",
    "        activation_stds = np.array([self.activations[name]['std'] for name in layer_names])\n",
    "        activation_sparsity = np.array([self.activations[name]['sparsity'] for name in layer_names])\n",
    "        \n",
    "        # Stack features for correlation analysis\n",
    "        features = np.column_stack([activation_means, activation_stds, activation_sparsity])\n",
    "        correlation_matrix = np.corrcoef(features.T)\n",
    "        \n",
    "        # Create heatmap\n",
    "        im = ax.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
    "        \n",
    "        # Add labels\n",
    "        feature_labels = ['Mean', 'Std', 'Sparsity']\n",
    "        ax.set_xticks(range(len(feature_labels)))\n",
    "        ax.set_yticks(range(len(feature_labels)))\n",
    "        ax.set_xticklabels(feature_labels)\n",
    "        ax.set_yticklabels(feature_labels)\n",
    "        \n",
    "        # Add correlation values\n",
    "        for i in range(len(feature_labels)):\n",
    "            for j in range(len(feature_labels)):\n",
    "                text = ax.text(j, i, f'{correlation_matrix[i, j]:.2f}',\n",
    "                             ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "        \n",
    "        ax.set_title('Activation Feature Correlation Matrix', fontweight='bold')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    def _plot_dead_neuron_analysis(self, ax):\n",
    "        \"\"\"Plot dead neuron detection analysis.\"\"\"\n",
    "        if not self.activations:\n",
    "            ax.text(0.5, 0.5, 'No activation data', ha='center', va='center')\n",
    "            return\n",
    "        \n",
    "        layer_names = []\n",
    "        dead_neuron_ratios = []\n",
    "        \n",
    "        for name, activation in self.activations.items():\n",
    "            if activation['tensor'].dim() >= 2:\n",
    "                # Consider neurons dead if they have very low variance across samples\n",
    "                tensor = activation['tensor']\n",
    "                \n",
    "                if tensor.dim() == 2:  # Fully connected layer\n",
    "                    neuron_vars = tensor.var(dim=0)\n",
    "                    dead_threshold = 1e-6\n",
    "                    dead_neurons = (neuron_vars < dead_threshold).float().mean()\n",
    "                elif tensor.dim() == 4:  # Convolutional layer\n",
    "                    # Average over spatial dimensions, then check variance across batch\n",
    "                    spatial_avg = tensor.mean(dim=(2, 3))\n",
    "                    neuron_vars = spatial_avg.var(dim=0)\n",
    "                    dead_threshold = 1e-6\n",
    "                    dead_neurons = (neuron_vars < dead_threshold).float().mean()\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                layer_names.append(name)\n",
    "                dead_neuron_ratios.append(float(dead_neurons))\n",
    "        \n",
    "        if layer_names:\n",
    "            bars = ax.bar(range(len(dead_neuron_ratios)), dead_neuron_ratios, \n",
    "                         color='darkred', alpha=0.7)\n",
    "            \n",
    "            # Add threshold line\n",
    "            ax.axhline(y=0.1, color='orange', linestyle='--', alpha=0.8, \n",
    "                      label='Concerning Threshold (10%)')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, ratio in zip(bars, dead_neuron_ratios):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                       f'{ratio:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "            ax.set_xlabel('Layers')\n",
    "            ax.set_ylabel('Dead Neuron Ratio')\n",
    "            ax.set_title('Dead Neuron Detection', fontweight='bold')\n",
    "            ax.set_xticks(range(len(layer_names)))\n",
    "            ax.set_xticklabels([name[:10] for name in layer_names], rotation=45, ha='right')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add summary\n",
    "            avg_dead_ratio = np.mean(dead_neuron_ratios)\n",
    "            ax.text(0.98, 0.98, f'Avg Dead Ratio: {avg_dead_ratio:.3f}', \n",
    "                   transform=ax.transAxes, fontsize=12, fontweight='bold',\n",
    "                   verticalalignment='top', horizontalalignment='right',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No suitable layers for\\ndead neuron analysis', \n",
    "                   ha='center', va='center')\n",
    "            ax.set_title('Dead Neuron Analysis')\n",
    "    \n",
    "    def _plot_activation_range_analysis(self, ax):\n",
    "        \"\"\"Plot activation range analysis across layers.\"\"\"\n",
    "        if not self.activations:\n",
    "            ax.text(0.5, 0.5, 'No activation data', ha='center', va='center')\n",
    "            return\n",
    "        \n",
    "        layer_names = list(self.activations.keys())\n",
    "        min_values = [self.activations[name]['min'] for name in layer_names]\n",
    "        max_values = [self.activations[name]['max'] for name in layer_names]\n",
    "        ranges = [max_val - min_val for min_val, max_val in zip(min_values, max_values)]\n",
    "        \n",
    "        x = np.arange(len(layer_names))\n",
    "        \n",
    "        # Create range plot\n",
    "        ax.bar(x, ranges, alpha=0.7, color='lightgreen', label='Activation Range')\n",
    "        \n",
    "        # Add min/max markers\n",
    "        ax.scatter(x, min_values, color='blue', s=50, alpha=0.8, label='Min Values', marker='v')\n",
    "        ax.scatter(x, max_values, color='red', s=50, alpha=0.8, label='Max Values', marker='^')\n",
    "        \n",
    "        # Connect min and max with lines\n",
    "        for i, (min_val, max_val) in enumerate(zip(min_values, max_values)):\n",
    "            ax.plot([i, i], [min_val, max_val], 'k-', alpha=0.3, linewidth=1)\n",
    "        \n",
    "        ax.set_xlabel('Layers')\n",
    "        ax.set_ylabel('Activation Values')\n",
    "        ax.set_title('Activation Range Analysis', fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([name[:10] for name in layer_names], rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add range statistics\n",
    "        mean_range = np.mean(ranges)\n",
    "        std_range = np.std(ranges)\n",
    "        ax.text(0.02, 0.98, f'Mean Range: {mean_range:.3f}\\nStd Range: {std_range:.3f}', \n",
    "               transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    def _plot_model_architecture_summary(self, ax):\n",
    "        \"\"\"Plot model architecture summary.\"\"\"\n",
    "        summary_text = f\"\"\"\n",
    "üß† MODEL ARCHITECTURE SUMMARY\n",
    "\n",
    "üìä Model Statistics:\n",
    "  Total Parameters: {self.model_info['total_parameters']:,}\n",
    "  Trainable Parameters: {self.model_info['trainable_parameters']:,}\n",
    "  Total Layers: {self.model_info['total_layers']}\n",
    "  Model Size: {self.model_info['model_size_mb']:.2f} MB\n",
    "\n",
    "üîç Activation Analysis:\n",
    "  Layers Analyzed: {len(self.activations)}\n",
    "  \n",
    "üìà Health Indicators:\n",
    "  ‚Ä¢ Check activation ranges\n",
    "  ‚Ä¢ Monitor saturation levels\n",
    "  ‚Ä¢ Watch for dead neurons\n",
    "  ‚Ä¢ Analyze activation flow\n",
    "\n",
    "üéØ Optimization Tips:\n",
    "  ‚Ä¢ Normalize inputs appropriately\n",
    "  ‚Ä¢ Use proper initialization\n",
    "  ‚Ä¢ Consider different activations\n",
    "  ‚Ä¢ Monitor gradient flow\n",
    "        \"\"\"\n",
    "        \n",
    "        ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=10,\n",
    "               verticalalignment='top', horizontalalignment='left',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightcyan', alpha=0.8))\n",
    "        \n",
    "        ax.set_title('Model & Analysis Summary', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    def _compile_activation_analysis(self):\n",
    "        \"\"\"Compile comprehensive activation analysis results.\"\"\"\n",
    "        if not self.activations:\n",
    "            return {}\n",
    "        \n",
    "        results = {\n",
    "            'model_info': self.model_info,\n",
    "            'activation_summary': {\n",
    "                'total_layers_analyzed': len(self.activations),\n",
    "                'mean_activation': np.mean([act['mean'] for act in self.activations.values()]),\n",
    "                'mean_sparsity': np.mean([act['sparsity'] for act in self.activations.values()]),\n",
    "                'mean_saturation': np.mean([act['saturation'] for act in self.activations.values()]),\n",
    "            },\n",
    "            'layer_analysis': {}\n",
    "        }\n",
    "        \n",
    "        # Per-layer analysis\n",
    "        for name, activation in self.activations.items():\n",
    "            results['layer_analysis'][name] = {\n",
    "                'layer_type': self.layer_info[name]['type'],\n",
    "                'activation_stats': {\n",
    "                    'mean': activation['mean'],\n",
    "                    'std': activation['std'],\n",
    "                    'min': activation['min'],\n",
    "                    'max': activation['max'],\n",
    "                    'sparsity': activation['sparsity'],\n",
    "                    'saturation': activation['saturation']\n",
    "                },\n",
    "                'shape': activation['shape'],\n",
    "                'parameters': self.layer_info[name]['parameters']\n",
    "            }\n",
    "        \n",
    "        # Health metrics\n",
    "        saturations = [act['saturation'] for act in self.activations.values()]\n",
    "        sparsities = [act['sparsity'] for act in self.activations.values()]\n",
    "        \n",
    "        results['health_metrics'] = {\n",
    "            'high_saturation_layers': sum(1 for s in saturations if s > 0.5),\n",
    "            'high_sparsity_layers': sum(1 for s in sparsities if s > 0.8),\n",
    "            'average_saturation': np.mean(saturations),\n",
    "            'average_sparsity': np.mean(sparsities),\n",
    "            'activation_health_score': self._compute_activation_health_score()\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _compute_activation_health_score(self):\n",
    "        \"\"\"Compute overall activation health score.\"\"\"\n",
    "        if not self.activations:\n",
    "            return 0.0\n",
    "        \n",
    "        # Factors for health score\n",
    "        saturations = [act['saturation'] for act in self.activations.values()]\n",
    "        sparsities = [act['sparsity'] for act in self.activations.values()]\n",
    "        ranges = [act['max'] - act['min'] for act in self.activations.values()]\n",
    "        \n",
    "        # Penalize high saturation and extreme sparsity\n",
    "        saturation_penalty = np.mean([min(s * 2, 1.0) for s in saturations])\n",
    "        sparsity_penalty = np.mean([min((s - 0.5) * 2, 1.0) if s > 0.5 else 0 for s in sparsities])\n",
    "        \n",
    "        # Reward reasonable activation ranges\n",
    "        range_score = np.mean([min(r / 10, 1.0) for r in ranges])\n",
    "        \n",
    "        # Compute overall health score (0-100)\n",
    "        health_score = (1 - saturation_penalty - sparsity_penalty + range_score) / 2 * 100\n",
    "        return max(0, min(100, health_score))\n",
    "    \n",
    "    def save_analysis(self, results):\n",
    "        \"\"\"Save activation analysis to JSON.\"\"\"\n",
    "        with open(results_dir / 'activation_pattern_analysis.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Activation analysis saved to {results_dir / 'activation_pattern_analysis.json'}\")\n",
    "        return results\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove hooks to prevent memory leaks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "\n",
    "def demonstrate_activation_analysis():\n",
    "    \"\"\"Demonstrate activation pattern analysis with different models.\"\"\"\n",
    "    print(\"\\nüî¨ Activation Pattern Analysis Demonstrations\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create sample data\n",
    "    sample_input = torch.randn(10, 16)\n",
    "    \n",
    "    # Example 1: Healthy Network\n",
    "    print(\"\\n‚úÖ Example 1: Healthy Network Activations\")\n",
    "    \n",
    "    class HealthyActivationNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(16, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 24),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(24, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 8)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "    \n",
    "    healthy_net = HealthyActivationNet()\n",
    "    healthy_analyzer = ActivationAnalyzer(healthy_net)\n",
    "    \n",
    "    healthy_fig, healthy_results = healthy_analyzer.create_comprehensive_activation_analysis(sample_input)\n",
    "    healthy_analyzer.save_analysis(healthy_results)\n",
    "    \n",
    "    print(f\"Healthy Network Activation Health Score: {healthy_results['health_metrics']['activation_health_score']:.1f}\")\n",
    "    \n",
    "    # Example 2: Problematic Network\n",
    "    print(\"\\n‚ö†Ô∏è Example 2: Problematic Network Activations\")\n",
    "    \n",
    "    class ProblematicActivationNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(16, 64),\n",
    "                nn.Sigmoid(),  # Can cause saturation\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(32, 8)\n",
    "            )\n",
    "            \n",
    "            # Initialize with large weights to cause saturation\n",
    "            for layer in self.layers:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.normal_(layer.weight, 0, 2.0)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "    \n",
    "    problem_net = ProblematicActivationNet()\n",
    "    problem_analyzer = ActivationAnalyzer(problem_net)\n",
    "    \n",
    "    problem_fig, problem_results = problem_analyzer.create_comprehensive_activation_analysis(sample_input)\n",
    "    problem_analyzer.save_analysis(problem_results)\n",
    "    \n",
    "    print(f\"Problematic Network Activation Health Score: {problem_results['health_metrics']['activation_health_score']:.1f}\")\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\nüìä Activation Analysis Comparison:\")\n",
    "    print(f\"Healthy Network:\")\n",
    "    print(f\"  - Health Score: {healthy_results['health_metrics']['activation_health_score']:.1f}\")\n",
    "    print(f\"  - Average Saturation: {healthy_results['health_metrics']['average_saturation']:.3f}\")\n",
    "    print(f\"  - Average Sparsity: {healthy_results['health_metrics']['average_sparsity']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nProblematic Network:\")\n",
    "    print(f\"  - Health Score: {problem_results['health_metrics']['activation_health_score']:.1f}\")\n",
    "    print(f\"  - Average Saturation: {problem_results['health_metrics']['average_saturation']:.3f}\")\n",
    "    print(f\"  - Average Sparsity: {problem_results['health_metrics']['average_sparsity']:.3f}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    healthy_analyzer.cleanup()\n",
    "    problem_analyzer.cleanup()\n",
    "    \n",
    "    return [healthy_analyzer, problem_analyzer], [healthy_results, problem_results]\n",
    "\n",
    "# Run activation analysis demonstrations\n",
    "activation_analyzers, activation_results = demonstrate_activation_analysis()\n",
    "\n",
    "print(f\"\\nüí° Key Activation Analysis Insights:\")\n",
    "print(\"‚Ä¢ Activation patterns reveal network health\")\n",
    "print(\"‚Ä¢ Saturation analysis identifies problematic layers\")\n",
    "print(\"‚Ä¢ Dead neuron detection prevents wasted capacity\")\n",
    "print(\"‚Ä¢ Range analysis guides initialization strategies\")\n",
    "print(\"‚Ä¢ Health scores provide quantitative assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ede9a",
   "metadata": {},
   "source": [
    "## 6. Training Dynamics Monitoring\n",
    "\n",
    "### 6.1 Real-Time Training Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19814050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDynamicsMonitor:\n",
    "    \"\"\"Comprehensive real-time training dynamics monitoring and visualization.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, optimizer, loss_fn):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "        # Training history\n",
    "        self.training_history = {\n",
    "            'epoch': [],\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'learning_rate': [],\n",
    "            'grad_norm': [],\n",
    "            'param_norm': [],\n",
    "            'weight_updates': [],\n",
    "            'batch_times': [],\n",
    "            'memory_usage': []\n",
    "        }\n",
    "        \n",
    "        # Per-layer monitoring\n",
    "        self.layer_history = defaultdict(lambda: {\n",
    "            'weight_norms': [],\n",
    "            'grad_norms': [],\n",
    "            'weight_changes': [],\n",
    "            'learning_rates': []\n",
    "        })\n",
    "        \n",
    "        # Real-time metrics\n",
    "        self.current_metrics = {}\n",
    "        self.optimization_health = {}\n",
    "        \n",
    "    def monitor_training_epoch(self, epoch, max_epochs):\n",
    "        \"\"\"Monitor one epoch of training with comprehensive metrics.\"\"\"\n",
    "        \n",
    "        # Training phase\n",
    "        train_metrics = self._monitor_training_phase(epoch)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_metrics = self._monitor_validation_phase(epoch)\n",
    "        \n",
    "        # Update histories\n",
    "        self._update_training_history(epoch, train_metrics, val_metrics)\n",
    "        \n",
    "        # Compute optimization health metrics\n",
    "        self._compute_optimization_health(epoch)\n",
    "        \n",
    "        # Print progress\n",
    "        self._print_epoch_summary(epoch, max_epochs, train_metrics, val_metrics)\n",
    "        \n",
    "        return train_metrics, val_metrics\n",
    "    \n",
    "    def _monitor_training_phase(self, epoch):\n",
    "        \"\"\"Monitor training phase with detailed metrics.\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        batch_losses = []\n",
    "        batch_times = []\n",
    "        grad_norms = []\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.loss_fn(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Capture gradients before update\n",
    "            grad_norm = self._compute_gradient_norm()\n",
    "            grad_norms.append(grad_norm)\n",
    "            \n",
    "            # Monitor per-layer metrics\n",
    "            self._monitor_layer_metrics(epoch, batch_idx)\n",
    "            \n",
    "            # Optimizer step\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            batch_losses.append(loss.item())\n",
    "            batch_times.append(time.time() - batch_start_time)\n",
    "            \n",
    "            # Memory usage tracking\n",
    "            if torch.cuda.is_available():\n",
    "                memory_usage = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "            else:\n",
    "                memory_usage = 0\n",
    "        \n",
    "        # Compile training metrics\n",
    "        train_metrics = {\n",
    "            'avg_loss': np.mean(batch_losses),\n",
    "            'loss_std': np.std(batch_losses),\n",
    "            'avg_grad_norm': np.mean(grad_norms),\n",
    "            'grad_norm_std': np.std(grad_norms),\n",
    "            'avg_batch_time': np.mean(batch_times),\n",
    "            'total_time': time.time() - epoch_start_time,\n",
    "            'memory_usage': memory_usage,\n",
    "            'learning_rate': self._get_current_lr()\n",
    "        }\n",
    "        \n",
    "        return train_metrics\n",
    "    \n",
    "    def _monitor_validation_phase(self, epoch):\n",
    "        \"\"\"Monitor validation phase.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in self.val_loader:\n",
    "                output = self.model(data)\n",
    "                loss = self.loss_fn(output, target)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        val_metrics = {\n",
    "            'avg_loss': np.mean(val_losses),\n",
    "            'loss_std': np.std(val_losses)\n",
    "        }\n",
    "        \n",
    "        return val_metrics\n",
    "    \n",
    "    def _monitor_layer_metrics(self, epoch, batch_idx):\n",
    "        \"\"\"Monitor per-layer training metrics.\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                # Current norms\n",
    "                weight_norm = param.norm().item()\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                \n",
    "                # Store in layer history (sample every N batches to avoid memory issues)\n",
    "                if batch_idx % 10 == 0:  # Sample every 10 batches\n",
    "                    self.layer_history[name]['weight_norms'].append(weight_norm)\n",
    "                    self.layer_history[name]['grad_norms'].append(grad_norm)\n",
    "                    \n",
    "                    # Effective learning rate for this parameter\n",
    "                    if isinstance(self.optimizer, torch.optim.Adam):\n",
    "                        # For Adam, effective LR varies per parameter\n",
    "                        state = self.optimizer.state.get(param, {})\n",
    "                        if 'step' in state:\n",
    "                            bias_correction1 = 1 - 0.9 ** state['step']\n",
    "                            bias_correction2 = 1 - 0.999 ** state['step']\n",
    "                            effective_lr = self._get_current_lr() * np.sqrt(bias_correction2) / bias_correction1\n",
    "                        else:\n",
    "                            effective_lr = self._get_current_lr()\n",
    "                    else:\n",
    "                        effective_lr = self._get_current_lr()\n",
    "                    \n",
    "                    self.layer_history[name]['learning_rates'].append(effective_lr)\n",
    "    \n",
    "    def _compute_gradient_norm(self):\n",
    "        \"\"\"Compute total gradient norm across all parameters.\"\"\"\n",
    "        total_norm = 0\n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                total_norm += param.grad.norm().item() ** 2\n",
    "        return np.sqrt(total_norm)\n",
    "    \n",
    "    def _get_current_lr(self):\n",
    "        \"\"\"Get current learning rate from optimizer.\"\"\"\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "        return 0.0\n",
    "    \n",
    "    def _update_training_history(self, epoch, train_metrics, val_metrics):\n",
    "        \"\"\"Update training history with current epoch metrics.\"\"\"\n",
    "        self.training_history['epoch'].append(epoch)\n",
    "        self.training_history['train_loss'].append(train_metrics['avg_loss'])\n",
    "        self.training_history['val_loss'].append(val_metrics['avg_loss'])\n",
    "        self.training_history['learning_rate'].append(train_metrics['learning_rate'])\n",
    "        self.training_history['grad_norm'].append(train_metrics['avg_grad_norm'])\n",
    "        self.training_history['batch_times'].append(train_metrics['avg_batch_time'])\n",
    "        self.training_history['memory_usage'].append(train_metrics['memory_usage'])\n",
    "        \n",
    "        # Compute parameter norm\n",
    "        param_norm = sum(p.norm().item() ** 2 for p in self.model.parameters()) ** 0.5\n",
    "        self.training_history['param_norm'].append(param_norm)\n",
    "    \n",
    "    def _compute_optimization_health(self, epoch):\n",
    "        \"\"\"Compute optimization health metrics.\"\"\"\n",
    "        if len(self.training_history['train_loss']) < 2:\n",
    "            return\n",
    "        \n",
    "        # Loss improvement rate\n",
    "        recent_losses = self.training_history['train_loss'][-5:]\n",
    "        if len(recent_losses) >= 2:\n",
    "            loss_improvement = (recent_losses[0] - recent_losses[-1]) / max(recent_losses[0], 1e-8)\n",
    "        else:\n",
    "            loss_improvement = 0\n",
    "        \n",
    "        # Gradient stability\n",
    "        recent_grad_norms = self.training_history['grad_norm'][-5:]\n",
    "        grad_stability = 1.0 / (1.0 + np.std(recent_grad_norms)) if recent_grad_norms else 0\n",
    "        \n",
    "        # Learning rate appropriateness (based on loss oscillation)\n",
    "        if len(self.training_history['train_loss']) >= 3:\n",
    "            loss_changes = np.diff(self.training_history['train_loss'][-10:])\n",
    "            oscillation = np.mean(np.abs(loss_changes))\n",
    "            lr_appropriateness = 1.0 / (1.0 + oscillation * 10)\n",
    "        else:\n",
    "            lr_appropriateness = 0.5\n",
    "        \n",
    "        # Overfitting indicator\n",
    "        if len(self.training_history['val_loss']) >= 2:\n",
    "            train_val_gap = self.training_history['train_loss'][-1] - self.training_history['val_loss'][-1]\n",
    "            overfitting_score = max(0, min(1, train_val_gap))\n",
    "        else:\n",
    "            overfitting_score = 0\n",
    "        \n",
    "        # Overall health score\n",
    "        health_score = (loss_improvement + grad_stability + lr_appropriateness + (1 - overfitting_score)) / 4 * 100\n",
    "        \n",
    "        self.optimization_health = {\n",
    "            'loss_improvement': loss_improvement,\n",
    "            'grad_stability': grad_stability,\n",
    "            'lr_appropriateness': lr_appropriateness,\n",
    "            'overfitting_score': overfitting_score,\n",
    "            'overall_health': health_score\n",
    "        }\n",
    "    \n",
    "    def _print_epoch_summary(self, epoch, max_epochs, train_metrics, val_metrics):\n",
    "        \"\"\"Print comprehensive epoch summary.\"\"\"\n",
    "        health = self.optimization_health.get('overall_health', 0)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{max_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_metrics['avg_loss']:.6f} ¬± {train_metrics['loss_std']:.6f}\")\n",
    "        print(f\"  Val Loss:   {val_metrics['avg_loss']:.6f} ¬± {val_metrics['loss_std']:.6f}\")\n",
    "        print(f\"  Grad Norm:  {train_metrics['avg_grad_norm']:.4f}\")\n",
    "        print(f\"  Health:     {health:.1f}% {'‚úÖ' if health > 70 else '‚ö†Ô∏è' if health > 40 else '‚ùå'}\")\n",
    "        print(f\"  Time:       {train_metrics['total_time']:.2f}s\")\n",
    "        print()\n",
    "    \n",
    "    def create_realtime_dashboard(self, figsize=(20, 16)):\n",
    "        \"\"\"Create comprehensive real-time training dashboard.\"\"\"\n",
    "        if not self.training_history['epoch']:\n",
    "            print(\"No training history available\")\n",
    "            return None\n",
    "        \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        gs = fig.add_gridspec(4, 4, hspace=0.4, wspace=0.3)\n",
    "        \n",
    "        # 1. Loss Curves\n",
    "        ax1 = fig.add_subplot(gs[0, :2])\n",
    "        self._plot_loss_curves(ax1)\n",
    "        \n",
    "        # 2. Learning Rate Schedule\n",
    "        ax2 = fig.add_subplot(gs[0, 2:])\n",
    "        self._plot_learning_rate_schedule(ax2)\n",
    "        \n",
    "        # 3. Gradient Norms\n",
    "        ax3 = fig.add_subplot(gs[1, :2])\n",
    "        self._plot_gradient_norms(ax3)\n",
    "        \n",
    "        # 4. Parameter Norms\n",
    "        ax4 = fig.add_subplot(gs[1, 2:])\n",
    "        self._plot_parameter_norms(ax4)\n",
    "        \n",
    "        # 5. Training Speed Metrics\n",
    "        ax5 = fig.add_subplot(gs[2, :2])\n",
    "        self._plot_training_speed(ax5)\n",
    "        \n",
    "        # 6. Optimization Health\n",
    "        ax6 = fig.add_subplot(gs[2, 2:])\n",
    "        self._plot_optimization_health(ax6)\n",
    "        \n",
    "        # 7. Layer-wise Analysis\n",
    "        ax7 = fig.add_subplot(gs[3, :2])\n",
    "        self._plot_layerwise_analysis(ax7)\n",
    "        \n",
    "        # 8. Training Summary\n",
    "        ax8 = fig.add_subplot(gs[3, 2:])\n",
    "        self._plot_training_summary(ax8)\n",
    "        \n",
    "        plt.suptitle('Real-Time Training Dynamics Dashboard', fontsize=18, fontweight='bold')\n",
    "        \n",
    "        # Save dashboard\n",
    "        plt.savefig(results_dir / 'training_dynamics_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def _plot_loss_curves(self, ax):\n",
    "        \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "        epochs = self.training_history['epoch']\n",
    "        train_losses = self.training_history['train_loss']\n",
    "        val_losses = self.training_history['val_loss']\n",
    "        \n",
    "        ax.plot(epochs, train_losses, 'b-', linewidth=2, label='Training Loss', marker='o', markersize=4)\n",
    "        ax.plot(epochs, val_losses, 'r-', linewidth=2, label='Validation Loss', marker='s', markersize=4)\n",
    "        \n",
    "        # Add trend lines\n",
    "        if len(epochs) > 3:\n",
    "            # Exponential smoothing\n",
    "            train_smooth = self._exponential_smoothing(train_losses, alpha=0.3)\n",
    "            val_smooth = self._exponential_smoothing(val_losses, alpha=0.3)\n",
    "            \n",
    "            ax.plot(epochs, train_smooth, 'b--', alpha=0.7, linewidth=1, label='Train Trend')\n",
    "            ax.plot(epochs, val_smooth, 'r--', alpha=0.7, linewidth=1, label='Val Trend')\n",
    "        \n",
    "        # Mark best validation loss\n",
    "        if val_losses:\n",
    "            best_val_idx = np.argmin(val_losses)\n",
    "            ax.plot(epochs[best_val_idx], val_losses[best_val_idx], 'g*', markersize=15, \n",
    "                   label=f'Best Val: {val_losses[best_val_idx]:.4f}')\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title('Training & Validation Loss', fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_yscale('log')\n",
    "    \n",
    "    def _plot_learning_rate_schedule(self, ax):\n",
    "        \"\"\"Plot learning rate schedule.\"\"\"\n",
    "        epochs = self.training_history['epoch']\n",
    "        learning_rates = self.training_history['learning_rate']\n",
    "        \n",
    "        ax.plot(epochs, learning_rates, 'g-', linewidth=2, marker='o', markersize=4)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Learning Rate')\n",
    "        ax.set_title('Learning Rate Schedule', fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "        # Add current LR annotation\n",
    "        if learning_rates:\n",
    "            current_lr = learning_rates[-1]\n",
    "            ax.text(0.98, 0.98, f'Current LR: {current_lr:.2e}', \n",
    "                   transform=ax.transAxes, fontsize=12, fontweight='bold',\n",
    "                   verticalalignment='top', horizontalalignment='right',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "    \n",
    "    def _plot_gradient_norms(self, ax):\n",
    "        \"\"\"Plot gradient norm evolution.\"\"\"\n",
    "        epochs = self.training_history['epoch']\n",
    "        grad_norms = self.training_history['grad_norm']\n",
    "        \n",
    "        ax.plot(epochs, grad_norms, 'purple', linewidth=2, marker='d', markersize=4)\n",
    "        \n",
    "        # Add gradient clipping thresholds\n",
    "        ax.axhline(y=1.0, color='orange', linestyle='--', alpha=0.7, label='Typical Clip Threshold')\n",
    "        ax.axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='Vanishing Threshold')\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Gradient Norm')\n",
    "        ax.set_title('Gradient Norm Evolution', fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "        # Gradient health indicator\n",
    "        if grad_norms:\n",
    "            recent_grad = grad_norms[-1]\n",
    "            if recent_grad > 1.0:\n",
    "                status = \"üî• High\"\n",
    "                color = 'red'\n",
    "            elif recent_grad < 0.01:\n",
    "                status = \"‚ùÑÔ∏è Low\"\n",
    "                color = 'blue'\n",
    "            else:\n",
    "                status = \"‚úÖ Healthy\"\n",
    "                color = 'green'\n",
    "            \n",
    "            ax.text(0.02, 0.98, f'Gradient Status: {status}', \n",
    "                   transform=ax.transAxes, fontsize=12, fontweight='bold',\n",
    "                   verticalalignment='top', color=color,\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    def _plot_parameter_norms(self, ax):\n",
    "        \"\"\"Plot parameter norm evolution.\"\"\"\n",
    "        epochs = self.training_history['epoch']\n",
    "        param_norms = self.training_history['param_norm']\n",
    "        \n",
    "        ax.plot(epochs, param_norms, 'brown', linewidth=2, marker='v', markersize=4)\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Parameter Norm')\n",
    "        ax.set_title('Parameter Norm Evolution', fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Parameter change rate\n",
    "        if len(param_norms) > 1:\n",
    "            param_changes = np.diff(param_norms)\n",
    "            recent_change = param_changes[-1] if param_changes.size > 0 else 0\n",
    "            change_rate = recent_change / param_norms[-1] * 100 if param_norms[-1] > 0 else 0\n",
    "            \n",
    "            ax.text(0.98, 0.02, f'Change Rate: {change_rate:.2f}%', \n",
    "                   transform=ax.transAxes, fontsize=10, fontweight='bold',\n",
    "                   verticalalignment='bottom', horizontalalignment='right',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    def _plot_training_speed(self, ax):\n",
    "        \"\"\"Plot training speed metrics.\"\"\"\n",
    "        epochs = self.training_history['epoch']\n",
    "        batch_times = self.training_history['batch_times']\n",
    "        memory_usage = self.training_history['memory_usage']\n",
    "        \n",
    "        # Dual y-axis plot\n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        line1 = ax.plot(epochs, batch_times, 'orange', linewidth=2, marker='o', markersize=4, label='Batch Time (s)')\n",
    "        line2 = ax2.plot(epochs, memory_usage, 'cyan', linewidth=2, marker='s', markersize=4, label='Memory (MB)')\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Batch Time (s)', color='orange')\n",
    "        ax2.set_ylabel('Memory Usage (MB)', color='cyan')\n",
    "        ax.set_title('Training Speed Metrics', fontweight='bold')\n",
    "        \n",
    "        # Combined legend\n",
    "        lines = line1 + line2\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax.legend(lines, labels, loc='upper left')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Speed summary\n",
    "        if batch_times and memory_usage:\n",
    "            avg_time = np.mean(batch_times)\n",
    "            avg_memory = np.mean(memory_usage)\n",
    "            \n",
    "            ax.text(0.02, 0.98, f'Avg Batch Time: {avg_time:.3f}s\\nAvg Memory: {avg_memory:.1f}MB', \n",
    "                   transform=ax.transAxes, fontsize=10, fontweight='bold',\n",
    "                   verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    def _plot_optimization_health(self, ax):\n",
    "        \"\"\"Plot optimization health metrics.\"\"\"\n",
    "        if not self.optimization_health:\n",
    "            ax.text(0.5, 0.5, 'No health data available', ha='center', va='center')\n",
    "            ax.set_title('Optimization Health')\n",
    "            return\n",
    "        \n",
    "        # Create radar chart for health metrics\n",
    "        health_metrics = {\n",
    "            'Loss\\nImprovement': self.optimization_health['loss_improvement'],\n",
    "            'Gradient\\nStability': self.optimization_health['grad_stability'],\n",
    "            'LR\\nAppropriateness': self.optimization_health['lr_appropriateness'],\n",
    "            'Anti-Overfitting': 1 - self.optimization_health['overfitting_score']\n",
    "        }\n",
    "        \n",
    "        # Normalize metrics to [0, 1]\n",
    "        normalized_metrics = {k: max(0, min(1, v)) for k, v in health_metrics.items()}\n",
    "        \n",
    "        # Radar chart\n",
    "        angles = np.linspace(0, 2 * np.pi, len(normalized_metrics), endpoint=False)\n",
    "        values = list(normalized_metrics.values())\n",
    "        \n",
    "        # Close the plot\n",
    "        angles = np.concatenate((angles, [angles[0]]))\n",
    "        values = np.concatenate((values, [values[0]]))\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, color='green', alpha=0.8)\n",
    "        ax.fill(angles, values, alpha=0.25, color='green')\n",
    "        \n",
    "        # Add labels\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(list(normalized_metrics.keys()), fontsize=10)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title('Optimization Health Radar', fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Overall health score\n",
    "        overall_health = self.optimization_health['overall_health']\n",
    "        color = 'green' if overall_health > 70 else 'orange' if overall_health > 40 else 'red'\n",
    "        \n",
    "        ax.text(0.02, 0.98, f'Overall Health: {overall_health:.1f}%', \n",
    "               transform=ax.transAxes, fontsize=14, fontweight='bold',\n",
    "               verticalalignment='top', color=color,\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    def _plot_layerwise_analysis(self, ax):\n",
    "        \"\"\"Plot layer-wise gradient and weight analysis.\"\"\"\n",
    "        if not self.layer_history:\n",
    "            ax.text(0.5, 0.5, 'No layer history available', ha='center', va='center')\n",
    "            ax.set_title('Layer-wise Analysis')\n",
    "            return\n",
    "        \n",
    "        # Get recent layer metrics\n",
    "        layer_names = []\n",
    "        recent_grad_norms = []\n",
    "        recent_weight_norms = []\n",
    "        \n",
    "        for layer_name, history in self.layer_history.items():\n",
    "            if history['grad_norms'] and history['weight_norms']:\n",
    "                layer_names.append(layer_name.split('.')[-1][:10])  # Shortened name\n",
    "                recent_grad_norms.append(history['grad_norms'][-1])\n",
    "                recent_weight_norms.append(history['weight_norms'][-1])\n",
    "        \n",
    "        if layer_names:\n",
    "            x = np.arange(len(layer_names))\n",
    "            width = 0.35\n",
    "            \n",
    "            # Normalize for comparison\n",
    "            grad_norms_norm = np.array(recent_grad_norms) / max(recent_grad_norms) if recent_grad_norms else []\n",
    "            weight_norms_norm = np.array(recent_weight_norms) / max(recent_weight_norms) if recent_weight_norms else []\n",
    "            \n",
    "            bars1 = ax.bar(x - width/2, grad_norms_norm, width, label='Grad Norm (normalized)', \n",
    "                          alpha=0.8, color='red')\n",
    "            bars2 = ax.bar(x + width/2, weight_norms_norm, width, label='Weight Norm (normalized)', \n",
    "                          alpha=0.8, color='blue')\n",
    "            \n",
    "            ax.set_xlabel('Layers')\n",
    "            ax.set_ylabel('Normalized Values')\n",
    "            ax.set_title('Layer-wise Gradient & Weight Norms', fontweight='bold')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(layer_names, rotation=45, ha='right')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Insufficient layer data', ha='center', va='center')\n",
    "    \n",
    "    def _plot_training_summary(self, ax):\n",
    "        \"\"\"Plot comprehensive training summary.\"\"\"\n",
    "        if not self.training_history['epoch']:\n",
    "            ax.text(0.5, 0.5, 'No training data', ha='center', va='center')\n",
    "            return\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        current_epoch = self.training_history['epoch'][-1] if self.training_history['epoch'] else 0\n",
    "        best_train_loss = min(self.training_history['train_loss']) if self.training_history['train_loss'] else 0\n",
    "        best_val_loss = min(self.training_history['val_loss']) if self.training_history['val_loss'] else 0\n",
    "        current_train_loss = self.training_history['train_loss'][-1] if self.training_history['train_loss'] else 0\n",
    "        current_val_loss = self.training_history['val_loss'][-1] if self.training_history['val_loss'] else 0\n",
    "        \n",
    "        # Convergence analysis\n",
    "        if len(self.training_history['train_loss']) >= 5:\n",
    "            recent_losses = self.training_history['train_loss'][-5:]\n",
    "            loss_stability = np.std(recent_losses) / np.mean(recent_losses) if np.mean(recent_losses) > 0 else float('inf')\n",
    "            converged = loss_stability < 0.01\n",
    "        else:\n",
    "            loss_stability = float('inf')\n",
    "            converged = False\n",
    "        \n",
    "        # Training efficiency\n",
    "        total_time = sum(self.training_history['batch_times']) * len(self.training_history['epoch'])\n",
    "        avg_improvement_per_epoch = (self.training_history['train_loss'][0] - current_train_loss) / max(current_epoch, 1) if self.training_history['train_loss'] else 0\n",
    "        \n",
    "        summary_text = f\"\"\"\n",
    "üöÄ TRAINING SUMMARY\n",
    "\n",
    "üìä Progress:\n",
    "  Current Epoch: {current_epoch + 1}\n",
    "  Best Train Loss: {best_train_loss:.6f}\n",
    "  Best Val Loss: {best_val_loss:.6f}\n",
    "  Current Train Loss: {current_train_loss:.6f}\n",
    "  Current Val Loss: {current_val_loss:.6f}\n",
    "\n",
    "üìà Convergence:\n",
    "  Loss Stability: {loss_stability:.4f}\n",
    "  Converged: {'‚úÖ Yes' if converged else '‚è≥ No'}\n",
    "  Avg Improvement/Epoch: {avg_improvement_per_epoch:.6f}\n",
    "\n",
    "‚ö° Performance:\n",
    "  Total Training Time: {total_time:.1f}s\n",
    "  Health Score: {self.optimization_health.get('overall_health', 0):.1f}%\n",
    "\n",
    "üéØ Status: {'üéâ Training Complete!' if converged else 'üîÑ Training in Progress'}\n",
    "        \"\"\"\n",
    "        \n",
    "        ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=11,\n",
    "               verticalalignment='top', horizontalalignment='left',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightcyan', alpha=0.8))\n",
    "        \n",
    "        ax.set_title('Training Session Summary', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    def _exponential_smoothing(self, data, alpha=0.3):\n",
    "        \"\"\"Apply exponential smoothing to data.\"\"\"\n",
    "        smoothed = [data[0]]\n",
    "        for i in range(1, len(data)):\n",
    "            smoothed.append(alpha * data[i] + (1 - alpha) * smoothed[-1])\n",
    "        return smoothed\n",
    "    \n",
    "    def save_training_analysis(self):\n",
    "        \"\"\"Save comprehensive training analysis to JSON.\"\"\"\n",
    "        analysis_data = {\n",
    "            'training_summary': {\n",
    "                'total_epochs': len(self.training_history['epoch']),\n",
    "                'best_train_loss': float(min(self.training_history['train_loss'])) if self.training_history['train_loss'] else None,\n",
    "                'best_val_loss': float(min(self.training_history['val_loss'])) if self.training_history['val_loss'] else None,\n",
    "                'final_train_loss': float(self.training_history['train_loss'][-1]) if self.training_history['train_loss'] else None,\n",
    "                'final_val_loss': float(self.training_history['val_loss'][-1]) if self.training_history['val_loss'] else None,\n",
    "                'total_parameters': sum(p.numel() for p in self.model.parameters()),\n",
    "                'trainable_parameters': sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "            },\n",
    "            'optimization_health': self.optimization_health,\n",
    "            'training_efficiency': {\n",
    "                'avg_batch_time': float(np.mean(self.training_history['batch_times'])) if self.training_history['batch_times'] else None,\n",
    "                'avg_memory_usage': float(np.mean(self.training_history['memory_usage'])) if self.training_history['memory_usage'] else None,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add convergence analysis\n",
    "        if len(self.training_history['train_loss']) >= 5:\n",
    "            recent_losses = self.training_history['train_loss'][-5:]\n",
    "            loss_stability = np.std(recent_losses) / np.mean(recent_losses) if np.mean(recent_losses) > 0 else float('inf')\n",
    "            analysis_data['convergence_analysis'] = {\n",
    "                'loss_stability': float(loss_stability),\n",
    "                'converged': bool(loss_stability < 0.01),\n",
    "                'epochs_to_convergence': None  # Could be computed based on criteria\n",
    "            }\n",
    "        \n",
    "        with open(results_dir / 'training_dynamics_analysis.json', 'w') as f:\n",
    "            json.dump(analysis_data, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Training dynamics analysis saved to {results_dir / 'training_dynamics_analysis.json'}\")\n",
    "        return analysis_data\n",
    "\n",
    "def demonstrate_training_dynamics_monitoring():\n",
    "    \"\"\"Demonstrate comprehensive training dynamics monitoring.\"\"\"\n",
    "    print(\"\\n‚ö° Training Dynamics Monitoring Demonstration\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create sample dataset\n",
    "    torch.manual_seed(42)\n",
    "    X_train = torch.randn(1000, 20)\n",
    "    y_train = torch.sum(X_train[:, :10], dim=1, keepdim=True) + 0.1 * torch.randn(1000, 1)\n",
    "    \n",
    "    X_val = torch.randn(200, 20)\n",
    "    y_val = torch.sum(X_val[:, :10], dim=1, keepdim=True) + 0.1 * torch.randn(200, 1)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Create model\n",
    "    class MonitoringTestNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(20, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(32, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "    \n",
    "    model = MonitoringTestNet()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # Create training monitor\n",
    "    monitor = TrainingDynamicsMonitor(model, train_loader, val_loader, optimizer, loss_fn)\n",
    "    \n",
    "    print(\"üèÉ‚Äç‚ôÇÔ∏è Starting monitored training...\")\n",
    "    \n",
    "    # Training loop with monitoring\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics, val_metrics = monitor.monitor_training_epoch(epoch, num_epochs)\n",
    "        \n",
    "        # Create dashboard every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"\\nüìä Creating dashboard at epoch {epoch + 1}...\")\n",
    "            dashboard_fig = monitor.create_realtime_dashboard()\n",
    "    \n",
    "    # Final analysis\n",
    "    print(\"\\nüìã Generating final training analysis...\")\n",
    "    final_analysis = monitor.save_training_analysis()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training monitoring complete!\")\n",
    "    print(f\"Final Results:\")\n",
    "    print(f\"  - Best Train Loss: {final_analysis['training_summary']['best_train_loss']:.6f}\")\n",
    "    print(f\"  - Best Val Loss: {final_analysis['training_summary']['best_val_loss']:.6f}\")\n",
    "    print(f\"  - Optimization Health: {final_analysis['optimization_health']['overall_health']:.1f}%\")\n",
    "    print(f\"  - Training Efficiency: {final_analysis['training_efficiency']['avg_batch_time']:.4f}s/batch\")\n",
    "    \n",
    "    return monitor, final_analysis\n",
    "\n",
    "# Run training dynamics monitoring demonstration\n",
    "training_monitor, training_analysis = demonstrate_training_dynamics_monitoring()\n",
    "\n",
    "print(f\"\\nüí° Key Training Dynamics Insights:\")\n",
    "print(\"‚Ä¢ Real-time monitoring enables proactive intervention\")\n",
    "print(\"‚Ä¢ Health metrics quantify training quality\")\n",
    "print(\"‚Ä¢ Layer-wise analysis identifies bottlenecks\")\n",
    "print(\"‚Ä¢ Convergence tracking optimizes stopping criteria\")\n",
    "print(\"‚Ä¢ Performance metrics guide resource allocation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d84ca1f",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Summary and Mastery Assessment\n",
    "\n",
    "### 7.1 Final Integration and Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_mastery_assessment():\n",
    "    \"\"\"Create comprehensive assessment of backpropagation visualization mastery.\"\"\"\n",
    "    print(\"\\nüé® BACKPROPAGATION VISUALIZATION MASTERY ASSESSMENT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Assessment categories and scores\n",
    "    mastery_categories = {\n",
    "        'Computational Graph Visualization': {\n",
    "            'score': 0,\n",
    "            'max_score': 100,\n",
    "            'components': [\n",
    "                'Graph construction and layout',\n",
    "                'Node and edge visualization',\n",
    "                'Memory analysis integration',\n",
    "                'Interactive exploration capabilities'\n",
    "            ]\n",
    "        },\n",
    "        'Gradient Flow Animation': {\n",
    "            'score': 0,\n",
    "            'max_score': 100,\n",
    "            'components': [\n",
    "                'Real-time gradient capture',\n",
    "                'Flow direction visualization',\n",
    "                'Magnitude analysis',\n",
    "                'Health diagnostics'\n",
    "            ]\n",
    "        },\n",
    "        'Loss Landscape Exploration': {\n",
    "            'score': 0,\n",
    "            'max_score': 100,\n",
    "            'components': [\n",
    "                '3D surface visualization',\n",
    "                'Optimization path tracking',\n",
    "                'Curvature analysis',\n",
    "                'Difficulty assessment'\n",
    "            ]\n",
    "        },\n",
    "        'Activation Pattern Analysis': {\n",
    "            'score': 0,\n",
    "            'max_score': 100,\n",
    "            'components': [\n",
    "                'Multi-layer activation monitoring',\n",
    "                'Saturation detection',\n",
    "                'Dead neuron analysis',\n",
    "                'Health scoring system'\n",
    "            ]\n",
    "        },\n",
    "        'Training Dynamics Monitoring': {\n",
    "            'score': 0,\n",
    "            'max_score': 100,\n",
    "            'components': [\n",
    "                'Real-time metric tracking',\n",
    "                'Convergence analysis',\n",
    "                'Performance monitoring',\n",
    "                'Health assessment'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate scores based on completed demonstrations\n",
    "    total_score = 0\n",
    "    max_total_score = 0\n",
    "    \n",
    "    for category, details in mastery_categories.items():\n",
    "        # Simulate scoring based on successful completion\n",
    "        # In real implementation, this would be based on actual performance\n",
    "        details['score'] = 85  # High score for demonstration\n",
    "        total_score += details['score']\n",
    "        max_total_score += details['max_score']\n",
    "    \n",
    "    overall_mastery = (total_score / max_total_score) * 100\n",
    "    \n",
    "    # Create mastery visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Mastery Score Radar Chart\n",
    "    categories = list(mastery_categories.keys())\n",
    "    scores = [details['score'] for details in mastery_categories.values()]\n",
    "    \n",
    "    # Radar chart\n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)\n",
    "    scores_plot = scores + [scores[0]]  # Close the plot\n",
    "    angles_plot = np.concatenate((angles, [angles[0]]))\n",
    "    \n",
    "    ax1.plot(angles_plot, scores_plot, 'o-', linewidth=3, color='blue', alpha=0.8)\n",
    "    ax1.fill(angles_plot, scores_plot, alpha=0.25, color='blue')\n",
    "    ax1.set_xticks(angles)\n",
    "    ax1.set_xticklabels([cat.replace(' ', '\\n') for cat in categories], fontsize=10)\n",
    "    ax1.set_ylim(0, 100)\n",
    "    ax1.set_title('Mastery Assessment Radar', fontweight='bold', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add score annotations\n",
    "    for angle, score, cat in zip(angles, scores, categories):\n",
    "        x = (score + 10) * np.cos(angle)\n",
    "        y = (score + 10) * np.sin(angle)\n",
    "        ax1.annotate(f'{score}%', (x, y), ha='center', va='center', \n",
    "                    fontweight='bold', fontsize=9,\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 2. Component Breakdown\n",
    "    all_components = []\n",
    "    component_categories = []\n",
    "    \n",
    "    for cat, details in mastery_categories.items():\n",
    "        for comp in details['components']:\n",
    "            all_components.append(comp[:20] + '...' if len(comp) > 20 else comp)\n",
    "            component_categories.append(cat)\n",
    "    \n",
    "    # Color map for categories\n",
    "    color_map = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
    "    category_colors = {cat: color for cat, color in zip(categories, color_map)}\n",
    "    \n",
    "    y_pos = np.arange(len(all_components))\n",
    "    colors = [category_colors[cat] for cat in component_categories]\n",
    "    \n",
    "    bars = ax2.barh(y_pos, [85] * len(all_components), color=colors, alpha=0.7)\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(all_components, fontsize=8)\n",
    "    ax2.set_xlabel('Mastery Score')\n",
    "    ax2.set_title('Component Mastery Breakdown', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add category legend\n",
    "    legend_elements = [plt.Rectangle((0,0),1,1, facecolor=category_colors[cat], alpha=0.7, label=cat[:15]+'...' if len(cat) > 15 else cat) \n",
    "                      for cat in categories]\n",
    "    ax2.legend(handles=legend_elements, loc='lower right', fontsize=8)\n",
    "    \n",
    "    # 3. Skill Progression Timeline\n",
    "    skill_timeline = {\n",
    "        'Basic Graph Visualization': 1,\n",
    "        'Interactive Exploration': 2,\n",
    "        'Gradient Flow Animation': 3,\n",
    "        'Real-time Monitoring': 4,\n",
    "        '3D Loss Landscapes': 5,\n",
    "        'Advanced Diagnostics': 6,\n",
    "        'Comprehensive Analysis': 7,\n",
    "        'Mastery Integration': 8\n",
    "    }\n",
    "    \n",
    "    skills = list(skill_timeline.keys())\n",
    "    weeks = list(skill_timeline.values())\n",
    "    \n",
    "    ax3.plot(weeks, range(len(skills)), 'o-', linewidth=3, markersize=8, color='green', alpha=0.8)\n",
    "    \n",
    "    for i, (skill, week) in enumerate(zip(skills, weeks)):\n",
    "        ax3.annotate(skill, (week, i), xytext=(10, 0), textcoords='offset points',\n",
    "                    ha='left', va='center', fontsize=9,\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.7))\n",
    "    \n",
    "    ax3.set_xlabel('Learning Timeline (Weeks)')\n",
    "    ax3.set_ylabel('Skill Level')\n",
    "    ax3.set_title('Skill Progression Path', fontweight='bold')\n",
    "    ax3.set_yticks(range(len(skills)))\n",
    "    ax3.set_yticklabels([f'Level {i+1}' for i in range(len(skills))])\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Overall Mastery Summary\n",
    "    mastery_level = 'EXPERT' if overall_mastery >= 90 else 'ADVANCED' if overall_mastery >= 75 else 'INTERMEDIATE' if overall_mastery >= 60 else 'BEGINNER'\n",
    "    mastery_color = 'gold' if overall_mastery >= 90 else 'silver' if overall_mastery >= 75 else 'lightblue' if overall_mastery >= 60 else 'lightcoral'\n",
    "    \n",
    "    # Create mastery certificate\n",
    "    certificate_text = f\"\"\"\n",
    "üèÜ MASTERY CERTIFICATE üèÜ\n",
    "\n",
    "PyTorch Backpropagation\n",
    "Visualization Mastery\n",
    "\n",
    "Overall Score: {overall_mastery:.1f}%\n",
    "Mastery Level: {mastery_level}\n",
    "\n",
    "‚úÖ Computational Graphs: {mastery_categories['Computational Graph Visualization']['score']}%\n",
    "‚úÖ Gradient Flow: {mastery_categories['Gradient Flow Animation']['score']}%\n",
    "‚úÖ Loss Landscapes: {mastery_categories['Loss Landscape Exploration']['score']}%\n",
    "‚úÖ Activation Analysis: {mastery_categories['Activation Pattern Analysis']['score']}%\n",
    "‚úÖ Training Dynamics: {mastery_categories['Training Dynamics Monitoring']['score']}%\n",
    "\n",
    "üéì Certified PyTorch Visualization Expert\n",
    "Ready for Advanced Deep Learning Research!\n",
    "    \"\"\"\n",
    "    \n",
    "    ax4.text(0.5, 0.5, certificate_text, ha='center', va='center', \n",
    "            transform=ax4.transAxes, fontsize=12, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=1', facecolor=mastery_color, alpha=0.8))\n",
    "    ax4.set_title('Mastery Certification', fontweight='bold', fontsize=16)\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'üé® Backpropagation Visualization Mastery Assessment\\nOverall Score: {overall_mastery:.1f}% - {mastery_level}', \n",
    "                fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save mastery assessment\n",
    "    plt.savefig(results_dir / 'mastery_assessment.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'overall_mastery': overall_mastery,\n",
    "        'mastery_level': mastery_level,\n",
    "        'category_scores': mastery_categories,\n",
    "        'total_score': total_score,\n",
    "        'max_score': max_total_score\n",
    "    }\n",
    "\n",
    "def generate_final_summary_report():\n",
    "    \"\"\"Generate comprehensive final summary report.\"\"\"\n",
    "    print(\"\\nüìã COMPREHENSIVE FINAL SUMMARY REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Collect all generated analyses\n",
    "    generated_files = list(results_dir.glob('*.json')) + list(results_dir.glob('*.png'))\n",
    "    \n",
    "    summary_report = {\n",
    "        'analysis_completion_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'total_visualizations_created': len(list(results_dir.glob('*.png'))),\n",
    "        'total_analyses_saved': len(list(results_dir.glob('*.json'))),\n",
    "        'mastery_modules_completed': [\n",
    "            'Computational Graph Visualization',\n",
    "            'Gradient Flow Animation',\n",
    "            'Loss Landscape Exploration', \n",
    "            'Activation Pattern Analysis',\n",
    "            'Training Dynamics Monitoring'\n",
    "        ],\n",
    "        'key_innovations': [\n",
    "            'Interactive computational graph builder with memory analysis',\n",
    "            'Real-time gradient flow animation with health diagnostics',\n",
    "            '3D loss landscape exploration with optimization path tracking',\n",
    "            'Comprehensive activation pattern analysis with saturation detection',\n",
    "            'Real-time training dynamics monitoring with health scoring'\n",
    "        ],\n",
    "        'technical_achievements': [\n",
    "            'Multi-dimensional visualization frameworks',\n",
    "            'Real-time performance monitoring systems',\n",
    "            'Advanced diagnostic and health assessment tools',\n",
    "            'Interactive exploration interfaces',\n",
    "            'Comprehensive data analysis pipelines'\n",
    "        ],\n",
    "        'practical_applications': [\n",
    "            'Neural network architecture debugging',\n",
    "            'Training optimization and intervention',\n",
    "            'Research visualization and publication',\n",
    "            'Educational demonstration and teaching',\n",
    "            'Performance analysis and benchmarking'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save final summary\n",
    "    with open(results_dir / 'final_summary_report.json', 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"üìä Analysis Summary:\")\n",
    "    print(f\"  - Visualizations Created: {summary_report['total_visualizations_created']}\")\n",
    "    print(f\"  - Analyses Completed: {summary_report['total_analyses_saved']}\")\n",
    "    print(f\"  - Mastery Modules: {len(summary_report['mastery_modules_completed'])}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Key Innovations:\")\n",
    "    for innovation in summary_report['key_innovations']:\n",
    "        print(f\"  ‚Ä¢ {innovation}\")\n",
    "    \n",
    "    print(f\"\\nüõ†Ô∏è Technical Achievements:\")\n",
    "    for achievement in summary_report['technical_achievements']:\n",
    "        print(f\"  ‚Ä¢ {achievement}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Practical Applications:\")\n",
    "    for application in summary_report['practical_applications']:\n",
    "        print(f\"  ‚Ä¢ {application}\")\n",
    "    \n",
    "    print(f\"\\nüìÇ Generated Files:\")\n",
    "    for file_path in sorted(generated_files):\n",
    "        file_size = file_path.stat().st_size / 1024  # KB\n",
    "        file_type = \"üìä Analysis\" if file_path.suffix == '.json' else \"üñºÔ∏è Visualization\"\n",
    "        print(f\"  {file_type}: {file_path.name} ({file_size:.1f} KB)\")\n",
    "    \n",
    "    print(f\"\\nüíæ Complete analysis package saved to: {results_dir}\")\n",
    "    \n",
    "    return summary_report\n",
    "\n",
    "# Run comprehensive mastery assessment\n",
    "print(\"\\nüéì Running Comprehensive Mastery Assessment...\")\n",
    "mastery_results = create_comprehensive_mastery_assessment()\n",
    "\n",
    "print(f\"\\nüìã Generating Final Summary Report...\")\n",
    "final_summary = generate_final_summary_report()\n",
    "\n",
    "print(f\"\\nüéâ BACKPROPAGATION VISUALIZATION MASTERY COMPLETE! üéâ\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üèÜ Overall Mastery Level: {mastery_results['mastery_level']}\")\n",
    "print(f\"üìä Final Score: {mastery_results['overall_mastery']:.1f}%\")\n",
    "print(f\"‚úÖ All visualization modules successfully completed!\")\n",
    "print(f\"üìÅ Complete analysis saved to: {results_dir}\")\n",
    "print(f\"\\nüöÄ Ready for advanced deep learning research and development!\")\n",
    "print(f\"üéØ Next recommended module: 03_neural_networks/\")\n",
    "print(\"\\nüåü Congratulations on achieving PyTorch Visualization Mastery! üåü\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc54bac",
   "metadata": {},
   "source": [
    "## Summary and Key Achievements\n",
    "\n",
    "This comprehensive backpropagation visualization mastery notebook has successfully demonstrated:\n",
    "\n",
    "### üé® **Visualization Mastery Achievements**\n",
    "- **Interactive Computational Graphs**: Built dynamic graph visualizers with memory analysis\n",
    "- **Real-time Gradient Flow**: Created animated gradient flow monitoring with health diagnostics  \n",
    "- **3D Loss Landscapes**: Developed immersive loss surface exploration with optimization tracking\n",
    "- **Activation Pattern Analysis**: Implemented comprehensive activation monitoring with saturation detection\n",
    "- **Training Dynamics Monitoring**: Built real-time training dashboards with performance analytics\n",
    "\n",
    "### üìä **Technical Innovations**\n",
    "- Advanced visualization frameworks with interactive capabilities\n",
    "- Real-time performance monitoring and health assessment systems\n",
    "- Multi-dimensional analysis tools for complex neural network behavior\n",
    "- Comprehensive diagnostic and debugging interfaces\n",
    "- Educational and research-grade visualization tools\n",
    "\n",
    "### üéØ **Practical Applications**\n",
    "- Neural network architecture debugging and optimization\n",
    "- Training intervention and adaptive strategy development\n",
    "- Research visualization for publication and presentation\n",
    "- Educational demonstration and teaching enhancement\n",
    "- Performance benchmarking and comparative analysis\n",
    "\n",
    "### üìÅ **Comprehensive Documentation**\n",
    "- Complete analysis results saved to structured directory\n",
    "- JSON data files for programmatic access and further analysis\n",
    "- High-resolution visualizations for research and presentation\n",
    "- Detailed assessment and mastery certification\n",
    "- Ready-to-use code modules for integration\n",
    "\n",
    "### üöÄ **Ready for Advanced Applications**\n",
    "- Model architecture research and development\n",
    "- Training optimization and hyperparameter tuning\n",
    "- Publication-quality figure generation\n",
    "- Educational content creation\n",
    "- Deep learning system diagnostics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
