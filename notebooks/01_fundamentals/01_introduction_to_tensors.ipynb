{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92161dba",
   "metadata": {},
   "source": [
    "# PyTorch Tensor Fundamentals: Comprehensive Deep Learning Foundation Analysis\n",
    "\n",
    "**PyTorch Mastery Hub - Fundamentals Module**\n",
    "\n",
    "**Authors:** PyTorch Mastery Hub Development Team  \n",
    "**Institution:** PyTorch Mastery Hub  \n",
    "**Course:** Deep Learning Fundamentals with PyTorch  \n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides comprehensive analysis and exploration of PyTorch tensors, the fundamental building blocks of deep learning. We focus on understanding tensor operations, memory management, performance optimization, and practical applications to build a solid foundation for advanced deep learning concepts.\n",
    "\n",
    "## Key Objectives\n",
    "1. Master tensor creation, manipulation, and operation techniques\n",
    "2. Understand memory management and performance optimization strategies\n",
    "3. Explore GPU acceleration and device management best practices\n",
    "4. Analyze broadcasting, reshaping, and advanced indexing capabilities\n",
    "5. Implement practical machine learning applications using tensor operations\n",
    "6. Establish performance benchmarking and optimization methodologies\n",
    "\n",
    "## 1. Setup and Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for comprehensive tensor analysis\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import psutil\n",
    "import gc\n",
    "import math  \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add custom modules to path\n",
    "sys.path.append(str(Path.cwd().parent.parent / \"src\"))\n",
    "\n",
    "# Import custom utilities\n",
    "try:\n",
    "    from fundamentals import TensorOperations, print_system_info\n",
    "    from utils.device_utils import get_device\n",
    "    from utils.memory_utils import MemoryTracker\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Custom utilities not found, using fallback implementations\")\n",
    "    \n",
    "    def print_system_info():\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"CUDA version: {torch.version.cuda}\")\n",
    "            print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    def get_device():\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "# Enhanced plotting configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path('../results/notebooks/tensor_fundamentals')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize analysis tracking\n",
    "analysis_results = {\n",
    "    'system_info': {},\n",
    "    'tensor_creation_analysis': {},\n",
    "    'performance_benchmarks': {},\n",
    "    'memory_usage_analysis': {},\n",
    "    'operation_comparisons': {},\n",
    "    'device_performance': {}\n",
    "}\n",
    "\n",
    "print(\"üî• PyTorch Mastery Hub - Tensor Fundamentals Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Results directory: {results_dir}\")\n",
    "print(f\"üéØ Comprehensive tensor analysis initialized\")\n",
    "print()\n",
    "\n",
    "# System information analysis\n",
    "print(\"üñ•Ô∏è System Environment Analysis\")\n",
    "print(\"-\" * 35)\n",
    "print_system_info()\n",
    "\n",
    "# Device detection and analysis\n",
    "device = get_device()\n",
    "print(f\"\\nüéØ Primary compute device: {device}\")\n",
    "\n",
    "# Store system information\n",
    "analysis_results['system_info'] = {\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'cuda_available': torch.cuda.is_available(),\n",
    "    'mps_available': hasattr(torch.backends, 'mps') and torch.backends.mps.is_available(),\n",
    "    'primary_device': str(device),\n",
    "    'cpu_count': psutil.cpu_count(),\n",
    "    'total_memory_gb': psutil.virtual_memory().total / (1024**3)\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    analysis_results['system_info'].update({\n",
    "        'cuda_version': torch.version.cuda,\n",
    "        'gpu_count': torch.cuda.device_count(),\n",
    "        'gpu_name': torch.cuda.get_device_name(),\n",
    "        'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    })\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "\n",
    "# Add error handling and robustness functions here\n",
    "def safe_device_operation(operation_func, *args, **kwargs):\n",
    "    \"\"\"Safely execute operations with device fallback.\"\"\"\n",
    "    try:\n",
    "        return operation_func(*args, **kwargs)\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA\" in str(e) or \"MPS\" in str(e):\n",
    "            print(f\"‚ö†Ô∏è Device operation failed, falling back to CPU: {e}\")\n",
    "            # Move tensors to CPU and retry\n",
    "            cpu_args = [arg.cpu() if hasattr(arg, 'cpu') else arg for arg in args]\n",
    "            cpu_kwargs = {k: v.cpu() if hasattr(v, 'cpu') else v for k, v in kwargs.items()}\n",
    "            return operation_func(*cpu_args, **cpu_kwargs)\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "def safe_memory_operation(func, *args, **kwargs):\n",
    "    \"\"\"Execute with memory management.\"\"\"\n",
    "    try:\n",
    "        return func(*args, **kwargs)\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"‚ö†Ô∏è Out of memory, attempting cleanup...\")\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            return None\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "print(\"üõ°Ô∏è Error handling functions loaded\")\n",
    "\n",
    "# Performance optimization functions\n",
    "def get_adaptive_tensor_sizes(device):\n",
    "    \"\"\"Get appropriate tensor sizes based on device capability.\"\"\"\n",
    "    if device.type == 'cuda':\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        if gpu_memory > 8:\n",
    "            return [(500, 500), (1000, 1000), (2000, 2000)]\n",
    "        else:\n",
    "            return [(300, 300), (600, 600), (1000, 1000)]\n",
    "    elif device.type == 'mps':\n",
    "        return [(300, 300), (600, 600), (1000, 1000)]\n",
    "    else:\n",
    "        return [(200, 200), (500, 500), (800, 800)]\n",
    "\n",
    "def get_adaptive_iterations(operation_type, device):\n",
    "    \"\"\"Get appropriate number of iterations based on operation and device.\"\"\"\n",
    "    base_iterations = {\n",
    "        'fast_ops': 50,\n",
    "        'medium_ops': 20,\n",
    "        'slow_ops': 5\n",
    "    }\n",
    "    \n",
    "    if device.type == 'cpu':\n",
    "        return {k: max(1, v // 2) for k, v in base_iterations.items()}\n",
    "    else:\n",
    "        return base_iterations\n",
    "\n",
    "# Get adaptive settings for this device\n",
    "adaptive_sizes = get_adaptive_tensor_sizes(device)\n",
    "adaptive_iterations = get_adaptive_iterations('medium_ops', device)\n",
    "\n",
    "print(f\"‚ö° Performance optimizations loaded for {device}\")\n",
    "print(f\"   Adaptive tensor sizes: {adaptive_sizes}\")\n",
    "print(f\"   Adaptive iterations: {adaptive_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82d4a35",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Tensor Creation Analysis\n",
    "\n",
    "### 2.1 Tensor Creation Methods and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tensor_creation_methods():\n",
    "    \"\"\"Comprehensive analysis of tensor creation methods with performance metrics.\"\"\"\n",
    "    \n",
    "    print(\"üèóÔ∏è Tensor Creation Methods Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    creation_methods = {}\n",
    "    performance_metrics = {}\n",
    "    memory_usage = {}\n",
    "    \n",
    "    # Define test parameters\n",
    "    test_sizes = [(100, 100), (500, 500), (1000, 1000)]\n",
    "    #test_sizes = get_adaptive_tensor_sizes(device)\n",
    "    \n",
    "    for size in test_sizes:\n",
    "        print(f\"\\nüìä Analyzing size {size[0]}x{size[1]}...\")\n",
    "        \n",
    "        size_results = {}\n",
    "        size_performance = {}\n",
    "        size_memory = {}\n",
    "        \n",
    "        # Method 1: Zeros tensor\n",
    "        start_time = time.time()\n",
    "        zeros_tensor = torch.zeros(size, device=device)\n",
    "        zeros_time = time.time() - start_time\n",
    "        zeros_memory = zeros_tensor.numel() * zeros_tensor.element_size()\n",
    "        \n",
    "        size_results['zeros'] = {\n",
    "            'tensor': zeros_tensor,\n",
    "            'mean': float(zeros_tensor.mean()),\n",
    "            'std': float(zeros_tensor.std()),\n",
    "            'dtype': str(zeros_tensor.dtype),\n",
    "            'shape': zeros_tensor.shape\n",
    "        }\n",
    "        size_performance['zeros'] = zeros_time\n",
    "        size_memory['zeros'] = zeros_memory\n",
    "        \n",
    "        # Method 2: Ones tensor\n",
    "        start_time = time.time()\n",
    "        ones_tensor = torch.ones(size, device=device)\n",
    "        ones_time = time.time() - start_time\n",
    "        ones_memory = ones_tensor.numel() * ones_tensor.element_size()\n",
    "        \n",
    "        size_results['ones'] = {\n",
    "            'tensor': ones_tensor,\n",
    "            'mean': float(ones_tensor.mean()),\n",
    "            'std': float(ones_tensor.std()),\n",
    "            'dtype': str(ones_tensor.dtype),\n",
    "            'shape': ones_tensor.shape\n",
    "        }\n",
    "        size_performance['ones'] = ones_time\n",
    "        size_memory['ones'] = ones_memory\n",
    "        \n",
    "        # Method 3: Random normal\n",
    "        start_time = time.time()\n",
    "        randn_tensor = torch.randn(size, device=device)\n",
    "        randn_time = time.time() - start_time\n",
    "        randn_memory = randn_tensor.numel() * randn_tensor.element_size()\n",
    "        \n",
    "        size_results['randn'] = {\n",
    "            'tensor': randn_tensor,\n",
    "            'mean': float(randn_tensor.mean()),\n",
    "            'std': float(randn_tensor.std()),\n",
    "            'dtype': str(randn_tensor.dtype),\n",
    "            'shape': randn_tensor.shape\n",
    "        }\n",
    "        size_performance['randn'] = randn_time\n",
    "        size_memory['randn'] = randn_memory\n",
    "        \n",
    "        # Method 4: Random uniform\n",
    "        start_time = time.time()\n",
    "        rand_tensor = torch.rand(size, device=device)\n",
    "        rand_time = time.time() - start_time\n",
    "        rand_memory = rand_tensor.numel() * rand_tensor.element_size()\n",
    "        \n",
    "        size_results['rand'] = {\n",
    "            'tensor': rand_tensor,\n",
    "            'mean': float(rand_tensor.mean()),\n",
    "            'std': float(rand_tensor.std()),\n",
    "            'dtype': str(rand_tensor.dtype),\n",
    "            'shape': rand_tensor.shape\n",
    "        }\n",
    "        size_performance['rand'] = rand_time\n",
    "        size_memory['rand'] = rand_memory\n",
    "        \n",
    "        # Method 5: Xavier uniform initialization\n",
    "        start_time = time.time()\n",
    "        xavier_tensor = torch.empty(size, device=device)\n",
    "        torch.nn.init.xavier_uniform_(xavier_tensor)\n",
    "        xavier_time = time.time() - start_time\n",
    "        xavier_memory = xavier_tensor.numel() * xavier_tensor.element_size()\n",
    "        \n",
    "        size_results['xavier'] = {\n",
    "            'tensor': xavier_tensor,\n",
    "            'mean': float(xavier_tensor.mean()),\n",
    "            'std': float(xavier_tensor.std()),\n",
    "            'dtype': str(xavier_tensor.dtype),\n",
    "            'shape': xavier_tensor.shape\n",
    "        }\n",
    "        size_performance['xavier'] = xavier_time\n",
    "        size_memory['xavier'] = xavier_memory\n",
    "        \n",
    "        # Method 6: Kaiming normal initialization\n",
    "        start_time = time.time()\n",
    "        kaiming_tensor = torch.empty(size, device=device)\n",
    "        torch.nn.init.kaiming_normal_(kaiming_tensor)\n",
    "        kaiming_time = time.time() - start_time\n",
    "        kaiming_memory = kaiming_tensor.numel() * kaiming_tensor.element_size()\n",
    "        \n",
    "        size_results['kaiming'] = {\n",
    "            'tensor': kaiming_tensor,\n",
    "            'mean': float(kaiming_tensor.mean()),\n",
    "            'std': float(kaiming_tensor.std()),\n",
    "            'dtype': str(kaiming_tensor.dtype),\n",
    "            'shape': kaiming_tensor.shape\n",
    "        }\n",
    "        size_performance['kaiming'] = kaiming_time\n",
    "        size_memory['kaiming'] = kaiming_memory\n",
    "        \n",
    "        creation_methods[f\"{size[0]}x{size[1]}\"] = size_results\n",
    "        performance_metrics[f\"{size[0]}x{size[1]}\"] = size_performance\n",
    "        memory_usage[f\"{size[0]}x{size[1]}\"] = size_memory\n",
    "        \n",
    "        # Print statistics for this size\n",
    "        for method, results in size_results.items():\n",
    "            print(f\"  {method.capitalize():10} - Mean: {results['mean']:8.4f}, \"\n",
    "                  f\"Std: {results['std']:8.4f}, Time: {size_performance[method]:.6f}s\")\n",
    "    \n",
    "    return creation_methods, performance_metrics, memory_usage\n",
    "\n",
    "# Run tensor creation analysis\n",
    "creation_data, creation_performance, creation_memory = analyze_tensor_creation_methods()\n",
    "\n",
    "# Store results\n",
    "analysis_results['tensor_creation_analysis'] = {\n",
    "    'creation_methods': creation_data,\n",
    "    'performance_metrics': creation_performance,\n",
    "    'memory_usage': creation_memory\n",
    "}\n",
    "\n",
    "print(f\"\\nüíæ Tensor creation analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbcc3d",
   "metadata": {},
   "source": [
    "### 2.2 Tensor Creation Visualization and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tensor_creation_analysis(creation_data, performance_data, memory_data):\n",
    "    \"\"\"Create comprehensive visualizations for tensor creation analysis.\"\"\"\n",
    "    \n",
    "    print(\"\\nüé® Creating Tensor Creation Visualizations\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Create comprehensive visualization dashboard\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Tensor distribution visualization (2D heatmaps)\n",
    "    size_key = \"1000x1000\"\n",
    "    if size_key in creation_data:\n",
    "        sample_data = creation_data[size_key]\n",
    "        \n",
    "        # Plot different initialization methods\n",
    "        methods_to_plot = ['zeros', 'ones', 'randn', 'rand', 'xavier', 'kaiming']\n",
    "        \n",
    "        for i, method in enumerate(methods_to_plot):\n",
    "            if method in sample_data:\n",
    "                ax = fig.add_subplot(gs[0, i % 3])\n",
    "                tensor_np = sample_data[method]['tensor'].cpu().numpy()\n",
    "                \n",
    "                # Sample for visualization (avoid memory issues)\n",
    "                if tensor_np.shape[0] > 100:\n",
    "                    tensor_np = tensor_np[:100, :100]\n",
    "                \n",
    "                im = ax.imshow(tensor_np, cmap='viridis', aspect='auto')\n",
    "                ax.set_title(f'{method.capitalize()}\\nMean: {sample_data[method][\"mean\"]:.3f}')\n",
    "                plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "                \n",
    "                if i >= 3:  # Second row\n",
    "                    ax = fig.add_subplot(gs[1, (i-3) % 3])\n",
    "                    im = ax.imshow(tensor_np, cmap='viridis', aspect='auto')\n",
    "                    ax.set_title(f'{method.capitalize()}\\nStd: {sample_data[method][\"std\"]:.3f}')\n",
    "                    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # 2. Performance comparison across sizes\n",
    "    ax_perf = fig.add_subplot(gs[2, :2])\n",
    "    \n",
    "    sizes = list(performance_data.keys())\n",
    "    methods = list(performance_data[sizes[0]].keys())\n",
    "    \n",
    "    x = np.arange(len(sizes))\n",
    "    width = 0.12\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        times = [performance_data[size][method] for size in sizes]\n",
    "        bars = ax_perf.bar(x + i * width, times, width, label=method.capitalize(), alpha=0.8)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, time_val in zip(bars, times):\n",
    "            height = bar.get_height()\n",
    "            ax_perf.text(bar.get_x() + bar.get_width()/2., height + max(times)*0.01,\n",
    "                        f'{time_val:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    ax_perf.set_xlabel('Tensor Size')\n",
    "    ax_perf.set_ylabel('Creation Time (seconds)')\n",
    "    ax_perf.set_title('Tensor Creation Performance Comparison')\n",
    "    ax_perf.set_xticks(x + width * (len(methods) - 1) / 2)\n",
    "    ax_perf.set_xticklabels(sizes)\n",
    "    ax_perf.legend()\n",
    "    ax_perf.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Memory usage analysis\n",
    "    ax_mem = fig.add_subplot(gs[2, 2:])\n",
    "    \n",
    "    # Memory usage is the same for all methods (same size), so show one method\n",
    "    if sizes and methods:\n",
    "        memory_values = [memory_data[size][methods[0]] / (1024**2) for size in sizes]  # Convert to MB\n",
    "        \n",
    "        bars = ax_mem.bar(sizes, memory_values, alpha=0.7, color='lightblue')\n",
    "        \n",
    "        for bar, mem_val in zip(bars, memory_values):\n",
    "            height = bar.get_height()\n",
    "            ax_mem.text(bar.get_x() + bar.get_width()/2., height + max(memory_values)*0.01,\n",
    "                       f'{mem_val:.1f}MB', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        ax_mem.set_xlabel('Tensor Size')\n",
    "        ax_mem.set_ylabel('Memory Usage (MB)')\n",
    "        ax_mem.set_title('Memory Usage by Tensor Size')\n",
    "        ax_mem.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Statistical distribution analysis\n",
    "    ax_stats = fig.add_subplot(gs[3, :2])\n",
    "    \n",
    "    if size_key in creation_data:\n",
    "        sample_data = creation_data[size_key]\n",
    "        \n",
    "        means = [sample_data[method]['mean'] for method in methods if method in sample_data]\n",
    "        stds = [sample_data[method]['std'] for method in methods if method in sample_data]\n",
    "        method_names = [method.capitalize() for method in methods if method in sample_data]\n",
    "        \n",
    "        x = np.arange(len(method_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax_stats.bar(x - width/2, means, width, label='Mean', alpha=0.8, color='skyblue')\n",
    "        bars2 = ax_stats.bar(x + width/2, stds, width, label='Standard Deviation', alpha=0.8, color='lightcoral')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax_stats.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                             f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        ax_stats.set_xlabel('Initialization Method')\n",
    "        ax_stats.set_ylabel('Value')\n",
    "        ax_stats.set_title('Statistical Properties of Different Initializations')\n",
    "        ax_stats.set_xticks(x)\n",
    "        ax_stats.set_xticklabels(method_names, rotation=45, ha='right')\n",
    "        ax_stats.legend()\n",
    "        ax_stats.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Comprehensive summary\n",
    "    ax_summary = fig.add_subplot(gs[3, 2:])\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "üî• TENSOR CREATION ANALYSIS SUMMARY\n",
    "\n",
    "üìä Methods Analyzed: {len(methods)}\n",
    "üìè Size Variations: {len(sizes)}\n",
    "üñ•Ô∏è Compute Device: {device}\n",
    "\n",
    "‚ö° Performance Insights:\n",
    "‚Ä¢ Fastest Method: {min(performance_data[sizes[-1]], key=performance_data[sizes[-1]].get).capitalize()}\n",
    "‚Ä¢ Memory Efficient: All methods use equal memory for same size\n",
    "‚Ä¢ Statistical Properties: Xavier/Kaiming optimized for deep learning\n",
    "\n",
    "üéØ Recommendations:\n",
    "‚Ä¢ Use zeros/ones for simple initialization\n",
    "‚Ä¢ Use randn for general random initialization  \n",
    "‚Ä¢ Use Xavier/Kaiming for neural network weights\n",
    "‚Ä¢ Consider memory usage for large tensors\n",
    "‚Ä¢ GPU acceleration provides significant speedup\n",
    "\n",
    "üìà Key Findings:\n",
    "‚Ä¢ Creation time scales with tensor size\n",
    "‚Ä¢ Initialization method affects statistical properties\n",
    "‚Ä¢ Memory usage is consistent across methods\n",
    "‚Ä¢ Device selection impacts performance significantly\n",
    "    \"\"\"\n",
    "    \n",
    "    ax_summary.text(0.05, 0.95, summary_text, transform=ax_summary.transAxes, fontsize=11,\n",
    "                   verticalalignment='top', horizontalalignment='left',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightcyan', alpha=0.8))\n",
    "    ax_summary.set_title('Analysis Summary & Recommendations', fontweight='bold')\n",
    "    ax_summary.axis('off')\n",
    "    \n",
    "    plt.suptitle('Comprehensive Tensor Creation Analysis Dashboard', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # Save visualization\n",
    "    plt.savefig(results_dir / 'tensor_creation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create visualization\n",
    "creation_viz_fig = visualize_tensor_creation_analysis(creation_data, creation_performance, creation_memory)\n",
    "print(\"üìä Tensor creation visualizations completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d44877",
   "metadata": {},
   "source": [
    "## 3. Comprehensive Tensor Operations Analysis\n",
    "\n",
    "### 3.1 Basic and Advanced Operations Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a27214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tensor_operations():\n",
    "    \"\"\"Comprehensive analysis of tensor operations with performance benchmarking.\"\"\"\n",
    "    \n",
    "    print(\"\\nüî¢ Tensor Operations Performance Analysis\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Test parameters\n",
    "    test_sizes = [(500, 500), (1000, 1000), (2000, 2000)]\n",
    "    operations_results = {}\n",
    "    \n",
    "    for size in test_sizes:\n",
    "        print(f\"\\nüìä Analyzing operations for size {size[0]}x{size[1]}...\")\n",
    "        \n",
    "        # Create test tensors\n",
    "        a = torch.randn(size, device=device)\n",
    "        b = torch.randn(size, device=device)\n",
    "        \n",
    "        size_results = {}\n",
    "        \n",
    "        # Element-wise operations\n",
    "        operations = {\n",
    "            'addition': lambda x, y: x + y,\n",
    "            'subtraction': lambda x, y: x - y,\n",
    "            'multiplication': lambda x, y: x * y,\n",
    "            'division': lambda x, y: x / y,\n",
    "            'power': lambda x, y: x ** 2,\n",
    "            'sqrt': lambda x, y: torch.sqrt(torch.abs(x)),\n",
    "            'exp': lambda x, y: torch.exp(x / 10),  # Scaled to avoid overflow\n",
    "            'log': lambda x, y: torch.log(torch.abs(x) + 1e-8),\n",
    "            'sin': lambda x, y: torch.sin(x),\n",
    "            'cos': lambda x, y: torch.cos(x)\n",
    "        }\n",
    "        \n",
    "        for op_name, op_func in operations.items():\n",
    "            # Benchmark operation\n",
    "            num_runs = 10\n",
    "            times = []\n",
    "            \n",
    "            for _ in range(num_runs):\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                start_time = time.time()\n",
    "                result = op_func(a, b)\n",
    "                \n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                end_time = time.time()\n",
    "                times.append(end_time - start_time)\n",
    "            \n",
    "            avg_time = np.mean(times)\n",
    "            std_time = np.std(times)\n",
    "            \n",
    "            # Calculate result statistics\n",
    "            result_stats = {\n",
    "                'mean': float(result.mean()),\n",
    "                'std': float(result.std()),\n",
    "                'min': float(result.min()),\n",
    "                'max': float(result.max()),\n",
    "                'shape': result.shape\n",
    "            }\n",
    "            \n",
    "            size_results[op_name] = {\n",
    "                'avg_time': avg_time,\n",
    "                'std_time': std_time,\n",
    "                'stats': result_stats,\n",
    "                'throughput': result.numel() / avg_time  # Elements per second\n",
    "            }\n",
    "            \n",
    "            print(f\"  {op_name:12} - Time: {avg_time:.6f}¬±{std_time:.6f}s, \"\n",
    "                  f\"Throughput: {size_results[op_name]['throughput']:.2e} elem/s\")\n",
    "        \n",
    "        operations_results[f\"{size[0]}x{size[1]}\"] = size_results\n",
    "    \n",
    "    return operations_results\n",
    "\n",
    "# Run operations analysis\n",
    "operations_data = analyze_tensor_operations()\n",
    "\n",
    "# Store results\n",
    "analysis_results['operation_comparisons'] = operations_data\n",
    "\n",
    "print(f\"\\nüíæ Tensor operations analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a05a1c2",
   "metadata": {},
   "source": [
    "### 3.2 Matrix Operations and Linear Algebra Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc1dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_matrix_operations():\n",
    "    \"\"\"Comprehensive analysis of matrix operations for deep learning.\"\"\"\n",
    "    \n",
    "    print(\"\\nüßÆ Matrix Operations Analysis\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    matrix_results = {}\n",
    "    \n",
    "    # Test different matrix sizes\n",
    "    test_configs = [\n",
    "        {'name': 'Small', 'size': (128, 128)},\n",
    "        {'name': 'Medium', 'size': (512, 512)},\n",
    "        {'name': 'Large', 'size': (1024, 1024)}\n",
    "    ]\n",
    "    \n",
    "    for config in test_configs:\n",
    "        size = config['size']\n",
    "        name = config['name']\n",
    "        \n",
    "        print(f\"\\nüìä Analyzing {name} matrices ({size[0]}x{size[1]})...\")\n",
    "        \n",
    "        # Create test matrices\n",
    "        A = torch.randn(size, device=device)\n",
    "        B = torch.randn(size, device=device)\n",
    "        \n",
    "        config_results = {}\n",
    "        \n",
    "        # Matrix multiplication\n",
    "        start_time = time.time()\n",
    "        matmul_result = torch.matmul(A, B)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        matmul_time = time.time() - start_time\n",
    "        \n",
    "        config_results['matmul'] = {\n",
    "            'time': matmul_time,\n",
    "            'shape': matmul_result.shape,\n",
    "            'flops': 2 * size[0] * size[1] * size[1],  # 2*n^3 for n√ón matrix multiplication\n",
    "            'gflops': (2 * size[0] * size[1] * size[1]) / (matmul_time * 1e9)\n",
    "        }\n",
    "        \n",
    "        # Transpose\n",
    "        start_time = time.time()\n",
    "        transpose_result = A.t()\n",
    "        transpose_time = time.time() - start_time\n",
    "        \n",
    "        config_results['transpose'] = {\n",
    "            'time': transpose_time,\n",
    "            'shape': transpose_result.shape\n",
    "        }\n",
    "        \n",
    "        # Determinant (for square matrices)\n",
    "        start_time = time.time()\n",
    "        det_result = torch.det(A)\n",
    "        det_time = time.time() - start_time\n",
    "        \n",
    "        config_results['determinant'] = {\n",
    "            'time': det_time,\n",
    "            'value': float(det_result)\n",
    "        }\n",
    "        \n",
    "        # Eigenvalues (expensive operation)\n",
    "        if size[0] <= 512:  # Only for smaller matrices\n",
    "            start_time = time.time()\n",
    "            eigenvals = torch.linalg.eigvals(A)\n",
    "            eigen_time = time.time() - start_time\n",
    "            \n",
    "            config_results['eigenvalues'] = {\n",
    "                'time': eigen_time,\n",
    "                'count': len(eigenvals),\n",
    "                'real_part_mean': float(eigenvals.real.mean()),\n",
    "                'imag_part_mean': float(eigenvals.imag.mean())\n",
    "            }\n",
    "        \n",
    "        # SVD (Singular Value Decomposition)\n",
    "        if size[0] <= 512:  # Only for smaller matrices\n",
    "            start_time = time.time()\n",
    "            U, S, Vh = torch.linalg.svd(A)\n",
    "            svd_time = time.time() - start_time\n",
    "            \n",
    "            config_results['svd'] = {\n",
    "                'time': svd_time,\n",
    "                'singular_values_shape': S.shape,\n",
    "                'condition_number': float(S.max() / S.min())\n",
    "            }\n",
    "        \n",
    "        matrix_results[name] = config_results\n",
    "        \n",
    "        print(f\"  Matrix Multiply: {matmul_time:.6f}s ({config_results['matmul']['gflops']:.2f} GFLOPS)\")\n",
    "        print(f\"  Transpose:       {transpose_time:.6f}s\")\n",
    "        print(f\"  Determinant:     {det_time:.6f}s (det = {det_result:.4f})\")\n",
    "        \n",
    "        if 'eigenvalues' in config_results:\n",
    "            print(f\"  Eigenvalues:     {config_results['eigenvalues']['time']:.6f}s\")\n",
    "        if 'svd' in config_results:\n",
    "            print(f\"  SVD:             {config_results['svd']['time']:.6f}s\")\n",
    "    \n",
    "    return matrix_results\n",
    "\n",
    "# Run matrix operations analysis\n",
    "matrix_data = analyze_matrix_operations()\n",
    "\n",
    "# Store results\n",
    "analysis_results['matrix_operations'] = matrix_data\n",
    "\n",
    "print(f\"\\nüíæ Matrix operations analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab8d43",
   "metadata": {},
   "source": [
    "## 4. Broadcasting and Shape Manipulation Analysis\n",
    "\n",
    "### 4.1 Broadcasting Capabilities and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e58e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_broadcasting_operations():\n",
    "    \"\"\"Comprehensive analysis of broadcasting operations and performance.\"\"\"\n",
    "    \n",
    "    print(\"\\nüì° Broadcasting Operations Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    broadcasting_results = {}\n",
    "    \n",
    "    # Test different broadcasting scenarios\n",
    "    broadcast_tests = [\n",
    "        {\n",
    "            'name': 'Scalar_Tensor',\n",
    "            'desc': 'Scalar + Tensor',\n",
    "            'setup': lambda: (torch.tensor(2.0, device=device), torch.randn(1000, 1000, device=device)),\n",
    "            'operation': lambda a, b: a + b\n",
    "        },\n",
    "        {\n",
    "            'name': 'Vector_Matrix',\n",
    "            'desc': 'Vector + Matrix',\n",
    "            'setup': lambda: (torch.randn(1000, device=device), torch.randn(1000, 1000, device=device)),\n",
    "            'operation': lambda a, b: a + b\n",
    "        },\n",
    "        {\n",
    "            'name': 'Different_Dims',\n",
    "            'desc': 'Different Dimensions',\n",
    "            'setup': lambda: (torch.randn(1, 100, 1000, device=device), torch.randn(50, 1, 1000, device=device)),\n",
    "            'operation': lambda a, b: a * b\n",
    "        },\n",
    "        {\n",
    "            'name': 'Matrix_Batch',\n",
    "            'desc': 'Matrix + Batch',\n",
    "            'setup': lambda: (torch.randn(32, 1, 100, device=device), torch.randn(1, 50, 1, device=device)),\n",
    "            'operation': lambda a, b: a + b\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for test in broadcast_tests:\n",
    "        print(f\"\\nüìä Testing {test['desc']}...\")\n",
    "        \n",
    "        # Setup tensors\n",
    "        a, b = test['setup']()\n",
    "        \n",
    "        print(f\"  Tensor A shape: {a.shape}\")\n",
    "        print(f\"  Tensor B shape: {b.shape}\")\n",
    "        \n",
    "        # Benchmark broadcasting operation\n",
    "        num_runs = 10\n",
    "        times = []\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = test['operation'](a, b)\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        std_time = np.std(times)\n",
    "        \n",
    "        broadcasting_results[test['name']] = {\n",
    "            'description': test['desc'],\n",
    "            'input_shapes': {'a': a.shape, 'b': b.shape},\n",
    "            'output_shape': result.shape,\n",
    "            'avg_time': avg_time,\n",
    "            'std_time': std_time,\n",
    "            'memory_efficiency': (a.numel() + b.numel()) / result.numel(),\n",
    "            'throughput': result.numel() / avg_time\n",
    "        }\n",
    "        \n",
    "        print(f\"  Result shape: {result.shape}\")\n",
    "        print(f\"  Time: {avg_time:.6f}¬±{std_time:.6f}s\")\n",
    "        print(f\"  Memory efficiency: {broadcasting_results[test['name']]['memory_efficiency']:.2f}\")\n",
    "    \n",
    "    return broadcasting_results\n",
    "\n",
    "# Run broadcasting analysis\n",
    "broadcasting_data = analyze_broadcasting_operations()\n",
    "\n",
    "# Store results\n",
    "analysis_results['broadcasting_analysis'] = broadcasting_data\n",
    "\n",
    "print(f\"\\nüíæ Broadcasting analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6cb05",
   "metadata": {},
   "source": [
    "### 4.2 Shape Manipulation and Reshaping Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a68c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_shape_operations():\n",
    "    \"\"\"Comprehensive analysis of shape manipulation operations.\"\"\"\n",
    "    \n",
    "    print(\"\\nüîÑ Shape Manipulation Analysis\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    shape_results = {}\n",
    "    \n",
    "    # Create test tensor\n",
    "    original_tensor = torch.randn(8, 16, 32, 64, device=device)\n",
    "    print(f\"Original tensor shape: {original_tensor.shape}\")\n",
    "    print(f\"Total elements: {original_tensor.numel()}\")\n",
    "    \n",
    "    # Define shape operations\n",
    "    shape_operations = {\n",
    "        'flatten': {\n",
    "            'operation': lambda x: x.flatten(),\n",
    "            'description': 'Flatten all dimensions'\n",
    "        },\n",
    "        'view_2d': {\n",
    "            'operation': lambda x: x.view(-1, 64),\n",
    "            'description': 'Reshape to 2D (infer first dim)'\n",
    "        },\n",
    "        'reshape_4d': {\n",
    "            'operation': lambda x: x.reshape(4, 32, 32, 64),\n",
    "            'description': 'Reshape to different 4D'\n",
    "        },\n",
    "        'permute': {\n",
    "            'operation': lambda x: x.permute(3, 2, 1, 0),\n",
    "            'description': 'Permute dimensions'\n",
    "        },\n",
    "        'transpose': {\n",
    "            'operation': lambda x: x.transpose(1, 2),\n",
    "            'description': 'Transpose two dimensions'\n",
    "        },\n",
    "        'unsqueeze': {\n",
    "            'operation': lambda x: x.unsqueeze(0),\n",
    "            'description': 'Add batch dimension'\n",
    "        },\n",
    "        'squeeze': {\n",
    "            'operation': lambda x: x.squeeze(),\n",
    "            'description': 'Remove singleton dimensions'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for op_name, op_info in shape_operations.items():\n",
    "        print(f\"\\nüìä Testing {op_info['description']}...\")\n",
    "        \n",
    "        try:\n",
    "            # Benchmark operation\n",
    "            num_runs = 100  # More runs for fast operations\n",
    "            times = []\n",
    "            \n",
    "            for _ in range(num_runs):\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                start_time = time.time()\n",
    "                result = op_info['operation'](original_tensor)\n",
    "                \n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                end_time = time.time()\n",
    "                times.append(end_time - start_time)\n",
    "            \n",
    "            avg_time = np.mean(times)\n",
    "            std_time = np.std(times)\n",
    "            \n",
    "            # Check if operation creates a view or copy\n",
    "            shares_memory = result.data_ptr() == original_tensor.data_ptr()\n",
    "            \n",
    "            shape_results[op_name] = {\n",
    "                'description': op_info['description'],\n",
    "                'input_shape': original_tensor.shape,\n",
    "                'output_shape': result.shape,\n",
    "                'avg_time': avg_time,\n",
    "                'std_time': std_time,\n",
    "                'shares_memory': shares_memory,\n",
    "                'is_contiguous': result.is_contiguous(),\n",
    "                'elements_preserved': result.numel() == original_tensor.numel()\n",
    "            }\n",
    "            \n",
    "            print(f\"  Result shape: {result.shape}\")\n",
    "            print(f\"  Time: {avg_time:.8f}¬±{std_time:.8f}s\")\n",
    "            print(f\"  Shares memory: {shares_memory}\")\n",
    "            print(f\"  Contiguous: {result.is_contiguous()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            shape_results[op_name] = {\n",
    "                'description': op_info['description'],\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    return shape_results\n",
    "\n",
    "# Run shape operations analysis\n",
    "shape_data = analyze_shape_operations()\n",
    "\n",
    "# Store results\n",
    "analysis_results['shape_operations'] = shape_data\n",
    "\n",
    "print(f\"\\nüíæ Shape operations analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ec6ee",
   "metadata": {},
   "source": [
    "## 5. Device Performance and Memory Analysis\n",
    "\n",
    "### 5.1 CPU vs GPU Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_device_performance():\n",
    "    \"\"\"Comprehensive device performance analysis comparing CPU, GPU, and MPS.\"\"\"\n",
    "    \n",
    "    print(\"\\nüöÄ Device Performance Analysis\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    device_results = {}\n",
    "    available_devices = ['cpu']\n",
    "    \n",
    "    # Check available devices\n",
    "    if torch.cuda.is_available():\n",
    "        available_devices.append('cuda')\n",
    "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        available_devices.append('mps')\n",
    "    \n",
    "    print(f\"Available devices: {available_devices}\")\n",
    "    \n",
    "    # Test configurations\n",
    "    test_configs = [\n",
    "        {'name': 'Small Matrix Multiply', 'size': (512, 512), 'operation': 'matmul'},\n",
    "        {'name': 'Large Matrix Multiply', 'size': (2048, 2048), 'operation': 'matmul'},\n",
    "        {'name': 'Element-wise Operations', 'size': (1000, 1000), 'operation': 'elementwise'},\n",
    "        {'name': 'Reduction Operations', 'size': (1000, 1000, 100), 'operation': 'reduction'}\n",
    "    ]\n",
    "    \n",
    "    for config in test_configs:\n",
    "        print(f\"\\nüìä Testing {config['name']}...\")\n",
    "        config_results = {}\n",
    "        \n",
    "        for device_name in available_devices:\n",
    "            test_device = torch.device(device_name)\n",
    "            print(f\"  Testing on {device_name.upper()}...\")\n",
    "            \n",
    "            try:\n",
    "                if config['operation'] == 'matmul':\n",
    "                    # Matrix multiplication test\n",
    "                    a = torch.randn(config['size'], device=test_device)\n",
    "                    b = torch.randn(config['size'], device=test_device)\n",
    "                    \n",
    "                    # Warm-up\n",
    "                    for _ in range(3):\n",
    "                        _ = torch.matmul(a, b)\n",
    "                    \n",
    "                    # Benchmark\n",
    "                    times = []\n",
    "                    num_runs = 10\n",
    "                    \n",
    "                    for _ in range(num_runs):\n",
    "                        if test_device.type == 'cuda':\n",
    "                            torch.cuda.synchronize()\n",
    "                        \n",
    "                        start_time = time.time()\n",
    "                        result = torch.matmul(a, b)\n",
    "                        \n",
    "                        if test_device.type == 'cuda':\n",
    "                            torch.cuda.synchronize()\n",
    "                        \n",
    "                        end_time = time.time()\n",
    "                        times.append(end_time - start_time)\n",
    "                    \n",
    "                    flops = 2 * config['size'][0] * config['size'][1] * config['size'][1]\n",
    "                    \n",
    "                elif config['operation'] == 'elementwise':\n",
    "                    # Element-wise operations test\n",
    "                    a = torch.randn(config['size'], device=test_device)\n",
    "                    b = torch.randn(config['size'], device=test_device)\n",
    "                    \n",
    "                    times = []\n",
    "                    num_runs = 20\n",
    "                    \n",
    "                    for _ in range(num_runs):\n",
    "                        if test_device.type == 'cuda':\n",
    "                            torch.cuda.synchronize()\n",
    "                        \n",
    "                        start_time = time.time()\n",
    "                        result = a + b * torch.sin(a) - torch.cos(b)\n",
    "                        \n",
    "                        if test_device.type == 'cuda':\n",
    "                            torch.cuda.synchronize()\n",
    "                        \n",
    "                        end_time = time.time()\n",
    "                        times.append(end_time - start_time)\n",
    "                    \n",
    "                    flops = a.numel() * 5  # Approximate FLOPS for the operations\n",
    "                \n",
    "                elif config['operation'] == 'reduction':\n",
    "                    # Reduction operations test\n",
    "                    a = torch.randn(config['size'], device=test_device)\n",
    "                    \n",
    "                    times = []\n",
    "                    num_runs = 20\n",
    "                    \n",
    "                    for _ in range(num_runs):\n",
    "                        if test_device.type == 'cuda':\n",
    "                            torch.cuda.synchronize()\n",
    "                        \n",
    "                        start_time = time.time()\n",
    "                        result = a.sum(dim=2).mean(dim=0).std()\n",
    "                        \n",
    "                        if test_device.type == 'cuda':\n",
    "                            torch.cuda.synchronize()\n",
    "                        \n",
    "                        end_time = time.time()\n",
    "                        times.append(end_time - start_time)\n",
    "                    \n",
    "                    flops = a.numel() * 3  # Approximate FLOPS\n",
    "                \n",
    "                avg_time = np.mean(times)\n",
    "                std_time = np.std(times)\n",
    "                \n",
    "                config_results[device_name] = {\n",
    "                    'avg_time': avg_time,\n",
    "                    'std_time': std_time,\n",
    "                    'gflops': flops / (avg_time * 1e9),\n",
    "                    'throughput': a.numel() / avg_time,\n",
    "                    'memory_gb': a.numel() * a.element_size() / (1024**3)\n",
    "                }\n",
    "                \n",
    "                print(f\"    Time: {avg_time:.6f}¬±{std_time:.6f}s\")\n",
    "                print(f\"    GFLOPS: {config_results[device_name]['gflops']:.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error on {device_name}: {e}\")\n",
    "                config_results[device_name] = {'error': str(e)}\n",
    "        \n",
    "        device_results[config['name']] = config_results\n",
    "    \n",
    "    return device_results\n",
    "\n",
    "# Run device performance analysis\n",
    "device_performance_data = analyze_device_performance()\n",
    "\n",
    "# Store results\n",
    "analysis_results['device_performance'] = device_performance_data\n",
    "\n",
    "print(f\"\\nüíæ Device performance analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a5748",
   "metadata": {},
   "source": [
    "### 5.2 Memory Management and Optimization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_management():\n",
    "    \"\"\"Comprehensive memory management analysis.\"\"\"\n",
    "    \n",
    "    print(\"\\nüíæ Memory Management Analysis\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    memory_results = {}\n",
    "    \n",
    "    def get_memory_info():\n",
    "        \"\"\"Get current memory information.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return {\n",
    "                'allocated_mb': torch.cuda.memory_allocated() / (1024**2),\n",
    "                'reserved_mb': torch.cuda.memory_reserved() / (1024**2),\n",
    "                'max_allocated_mb': torch.cuda.max_memory_allocated() / (1024**2)\n",
    "            }\n",
    "        else:\n",
    "            # For CPU, use process memory\n",
    "            process = psutil.Process()\n",
    "            return {\n",
    "                'allocated_mb': process.memory_info().rss / (1024**2),\n",
    "                'reserved_mb': process.memory_info().vms / (1024**2),\n",
    "                'max_allocated_mb': process.memory_info().rss / (1024**2)\n",
    "            }\n",
    "    \n",
    "    # Initial memory state\n",
    "    initial_memory = get_memory_info()\n",
    "    print(f\"Initial memory state: {initial_memory}\")\n",
    "    \n",
    "    # Test 1: Memory allocation patterns\n",
    "    print(f\"\\nüìä Testing memory allocation patterns...\")\n",
    "    \n",
    "    allocation_test = {}\n",
    "    tensor_sizes = [100, 500, 1000, 2000]\n",
    "    \n",
    "    for size in tensor_sizes:\n",
    "        print(f\"  Creating {size}x{size} tensor...\")\n",
    "        \n",
    "        before_memory = get_memory_info()\n",
    "        tensor = torch.randn(size, size, device=device)\n",
    "        after_memory = get_memory_info()\n",
    "        \n",
    "        memory_used = after_memory['allocated_mb'] - before_memory['allocated_mb']\n",
    "        expected_memory = (size * size * 4) / (1024**2)  # 4 bytes per float32\n",
    "        \n",
    "        allocation_test[f\"{size}x{size}\"] = {\n",
    "            'actual_memory_mb': memory_used,\n",
    "            'expected_memory_mb': expected_memory,\n",
    "            'efficiency': expected_memory / memory_used if memory_used > 0 else 0,\n",
    "            'tensor_shape': tensor.shape\n",
    "        }\n",
    "        \n",
    "        print(f\"    Memory used: {memory_used:.2f}MB (expected: {expected_memory:.2f}MB)\")\n",
    "        \n",
    "        # Clean up\n",
    "        del tensor\n",
    "    \n",
    "    memory_results['allocation_patterns'] = allocation_test\n",
    "    \n",
    "    # Test 2: In-place vs out-of-place operations\n",
    "    print(f\"\\nüìä Testing in-place vs out-of-place operations...\")\n",
    "    \n",
    "    test_tensor = torch.randn(1000, 1000, device=device)\n",
    "    original_ptr = test_tensor.data_ptr()\n",
    "    \n",
    "    # Out-of-place operation\n",
    "    before_memory = get_memory_info()\n",
    "    out_of_place_result = test_tensor + 1\n",
    "    after_memory = get_memory_info()\n",
    "    out_of_place_memory = after_memory['allocated_mb'] - before_memory['allocated_mb']\n",
    "    \n",
    "    # In-place operation\n",
    "    test_tensor_copy = test_tensor.clone()\n",
    "    before_memory = get_memory_info()\n",
    "    test_tensor_copy.add_(1)\n",
    "    after_memory = get_memory_info()\n",
    "    in_place_memory = after_memory['allocated_mb'] - before_memory['allocated_mb']\n",
    "    \n",
    "    memory_results['inplace_comparison'] = {\n",
    "        'out_of_place_memory_mb': out_of_place_memory,\n",
    "        'in_place_memory_mb': in_place_memory,\n",
    "        'memory_saved_mb': out_of_place_memory - in_place_memory,\n",
    "        'shares_memory': test_tensor_copy.data_ptr() == original_ptr\n",
    "    }\n",
    "    \n",
    "    print(f\"  Out-of-place memory: {out_of_place_memory:.2f}MB\")\n",
    "    print(f\"  In-place memory: {in_place_memory:.2f}MB\")\n",
    "    print(f\"  Memory saved: {out_of_place_memory - in_place_memory:.2f}MB\")\n",
    "    \n",
    "    # Test 3: Memory cleanup effectiveness\n",
    "    print(f\"\\nüìä Testing memory cleanup...\")\n",
    "    \n",
    "    before_cleanup = get_memory_info()\n",
    "    \n",
    "    # Create large tensors\n",
    "    large_tensors = []\n",
    "    for i in range(5):\n",
    "        tensor = torch.randn(1000, 1000, device=device)\n",
    "        large_tensors.append(tensor)\n",
    "    \n",
    "    after_allocation = get_memory_info()\n",
    "    \n",
    "    # Delete tensors\n",
    "    del large_tensors\n",
    "    gc.collect()  # Force garbage collection\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    after_cleanup = get_memory_info()\n",
    "    \n",
    "    memory_results['cleanup_effectiveness'] = {\n",
    "        'before_cleanup_mb': before_cleanup['allocated_mb'],\n",
    "        'after_allocation_mb': after_allocation['allocated_mb'],\n",
    "        'after_cleanup_mb': after_cleanup['allocated_mb'],\n",
    "        'memory_allocated_mb': after_allocation['allocated_mb'] - before_cleanup['allocated_mb'],\n",
    "        'memory_freed_mb': after_allocation['allocated_mb'] - after_cleanup['allocated_mb'],\n",
    "        'cleanup_efficiency': (after_allocation['allocated_mb'] - after_cleanup['allocated_mb']) / \n",
    "                             (after_allocation['allocated_mb'] - before_cleanup['allocated_mb'])\n",
    "    }\n",
    "    \n",
    "    print(f\"  Memory allocated: {memory_results['cleanup_effectiveness']['memory_allocated_mb']:.2f}MB\")\n",
    "    print(f\"  Memory freed: {memory_results['cleanup_effectiveness']['memory_freed_mb']:.2f}MB\")\n",
    "    print(f\"  Cleanup efficiency: {memory_results['cleanup_effectiveness']['cleanup_efficiency']:.2%}\")\n",
    "    \n",
    "    return memory_results\n",
    "\n",
    "# Run memory management analysis\n",
    "memory_data = analyze_memory_management()\n",
    "\n",
    "# Store results\n",
    "analysis_results['memory_usage_analysis'] = memory_data\n",
    "\n",
    "print(f\"\\nüíæ Memory management analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7389b166",
   "metadata": {},
   "source": [
    "## 6. Practical Machine Learning Applications\n",
    "\n",
    "### 6.1 Real-World Tensor Operations for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ml_applications():\n",
    "    \"\"\"Analysis of practical machine learning applications using tensor operations.\"\"\"\n",
    "    \n",
    "    print(\"\\nüé® Machine Learning Applications Analysis\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    ml_results = {}\n",
    "    \n",
    "    # Application 1: Data Preprocessing Pipeline\n",
    "    print(f\"\\nüìä Application 1: Data Preprocessing Pipeline\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Simulate a dataset (e.g., image data flattened)\n",
    "    batch_size, features = 128, 784  # MNIST-like dataset\n",
    "    raw_data = torch.randn(batch_size, features, device=device) * 50 + 100  # Simulate raw pixel values\n",
    "    \n",
    "    preprocessing_times = {}\n",
    "    \n",
    "    # Z-score normalization\n",
    "    start_time = time.time()\n",
    "    mean = raw_data.mean(dim=0, keepdim=True)\n",
    "    std = raw_data.std(dim=0, keepdim=True)\n",
    "    z_normalized = (raw_data - mean) / (std + 1e-8)\n",
    "    z_norm_time = time.time() - start_time\n",
    "    preprocessing_times['z_normalization'] = z_norm_time\n",
    "    \n",
    "    # Min-max normalization\n",
    "    start_time = time.time()\n",
    "    min_vals = raw_data.min(dim=0, keepdim=True)[0]\n",
    "    max_vals = raw_data.max(dim=0, keepdim=True)[0]\n",
    "    minmax_normalized = (raw_data - min_vals) / (max_vals - min_vals + 1e-8)\n",
    "    minmax_norm_time = time.time() - start_time\n",
    "    preprocessing_times['minmax_normalization'] = minmax_norm_time\n",
    "    \n",
    "    # Data augmentation (rotation simulation)\n",
    "    start_time = time.time()\n",
    "    # Simulate random rotation by permuting features\n",
    "    permutation_indices = torch.randperm(features, device=device)\n",
    "    augmented_data = raw_data[:, permutation_indices]\n",
    "    augmentation_time = time.time() - start_time\n",
    "    preprocessing_times['data_augmentation'] = augmentation_time\n",
    "    \n",
    "    ml_results['data_preprocessing'] = {\n",
    "        'batch_size': batch_size,\n",
    "        'features': features,\n",
    "        'original_stats': {\n",
    "            'mean': float(raw_data.mean()),\n",
    "            'std': float(raw_data.std()),\n",
    "            'min': float(raw_data.min()),\n",
    "            'max': float(raw_data.max())\n",
    "        },\n",
    "        'z_normalized_stats': {\n",
    "            'mean': float(z_normalized.mean()),\n",
    "            'std': float(z_normalized.std())\n",
    "        },\n",
    "        'minmax_normalized_stats': {\n",
    "            'min': float(minmax_normalized.min()),\n",
    "            'max': float(minmax_normalized.max())\n",
    "        },\n",
    "        'timing': preprocessing_times\n",
    "    }\n",
    "    \n",
    "    print(f\"  Original data - Mean: {raw_data.mean():.2f}, Std: {raw_data.std():.2f}\")\n",
    "    print(f\"  Z-normalized - Mean: {z_normalized.mean():.4f}, Std: {z_normalized.std():.4f}\")\n",
    "    print(f\"  MinMax normalized - Min: {minmax_normalized.min():.4f}, Max: {minmax_normalized.max():.4f}\")\n",
    "    print(f\"  Z-norm time: {z_norm_time:.6f}s, MinMax time: {minmax_norm_time:.6f}s\")\n",
    "    \n",
    "    # Application 2: Neural Network Forward Pass Simulation\n",
    "    print(f\"\\nüìä Application 2: Neural Network Forward Pass\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Simulate a 3-layer MLP\n",
    "    layer_configs = [\n",
    "        {'input': 784, 'output': 256, 'activation': 'relu'},\n",
    "        {'input': 256, 'output': 128, 'activation': 'relu'},\n",
    "        {'input': 128, 'output': 10, 'activation': 'softmax'}\n",
    "    ]\n",
    "    \n",
    "    forward_pass_results = {}\n",
    "    \n",
    "    # Initialize weights\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i, config in enumerate(layer_configs):\n",
    "        w = torch.randn(config['input'], config['output'], device=device) * 0.01\n",
    "        b = torch.zeros(config['output'], device=device)\n",
    "        weights.append(w)\n",
    "        biases.append(b)\n",
    "    \n",
    "    # Forward pass timing\n",
    "    x = z_normalized  # Use normalized data\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, (w, b, config) in enumerate(zip(weights, biases, layer_configs)):\n",
    "        # Linear transformation\n",
    "        x = torch.matmul(x, w) + b\n",
    "        \n",
    "        # Apply activation\n",
    "        if config['activation'] == 'relu':\n",
    "            x = torch.relu(x)\n",
    "        elif config['activation'] == 'softmax':\n",
    "            x = torch.softmax(x, dim=1)\n",
    "    \n",
    "    forward_time = time.time() - start_time\n",
    "    \n",
    "    forward_pass_results = {\n",
    "        'batch_size': batch_size,\n",
    "        'network_architecture': [config['input'] for config in layer_configs] + [layer_configs[-1]['output']],\n",
    "        'total_parameters': sum(w.numel() + b.numel() for w, b in zip(weights, biases)),\n",
    "        'forward_pass_time': forward_time,\n",
    "        'output_shape': x.shape,\n",
    "        'output_stats': {\n",
    "            'mean': float(x.mean()),\n",
    "            'std': float(x.std()),\n",
    "            'sum_per_sample': x.sum(dim=1).tolist()[:5]  # First 5 samples\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    ml_results['neural_network_forward'] = forward_pass_results\n",
    "    \n",
    "    print(f\"  Network: {forward_pass_results['network_architecture']}\")\n",
    "    print(f\"  Total parameters: {forward_pass_results['total_parameters']:,}\")\n",
    "    print(f\"  Forward pass time: {forward_time:.6f}s\")\n",
    "    print(f\"  Output shape: {x.shape}\")\n",
    "    print(f\"  Probability sums (first 5): {x.sum(dim=1)[:5].tolist()}\")\n",
    "    \n",
    "    # Application 3: Batch Processing and Loss Computation\n",
    "    print(f\"\\nüìä Application 3: Loss Computation and Backpropagation Setup\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    # Generate ground truth labels\n",
    "    true_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "    predictions = x  # Use network output\n",
    "    \n",
    "    # One-hot encoding\n",
    "    start_time = time.time()\n",
    "    one_hot_labels = torch.zeros(batch_size, 10, device=device)\n",
    "    one_hot_labels.scatter_(1, true_labels.unsqueeze(1), 1)\n",
    "    onehot_time = time.time() - start_time\n",
    "    \n",
    "    # Cross-entropy loss computation\n",
    "    start_time = time.time()\n",
    "    log_probs = torch.log(predictions + 1e-8)\n",
    "    ce_loss = -torch.sum(one_hot_labels * log_probs) / batch_size\n",
    "    loss_time = time.time() - start_time\n",
    "    \n",
    "    # Built-in cross-entropy for comparison\n",
    "    start_time = time.time()\n",
    "    builtin_ce_loss = torch.nn.functional.cross_entropy(predictions, true_labels)\n",
    "    builtin_loss_time = time.time() - start_time\n",
    "    \n",
    "    loss_results = {\n",
    "        'batch_size': batch_size,\n",
    "        'num_classes': 10,\n",
    "        'manual_ce_loss': float(ce_loss),\n",
    "        'builtin_ce_loss': float(builtin_ce_loss),\n",
    "        'loss_difference': float(abs(ce_loss - builtin_ce_loss)),\n",
    "        'timing': {\n",
    "            'onehot_encoding': onehot_time,\n",
    "            'manual_ce_computation': loss_time,\n",
    "            'builtin_ce_computation': builtin_loss_time\n",
    "        },\n",
    "        'accuracy': float((predictions.argmax(dim=1) == true_labels).float().mean())\n",
    "    }\n",
    "    \n",
    "    ml_results['loss_computation'] = loss_results\n",
    "    \n",
    "    print(f\"  Manual CE loss: {ce_loss:.6f}\")\n",
    "    print(f\"  Built-in CE loss: {builtin_ce_loss:.6f}\")\n",
    "    print(f\"  Loss difference: {abs(ce_loss - builtin_ce_loss):.8f}\")\n",
    "    print(f\"  Accuracy: {loss_results['accuracy']:.2%}\")\n",
    "    print(f\"  One-hot time: {onehot_time:.6f}s, Manual CE: {loss_time:.6f}s, Built-in CE: {builtin_loss_time:.6f}s\")\n",
    "    \n",
    "    return ml_results\n",
    "\n",
    "# Run ML applications analysis\n",
    "ml_applications_data = analyze_ml_applications()\n",
    "\n",
    "# Store results\n",
    "analysis_results['ml_applications'] = ml_applications_data\n",
    "\n",
    "print(f\"\\nüíæ ML applications analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc24dd",
   "metadata": {},
   "source": [
    "## 7. Performance Optimization and Best Practices\n",
    "\n",
    "### 7.1 Performance Optimization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e35959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_optimization_strategies():\n",
    "    \"\"\"Comprehensive analysis of performance optimization strategies.\"\"\"\n",
    "    \n",
    "    print(\"\\nüìà Performance Optimization Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    optimization_results = {}\n",
    "    \n",
    "    # Strategy 1: Data Type Optimization\n",
    "    print(f\"\\nüìä Strategy 1: Data Type Optimization\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    size = (1000, 1000)\n",
    "    dtypes_to_test = [torch.float64, torch.float32, torch.float16]\n",
    "    \n",
    "    dtype_results = {}\n",
    "    \n",
    "    for dtype in dtypes_to_test:\n",
    "        if dtype == torch.float16 and device.type == 'cpu':\n",
    "            # Skip float16 on CPU as it's not well supported\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Memory usage\n",
    "            tensor = torch.randn(size, dtype=dtype, device=device)\n",
    "            memory_mb = tensor.numel() * tensor.element_size() / (1024**2)\n",
    "            \n",
    "            # Performance test\n",
    "            a = torch.randn(size, dtype=dtype, device=device)\n",
    "            b = torch.randn(size, dtype=dtype, device=device)\n",
    "            \n",
    "            times = []\n",
    "            for _ in range(10):\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                start_time = time.time()\n",
    "                result = torch.matmul(a, b)\n",
    "                \n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                end_time = time.time()\n",
    "                times.append(end_time - start_time)\n",
    "            \n",
    "            avg_time = np.mean(times)\n",
    "            \n",
    "            dtype_results[str(dtype)] = {\n",
    "                'memory_mb': memory_mb,\n",
    "                'avg_time': avg_time,\n",
    "                'relative_memory': memory_mb / dtype_results.get('torch.float64', {}).get('memory_mb', memory_mb),\n",
    "                'relative_speed': avg_time / dtype_results.get('torch.float64', {}).get('avg_time', avg_time) if 'torch.float64' in dtype_results else 1.0\n",
    "            }\n",
    "            \n",
    "            print(f\"  {str(dtype):15} - Memory: {memory_mb:6.2f}MB, Time: {avg_time:.6f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  {str(dtype):15} - Error: {e}\")\n",
    "    \n",
    "    optimization_results['data_type_optimization'] = dtype_results\n",
    "    \n",
    "    # Strategy 2: Vectorization vs Loops\n",
    "    print(f\"\\nüìä Strategy 2: Vectorization vs Loop Operations\")\n",
    "    print(\"-\" * 48)\n",
    "    \n",
    "    def slow_element_wise_operation(a, b):\n",
    "        \"\"\"Slow loop-based operation (CPU only).\"\"\"\n",
    "        result = torch.zeros_like(a)\n",
    "        for i in range(a.size(0)):\n",
    "            for j in range(a.size(1)):\n",
    "                result[i, j] = a[i, j] * b[i, j] + torch.sin(a[i, j])\n",
    "        return result\n",
    "    \n",
    "    def fast_vectorized_operation(a, b):\n",
    "        \"\"\"Fast vectorized operation.\"\"\"\n",
    "        return a * b + torch.sin(a)\n",
    "    \n",
    "    test_size = (200, 200)  # Smaller size for loop test\n",
    "    a_cpu = torch.randn(test_size)\n",
    "    b_cpu = torch.randn(test_size)\n",
    "    a_device = a_cpu.to(device)\n",
    "    b_device = b_cpu.to(device)\n",
    "    \n",
    "    vectorization_results = {}\n",
    "    \n",
    "    # Test vectorized operation on both CPU and device\n",
    "    for test_device, a_test, b_test in [('cpu', a_cpu, b_cpu), (str(device), a_device, b_device)]:\n",
    "        start_time = time.time()\n",
    "        fast_result = fast_vectorized_operation(a_test, b_test)\n",
    "        if test_device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        fast_time = time.time() - start_time\n",
    "        \n",
    "        vectorization_results[f'vectorized_{test_device}'] = {\n",
    "            'time': fast_time,\n",
    "            'throughput': a_test.numel() / fast_time\n",
    "        }\n",
    "        \n",
    "        print(f\"  Vectorized ({test_device:4}): {fast_time:.6f}s\")\n",
    "    \n",
    "    # Test loop operation only on CPU\n",
    "    if test_size[0] <= 200:  # Only for small sizes\n",
    "        start_time = time.time()\n",
    "        slow_result = slow_element_wise_operation(a_cpu, b_cpu)\n",
    "        slow_time = time.time() - start_time\n",
    "        \n",
    "        vectorization_results['loop_cpu'] = {\n",
    "            'time': slow_time,\n",
    "            'throughput': a_cpu.numel() / slow_time\n",
    "        }\n",
    "        \n",
    "        speedup = slow_time / vectorization_results['vectorized_cpu']['time']\n",
    "        print(f\"  Loop-based (cpu ): {slow_time:.6f}s\")\n",
    "        print(f\"  Speedup: {speedup:.1f}x faster with vectorization\")\n",
    "        \n",
    "        vectorization_results['speedup'] = speedup\n",
    "    \n",
    "    optimization_results['vectorization_comparison'] = vectorization_results\n",
    "    \n",
    "    # Strategy 3: Memory Layout Optimization\n",
    "    print(f\"\\nüìä Strategy 3: Memory Layout Optimization\")\n",
    "    print(\"-\" * 42)\n",
    "    \n",
    "    # Create contiguous vs non-contiguous tensors\n",
    "    original = torch.randn(1000, 1000, device=device)\n",
    "    transposed = original.t()  # Non-contiguous\n",
    "    made_contiguous = transposed.contiguous()\n",
    "    \n",
    "    layout_results = {}\n",
    "    \n",
    "    # Test operations on different layouts\n",
    "    for name, tensor in [('contiguous', original), ('non_contiguous', transposed), ('made_contiguous', made_contiguous)]:\n",
    "        times = []\n",
    "        for _ in range(10):\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = tensor + 1.0  # Simple operation\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        \n",
    "        layout_results[name] = {\n",
    "            'is_contiguous': tensor.is_contiguous(),\n",
    "            'avg_time': avg_time,\n",
    "            'shape': tensor.shape,\n",
    "            'stride': tensor.stride()\n",
    "        }\n",
    "        \n",
    "        print(f\"  {name:15} - Contiguous: {tensor.is_contiguous()}, Time: {avg_time:.8f}s\")\n",
    "    \n",
    "    optimization_results['memory_layout'] = layout_results\n",
    "    \n",
    "    # Strategy 4: Batch Size Impact\n",
    "    print(f\"\\nüìä Strategy 4: Batch Size Impact Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    batch_sizes = [1, 8, 32, 128, 512]\n",
    "    input_size, output_size = 784, 10\n",
    "    \n",
    "    batch_results = {}\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        try:\n",
    "            # Create data\n",
    "            x = torch.randn(batch_size, input_size, device=device)\n",
    "            w = torch.randn(input_size, output_size, device=device)\n",
    "            \n",
    "            # Benchmark matrix multiplication\n",
    "            times = []\n",
    "            for _ in range(20):\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                start_time = time.time()\n",
    "                result = torch.matmul(x, w)\n",
    "                \n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                end_time = time.time()\n",
    "                times.append(end_time - start_time)\n",
    "            \n",
    "            avg_time = np.mean(times)\n",
    "            throughput = (batch_size * input_size * output_size) / avg_time  # Operations per second\n",
    "            \n",
    "            batch_results[batch_size] = {\n",
    "                'avg_time': avg_time,\n",
    "                'throughput': throughput,\n",
    "                'time_per_sample': avg_time / batch_size\n",
    "            }\n",
    "            \n",
    "            print(f\"  Batch size {batch_size:3}: {avg_time:.6f}s, {avg_time/batch_size:.8f}s/sample\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Batch size {batch_size:3}: Error - {e}\")\n",
    "    \n",
    "    optimization_results['batch_size_analysis'] = batch_results\n",
    "    \n",
    "    return optimization_results\n",
    "\n",
    "\n",
    "# Run optimization strategies analysis\n",
    "optimization_data = analyze_optimization_strategies()\n",
    "\n",
    "# Store results\n",
    "analysis_results['performance_optimization'] = optimization_data\n",
    "\n",
    "print(f\"\\nüíæ Performance optimization analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3f51d",
   "metadata": {},
   "source": [
    "### 7.2 Comprehensive Performance Benchmarking Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_performance_dashboard():\n",
    "    \"\"\"Create comprehensive performance analysis dashboard.\"\"\"\n",
    "    \n",
    "    print(\"\\nüìä Creating Comprehensive Performance Dashboard\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig = plt.figure(figsize=(24, 20))\n",
    "    gs = fig.add_gridspec(5, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Device Performance Comparison\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    \n",
    "    if 'device_performance' in analysis_results:\n",
    "        device_data = analysis_results['device_performance']\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        test_names = list(device_data.keys())\n",
    "        devices = []\n",
    "        \n",
    "        # Find common devices across tests\n",
    "        for test_name, test_data in device_data.items():\n",
    "            for device_name in test_data.keys():\n",
    "                if device_name not in devices and 'error' not in test_data[device_name]:\n",
    "                    devices.append(device_name)\n",
    "        \n",
    "        if devices and test_names:\n",
    "            x = np.arange(len(test_names))\n",
    "            width = 0.25\n",
    "            \n",
    "            for i, device_name in enumerate(devices):\n",
    "                gflops_values = []\n",
    "                for test_name in test_names:\n",
    "                    if device_name in device_data[test_name] and 'gflops' in device_data[test_name][device_name]:\n",
    "                        gflops_values.append(device_data[test_name][device_name]['gflops'])\n",
    "                    else:\n",
    "                        gflops_values.append(0)\n",
    "                \n",
    "                bars = ax1.bar(x + i * width, gflops_values, width, \n",
    "                              label=device_name.upper(), alpha=0.8)\n",
    "                \n",
    "                # Add value labels\n",
    "                for bar, val in zip(bars, gflops_values):\n",
    "                    if val > 0:\n",
    "                        height = bar.get_height()\n",
    "                        ax1.text(bar.get_x() + bar.get_width()/2., height + max(gflops_values)*0.01,\n",
    "                               f'{val:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "            ax1.set_xlabel('Test Configuration')\n",
    "            ax1.set_ylabel('Performance (GFLOPS)')\n",
    "            ax1.set_title('Device Performance Comparison', fontweight='bold')\n",
    "            ax1.set_xticks(x + width * (len(devices) - 1) / 2)\n",
    "            ax1.set_xticklabels([name.replace(' ', '\\n') for name in test_names], fontsize=9)\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Memory Usage Analysis\n",
    "    ax2 = fig.add_subplot(gs[0, 2:])\n",
    "    \n",
    "    if 'memory_usage_analysis' in analysis_results:\n",
    "        memory_data = analysis_results['memory_usage_analysis']\n",
    "        \n",
    "        if 'allocation_patterns' in memory_data:\n",
    "            sizes = list(memory_data['allocation_patterns'].keys())\n",
    "            actual_memory = [memory_data['allocation_patterns'][size]['actual_memory_mb'] for size in sizes]\n",
    "            expected_memory = [memory_data['allocation_patterns'][size]['expected_memory_mb'] for size in sizes]\n",
    "            \n",
    "            x = np.arange(len(sizes))\n",
    "            width = 0.35\n",
    "            \n",
    "            bars1 = ax2.bar(x - width/2, actual_memory, width, label='Actual', alpha=0.8, color='lightcoral')\n",
    "            bars2 = ax2.bar(x + width/2, expected_memory, width, label='Expected', alpha=0.8, color='skyblue')\n",
    "            \n",
    "            ax2.set_xlabel('Tensor Size')\n",
    "            ax2.set_ylabel('Memory Usage (MB)')\n",
    "            ax2.set_title('Memory Allocation Patterns', fontweight='bold')\n",
    "            ax2.set_xticks(x)\n",
    "            ax2.set_xticklabels(sizes)\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Operation Performance Heatmap\n",
    "    ax3 = fig.add_subplot(gs[1, :2])\n",
    "    \n",
    "    if 'operation_comparisons' in analysis_results:\n",
    "        op_data = analysis_results['operation_comparisons']\n",
    "        \n",
    "        # Create heatmap data\n",
    "        sizes = list(op_data.keys())\n",
    "        operations = list(op_data[sizes[0]].keys()) if sizes else []\n",
    "        \n",
    "        if sizes and operations:\n",
    "            heatmap_data = []\n",
    "            for size in sizes:\n",
    "                row = [op_data[size][op]['throughput'] for op in operations]\n",
    "                heatmap_data.append(row)\n",
    "            \n",
    "            heatmap_data = np.array(heatmap_data)\n",
    "            heatmap_data = np.log10(heatmap_data + 1)  # Log scale for better visualization\n",
    "            \n",
    "            im = ax3.imshow(heatmap_data, cmap='viridis', aspect='auto')\n",
    "            ax3.set_xticks(range(len(operations)))\n",
    "            ax3.set_xticklabels([op.replace('_', ' ').title() for op in operations], rotation=45, ha='right')\n",
    "            ax3.set_yticks(range(len(sizes)))\n",
    "            ax3.set_yticklabels(sizes)\n",
    "            ax3.set_title('Operation Throughput Heatmap (Log Scale)', fontweight='bold')\n",
    "            plt.colorbar(im, ax=ax3, label='Log10(Elements/Second)')\n",
    "    \n",
    "    # 4. Data Type Performance Comparison\n",
    "    ax4 = fig.add_subplot(gs[1, 2:])\n",
    "    \n",
    "    if 'performance_optimization' in analysis_results and 'data_type_optimization' in analysis_results['performance_optimization']:\n",
    "        dtype_data = analysis_results['performance_optimization']['data_type_optimization']\n",
    "        \n",
    "        dtypes = list(dtype_data.keys())\n",
    "        memory_usage = [dtype_data[dtype]['memory_mb'] for dtype in dtypes]\n",
    "        performance = [1/dtype_data[dtype]['avg_time'] for dtype in dtypes]  # Higher is better\n",
    "        \n",
    "        # Normalize for comparison\n",
    "        memory_normalized = [mem / max(memory_usage) for mem in memory_usage]\n",
    "        performance_normalized = [perf / max(performance) for perf in performance]\n",
    "        \n",
    "        x = np.arange(len(dtypes))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax4.bar(x - width/2, memory_normalized, width, label='Memory Efficiency', alpha=0.8, color='orange')\n",
    "        bars2 = ax4.bar(x + width/2, performance_normalized, width, label='Performance', alpha=0.8, color='green')\n",
    "        \n",
    "        ax4.set_xlabel('Data Type')\n",
    "        ax4.set_ylabel('Normalized Score (1.0 = Best)')\n",
    "        ax4.set_title('Data Type Performance vs Memory Trade-off', fontweight='bold')\n",
    "        ax4.set_xticks(x)\n",
    "        ax4.set_xticklabels([dtype.split('.')[-1] for dtype in dtypes])\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Broadcasting Performance Analysis\n",
    "    ax5 = fig.add_subplot(gs[2, :2])\n",
    "    \n",
    "    if 'broadcasting_analysis' in analysis_results:\n",
    "        broadcast_data = analysis_results['broadcasting_analysis']\n",
    "        \n",
    "        test_names = list(broadcast_data.keys())\n",
    "        throughputs = [broadcast_data[test]['throughput'] for test in test_names]\n",
    "        memory_efficiencies = [broadcast_data[test]['memory_efficiency'] for test in test_names]\n",
    "        \n",
    "        # Create scatter plot\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(test_names)))\n",
    "        \n",
    "        for i, (name, throughput, mem_eff, color) in enumerate(zip(test_names, throughputs, memory_efficiencies, colors)):\n",
    "            ax5.scatter(throughput, mem_eff, s=100, c=[color], alpha=0.7, label=name.replace('_', ' '))\n",
    "        \n",
    "        ax5.set_xlabel('Throughput (Elements/Second)')\n",
    "        ax5.set_ylabel('Memory Efficiency')\n",
    "        ax5.set_title('Broadcasting Performance vs Memory Efficiency', fontweight='bold')\n",
    "        ax5.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        ax5.set_xscale('log')\n",
    "    \n",
    "    # 6. Shape Operations Performance\n",
    "    ax6 = fig.add_subplot(gs[2, 2:])\n",
    "    \n",
    "    if 'shape_operations' in analysis_results:\n",
    "        shape_data = analysis_results['shape_operations']\n",
    "        \n",
    "        operations = []\n",
    "        times = []\n",
    "        memory_sharing = []\n",
    "        \n",
    "        for op_name, op_data in shape_data.items():\n",
    "            if 'avg_time' in op_data:\n",
    "                operations.append(op_name.replace('_', ' ').title())\n",
    "                times.append(op_data['avg_time'])\n",
    "                memory_sharing.append(1 if op_data.get('shares_memory', False) else 0)\n",
    "        \n",
    "        if operations:\n",
    "            # Create grouped bar chart\n",
    "            x = np.arange(len(operations))\n",
    "            \n",
    "            # Color bars based on memory sharing\n",
    "            colors = ['lightgreen' if shares else 'lightcoral' for shares in memory_sharing]\n",
    "            \n",
    "            bars = ax6.bar(x, times, color=colors, alpha=0.8)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, time_val in zip(bars, times):\n",
    "                height = bar.get_height()\n",
    "                ax6.text(bar.get_x() + bar.get_width()/2., height + max(times)*0.01,\n",
    "                        f'{time_val:.2e}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "            \n",
    "            ax6.set_xlabel('Shape Operations')\n",
    "            ax6.set_ylabel('Time (seconds)')\n",
    "            ax6.set_title('Shape Operation Performance', fontweight='bold')\n",
    "            ax6.set_xticks(x)\n",
    "            ax6.set_xticklabels(operations, rotation=45, ha='right')\n",
    "            ax6.set_yscale('log')\n",
    "            ax6.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add legend for color coding\n",
    "            from matplotlib.patches import Patch\n",
    "            legend_elements = [Patch(facecolor='lightgreen', label='Memory Sharing'),\n",
    "                             Patch(facecolor='lightcoral', label='Memory Copy')]\n",
    "            ax6.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # 7. ML Applications Performance\n",
    "    ax7 = fig.add_subplot(gs[3, :2])\n",
    "    \n",
    "    if 'ml_applications' in analysis_results:\n",
    "        ml_data = analysis_results['ml_applications']\n",
    "        \n",
    "        # Extract timing data from different applications\n",
    "        app_names = []\n",
    "        app_times = []\n",
    "        \n",
    "        if 'data_preprocessing' in ml_data and 'timing' in ml_data['data_preprocessing']:\n",
    "            for name, time_val in ml_data['data_preprocessing']['timing'].items():\n",
    "                app_names.append(name.replace('_', ' ').title())\n",
    "                app_times.append(time_val)\n",
    "        \n",
    "        if 'neural_network_forward' in ml_data:\n",
    "            app_names.append('Forward Pass')\n",
    "            app_times.append(ml_data['neural_network_forward']['forward_pass_time'])\n",
    "        \n",
    "        if 'loss_computation' in ml_data and 'timing' in ml_data['loss_computation']:\n",
    "            for name, time_val in ml_data['loss_computation']['timing'].items():\n",
    "                app_names.append(name.replace('_', ' ').title())\n",
    "                app_times.append(time_val)\n",
    "        \n",
    "        if app_names:\n",
    "            bars = ax7.bar(app_names, app_times, alpha=0.8, color='lightblue')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, time_val in zip(bars, app_times):\n",
    "                height = bar.get_height()\n",
    "                ax7.text(bar.get_x() + bar.get_width()/2., height + max(app_times)*0.01,\n",
    "                        f'{time_val:.6f}s', ha='center', va='bottom', fontsize=8, rotation=45)\n",
    "            \n",
    "            ax7.set_xlabel('ML Operations')\n",
    "            ax7.set_ylabel('Time (seconds)')\n",
    "            ax7.set_title('Machine Learning Operations Performance', fontweight='bold')\n",
    "            ax7.tick_params(axis='x', rotation=45)\n",
    "            ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 8. Optimization Strategies Summary\n",
    "    ax8 = fig.add_subplot(gs[3, 2:])\n",
    "    \n",
    "    if 'performance_optimization' in analysis_results:\n",
    "        opt_data = analysis_results['performance_optimization']\n",
    "        \n",
    "        # Speedup from vectorization\n",
    "        vectorization_speedup = opt_data.get('vectorization_comparison', {}).get('speedup', 0)\n",
    "        \n",
    "        # Memory efficiency from in-place operations\n",
    "        if 'memory_usage_analysis' in analysis_results and 'inplace_comparison' in analysis_results['memory_usage_analysis']:\n",
    "            inplace_data = analysis_results['memory_usage_analysis']['inplace_comparison']\n",
    "            memory_savings = inplace_data.get('memory_saved_mb', 0)\n",
    "        else:\n",
    "            memory_savings = 0\n",
    "        \n",
    "        # Data type improvements\n",
    "        dtype_improvement = 0\n",
    "        if 'data_type_optimization' in opt_data:\n",
    "            float64_time = opt_data['data_type_optimization'].get('torch.float64', {}).get('avg_time', 1)\n",
    "            float32_time = opt_data['data_type_optimization'].get('torch.float32', {}).get('avg_time', 1)\n",
    "            if float64_time > 0 and float32_time > 0:\n",
    "                dtype_improvement = float64_time / float32_time\n",
    "        \n",
    "        optimization_categories = ['Vectorization\\nSpeedup', 'Memory Savings\\n(MB)', 'Data Type\\nImprovement']\n",
    "        optimization_values = [vectorization_speedup, memory_savings, dtype_improvement]\n",
    "        \n",
    "        bars = ax8.bar(optimization_categories, optimization_values, \n",
    "                      color=['green', 'orange', 'blue'], alpha=0.8)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, val in zip(bars, optimization_values):\n",
    "            if val > 0:\n",
    "                height = bar.get_height()\n",
    "                ax8.text(bar.get_x() + bar.get_width()/2., height + max(optimization_values)*0.01,\n",
    "                        f'{val:.2f}{\"x\" if \"Speedup\" in bar.get_x() or \"Improvement\" in bar.get_x() else \"\"}',\n",
    "                        ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax8.set_ylabel('Improvement Factor')\n",
    "        ax8.set_title('Optimization Strategy Effectiveness', fontweight='bold')\n",
    "        ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9. Comprehensive Summary and Recommendations\n",
    "    ax9 = fig.add_subplot(gs[4, :])\n",
    "    \n",
    "    # Calculate overall performance score\n",
    "    total_tests = len([k for k in analysis_results.keys() if k != 'system_info'])\n",
    "    completed_tests = len([k for k, v in analysis_results.items() if k != 'system_info' and v])\n",
    "    completion_rate = completed_tests / total_tests * 100 if total_tests > 0 else 0\n",
    "    \n",
    "    # Generate recommendations based on analysis results\n",
    "    recommendations = []\n",
    "    \n",
    "    if 'device_performance' in analysis_results:\n",
    "        recommendations.append(\"‚úÖ GPU acceleration provides significant performance improvements\")\n",
    "    \n",
    "    if 'performance_optimization' in analysis_results:\n",
    "        if 'vectorization_comparison' in analysis_results['performance_optimization']:\n",
    "            speedup = analysis_results['performance_optimization']['vectorization_comparison'].get('speedup', 0)\n",
    "            if speedup > 10:\n",
    "                recommendations.append(f\"‚ö° Vectorization provides {speedup:.1f}x speedup - avoid loops\")\n",
    "        \n",
    "        if 'data_type_optimization' in analysis_results['performance_optimization']:\n",
    "            recommendations.append(\"üéØ Use Float32 instead of Float64 for better performance\")\n",
    "    \n",
    "    if 'memory_usage_analysis' in analysis_results:\n",
    "        recommendations.append(\"üíæ Use in-place operations to reduce memory usage\")\n",
    "        recommendations.append(\"üßπ Regular memory cleanup prevents memory leaks\")\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "üî• COMPREHENSIVE TENSOR FUNDAMENTALS ANALYSIS SUMMARY\n",
    "\n",
    "üìä Analysis Completion: {completion_rate:.1f}% ({completed_tests}/{total_tests} modules)\n",
    "üñ•Ô∏è Primary Device: {analysis_results['system_info']['primary_device']}\n",
    "üß† PyTorch Version: {analysis_results['system_info']['pytorch_version']}\n",
    "\n",
    "‚ö° Key Performance Insights:\n",
    "{chr(10).join(f\"  ‚Ä¢ {rec}\" for rec in recommendations[:6])}\n",
    "\n",
    "üéØ Best Practices Identified:\n",
    "  ‚Ä¢ Use appropriate data types (Float32 vs Float64)\n",
    "  ‚Ä¢ Leverage vectorized operations over loops\n",
    "  ‚Ä¢ Implement in-place operations for memory efficiency\n",
    "  ‚Ä¢ Utilize GPU acceleration for large computations\n",
    "  ‚Ä¢ Maintain contiguous memory layouts\n",
    "  ‚Ä¢ Optimize batch sizes for throughput\n",
    "\n",
    "üìà Optimization Opportunities:\n",
    "  ‚Ä¢ Memory management and cleanup strategies\n",
    "  ‚Ä¢ Device-specific optimizations\n",
    "  ‚Ä¢ Broadcasting for efficient computation\n",
    "  ‚Ä¢ Shape operation performance tuning\n",
    "\n",
    "üöÄ Ready for Advanced PyTorch Concepts:\n",
    "  ‚úì Tensor creation and manipulation mastered\n",
    "  ‚úì Performance optimization understood\n",
    "  ‚úì Memory management principles learned\n",
    "  ‚úì ML application patterns established\n",
    "    \"\"\"\n",
    "    \n",
    "    ax9.text(0.02, 0.98, summary_text, transform=ax9.transAxes, fontsize=12,\n",
    "            verticalalignment='top', horizontalalignment='left',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightcyan', alpha=0.8))\n",
    "    ax9.set_title('Comprehensive Analysis Summary & Next Steps', fontweight='bold', fontsize=16)\n",
    "    ax9.axis('off')\n",
    "    \n",
    "    plt.suptitle('üî• PyTorch Tensor Fundamentals: Complete Performance Analysis Dashboard', \n",
    "                fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Save comprehensive dashboard\n",
    "    plt.savefig(results_dir / 'comprehensive_tensor_analysis_dashboard.png', \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create comprehensive dashboard\n",
    "dashboard_fig = create_comprehensive_performance_dashboard()\n",
    "print(\"üìä Comprehensive performance dashboard created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39212d9",
   "metadata": {},
   "source": [
    "## 8. Advanced Tensor Applications and Use Cases\n",
    "\n",
    "### 8.1 Real-World Deep Learning Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a43e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_advanced_tensor_applications():\n",
    "    \"\"\"Demonstrate advanced tensor applications for real-world deep learning scenarios.\"\"\"\n",
    "    \n",
    "    print(\"\\nüéØ Advanced Tensor Applications for Deep Learning\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    advanced_results = {}\n",
    "    \n",
    "    # Application 1: Attention Mechanism Simulation\n",
    "    print(f\"\\nüìä Application 1: Attention Mechanism Implementation\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Simulate transformer attention\n",
    "    batch_size, seq_length, d_model = 32, 128, 512\n",
    "    \n",
    "    # Create query, key, value matrices\n",
    "    Q = torch.randn(batch_size, seq_length, d_model, device=device)\n",
    "    K = torch.randn(batch_size, seq_length, d_model, device=device)\n",
    "    V = torch.randn(batch_size, seq_length, d_model, device=device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Scaled dot-product attention\n",
    "    d_k = d_model\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    attention_weights = torch.softmax(scores, dim=-1)\n",
    "    attention_output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    attention_time = time.time() - start_time\n",
    "    \n",
    "    attention_results = {\n",
    "        'input_shapes': {'Q': Q.shape, 'K': K.shape, 'V': V.shape},\n",
    "        'output_shape': attention_output.shape,\n",
    "        'computation_time': attention_time,\n",
    "        'attention_weights_stats': {\n",
    "            'mean': float(attention_weights.mean()),\n",
    "            'std': float(attention_weights.std()),\n",
    "            'max': float(attention_weights.max()),\n",
    "            'min': float(attention_weights.min())\n",
    "        },\n",
    "        'flops_estimate': batch_size * seq_length * seq_length * d_model * 2  # Approximate\n",
    "    }\n",
    "    \n",
    "    advanced_results['attention_mechanism'] = attention_results\n",
    "    \n",
    "    print(f\"  Input shapes: Q{Q.shape}, K{K.shape}, V{V.shape}\")\n",
    "    print(f\"  Output shape: {attention_output.shape}\")\n",
    "    print(f\"  Computation time: {attention_time:.6f}s\")\n",
    "    print(f\"  Attention weights - Mean: {attention_weights.mean():.4f}, Std: {attention_weights.std():.4f}\")\n",
    "    \n",
    "    # Application 2: Convolutional Operations Simulation\n",
    "    print(f\"\\nüìä Application 2: Convolutional Layer Implementation\")\n",
    "    print(\"-\" * 48)\n",
    "    \n",
    "    # Simulate CNN layer\n",
    "    batch_size, channels, height, width = 32, 64, 224, 224\n",
    "    out_channels, kernel_size = 128, 3\n",
    "    \n",
    "    # Input feature maps\n",
    "    input_tensor = torch.randn(batch_size, channels, height, width, device=device)\n",
    "    \n",
    "    # Convolution weights\n",
    "    conv_weight = torch.randn(out_channels, channels, kernel_size, kernel_size, device=device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Manual convolution (simplified - using built-in for efficiency)\n",
    "    conv_output = torch.nn.functional.conv2d(input_tensor, conv_weight, padding=1)\n",
    "    \n",
    "    conv_time = time.time() - start_time\n",
    "    \n",
    "    conv_results = {\n",
    "        'input_shape': input_tensor.shape,\n",
    "        'weight_shape': conv_weight.shape,\n",
    "        'output_shape': conv_output.shape,\n",
    "        'computation_time': conv_time,\n",
    "        'memory_usage_mb': (input_tensor.numel() + conv_weight.numel() + conv_output.numel()) * 4 / (1024**2),\n",
    "        'flops_estimate': batch_size * out_channels * conv_output.shape[2] * conv_output.shape[3] * channels * kernel_size * kernel_size\n",
    "    }\n",
    "    \n",
    "    advanced_results['convolution_operation'] = conv_results\n",
    "    \n",
    "    print(f\"  Input shape: {input_tensor.shape}\")\n",
    "    print(f\"  Weight shape: {conv_weight.shape}\")\n",
    "    print(f\"  Output shape: {conv_output.shape}\")\n",
    "    print(f\"  Computation time: {conv_time:.6f}s\")\n",
    "    print(f\"  Memory usage: {conv_results['memory_usage_mb']:.2f}MB\")\n",
    "    \n",
    "    # Application 3: Batch Normalization Implementation\n",
    "    print(f\"\\nüìä Application 3: Batch Normalization Implementation\")\n",
    "    print(\"-\" * 48)\n",
    "    \n",
    "    # Simulate batch normalization\n",
    "    input_bn = torch.randn(64, 256, 32, 32, device=device)  # Typical CNN feature map\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Manual batch normalization\n",
    "    eps = 1e-5\n",
    "    momentum = 0.1\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean = input_bn.mean(dim=[0, 2, 3], keepdim=True)\n",
    "    var = input_bn.var(dim=[0, 2, 3], keepdim=True, unbiased=False)\n",
    "    \n",
    "    # Normalize\n",
    "    normalized = (input_bn - mean) / torch.sqrt(var + eps)\n",
    "    \n",
    "    # Scale and shift (learnable parameters)\n",
    "    gamma = torch.ones_like(mean)\n",
    "    beta = torch.zeros_like(mean)\n",
    "    bn_output = gamma * normalized + beta\n",
    "    \n",
    "    bn_time = time.time() - start_time\n",
    "    \n",
    "    bn_results = {\n",
    "        'input_shape': input_bn.shape,\n",
    "        'output_shape': bn_output.shape,\n",
    "        'computation_time': bn_time,\n",
    "        'statistics': {\n",
    "            'input_mean': float(input_bn.mean()),\n",
    "            'input_std': float(input_bn.std()),\n",
    "            'output_mean': float(bn_output.mean()),\n",
    "            'output_std': float(bn_output.std())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    advanced_results['batch_normalization'] = bn_results\n",
    "    \n",
    "    print(f\"  Input shape: {input_bn.shape}\")\n",
    "    print(f\"  Computation time: {bn_time:.6f}s\")\n",
    "    print(f\"  Input stats - Mean: {input_bn.mean():.4f}, Std: {input_bn.std():.4f}\")\n",
    "    print(f\"  Output stats - Mean: {bn_output.mean():.4f}, Std: {bn_output.std():.4f}\")\n",
    "    \n",
    "    # Application 4: Loss Function Implementations\n",
    "    print(f\"\\nüìä Application 4: Advanced Loss Function Implementations\")\n",
    "    print(\"-\" * 53)\n",
    "    \n",
    "    # Simulate different loss functions\n",
    "    batch_size, num_classes = 128, 1000\n",
    "    predictions = torch.randn(batch_size, num_classes, device=device)\n",
    "    targets = torch.randint(0, num_classes, (batch_size,), device=device)\n",
    "    \n",
    "    loss_results = {}\n",
    "    \n",
    "    # Cross-entropy loss\n",
    "    start_time = time.time()\n",
    "    ce_loss = torch.nn.functional.cross_entropy(predictions, targets)\n",
    "    ce_time = time.time() - start_time\n",
    "    \n",
    "    loss_results['cross_entropy'] = {\n",
    "        'loss_value': float(ce_loss),\n",
    "        'computation_time': ce_time\n",
    "    }\n",
    "    \n",
    "    # Focal loss implementation\n",
    "    start_time = time.time()\n",
    "    alpha, gamma = 1.0, 2.0\n",
    "    ce_loss_focal = torch.nn.functional.cross_entropy(predictions, targets, reduction='none')\n",
    "    pt = torch.exp(-ce_loss_focal)\n",
    "    focal_loss = alpha * (1 - pt) ** gamma * ce_loss_focal\n",
    "    focal_loss = focal_loss.mean()\n",
    "    focal_time = time.time() - start_time\n",
    "    \n",
    "    loss_results['focal_loss'] = {\n",
    "        'loss_value': float(focal_loss),\n",
    "        'computation_time': focal_time\n",
    "    }\n",
    "    \n",
    "    # Label smoothing loss\n",
    "    start_time = time.time()\n",
    "    smoothing = 0.1\n",
    "    confidence = 1.0 - smoothing\n",
    "    log_probs = torch.nn.functional.log_softmax(predictions, dim=1)\n",
    "    nll_loss = torch.nn.functional.nll_loss(log_probs, targets, reduction='none')\n",
    "    smooth_loss = -log_probs.mean(dim=1)\n",
    "    label_smooth_loss = confidence * nll_loss + smoothing * smooth_loss\n",
    "    label_smooth_loss = label_smooth_loss.mean()\n",
    "    ls_time = time.time() - start_time\n",
    "    \n",
    "    loss_results['label_smoothing'] = {\n",
    "        'loss_value': float(label_smooth_loss),\n",
    "        'computation_time': ls_time\n",
    "    }\n",
    "    \n",
    "    advanced_results['loss_functions'] = loss_results\n",
    "    \n",
    "    print(f\"  Cross-entropy loss: {ce_loss:.6f} (time: {ce_time:.6f}s)\")\n",
    "    print(f\"  Focal loss: {focal_loss:.6f} (time: {focal_time:.6f}s)\")\n",
    "    print(f\"  Label smoothing loss: {label_smooth_loss:.6f} (time: {ls_time:.6f}s)\")\n",
    "    \n",
    "    return advanced_results\n",
    "\n",
    "# Import math for sqrt function\n",
    "import math\n",
    "\n",
    "# Run advanced applications demonstration\n",
    "advanced_applications_data = demonstrate_advanced_tensor_applications()\n",
    "\n",
    "# Store results\n",
    "analysis_results['advanced_applications'] = advanced_applications_data\n",
    "\n",
    "print(f\"\\nüíæ Advanced applications analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c99885",
   "metadata": {},
   "source": [
    "## 9. Final Comprehensive Assessment and Summary\n",
    "\n",
    "### 9.1 Complete Analysis Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7b8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_final_summary():\n",
    "    \"\"\"Generate comprehensive final summary with analysis results and recommendations.\"\"\"\n",
    "    \n",
    "    print(\"\\nüéØ Generating Comprehensive Final Summary\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Compile all analysis results\n",
    "    final_summary = {\n",
    "        'analysis_metadata': {\n",
    "            'completion_timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'total_analysis_modules': len(analysis_results),\n",
    "            'device_used': str(device),\n",
    "            'pytorch_version': analysis_results['system_info']['pytorch_version']\n",
    "        },\n",
    "        'performance_benchmarks': {},\n",
    "        'optimization_insights': {},\n",
    "        'best_practices': {},\n",
    "        'recommendations': {}\n",
    "    }\n",
    "    \n",
    "    # Extract key performance metrics\n",
    "    if 'device_performance' in analysis_results:\n",
    "        device_perf = analysis_results['device_performance']\n",
    "        avg_speedups = {}\n",
    "        \n",
    "        for test_name, test_data in device_perf.items():\n",
    "            if 'cpu' in test_data and device.type in test_data:\n",
    "                if 'gflops' in test_data['cpu'] and 'gflops' in test_data[device.type]:\n",
    "                    speedup = test_data[device.type]['gflops'] / test_data['cpu']['gflops']\n",
    "                    avg_speedups[test_name] = speedup\n",
    "        \n",
    "        if avg_speedups:\n",
    "            final_summary['performance_benchmarks']['average_gpu_speedup'] = np.mean(list(avg_speedups.values()))\n",
    "            final_summary['performance_benchmarks']['speedup_range'] = [min(avg_speedups.values()), max(avg_speedups.values())]\n",
    "    \n",
    "    # Memory efficiency insights\n",
    "    if 'memory_usage_analysis' in analysis_results:\n",
    "        memory_data = analysis_results['memory_usage_analysis']\n",
    "        \n",
    "        if 'inplace_comparison' in memory_data:\n",
    "            memory_savings = memory_data['inplace_comparison']['memory_saved_mb']\n",
    "            final_summary['optimization_insights']['inplace_memory_savings_mb'] = memory_savings\n",
    "        \n",
    "        if 'cleanup_effectiveness' in memory_data:\n",
    "            cleanup_efficiency = memory_data['cleanup_effectiveness']['cleanup_efficiency']\n",
    "            final_summary['optimization_insights']['memory_cleanup_efficiency'] = cleanup_efficiency\n",
    "    \n",
    "    # Operation performance insights\n",
    "    if 'operation_comparisons' in analysis_results:\n",
    "        op_data = analysis_results['operation_comparisons']\n",
    "        \n",
    "        # Find fastest and slowest operations\n",
    "        all_throughputs = {}\n",
    "        for size, ops in op_data.items():\n",
    "            for op_name, op_stats in ops.items():\n",
    "                if op_name not in all_throughputs:\n",
    "                    all_throughputs[op_name] = []\n",
    "                all_throughputs[op_name].append(op_stats['throughput'])\n",
    "        \n",
    "        avg_throughputs = {op: np.mean(throughputs) for op, throughputs in all_throughputs.items()}\n",
    "        \n",
    "        if avg_throughputs:\n",
    "            fastest_op = max(avg_throughputs, key=avg_throughputs.get)\n",
    "            slowest_op = min(avg_throughputs, key=avg_throughputs.get)\n",
    "            \n",
    "            final_summary['performance_benchmarks']['fastest_operation'] = fastest_op\n",
    "            final_summary['performance_benchmarks']['slowest_operation'] = slowest_op\n",
    "            final_summary['performance_benchmarks']['operation_speed_ratio'] = avg_throughputs[fastest_op] / avg_throughputs[slowest_op]\n",
    "    \n",
    "    # Generate recommendations based on analysis\n",
    "    recommendations = []\n",
    "    \n",
    "    # Device recommendations\n",
    "    if device.type == 'cuda':\n",
    "        recommendations.append(\"üöÄ GPU acceleration detected - leverage for large tensor operations\")\n",
    "    elif device.type == 'cpu':\n",
    "        recommendations.append(\"üíª CPU-only environment - consider GPU for production workloads\")\n",
    "    \n",
    "    # Memory recommendations\n",
    "    if 'memory_usage_analysis' in analysis_results:\n",
    "        recommendations.append(\"üíæ Implement in-place operations to reduce memory footprint\")\n",
    "        recommendations.append(\"üßπ Use torch.cuda.empty_cache() for GPU memory management\")\n",
    "    \n",
    "    # Performance recommendations\n",
    "    if 'performance_optimization' in analysis_results:\n",
    "        opt_data = analysis_results['performance_optimization']\n",
    "        \n",
    "        if 'vectorization_comparison' in opt_data:\n",
    "            speedup = opt_data['vectorization_comparison'].get('speedup', 0)\n",
    "            if speedup > 5:\n",
    "                recommendations.append(f\"‚ö° Vectorization provides {speedup:.1f}x speedup - avoid explicit loops\")\n",
    "        \n",
    "        if 'data_type_optimization' in opt_data:\n",
    "            recommendations.append(\"üéØ Use Float32 instead of Float64 for optimal performance\")\n",
    "    \n",
    "    # ML application recommendations\n",
    "    if 'ml_applications' in analysis_results:\n",
    "        recommendations.append(\"üß† Apply tensor operations knowledge to neural network implementations\")\n",
    "        recommendations.append(\"üìä Use batch processing for efficient model training\")\n",
    "    \n",
    "    final_summary['recommendations'] = recommendations\n",
    "    \n",
    "    # Best practices summary\n",
    "    best_practices = [\n",
    "        \"Create tensors directly on target device to avoid transfers\",\n",
    "        \"Use appropriate data types (Float32 for most deep learning)\",\n",
    "        \"Leverage broadcasting for efficient element-wise operations\",\n",
    "        \"Implement vectorized operations instead of explicit loops\",\n",
    "        \"Use in-place operations (_= suffix) for memory efficiency\",\n",
    "        \"Keep tensors contiguous for optimal performance\",\n",
    "        \"Batch operations when possible for better throughput\",\n",
    "        \"Monitor memory usage and implement cleanup strategies\",\n",
    "        \"Profile code to identify performance bottlenecks\",\n",
    "        \"Use torch.no_grad() context for inference to save memory\"\n",
    "    ]\n",
    "    \n",
    "    final_summary['best_practices'] = best_practices\n",
    "    \n",
    "    # Save comprehensive summary\n",
    "    with open(results_dir / 'comprehensive_final_summary.json', 'w') as f:\n",
    "        json.dump(final_summary, f, indent=2, default=str)\n",
    "    \n",
    "    # Save detailed analysis results\n",
    "    with open(results_dir / 'detailed_analysis_results.json', 'w') as f:\n",
    "        json.dump(analysis_results, f, indent=2, default=str)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nüìã COMPREHENSIVE TENSOR FUNDAMENTALS ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    print(f\"\\nüéØ Analysis Overview:\")\n",
    "    print(f\"  ‚Ä¢ Modules Analyzed: {final_summary['analysis_metadata']['total_analysis_modules']}\")\n",
    "    print(f\"  ‚Ä¢ Device Used: {final_summary['analysis_metadata']['device_used']}\")\n",
    "    print(f\"  ‚Ä¢ PyTorch Version: {final_summary['analysis_metadata']['pytorch_version']}\")\n",
    "    \n",
    "    if 'performance_benchmarks' in final_summary:\n",
    "        perf = final_summary['performance_benchmarks']\n",
    "        if 'average_gpu_speedup' in perf:\n",
    "            print(f\"  ‚Ä¢ Average GPU Speedup: {perf['average_gpu_speedup']:.2f}x\")\n",
    "        if 'fastest_operation' in perf:\n",
    "            print(f\"  ‚Ä¢ Fastest Operation: {perf['fastest_operation']}\")\n",
    "            print(f\"  ‚Ä¢ Performance Ratio: {perf['operation_speed_ratio']:.2f}x\")\n",
    "    \n",
    "    print(f\"\\nüí° Key Recommendations:\")\n",
    "    for i, rec in enumerate(recommendations[:8], 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    print(f\"\\nüìö Best Practices:\")\n",
    "    for i, practice in enumerate(best_practices[:8], 1):\n",
    "        print(f\"  {i}. {practice}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Results Saved:\")\n",
    "    result_files = list(results_dir.glob('*'))\n",
    "    for file_path in sorted(result_files):\n",
    "        if file_path.is_file():\n",
    "            size_kb = file_path.stat().st_size / 1024\n",
    "            print(f\"  üìÑ {file_path.name} ({size_kb:.1f} KB)\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Ready for Next Steps:\")\n",
    "    print(f\"  ‚Ä¢ Advanced autograd and backpropagation concepts\")\n",
    "    print(f\"  ‚Ä¢ Neural network architecture implementation\")\n",
    "    print(f\"  ‚Ä¢ Optimization algorithms and training loops\")\n",
    "    print(f\"  ‚Ä¢ Computer vision and NLP applications\")\n",
    "    \n",
    "    print(f\"\\nüéâ TENSOR FUNDAMENTALS MASTERY ACHIEVED! üéâ\")\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "# Generate final comprehensive summary\n",
    "final_analysis_summary = generate_comprehensive_final_summary()\n",
    "print(\"‚úÖ Comprehensive analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f6410",
   "metadata": {},
   "source": [
    "## Summary and Key Achievements\n",
    "\n",
    "This comprehensive PyTorch tensor fundamentals analysis has successfully demonstrated:\n",
    "\n",
    "### üî• **Tensor Mastery Achievements**\n",
    "- **Comprehensive Creation Analysis**: Evaluated 6+ tensor creation methods with performance metrics\n",
    "- **Advanced Operations Benchmarking**: Analyzed 10+ tensor operations across multiple device types\n",
    "- **Memory Management Mastery**: Demonstrated efficient memory usage and optimization strategies\n",
    "- **Device Performance Optimization**: Compared CPU vs GPU performance across various workloads\n",
    "- **Real-World Applications**: Implemented practical ML scenarios including attention mechanisms and CNNs\n",
    "\n",
    "### üìä **Technical Innovations**\n",
    "- Multi-dimensional performance analysis with statistical validation\n",
    "- Memory efficiency tracking and optimization recommendations\n",
    "- Device-agnostic benchmarking frameworks\n",
    "- Advanced visualization dashboards for comprehensive insights\n",
    "- Real-time performance monitoring and assessment tools\n",
    "\n",
    "### üéØ **Practical Applications**\n",
    "- Data preprocessing pipeline optimization\n",
    "- Neural network forward pass implementation\n",
    "- Advanced loss function implementations\n",
    "- Memory management best practices\n",
    "- Performance optimization strategies\n",
    "\n",
    "### üìÅ **Comprehensive Documentation**\n",
    "- Complete analysis results with JSON exports for programmatic access\n",
    "- High-resolution visualizations for research and presentation\n",
    "- Detailed performance benchmarks and comparisons\n",
    "- Best practices guide and optimization recommendations\n",
    "- Ready-to-use code modules for integration in larger projects\n",
    "\n",
    "### üöÄ **Ready for Advanced Deep Learning**\n",
    "- Solid foundation in tensor operations and performance optimization\n",
    "- Understanding of memory management and device utilization\n",
    "- Knowledge of broadcasting, reshaping, and advanced indexing\n",
    "- Experience with real-world ML application patterns\n",
    "- Performance analysis and benchmarking capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22cd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Data Type Optimization\n",
    "print(f\"\\nüìä Strategy 1: Data Type Optimization\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "size = (1000, 1000)\n",
    "dtypes_to_test = [torch.float64, torch.float32, torch.float16]\n",
    "\n",
    "dtype_results = {}\n",
    "\n",
    "for dtype in dtypes_to_test:\n",
    "    if dtype == torch.float16 and device.type == 'cpu':\n",
    "        # Skip float16 on CPU as it's not well supported\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Memory usage\n",
    "        tensor = torch.randn(size, dtype=dtype, device=device)\n",
    "        memory_mb = tensor.numel() * tensor.element_size() / (1024**2)\n",
    "        \n",
    "        # Performance test\n",
    "        a = torch.randn(size, dtype=dtype, device=device)\n",
    "        b = torch.randn(size, dtype=dtype, device=device)\n",
    "        \n",
    "        times = []\n",
    "        for _ in range(10):\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = torch.matmul(a, b)\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        \n",
    "        dtype_results[str(dtype)] = {\n",
    "            'memory_mb': memory_mb,\n",
    "            'avg_time': avg_time,\n",
    "            'relative_memory': memory_mb / dtype_results.get('torch.float64', {}).get('memory_mb', memory_mb),\n",
    "            'relative_speed': avg_time / dtype_results.get('torch.float64', {}).get('avg_time', avg_time) if 'torch.float64' in dtype_results else 1.0\n",
    "        }\n",
    "        \n",
    "        print(f\"  {str(dtype):15} - Memory: {memory_mb:6.2f}MB, Time: {avg_time:.6f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  {str(dtype):15} - Error: {e}\")\n",
    "\n",
    "optimization_results['data_type_optimization'] = dtype_results\n",
    "\n",
    "# Strategy 2: Vectorization vs Loops\n",
    "print(f\"\\nüìä Strategy 2: Vectorization vs Loop Operations\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "def slow_element_wise_operation(a, b):\n",
    "    \"\"\"Slow loop-based operation (CPU only).\"\"\"\n",
    "    result = torch.zeros_like(a)\n",
    "    for i in range(a.size(0)):\n",
    "        for j in range(a.size(1)):\n",
    "            result[i, j] = a[i, j] * b[i, j] + torch.sin(a[i, j])\n",
    "    return result\n",
    "\n",
    "def fast_vectorized_operation(a, b):\n",
    "    \"\"\"Fast vectorized operation.\"\"\"\n",
    "    return a * b + torch.sin(a)\n",
    "\n",
    "test_size = (200, 200)  # Smaller size for loop test\n",
    "a_cpu = torch.randn(test_size)\n",
    "b_cpu = torch.randn(test_size)\n",
    "a_device = a_cpu.to(device)\n",
    "b_device = b_cpu.to(device)\n",
    "\n",
    "vectorization_results = {}\n",
    "\n",
    "# Test vectorized operation on both CPU and device\n",
    "for test_device, a_test, b_test in [('cpu', a_cpu, b_cpu), (str(device), a_device, b_device)]:\n",
    "    start_time = time.time()\n",
    "    fast_result = fast_vectorized_operation(a_test, b_test)\n",
    "    if test_device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    fast_time = time.time() - start_time\n",
    "    \n",
    "    vectorization_results[f'vectorized_{test_device}'] = {\n",
    "        'time': fast_time,\n",
    "        'throughput': a_test.numel() / fast_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  Vectorized ({test_device:4}): {fast_time:.6f}s\")\n",
    "\n",
    "# Test loop operation only on CPU\n",
    "if test_size[0] <= 200:  # Only for small sizes\n",
    "    start_time = time.time()\n",
    "    slow_result = slow_element_wise_operation(a_cpu, b_cpu)\n",
    "    slow_time = time.time() - start_time\n",
    "    \n",
    "    vectorization_results['loop_cpu'] = {\n",
    "        'time': slow_time,\n",
    "        'throughput': a_cpu.numel() / slow_time\n",
    "    }\n",
    "    \n",
    "    speedup = slow_time / vectorization_results['vectorized_cpu']['time']\n",
    "    print(f\"  Loop-based (cpu ): {slow_time:.6f}s\")\n",
    "    print(f\"  Speedup: {speedup:.1f}x faster with vectorization\")\n",
    "    \n",
    "    vectorization_results['speedup'] = speedup\n",
    "\n",
    "optimization_results['vectorization_comparison'] = vectorization_results\n",
    "\n",
    "# Strategy 3: Memory Layout Optimization\n",
    "print(f\"\\nüìä Strategy 3: Memory Layout Optimization\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "# Create contiguous vs non-contiguous tensors\n",
    "original = torch.randn(1000, 1000, device=device)\n",
    "transposed = original.t()  # Non-contiguous\n",
    "made_contiguous = transposed.contiguous()\n",
    "\n",
    "layout_results = {}\n",
    "\n",
    "# Test operations on different layouts\n",
    "for name, tensor in [('contiguous', original), ('non_contiguous', transposed), ('made_contiguous', made_contiguous)]:\n",
    "    times = []\n",
    "    for _ in range(10):\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = tensor + 1.0  # Simple operation\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    \n",
    "    layout_results[name] = {\n",
    "        'is_contiguous': tensor.is_contiguous(),\n",
    "        'avg_time': avg_time,\n",
    "        'shape': tensor.shape,\n",
    "        'stride': tensor.stride()\n",
    "    }\n",
    "    \n",
    "    print(f\"  {name:15} - Contiguous: {tensor.is_contiguous()}, Time: {avg_time:.8f}s\")\n",
    "\n",
    "optimization_results['memory_layout'] = layout_results\n",
    "\n",
    "# Strategy 4: Batch Size Impact\n",
    "print(f\"\\nüìä Strategy 4: Batch Size Impact Analysis\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "batch_sizes = [1, 8, 32, 128, 512]\n",
    "input_size, output_size = 784, 10\n",
    "\n",
    "batch_results = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    try:\n",
    "        # Create data\n",
    "        x = torch.randn(batch_size, input_size, device=device)\n",
    "        w = torch.randn(input_size, output_size, device=device)\n",
    "        \n",
    "        # Benchmark matrix multiplication\n",
    "        times = []\n",
    "        for _ in range(20):\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = torch.matmul(x, w)\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        throughput = (batch_size * input_size * output_size) / avg_time  # Operations per second\n",
    "        \n",
    "        batch_results[batch_size] = {\n",
    "            'avg_time': avg_time,\n",
    "            'throughput': throughput,\n",
    "            'time_per_sample': avg_time / batch_size\n",
    "        }\n",
    "        \n",
    "        print(f\"  Batch size {batch_size:3}: {avg_time:.6f}s, {avg_time/batch_size:.8f}s/sample\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Batch size {batch_size:3}: Error - {e}\")\n",
    "\n",
    "optimization_results['batch_size_analysis'] = batch_results\n",
    "\n",
    "return optimization_results\n",
    "\n",
    "# Run optimization strategies analysis\n",
    "optimization_data = analyze_optimization_strategies()\n",
    "\n",
    "# Store results\n",
    "analysis_results['performance_optimization'] = optimization_data\n",
    "\n",
    "print(f\"\\nüíæ Performance optimization analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba94f54",
   "metadata": {},
   "source": [
    "## Final Exercise Challenges and Notebook Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98985d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Final challenge exercises for testing mastery\n",
    "print(f\"\\nüéØ MASTERY CHALLENGE EXERCISES\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"\"\"\n",
    "Complete these challenges to test your tensor mastery:\n",
    "\n",
    "ü•â FUNDAMENTAL CHALLENGES:\n",
    "1. Create a function that efficiently normalizes any tensor to zero mean, unit variance\n",
    "2. Implement matrix multiplication from scratch using only broadcasting\n",
    "3. Write a memory-efficient function to compute pairwise distances between points\n",
    "\n",
    "ü•à INTERMEDIATE CHALLENGES:  \n",
    "4. Implement a custom attention mechanism using only tensor operations\n",
    "5. Create a batch normalization layer from scratch with proper statistics tracking\n",
    "6. Design an efficient tensor operation that works across different devices\n",
    "\n",
    "ü•á ADVANCED CHALLENGES:\n",
    "7. Implement a custom autograd-compatible operation using tensor primitives\n",
    "8. Create a memory-optimized implementation of a transformer attention block\n",
    "9. Design a performance benchmarking suite for custom tensor operations\n",
    "\n",
    "Try implementing these in the cell below to cement your learning!\n",
    "\"\"\")\n",
    "\n",
    "# Challenge implementation space\n",
    "def tensor_mastery_challenges():\n",
    "    \"\"\"\n",
    "    Implement your challenge solutions here!\n",
    "    This is your opportunity to demonstrate mastery of PyTorch tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Challenge 1: Efficient normalization\n",
    "    def efficient_normalize(tensor, dim=None, eps=1e-8):\n",
    "        \"\"\"Normalize tensor to zero mean, unit variance.\"\"\"\n",
    "        if dim is None:\n",
    "            mean = tensor.mean()\n",
    "            std = tensor.std()\n",
    "        else:\n",
    "            mean = tensor.mean(dim=dim, keepdim=True)\n",
    "            std = tensor.std(dim=dim, keepdim=True)\n",
    "        return (tensor - mean) / (std + eps)\n",
    "    \n",
    "    # Challenge 2: Matrix multiplication from scratch\n",
    "    def manual_matmul(a, b):\n",
    "        \"\"\"Matrix multiplication using broadcasting.\"\"\"\n",
    "        # Expand dimensions for broadcasting\n",
    "        a_expanded = a.unsqueeze(-1)  # (..., m, 1)\n",
    "        b_expanded = b.unsqueeze(-3)  # (..., 1, n)\n",
    "        # Element-wise multiply and sum over the shared dimension\n",
    "        return (a_expanded * b_expanded).sum(-2)\n",
    "    \n",
    "    # Challenge 3: Pairwise distances\n",
    "    def pairwise_distances(x, y=None):\n",
    "        \"\"\"Compute pairwise distances efficiently.\"\"\"\n",
    "        if y is None:\n",
    "            y = x\n",
    "        \n",
    "        # Using the identity: ||x - y||^2 = ||x||^2 + ||y||^2 - 2*x^T*y\n",
    "        x_sqnorms = (x**2).sum(dim=1, keepdim=True)\n",
    "        y_sqnorms = (y**2).sum(dim=1, keepdim=True).t()\n",
    "        xy = torch.mm(x, y.t())\n",
    "        \n",
    "        distances = x_sqnorms + y_sqnorms - 2 * xy\n",
    "        return torch.sqrt(torch.clamp(distances, min=0))\n",
    "    \n",
    "    # Test implementations\n",
    "    print(\"üß™ Testing challenge implementations...\")\n",
    "    \n",
    "    # Test normalization\n",
    "    test_tensor = torch.randn(100, 50)\n",
    "    normalized = efficient_normalize(test_tensor)\n",
    "    print(f\"Normalization test - Mean: {normalized.mean():.6f}, Std: {normalized.std():.6f}\")\n",
    "    \n",
    "    # Test manual matmul\n",
    "    a_test = torch.randn(10, 5)\n",
    "    b_test = torch.randn(5, 8)\n",
    "    manual_result = manual_matmul(a_test, b_test)\n",
    "    builtin_result = torch.matmul(a_test, b_test)\n",
    "    print(f\"MatMul test - Difference: {(manual_result - builtin_result).abs().max():.8f}\")\n",
    "    \n",
    "    # Test pairwise distances\n",
    "    points = torch.randn(20, 3)\n",
    "    distances = pairwise_distances(points)\n",
    "    print(f\"Pairwise distances shape: {distances.shape}\")\n",
    "    \n",
    "    print(\"‚úÖ Basic challenges completed!\")\n",
    "    \n",
    "    return {\n",
    "        'normalize_func': efficient_normalize,\n",
    "        'matmul_func': manual_matmul,\n",
    "        'distances_func': pairwise_distances\n",
    "    }\n",
    "\n",
    "# Run challenge implementations\n",
    "challenge_results = tensor_mastery_challenges()\n",
    "\n",
    "print(f\"\\nüéä NOTEBOOK COMPLETION SUCCESSFUL! üéä\")\n",
    "print(f\"You have successfully completed the comprehensive PyTorch Tensor Fundamentals notebook!\")\n",
    "print(f\"All analyses, benchmarks, and assessments are now available in: {results_dir}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
