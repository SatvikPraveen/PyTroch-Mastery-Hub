{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474eea55",
   "metadata": {},
   "source": [
    "# Modern CNN Architectures: From ResNet to Vision Transformers\n",
    "\n",
    "**Building State-of-the-Art Computer Vision Models with PyTorch**\n",
    "\n",
    "**Authors:** PyTorch Mastery Hub Team  \n",
    "**Institution:** Advanced Deep Learning Research  \n",
    "**Course:** Computer Vision and Deep Learning  \n",
    "**Date:** December 2024\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides comprehensive implementation and analysis of modern CNN architectures that revolutionized computer vision. We explore the evolution from traditional CNNs to cutting-edge Vision Transformers, implementing each architecture from scratch and conducting thorough performance comparisons.\n",
    "\n",
    "## Key Objectives\n",
    "1. Implement ResNet from scratch with skip connections and residual learning\n",
    "2. Build DenseNet with dense connectivity and feature reuse mechanisms\n",
    "3. Create EfficientNet with compound scaling and mobile optimization\n",
    "4. Develop Vision Transformer fundamentals with self-attention mechanisms\n",
    "5. Conduct comprehensive architecture comparisons and performance analysis\n",
    "6. Execute advanced transfer learning experiments with different strategies\n",
    "7. Generate professional visualizations and analysis reports\n",
    "\n",
    "## 1. Environment Setup and Configuration\n",
    "\n",
    "```python\n",
    "# Core PyTorch and Computer Vision Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Scientific Computing and Visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced Analysis Tools\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è torchinfo not available. Install with: pip install torchinfo\")\n",
    "    def summary(*args, **kwargs):\n",
    "        return \"Summary not available\"\n",
    "\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "# Configuration and Styling\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Professional Directory Structure\n",
    "def setup_comprehensive_directories():\n",
    "    \"\"\"Create comprehensive directory structure for modern CNN architectures project\"\"\"\n",
    "    base_dirs = [\n",
    "        \"../results/modern_cnn_architectures/analysis/architecture_comparisons\",\n",
    "        \"../results/modern_cnn_architectures/analysis/performance_benchmarks\", \n",
    "        \"../results/modern_cnn_architectures/experiments/transfer_learning\",\n",
    "        \"../results/modern_cnn_architectures/experiments/attention_visualization\",\n",
    "        \"../results/modern_cnn_architectures/experiments/scaling_analysis\",\n",
    "        \"../results/modern_cnn_architectures/visualizations/model_structures\",\n",
    "        \"../results/modern_cnn_architectures/visualizations/training_progress\",\n",
    "        \"../models/modern_cnn_architectures/pretrained\",\n",
    "        \"../models/modern_cnn_architectures/custom_trained\",\n",
    "        \"../data/modern_cnn_architectures/processed\"\n",
    "    ]\n",
    "    \n",
    "    created_dirs = {}\n",
    "    for dir_path in base_dirs:\n",
    "        Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "        dir_name = Path(dir_path).name\n",
    "        created_dirs[dir_name] = dir_path\n",
    "        print(f\"üìÅ Created: {dir_path}\")\n",
    "    \n",
    "    return created_dirs\n",
    "\n",
    "# Initialize directory structure\n",
    "project_dirs = setup_comprehensive_directories()\n",
    "print(f\"\\n‚úÖ Comprehensive directory structure initialized!\")\n",
    "print(f\"üìä Results will be saved to: ../results/modern_cnn_architectures/\")\n",
    "print(f\"üíæ Models will be saved to: ../models/modern_cnn_architectures/\")\n",
    "\n",
    "# Utility classes for benchmarking and analysis\n",
    "class Timer:\n",
    "    \"\"\"Professional timer for benchmarking operations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.history = []\n",
    "    \n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def stop(self):\n",
    "        if self.start_time is None:\n",
    "            return 0\n",
    "        elapsed = time.time() - self.start_time\n",
    "        self.history.append(elapsed)\n",
    "        self.start_time = None\n",
    "        return elapsed\n",
    "    \n",
    "    def average(self):\n",
    "        return np.mean(self.history) if self.history else 0\n",
    "\n",
    "class ModelAnalyzer:\n",
    "    \"\"\"Comprehensive model analysis utilities\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_parameters(model):\n",
    "        \"\"\"Count total and trainable parameters\"\"\"\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        return total_params, trainable_params\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_model_size(model):\n",
    "        \"\"\"Calculate model size in MB\"\"\"\n",
    "        total_params, _ = ModelAnalyzer.count_parameters(model)\n",
    "        # Assuming float32 (4 bytes per parameter)\n",
    "        size_mb = total_params * 4 / (1024 ** 2)\n",
    "        return size_mb\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_layer_info(model):\n",
    "        \"\"\"Extract detailed layer information\"\"\"\n",
    "        layer_info = []\n",
    "        for name, module in model.named_modules():\n",
    "            if len(list(module.children())) == 0:  # Leaf modules only\n",
    "                layer_info.append({\n",
    "                    'name': name,\n",
    "                    'type': type(module).__name__,\n",
    "                    'parameters': sum(p.numel() for p in module.parameters())\n",
    "                })\n",
    "        return layer_info\n",
    "\n",
    "print(\"\\nüîß Analysis utilities initialized successfully!\")\n",
    "```\n",
    "\n",
    "## 2. ResNet Implementation: Skip Connections and Residual Learning\n",
    "\n",
    "```python\n",
    "class BasicResNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic ResNet block implementing residual connections for gradient flow improvement\n",
    "    \n",
    "    This block solves the vanishing gradient problem through skip connections,\n",
    "    enabling training of very deep networks.\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicResNetBlock, self).__init__()\n",
    "        \n",
    "        # First convolution with potential stride for downsampling\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Second convolution (always stride 1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                              stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Downsample layer for dimension matching\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Track forward pass statistics\n",
    "        self.forward_count = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.forward_count += 1\n",
    "        identity = x\n",
    "        \n",
    "        # First convolution block\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        \n",
    "        # Second convolution block\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Apply downsampling to identity if dimensions don't match\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        # The key innovation: residual connection\n",
    "        out += identity\n",
    "        out = F.relu(out, inplace=True)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class BottleneckResNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bottleneck ResNet block for deeper networks (ResNet-50+)\n",
    "    \n",
    "    Uses 1x1 convolutions to reduce computational complexity while maintaining\n",
    "    representational power through bottleneck architecture.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BottleneckResNetBlock, self).__init__()\n",
    "        \n",
    "        # 1x1 convolution for dimension reduction\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # 3x3 convolution (main computation)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # 1x1 convolution for dimension expansion\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,\n",
    "                              kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        # Bottleneck: reduce ‚Üí process ‚Üí expand\n",
    "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        out = F.relu(self.bn2(self.conv2(out)), inplace=True)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        \n",
    "        # Skip connection\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = F.relu(out, inplace=True)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResNetArchitecture(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete ResNet implementation with comprehensive analysis capabilities\n",
    "    \n",
    "    Supports multiple ResNet variants (18, 34, 50, 101, 152) through\n",
    "    configurable block arrangements and types.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, block, layers, num_classes=10, input_channels=3, \n",
    "                 zero_init_residual=False):\n",
    "        super(ResNetArchitecture, self).__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        self.block_type = block.__name__\n",
    "        self.layer_config = layers\n",
    "        \n",
    "        # Initial convolution and pooling\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, \n",
    "                              padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual layers with progressive channel increase\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        \n",
    "        # Global average pooling and classification\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        # Initialize weights with modern best practices\n",
    "        self._initialize_weights(zero_init_residual)\n",
    "        \n",
    "        # Analysis attributes\n",
    "        self.feature_maps = {}\n",
    "        self.gradient_flows = {}\n",
    "    \n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        \"\"\"Create a residual layer with multiple blocks\"\"\"\n",
    "        downsample = None\n",
    "        \n",
    "        # Create downsampling layer if needed for dimension matching\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
    "                         kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion)\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        # First block handles stride and potential downsampling\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        \n",
    "        # Remaining blocks maintain dimensions\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self, zero_init_residual):\n",
    "        \"\"\"Initialize weights using He initialization for ReLU networks\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        # Zero-initialize last BN in residual branches for better training\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, BottleneckResNetBlock):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicResNetBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with optional feature extraction\"\"\"\n",
    "        # Initial processing\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Progressive residual processing\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def extract_features(self, x, layer_names=None):\n",
    "        \"\"\"Extract intermediate feature maps for analysis\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Initial layers\n",
    "        x = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        features['conv1'] = x.clone().detach()\n",
    "        x = self.maxpool(x)\n",
    "        features['maxpool'] = x.clone().detach()\n",
    "        \n",
    "        # Residual layers\n",
    "        x = self.layer1(x)\n",
    "        features['layer1'] = x.clone().detach()\n",
    "        x = self.layer2(x)\n",
    "        features['layer2'] = x.clone().detach()\n",
    "        x = self.layer3(x)\n",
    "        features['layer3'] = x.clone().detach()\n",
    "        x = self.layer4(x)\n",
    "        features['layer4'] = x.clone().detach()\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def get_architecture_info(self):\n",
    "        \"\"\"Get comprehensive architecture information\"\"\"\n",
    "        total_params, trainable_params = ModelAnalyzer.count_parameters(self)\n",
    "        model_size = ModelAnalyzer.calculate_model_size(self)\n",
    "        \n",
    "        return {\n",
    "            'architecture': 'ResNet',\n",
    "            'block_type': self.block_type,\n",
    "            'layer_config': self.layer_config,\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'model_size_mb': model_size,\n",
    "            'depth': sum(self.layer_config) * 2 + 2  # Approximate depth\n",
    "        }\n",
    "\n",
    "# Factory functions for different ResNet variants\n",
    "def create_resnet18(num_classes=10, input_channels=3):\n",
    "    \"\"\"Create ResNet-18 with 18 layers\"\"\"\n",
    "    return ResNetArchitecture(BasicResNetBlock, [2, 2, 2, 2], num_classes, input_channels)\n",
    "\n",
    "def create_resnet34(num_classes=10, input_channels=3):\n",
    "    \"\"\"Create ResNet-34 with 34 layers\"\"\"\n",
    "    return ResNetArchitecture(BasicResNetBlock, [3, 4, 6, 3], num_classes, input_channels)\n",
    "\n",
    "def create_resnet50(num_classes=10, input_channels=3):\n",
    "    \"\"\"Create ResNet-50 with bottleneck blocks\"\"\"\n",
    "    return ResNetArchitecture(BottleneckResNetBlock, [3, 4, 6, 3], num_classes, input_channels)\n",
    "\n",
    "def create_resnet101(num_classes=10, input_channels=3):\n",
    "    \"\"\"Create ResNet-101 for very deep learning\"\"\"\n",
    "    return ResNetArchitecture(BottleneckResNetBlock, [3, 4, 23, 3], num_classes, input_channels)\n",
    "\n",
    "# Initialize ResNet model collection\n",
    "print(\"üîó Creating ResNet Architecture Collection:\")\n",
    "resnet_models = {\n",
    "    'ResNet-18': create_resnet18(),\n",
    "    'ResNet-34': create_resnet34(),\n",
    "    'ResNet-50': create_resnet50()\n",
    "}\n",
    "\n",
    "# Store comprehensive model information\n",
    "models_info = {}\n",
    "for name, model in resnet_models.items():\n",
    "    model = model.to(device)\n",
    "    info = model.get_architecture_info()\n",
    "    models_info[name] = info\n",
    "    \n",
    "    print(f\"\\nüìä {name} Analysis:\")\n",
    "    print(f\"   Parameters: {info['total_parameters']:,}\")\n",
    "    print(f\"   Size: {info['model_size_mb']:.2f} MB\")\n",
    "    print(f\"   Depth: {info['depth']} layers\")\n",
    "    print(f\"   Block type: {info['block_type']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ ResNet collection initialized with {len(resnet_models)} variants!\")\n",
    "```\n",
    "\n",
    "## 3. DenseNet Implementation: Dense Connectivity and Feature Reuse\n",
    "\n",
    "```python\n",
    "class DenseLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Dense layer implementing feature concatenation for maximum information flow\n",
    "    \n",
    "    Each layer receives feature maps from ALL preceding layers, enabling\n",
    "    feature reuse and reducing the number of parameters needed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, growth_rate, bn_size=4, drop_rate=0.1):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        \n",
    "        self.growth_rate = growth_rate\n",
    "        self.drop_rate = drop_rate\n",
    "        \n",
    "        # Bottleneck layers for computational efficiency\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, bn_size * growth_rate, \n",
    "                              kernel_size=1, bias=False)\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
    "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                              kernel_size=3, padding=1, bias=False)\n",
    "        \n",
    "        # Track concatenation statistics\n",
    "        self.concat_count = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pre-activation design: BN ‚Üí ReLU ‚Üí Conv\n",
    "        bottleneck = self.conv1(F.relu(self.bn1(x), inplace=True))\n",
    "        new_features = self.conv2(F.relu(self.bn2(bottleneck), inplace=True))\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        \n",
    "        # The key innovation: concatenate with input (dense connection)\n",
    "        self.concat_count += 1\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Dense block containing multiple dense layers with progressive feature growth\n",
    "    \n",
    "    Features grow by growth_rate at each layer, creating rich feature representations\n",
    "    through dense connectivity patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, in_channels, growth_rate, bn_size=4, drop_rate=0.1):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.growth_rate = growth_rate\n",
    "        \n",
    "        # Create sequence of dense layers\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layer = DenseLayer(\n",
    "                in_channels + i * growth_rate, \n",
    "                growth_rate, \n",
    "                bn_size, \n",
    "                drop_rate\n",
    "            )\n",
    "            layers.append(layer)\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Progressive feature concatenation\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def get_output_channels(self, in_channels):\n",
    "        \"\"\"Calculate output channels after dense block\"\"\"\n",
    "        return in_channels + self.num_layers * self.growth_rate\n",
    "\n",
    "class DenseTransition(nn.Module):\n",
    "    \"\"\"\n",
    "    Transition layer between dense blocks for dimension reduction\n",
    "    \n",
    "    Reduces feature map size and number of channels to control model complexity\n",
    "    and memory usage between dense blocks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, compression_factor=0.5):\n",
    "        super(DenseTransition, self).__init__()\n",
    "        \n",
    "        self.compression_factor = compression_factor\n",
    "        \n",
    "        # Dimension reduction through 1x1 convolution\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(F.relu(self.bn(x), inplace=True))\n",
    "        out = self.pool(out)\n",
    "        return out\n",
    "\n",
    "class DenseNetArchitecture(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete DenseNet implementation with comprehensive feature analysis\n",
    "    \n",
    "    Implements dense connectivity where each layer connects to every other layer\n",
    "    in a feed-forward fashion, maximizing information flow and gradient propagation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0.1, \n",
    "                 num_classes=10, input_channels=3, compression_factor=0.5):\n",
    "        super(DenseNetArchitecture, self).__init__()\n",
    "        \n",
    "        self.growth_rate = growth_rate\n",
    "        self.block_config = block_config\n",
    "        self.compression_factor = compression_factor\n",
    "        \n",
    "        # Initial convolution and pooling\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, num_init_features, kernel_size=7, \n",
    "                     stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(num_init_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Progressive dense blocks with transitions\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            # Dense block\n",
    "            block = DenseBlock(num_layers, num_features, growth_rate, bn_size, drop_rate)\n",
    "            self.features.add_module(f'denseblock{i+1}', block)\n",
    "            num_features = block.get_output_channels(num_features)\n",
    "            \n",
    "            # Transition layer (except for last block)\n",
    "            if i != len(block_config) - 1:\n",
    "                trans_channels = int(num_features * compression_factor)\n",
    "                trans = DenseTransition(num_features, trans_channels, compression_factor)\n",
    "                self.features.add_module(f'transition{i+1}', trans)\n",
    "                num_features = trans_channels\n",
    "        \n",
    "        # Final batch normalization\n",
    "        self.features.add_module('norm_final', nn.BatchNorm2d(num_features))\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        # Store architecture info\n",
    "        self.final_feature_size = num_features\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights for optimal convergence\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    \n",
    "    def extract_dense_features(self, x):\n",
    "        \"\"\"Extract features from each dense block for analysis\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Initial features\n",
    "        x = self.features.conv1(x)\n",
    "        x = self.features.bn1(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        features['initial'] = x.clone().detach()\n",
    "        x = self.features.maxpool(x)\n",
    "        \n",
    "        # Extract from each dense block\n",
    "        for name, module in self.features.named_children():\n",
    "            if 'denseblock' in name:\n",
    "                x = module(x)\n",
    "                features[name] = x.clone().detach()\n",
    "            elif 'transition' in name:\n",
    "                x = module(x)\n",
    "                features[name] = x.clone().detach()\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def get_architecture_info(self):\n",
    "        \"\"\"Get comprehensive DenseNet architecture information\"\"\"\n",
    "        total_params, trainable_params = ModelAnalyzer.count_parameters(self)\n",
    "        model_size = ModelAnalyzer.calculate_model_size(self)\n",
    "        \n",
    "        return {\n",
    "            'architecture': 'DenseNet',\n",
    "            'growth_rate': self.growth_rate,\n",
    "            'block_config': self.block_config,\n",
    "            'compression_factor': self.compression_factor,\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'model_size_mb': model_size,\n",
    "            'final_feature_size': self.final_feature_size\n",
    "        }\n",
    "\n",
    "# Factory functions for different DenseNet variants\n",
    "def create_densenet121(num_classes=10, input_channels=3):\n",
    "    \"\"\"Create DenseNet-121 with efficient architecture\"\"\"\n",
    "    return DenseNetArchitecture(\n",
    "        growth_rate=32, \n",
    "        block_config=(6, 12, 24, 16), \n",
    "        num_init_features=64,\n",
    "        num_classes=num_classes, \n",
    "        input_channels=input_channels\n",
    "    )\n",
    "\n",
    "def create_densenet169(num_classes=10, input_channels=3):\n",
    "    \"\"\"Create DenseNet-169 with deeper blocks\"\"\"\n",
    "    return DenseNetArchitecture(\n",
    "        growth_rate=32, \n",
    "        block_config=(6, 12, 32, 32),\n",
    "        num_init_features=64,\n",
    "        num_classes=num_classes, \n",
    "        input_channels=input_channels\n",
    "    )\n",
    "\n",
    "def create_densenet201(num_classes=10, input_channels=3):\n",
    "    \"\"\"Create DenseNet-201 with maximum depth\"\"\"\n",
    "    return DenseNetArchitecture(\n",
    "        growth_rate=32, \n",
    "        block_config=(6, 12, 48, 32),\n",
    "        num_init_features=64,\n",
    "        num_classes=num_classes, \n",
    "        input_channels=input_channels\n",
    "    )\n",
    "\n",
    "# Initialize DenseNet model collection\n",
    "print(\"\\nüåü Creating DenseNet Architecture Collection:\")\n",
    "densenet_models = {\n",
    "    'DenseNet-121': create_densenet121(),\n",
    "    'DenseNet-169': create_densenet169()\n",
    "}\n",
    "\n",
    "for name, model in densenet_models.items():\n",
    "    model = model.to(device)\n",
    "    info = model.get_architecture_info()\n",
    "    models_info[name] = info\n",
    "    \n",
    "    print(f\"\\nüìä {name} Analysis:\")\n",
    "    print(f\"   Parameters: {info['total_parameters']:,}\")\n",
    "    print(f\"   Size: {info['model_size_mb']:.2f} MB\")\n",
    "    print(f\"   Growth rate: {info['growth_rate']}\")\n",
    "    print(f\"   Block config: {info['block_config']}\")\n",
    "    print(f\"   Compression factor: {info['compression_factor']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ DenseNet collection initialized with {len(densenet_models)} variants!\")\n",
    "```\n",
    "\n",
    "## 4. EfficientNet Implementation: Compound Scaling and Mobile Optimization\n",
    "\n",
    "```python\n",
    "class SqueezeExcitationBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation block for channel attention mechanism\n",
    "    \n",
    "    Implements adaptive recalibration of channel-wise feature responses\n",
    "    by explicitly modelling interdependencies between channels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(SqueezeExcitationBlock, self).__init__()\n",
    "        \n",
    "        reduced_channels = max(1, in_channels // reduction_ratio)\n",
    "        \n",
    "        # Global average pooling for squeeze operation\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Excitation mechanism with dimensionality reduction and expansion\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, reduced_channels, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(reduced_channels, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Track attention statistics\n",
    "        self.attention_weights = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, channels, _, _ = x.size()\n",
    "        \n",
    "        # Squeeze: Global spatial information compression\n",
    "        squeeze = self.avg_pool(x).view(batch_size, channels)\n",
    "        \n",
    "        # Excitation: Channel importance weights\n",
    "        excitation = self.excitation(squeeze).view(batch_size, channels, 1, 1)\n",
    "        self.attention_weights = excitation.clone().detach()\n",
    "        \n",
    "        # Scale: Apply attention weights\n",
    "        return x * excitation.expand_as(x)\n",
    "\n",
    "class MobileInvertedBottleneckBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Mobile Inverted Bottleneck Convolution block with efficiency optimizations\n",
    "    \n",
    "    Key innovations:\n",
    "    - Depthwise separable convolutions for efficiency\n",
    "    - Inverted residuals for better gradient flow\n",
    "    - Linear bottlenecks to preserve information\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, \n",
    "                 expand_ratio, se_ratio=0.25, drop_rate=0.1):\n",
    "        super(MobileInvertedBottleneckBlock, self).__init__()\n",
    "        \n",
    "        self.stride = stride\n",
    "        self.drop_rate = drop_rate\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "        self.expand_ratio = expand_ratio\n",
    "        \n",
    "        # Expansion phase (inverted residual)\n",
    "        expanded_channels = in_channels * expand_ratio\n",
    "        \n",
    "        if expand_ratio != 1:\n",
    "            self.expand_conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, expanded_channels, 1, bias=False),\n",
    "                nn.BatchNorm2d(expanded_channels),\n",
    "                nn.ReLU6(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.expand_conv = nn.Identity()\n",
    "        \n",
    "        # Depthwise convolution for spatial processing\n",
    "        self.depthwise_conv = nn.Sequential(\n",
    "            nn.Conv2d(expanded_channels, expanded_channels, kernel_size, stride, \n",
    "                     kernel_size//2, groups=expanded_channels, bias=False),\n",
    "            nn.BatchNorm2d(expanded_channels),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Squeeze-and-Excitation for channel attention\n",
    "        if se_ratio > 0:\n",
    "            se_channels = max(1, int(in_channels * se_ratio))\n",
    "            self.se = SqueezeExcitationBlock(expanded_channels, \n",
    "                                           expanded_channels // se_channels)\n",
    "        else:\n",
    "            self.se = nn.Identity()\n",
    "        \n",
    "        # Pointwise convolution for channel mixing (linear bottleneck)\n",
    "        self.pointwise_conv = nn.Sequential(\n",
    "            nn.Conv2d(expanded_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "            # Note: No activation here (linear bottleneck)\n",
    "        )\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.forward_count = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.forward_count += 1\n",
    "        identity = x\n",
    "        \n",
    "        # Expansion phase\n",
    "        x = self.expand_conv(x)\n",
    "        \n",
    "        # Depthwise spatial processing\n",
    "        x = self.depthwise_conv(x)\n",
    "        \n",
    "        # Channel attention\n",
    "        x = self.se(x)\n",
    "        \n",
    "        # Pointwise channel mixing (linear bottleneck)\n",
    "        x = self.pointwise_conv(x)\n",
    "        \n",
    "        # Residual connection with stochastic depth\n",
    "        if self.use_residual:\n",
    "            if self.drop_rate > 0 and self.training:\n",
    "                x = F.dropout(x, p=self.drop_rate, training=self.training)\n",
    "            x = x + identity\n",
    "        \n",
    "        return x\n",
    "\n",
    "class EfficientNetArchitecture(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet implementation with compound scaling methodology\n",
    "    \n",
    "    Systematically scales network depth, width, and resolution using\n",
    "    compound scaling coefficients for optimal accuracy-efficiency trade-offs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, width_mult=1.0, depth_mult=1.0, resolution=224, \n",
    "                 num_classes=10, input_channels=3, drop_rate=0.2, \n",
    "                 stochastic_depth_rate=0.2):\n",
    "        super(EfficientNetArchitecture, self).__init__()\n",
    "        \n",
    "        self.width_mult = width_mult\n",
    "        self.depth_mult = depth_mult\n",
    "        self.resolution = resolution\n",
    "        self.drop_rate = drop_rate\n",
    "        \n",
    "        # Base configuration for EfficientNet-B0\n",
    "        # (expand_ratio, channels, num_layers, stride, kernel_size)\n",
    "        base_config = [\n",
    "            (1, 16, 1, 1, 3),   # Stage 1\n",
    "            (6, 24, 2, 2, 3),   # Stage 2\n",
    "            (6, 40, 2, 2, 5),   # Stage 3\n",
    "            (6, 80, 3, 2, 3),   # Stage 4\n",
    "            (6, 112, 3, 1, 5),  # Stage 5\n",
    "            (6, 192, 4, 2, 5),  # Stage 6\n",
    "            (6, 320, 1, 1, 3),  # Stage 7\n",
    "        ]\n",
    "        \n",
    "        # Apply compound scaling to base configuration\n",
    "        def round_filters(filters, width_mult):\n",
    "            \"\"\"Round number of filters based on width multiplier\"\"\"\n",
    "            if width_mult == 1.0:\n",
    "                return filters\n",
    "            filters *= width_mult\n",
    "            new_filters = max(8, int(filters + 4) // 8 * 8)\n",
    "            if new_filters < 0.9 * filters:\n",
    "                new_filters += 8\n",
    "            return int(new_filters)\n",
    "        \n",
    "        def round_repeats(repeats, depth_mult):\n",
    "            \"\"\"Round number of layer repeats based on depth multiplier\"\"\"\n",
    "            if depth_mult == 1.0:\n",
    "                return repeats\n",
    "            return int(math.ceil(depth_mult * repeats))\n",
    "        \n",
    "        # Stem convolution\n",
    "        out_channels = round_filters(32, width_mult)\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, out_channels, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Build inverted bottleneck blocks\n",
    "        features = []\n",
    "        in_channels = out_channels\n",
    "        total_blocks = sum(round_repeats(layers, depth_mult) for _, _, layers, _, _ in base_config)\n",
    "        block_idx = 0\n",
    "        \n",
    "        for expand_ratio, channels, num_layers, stride, kernel_size in base_config:\n",
    "            out_channels = round_filters(channels, width_mult)\n",
    "            num_layers = round_repeats(num_layers, depth_mult)\n",
    "            \n",
    "            for i in range(num_layers):\n",
    "                # Stochastic depth probability increases linearly\n",
    "                drop_path_rate = stochastic_depth_rate * block_idx / total_blocks\n",
    "                \n",
    "                features.append(MobileInvertedBottleneckBlock(\n",
    "                    in_channels, out_channels, kernel_size,\n",
    "                    stride if i == 0 else 1, expand_ratio,\n",
    "                    se_ratio=0.25, drop_rate=drop_path_rate\n",
    "                ))\n",
    "                in_channels = out_channels\n",
    "                block_idx += 1\n",
    "        \n",
    "        self.features = nn.Sequential(*features)\n",
    "        \n",
    "        # Head: final convolution and classification\n",
    "        last_channels = round_filters(1280, width_mult)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, last_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(last_channels),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Dropout(drop_rate) if drop_rate > 0 else nn.Identity(),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(last_channels, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        # Store scaling information\n",
    "        self.last_channels = last_channels\n",
    "        self.total_blocks = total_blocks\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights for efficient training\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.features(x)\n",
    "        x = self.head(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def extract_efficient_features(self, x):\n",
    "        \"\"\"Extract features at multiple scales for analysis\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Stem\n",
    "        x = self.stem(x)\n",
    "        features['stem'] = x.clone().detach()\n",
    "        \n",
    "        # Sample features from different stages\n",
    "        stage_indices = [0, 3, 6, 10, 16, 23, 30]  # Approximate stage boundaries\n",
    "        \n",
    "        for i, layer in enumerate(self.features):\n",
    "            x = layer(x)\n",
    "            if i in stage_indices:\n",
    "                features[f'stage_{stage_indices.index(i)}'] = x.clone().detach()\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def get_architecture_info(self):\n",
    "        \"\"\"Get comprehensive EfficientNet architecture information\"\"\"\n",
    "        total_params, trainable_params = ModelAnalyzer.count_parameters(self)\n",
    "        model_size = ModelAnalyzer.calculate_model_size(self)\n",
    "        \n",
    "        return {\n",
    "            'architecture': 'EfficientNet',\n",
    "            'width_multiplier': self.width_mult,\n",
    "            'depth_multiplier': self.depth_mult,\n",
    "            'resolution': self.resolution,\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'model_size_mb': model_size,\n",
    "            'total_blocks': self.total_blocks,\n",
    "            'final_channels': self.last_channels\n",
    "        }\n",
    "\n",
    "# Factory functions for different EfficientNet variants\n",
    "def create_efficientnet_b0(num_classes=10, input_channels=3):\n",
    "    \"\"\"Create EfficientNet-B0 baseline model\"\"\"\n",
    "    return EfficientNetArchitecture(\n",
    "        width_mult=1.0, depth_mult=1.0, resolution=224,\n",
    "        num_classes=num_classes, input_channels=input_channels\n",
    "    )\n",
    "\n",
    "def create_efficientnet_b1(num_classes=10, input_channels=3):\n",
    "    \"\"\"Create EfficientNet-B1 with light scaling\"\"\"\n",
    "    return EfficientNetArchitecture(\n",
    "        width_mult=1.0, depth_mult=1.1, resolution=240,\n",
    "        num_classes=num_classes, input_channels=input_channels\n",
    "    )\n",
    "\n",
    "def create_efficientnet_b2(num_classes=10, input_channels=3):\n",
    "    \"\"\"Create EfficientNet-B2 with moderate scaling\"\"\"\n",
    "    return EfficientNetArchitecture(\n",
    "        width_mult=1.1, depth_mult=1.2, resolution=260,\n",
    "        num_classes=num_classes, input_channels=input_channels\n",
    "    )\n",
    "\n",
    "# Initialize EfficientNet model collection\n",
    "print(\"\\n‚ö° Creating EfficientNet Architecture Collection:\")\n",
    "efficientnet_models = {\n",
    "    'EfficientNet-B0': create_efficientnet_b0(),\n",
    "    'EfficientNet-B1': create_efficientnet_b1()\n",
    "}\n",
    "\n",
    "for name, model in efficientnet_models.items():\n",
    "    model = model.to(device)\n",
    "    info = model.get_architecture_info()\n",
    "    models_info[name] = info\n",
    "    \n",
    "    print(f\"\\nüìä {name} Analysis:\")\n",
    "    print(f\"   Parameters: {info['total_parameters']:,}\")\n",
    "    print(f\"   Size: {info['model_size_mb']:.2f} MB\")\n",
    "    print(f\"   Width multiplier: {info['width_multiplier']}\")\n",
    "    print(f\"   Depth multiplier: {info['depth_multiplier']}\")\n",
    "    print(f\"   Total blocks: {info['total_blocks']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ EfficientNet collection initialized with {len(efficientnet_models)} variants!\")\n",
    "```\n",
    "\n",
    "## 5. Vision Transformer Implementation: Self-Attention for Computer Vision\n",
    "\n",
    "```python\n",
    "class PatchEmbeddingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert image patches to embeddings for transformer processing\n",
    "    \n",
    "    Divides input image into non-overlapping patches and linearly embeds\n",
    "    each patch, treating them as tokens for the transformer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=192):\n",
    "        super(PatchEmbeddingLayer, self).__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Convolutional layer for patch extraction and embedding\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels, embed_dim, \n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        \n",
    "        # Layer normalization for stable training\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (Batch, Channels, Height, Width)\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Extract patches and embed: (B, embed_dim, H/patch_size, W/patch_size)\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        # Flatten spatial dimensions: (B, embed_dim, num_patches)\n",
    "        x = x.flatten(2)\n",
    "        \n",
    "        # Transpose for transformer: (B, num_patches, embed_dim)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention mechanism for capturing global dependencies\n",
    "    \n",
    "    Enables the model to attend to different parts of the input sequence\n",
    "    simultaneously through multiple attention heads.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1, qkv_bias=True):\n",
    "        super(MultiHeadSelfAttentionBlock, self).__init__()\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Linear projections for queries, keys, and values\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Attention weights for visualization\n",
    "        self.attention_weights = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        \n",
    "        # Generate Q, K, V matrices\n",
    "        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, seq_len, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attention_scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attention_weights = attention_scores.softmax(dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Store attention weights for visualization\n",
    "        self.attention_weights = attention_weights.clone().detach()\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attention_output = (attention_weights @ v).transpose(1, 2).reshape(\n",
    "            batch_size, seq_len, embed_dim\n",
    "        )\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.proj(attention_output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TransformerMLPBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP block with GELU activation for transformer layers\n",
    "    \n",
    "    Provides non-linear transformation with expansion and contraction\n",
    "    for enhanced representational capacity.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, mlp_ratio=4.0, dropout=0.1):\n",
    "        super(TransformerMLPBlock, self).__init__()\n",
    "        \n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        \n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.activation = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete transformer encoder block with attention and MLP\n",
    "    \n",
    "    Implements the standard transformer architecture with:\n",
    "    - Multi-head self-attention\n",
    "    - Layer normalization\n",
    "    - MLP with residual connections\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        \n",
    "        # Pre-normalization design for better gradient flow\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = MultiHeadSelfAttentionBlock(embed_dim, num_heads, dropout)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = TransformerMLPBlock(embed_dim, mlp_ratio, dropout)\n",
    "        \n",
    "        # Track block statistics\n",
    "        self.forward_count = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.forward_count += 1\n",
    "        \n",
    "        # Self-attention with residual connection (pre-norm)\n",
    "        attention_output = self.attention(self.norm1(x))\n",
    "        x = x + attention_output\n",
    "        \n",
    "        # MLP with residual connection (pre-norm)\n",
    "        mlp_output = self.mlp(self.norm2(x))\n",
    "        x = x + mlp_output\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VisionTransformerArchitecture(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Vision Transformer implementation for image classification\n",
    "    \n",
    "    Adapts the transformer architecture for computer vision by:\n",
    "    - Converting images to patch sequences\n",
    "    - Adding learnable position embeddings\n",
    "    - Using a class token for classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
    "                 embed_dim=192, depth=12, num_heads=3, mlp_ratio=4.0, \n",
    "                 dropout=0.1, attention_dropout=0.1):\n",
    "        super(VisionTransformerArchitecture, self).__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbeddingLayer(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer encoder blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, attention_dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        # Store architecture info\n",
    "        self.num_patches = num_patches\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize transformer weights using appropriate distributions\"\"\"\n",
    "        # Initialize position embeddings and class token\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        \n",
    "        # Initialize other weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "            elif isinstance(m, nn.Conv2d):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Convert image to patch embeddings\n",
    "        x = self.patch_embed(x)  # (B, num_patches, embed_dim)\n",
    "        \n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        attention_weights = []\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "            if hasattr(block.attention, 'attention_weights') and block.attention.attention_weights is not None:\n",
    "                attention_weights.append(block.attention.attention_weights)\n",
    "        \n",
    "        # Apply final layer norm\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Classification using class token\n",
    "        cls_token_final = x[:, 0]  # First token is class token\n",
    "        logits = self.head(cls_token_final)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "    \n",
    "    def extract_transformer_features(self, x):\n",
    "        \"\"\"Extract features from different transformer layers\"\"\"\n",
    "        features = {}\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        features['patch_embed'] = x.clone().detach()\n",
    "        \n",
    "        # Add tokens and positions\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x)\n",
    "        features['input_with_pos'] = x.clone().detach()\n",
    "        \n",
    "        # Extract from transformer blocks\n",
    "        for i, block in enumerate(self.transformer_blocks):\n",
    "            x = block(x)\n",
    "            if i % 3 == 0:  # Sample every 3rd layer\n",
    "                features[f'transformer_block_{i}'] = x.clone().detach()\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def get_architecture_info(self):\n",
    "        \"\"\"Get comprehensive Vision Transformer architecture information\"\"\"\n",
    "        total_params, trainable_params = ModelAnalyzer.count_parameters(self)\n",
    "        model_size = ModelAnalyzer.calculate_model_size(self)\n",
    "        \n",
    "        return {\n",
    "            'architecture': 'Vision Transformer',\n",
    "            'img_size': self.img_size,\n",
    "            'patch_size': self.patch_size,\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'depth': self.depth,\n",
    "            'num_heads': self.num_heads,\n",
    "            'num_patches': self.num_patches,\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'model_size_mb': model_size\n",
    "        }\n",
    "\n",
    "# Factory functions for different ViT variants\n",
    "def create_vit_tiny(num_classes=10, input_channels=3):\n",
    "    \"\"\"Create ViT-Tiny for efficient processing\"\"\"\n",
    "    return VisionTransformerArchitecture(\n",
    "        img_size=32, patch_size=4, in_channels=input_channels, num_classes=num_classes,\n",
    "        embed_dim=192, depth=12, num_heads=3, mlp_ratio=4.0\n",
    "    )\n",
    "\n",
    "def create_vit_small(num_classes=10, input_channels=3):\n",
    "    \"\"\"Create ViT-Small for better performance\"\"\"\n",
    "    return VisionTransformerArchitecture(\n",
    "        img_size=32, patch_size=4, in_channels=input_channels, num_classes=num_classes,\n",
    "        embed_dim=384, depth=12, num_heads=6, mlp_ratio=4.0\n",
    "    )\n",
    "\n",
    "def create_vit_base(num_classes=10, input_channels=3):\n",
    "    \"\"\"Create ViT-Base for high performance\"\"\"\n",
    "    return VisionTransformerArchitecture(\n",
    "        img_size=32, patch_size=4, in_channels=input_channels, num_classes=num_classes,\n",
    "        embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0\n",
    "    )\n",
    "\n",
    "# Initialize Vision Transformer model collection\n",
    "print(\"\\nüëÅÔ∏è Creating Vision Transformer Architecture Collection:\")\n",
    "vit_models = {\n",
    "    'ViT-Tiny': create_vit_tiny(),\n",
    "    'ViT-Small': create_vit_small()\n",
    "}\n",
    "\n",
    "for name, model in vit_models.items():\n",
    "    model = model.to(device)\n",
    "    info = model.get_architecture_info()\n",
    "    models_info[name] = info\n",
    "    \n",
    "    print(f\"\\nüìä {name} Analysis:\")\n",
    "    print(f\"   Parameters: {info['total_parameters']:,}\")\n",
    "    print(f\"   Size: {info['model_size_mb']:.2f} MB\")\n",
    "    print(f\"   Embed dim: {info['embed_dim']}\")\n",
    "    print(f\"   Depth: {info['depth']} layers\")\n",
    "    print(f\"   Attention heads: {info['num_heads']}\")\n",
    "    print(f\"   Patches: {info['num_patches']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Vision Transformer collection initialized with {len(vit_models)} variants!\")\n",
    "\n",
    "# Combine all models for comprehensive analysis\n",
    "all_models = {**resnet_models, **densenet_models, **efficientnet_models, **vit_models}\n",
    "print(f\"\\nüèÜ Complete architecture collection: {len(all_models)} models across 4 architecture families!\")\n",
    "```\n",
    "\n",
    "## 6. Comprehensive Architecture Analysis and Benchmarking\n",
    "\n",
    "```python\n",
    "def create_comprehensive_architecture_analysis(models_info, save_path):\n",
    "    \"\"\"\n",
    "    Create detailed architectural comparison across all model families\n",
    "    \n",
    "    Args:\n",
    "        models_info: Dictionary containing model architecture information\n",
    "        save_path: Path to save the comprehensive analysis visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Extract data for analysis\n",
    "    model_names = list(models_info.keys())\n",
    "    param_counts = [info['total_parameters'] / 1e6 for info in models_info.values()]  # Convert to millions\n",
    "    model_sizes = [info['model_size_mb'] for info in models_info.values()]\n",
    "    \n",
    "    # Define color scheme by architecture family\n",
    "    color_map = {\n",
    "        'ResNet': '#FF6B6B',      # Red family\n",
    "        'DenseNet': '#4ECDC4',    # Teal family  \n",
    "        'EfficientNet': '#45B7D1', # Blue family\n",
    "        'ViT': '#96CEB4'          # Green family\n",
    "    }\n",
    "    \n",
    "    colors = []\n",
    "    for name in model_names:\n",
    "        for family in color_map.keys():\n",
    "            if family in name or family.replace('Net', '') in name:\n",
    "                colors.append(color_map[family])\n",
    "                break\n",
    "        else:\n",
    "            colors.append('#FFEAA7')  # Default yellow\n",
    "    \n",
    "    # 1. Parameter Count Comparison\n",
    "    bars1 = axes[0].bar(range(len(model_names)), param_counts, color=colors, alpha=0.8)\n",
    "    axes[0].set_title('Model Parameter Count Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Parameters (Millions)', fontsize=12)\n",
    "    axes[0].set_xticks(range(len(model_names)))\n",
    "    axes[0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars1, param_counts):\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height + max(param_counts)*0.01,\n",
    "                    f'{count:.1f}M', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 2. Model Size Comparison\n",
    "    bars2 = axes[1].bar(range(len(model_names)), model_sizes, color=colors, alpha=0.8)\n",
    "    axes[1].set_title('Model Memory Footprint', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('Size (MB)', fontsize=12)\n",
    "    axes[1].set_xticks(range(len(model_names)))\n",
    "    axes[1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    \n",
    "    for bar, size in zip(bars2, model_sizes):\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height + max(model_sizes)*0.01,\n",
    "                    f'{size:.1f}MB', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 3. Architecture Family Grouping\n",
    "    families = {}\n",
    "    for name, info in models_info.items():\n",
    "        family = info.get('architecture', name.split('-')[0])\n",
    "        if family not in families:\n",
    "            families[family] = []\n",
    "        families[family].append(info['total_parameters'])\n",
    "    \n",
    "    family_names = list(families.keys())\n",
    "    family_avg_params = [np.mean(families[family]) / 1e6 for family in family_names]\n",
    "    family_colors = [color_map.get(family, '#FFEAA7') for family in family_names]\n",
    "    \n",
    "    bars3 = axes[2].bar(family_names, family_avg_params, color=family_colors, alpha=0.8)\n",
    "    axes[2].set_title('Average Parameters by Architecture Family', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_ylabel('Average Parameters (Millions)', fontsize=12)\n",
    "    \n",
    "    for bar, params in zip(bars3, family_avg_params):\n",
    "        height = bar.get_height()\n",
    "        axes[2].text(bar.get_x() + bar.get_width()/2., height + max(family_avg_params)*0.01,\n",
    "                    f'{params:.1f}M', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 4. Parameter Efficiency Analysis\n",
    "    # Calculate parameters per MB (efficiency metric)\n",
    "    efficiency_ratios = [params / size for params, size in zip(param_counts, model_sizes)]\n",
    "    bars4 = axes[3].bar(range(len(model_names)), efficiency_ratios, color=colors, alpha=0.8)\n",
    "    axes[3].set_title('Parameter Density (Parameters per MB)', fontsize=14, fontweight='bold')\n",
    "    axes[3].set_ylabel('Parameters/MB (Millions)', fontsize=12)\n",
    "    axes[3].set_xticks(range(len(model_names)))\n",
    "    axes[3].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    \n",
    "    # 5. Architecture Innovation Timeline\n",
    "    innovation_years = {\n",
    "        'ResNet': 2015, 'DenseNet': 2016, 'EfficientNet': 2019, 'Vision Transformer': 2020\n",
    "    }\n",
    "    \n",
    "    timeline_families = list(innovation_years.keys())\n",
    "    timeline_years = list(innovation_years.values())\n",
    "    timeline_colors = [color_map.get(family, '#FFEAA7') for family in timeline_families]\n",
    "    \n",
    "    axes[4].scatter(timeline_years, range(len(timeline_families)), c=timeline_colors, s=200, alpha=0.8)\n",
    "    for i, (family, year) in enumerate(innovation_years.items()):\n",
    "        axes[4].annotate(family, (year, i), xytext=(10, 0), textcoords='offset points',\n",
    "                        fontsize=12, fontweight='bold')\n",
    "    \n",
    "    axes[4].set_title('Architecture Innovation Timeline', fontsize=14, fontweight='bold')\n",
    "    axes[4].set_xlabel('Year of Introduction')\n",
    "    axes[4].set_yticks(range(len(timeline_families)))\n",
    "    axes[4].set_yticklabels(timeline_families)\n",
    "    axes[4].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Complexity vs Capability Analysis\n",
    "    # Create a proxy for model capability based on parameters and architecture type\n",
    "    capability_scores = []\n",
    "    for name, info in models_info.items():\n",
    "        base_score = np.log10(info['total_parameters'])  # Log scale for parameters\n",
    "        \n",
    "        # Architecture-specific bonuses\n",
    "        if 'ResNet' in name:\n",
    "            if '50' in name: base_score += 0.5  # Deeper models\n",
    "        elif 'DenseNet' in name:\n",
    "            base_score += 0.3  # Dense connectivity bonus\n",
    "        elif 'EfficientNet' in name:\n",
    "            base_score += 0.7  # Efficiency innovation bonus\n",
    "        elif 'ViT' in name:\n",
    "            base_score += 0.8  # Attention mechanism bonus\n",
    "        \n",
    "        capability_scores.append(base_score)\n",
    "    \n",
    "    scatter = axes[5].scatter(param_counts, capability_scores, c=colors, s=150, alpha=0.8)\n",
    "    for i, name in enumerate(model_names):\n",
    "        axes[5].annotate(name, (param_counts[i], capability_scores[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "    \n",
    "    axes[5].set_title('Model Complexity vs Estimated Capability', fontsize=14, fontweight='bold')\n",
    "    axes[5].set_xlabel('Parameters (Millions)')\n",
    "    axes[5].set_ylabel('Capability Score (Log Scale)')\n",
    "    axes[5].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Architecture Characteristics Matrix\n",
    "    characteristics = {\n",
    "        'ResNet': ['Skip Connections', 'Deep Training', 'Gradient Flow', 'Residual Learning'],\n",
    "        'DenseNet': ['Dense Connectivity', 'Feature Reuse', 'Parameter Efficiency', 'Memory Intensive'],\n",
    "        'EfficientNet': ['Compound Scaling', 'Mobile Optimized', 'SE Attention', 'Efficient Design'],\n",
    "        'Vision Transformer': ['Self-Attention', 'Global Context', 'Patch Processing', 'Scalable']\n",
    "    }\n",
    "    \n",
    "    axes[6].axis('off')\n",
    "    y_start = 0.9\n",
    "    for family, chars in characteristics.items():\n",
    "        color = color_map.get(family, '#FFEAA7')\n",
    "        axes[6].text(0.05, y_start, f'{family}:', fontsize=14, fontweight='bold', color=color)\n",
    "        for i, char in enumerate(chars):\n",
    "            axes[6].text(0.1, y_start - 0.06 * (i + 1), f'‚Ä¢ {char}', fontsize=11)\n",
    "        y_start -= 0.35\n",
    "    \n",
    "    axes[6].set_title('Key Architecture Characteristics', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 8. Model Selection Guide\n",
    "    use_cases = {\n",
    "        'High Accuracy\\nLarge Datasets': ['ResNet-50', 'ViT-Small'],\n",
    "        'Efficient Mobile\\nDeployment': ['EfficientNet-B0', 'EfficientNet-B1'],\n",
    "        'Memory Constrained\\nEnvironments': ['ResNet-18', 'EfficientNet-B0'],\n",
    "        'Research & \\nExperimentation': ['DenseNet-121', 'ViT-Tiny']\n",
    "    }\n",
    "    \n",
    "    axes[7].axis('off')\n",
    "    y_pos = 0.9\n",
    "    axes[7].text(0.5, 0.95, 'Model Selection Guide', fontsize=14, fontweight='bold', ha='center')\n",
    "    \n",
    "    for use_case, recommended_models in use_cases.items():\n",
    "        axes[7].text(0.05, y_pos, f'{use_case}:', fontsize=12, fontweight='bold')\n",
    "        axes[7].text(0.55, y_pos, f'{\", \".join(recommended_models)}', fontsize=11)\n",
    "        y_pos -= 0.2\n",
    "    \n",
    "    # 9. Performance Trade-offs Summary\n",
    "    trade_offs = [\n",
    "        \"üéØ Accuracy vs Efficiency: ViT > ResNet > DenseNet > EfficientNet\",\n",
    "        \"‚ö° Speed vs Quality: EfficientNet > ResNet > DenseNet > ViT\", \n",
    "        \"üíæ Memory vs Performance: EfficientNet > ResNet > ViT > DenseNet\",\n",
    "        \"üîß Training vs Inference: ResNet (balanced) > others (specialized)\",\n",
    "        \"üì± Mobile vs Desktop: EfficientNet (mobile) > ResNet (desktop)\"\n",
    "    ]\n",
    "    \n",
    "    axes[8].axis('off')\n",
    "    axes[8].text(0.5, 0.9, 'Architecture Trade-offs', fontsize=14, fontweight='bold', ha='center')\n",
    "    \n",
    "    for i, trade_off in enumerate(trade_offs):\n",
    "        axes[8].text(0.05, 0.8 - i*0.15, trade_off, fontsize=11, wrap=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üíæ Comprehensive architecture analysis saved to: {save_path}\")\n",
    "\n",
    "def benchmark_inference_performance(models_dict, input_size=(1, 3, 32, 32), num_runs=100):\n",
    "    \"\"\"\n",
    "    Comprehensive inference speed benchmarking across all architectures\n",
    "    \n",
    "    Args:\n",
    "        models_dict: Dictionary of models to benchmark\n",
    "        input_size: Input tensor size for testing\n",
    "        num_runs: Number of inference runs for averaging\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with detailed performance metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚ö° Benchmarking Inference Performance:\")\n",
    "    print(f\"   Input size: {input_size}\")\n",
    "    print(f\"   Number of runs: {num_runs}\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    \n",
    "    results = {}\n",
    "    dummy_input = torch.randn(input_size).to(device)\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        print(f\"\\nüîç Testing {name}...\")\n",
    "        model.eval()\n",
    "        \n",
    "        # Warmup runs\n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):\n",
    "                if 'ViT' in name:\n",
    "                    output, _ = model(dummy_input)\n",
    "                else:\n",
    "                    output = model(dummy_input)\n",
    "        \n",
    "        # Synchronize CUDA operations\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark timing\n",
    "        timer = Timer()\n",
    "        timer.start()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_runs):\n",
    "                if 'ViT' in name:\n",
    "                    output, _ = model(dummy_input)\n",
    "                else:\n",
    "                    output = model(dummy_input)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        total_time = timer.stop()\n",
    "        avg_time = total_time / num_runs * 1000  # Convert to milliseconds\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        params = models_info[name]['total_parameters']\n",
    "        size_mb = models_info[name]['model_size_mb']\n",
    "        \n",
    "        results[name] = {\n",
    "            'avg_inference_time_ms': avg_time,\n",
    "            'throughput_fps': 1000 / avg_time,\n",
    "            'params_per_ms': params / avg_time,\n",
    "            'efficiency_score': (params / 1e6) / avg_time,  # Normalized efficiency\n",
    "            'memory_efficiency': params / (size_mb * 1024 * 1024)  # Params per byte\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚è±Ô∏è  Avg inference time: {avg_time:.2f} ms\")\n",
    "        print(f\"   üöÄ Throughput: {results[name]['throughput_fps']:.1f} FPS\")\n",
    "        print(f\"   ‚öñÔ∏è  Efficiency score: {results[name]['efficiency_score']:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_performance_analysis_dashboard(models_info, performance_results, save_path):\n",
    "    \"\"\"\n",
    "    Create comprehensive performance analysis dashboard\n",
    "    \n",
    "    Args:\n",
    "        models_info: Model architecture information\n",
    "        performance_results: Benchmarking results\n",
    "        save_path: Path to save the dashboard\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Extract data\n",
    "    model_names = list(performance_results.keys())\n",
    "    inference_times = [results['avg_inference_time_ms'] for results in performance_results.values()]\n",
    "    throughputs = [results['throughput_fps'] for results in performance_results.values()]\n",
    "    efficiency_scores = [results['efficiency_score'] for results in performance_results.values()]\n",
    "    param_counts = [models_info[name]['total_parameters'] / 1e6 for name in model_names]\n",
    "    model_sizes = [models_info[name]['model_size_mb'] for name in model_names]\n",
    "    \n",
    "    # Color coding by architecture family\n",
    "    colors = []\n",
    "    for name in model_names:\n",
    "        if 'ResNet' in name:\n",
    "            colors.append('#FF6B6B')\n",
    "        elif 'DenseNet' in name:\n",
    "            colors.append('#4ECDC4')\n",
    "        elif 'EfficientNet' in name:\n",
    "            colors.append('#45B7D1')\n",
    "        elif 'ViT' in name:\n",
    "            colors.append('#96CEB4')\n",
    "        else:\n",
    "            colors.append('#FFEAA7')\n",
    "    \n",
    "    # 1. Inference Time Comparison\n",
    "    bars1 = axes[0, 0].bar(range(len(model_names)), inference_times, color=colors, alpha=0.8)\n",
    "    axes[0, 0].set_title('Inference Time Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Inference Time (ms)', fontsize=12)\n",
    "    axes[0, 0].set_xticks(range(len(model_names)))\n",
    "    axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    \n",
    "    for bar, time in zip(bars1, inference_times):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + max(inference_times)*0.01,\n",
    "                       f'{time:.1f}ms', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 2. Throughput Analysis\n",
    "    bars2 = axes[0, 1].bar(range(len(model_names)), throughputs, color=colors, alpha=0.8)\n",
    "    axes[0, 1].set_title('Model Throughput (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Throughput (FPS)', fontsize=12)\n",
    "    axes[0, 1].set_xticks(range(len(model_names)))\n",
    "    axes[0, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    \n",
    "    for bar, fps in zip(bars2, throughputs):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + max(throughputs)*0.01,\n",
    "                       f'{fps:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 3. Efficiency Score Analysis\n",
    "    bars3 = axes[0, 2].bar(range(len(model_names)), efficiency_scores, color=colors, alpha=0.8)\n",
    "    axes[0, 2].set_title('Computational Efficiency Score', fontsize=14, fontweight='bold')\n",
    "    axes[0, 2].set_ylabel('Efficiency Score', fontsize=12)\n",
    "    axes[0, 2].set_xticks(range(len(model_names)))\n",
    "    axes[0, 2].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    \n",
    "    # 4. Parameters vs Speed Trade-off\n",
    "    scatter1 = axes[1, 0].scatter(param_counts, inference_times, c=colors, s=150, alpha=0.8)\n",
    "    for i, name in enumerate(model_names):\n",
    "        axes[1, 0].annotate(name, (param_counts[i], inference_times[i]), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Parameters (Millions)', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Inference Time (ms)', fontsize=12)\n",
    "    axes[1, 0].set_title('Model Complexity vs Speed Trade-off', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Memory vs Speed Trade-off\n",
    "    scatter2 = axes[1, 1].scatter(model_sizes, inference_times, c=colors, s=150, alpha=0.8)\n",
    "    for i, name in enumerate(model_names):\n",
    "        axes[1, 1].annotate(name, (model_sizes[i], inference_times[i]), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Model Size (MB)', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Inference Time (ms)', fontsize=12)\n",
    "    axes[1, 1].set_title('Memory vs Speed Trade-off', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Performance Ranking Summary\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    # Create performance rankings\n",
    "    speed_ranking = sorted(model_names, key=lambda x: performance_results[x]['avg_inference_time_ms'])\n",
    "    efficiency_ranking = sorted(model_names, key=lambda x: performance_results[x]['efficiency_score'], reverse=True)\n",
    "    \n",
    "    axes[1, 2].text(0.5, 0.95, 'Performance Rankings', fontsize=14, fontweight='bold', ha='center')\n",
    "    \n",
    "    axes[1, 2].text(0.05, 0.85, 'Fastest Models:', fontsize=12, fontweight='bold')\n",
    "    for i, model in enumerate(speed_ranking[:3]):\n",
    "        time = performance_results[model]['avg_inference_time_ms']\n",
    "        axes[1, 2].text(0.1, 0.80 - i*0.08, f'{i+1}. {model}: {time:.1f}ms', fontsize=11)\n",
    "    \n",
    "    axes[1, 2].text(0.05, 0.55, 'Most Efficient:', fontsize=12, fontweight='bold')\n",
    "    for i, model in enumerate(efficiency_ranking[:3]):\n",
    "        score = performance_results[model]['efficiency_score']\n",
    "        axes[1, 2].text(0.1, 0.50 - i*0.08, f'{i+1}. {model}: {score:.2f}', fontsize=11)\n",
    "    \n",
    "    # Add performance insights\n",
    "    axes[1, 2].text(0.05, 0.25, 'Key Insights:', fontsize=12, fontweight='bold')\n",
    "    insights = [\n",
    "        \"‚Ä¢ EfficientNet optimized for mobile\",\n",
    "        \"‚Ä¢ ResNet balanced performance\", \n",
    "        \"‚Ä¢ ViT requires more computation\",\n",
    "        \"‚Ä¢ DenseNet memory intensive\"\n",
    "    ]\n",
    "    \n",
    "    for i, insight in enumerate(insights):\n",
    "        axes[1, 2].text(0.1, 0.20 - i*0.05, insight, fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üíæ Performance analysis dashboard saved to: {save_path}\")\n",
    "\n",
    "# Execute comprehensive architecture analysis\n",
    "print(\"\\nüìä Creating Comprehensive Architecture Analysis...\")\n",
    "create_comprehensive_architecture_analysis(\n",
    "    models_info,\n",
    "    f\"{project_dirs['architecture_comparisons']}/comprehensive_architecture_analysis.png\"\n",
    ")\n",
    "\n",
    "# Benchmark inference performance\n",
    "selected_models = {\n",
    "    'ResNet-18': resnet_models['ResNet-18'],\n",
    "    'DenseNet-121': densenet_models['DenseNet-121'], \n",
    "    'EfficientNet-B0': efficientnet_models['EfficientNet-B0'],\n",
    "    'ViT-Tiny': vit_models['ViT-Tiny']\n",
    "}\n",
    "\n",
    "performance_results = benchmark_inference_performance(selected_models)\n",
    "\n",
    "# Create performance analysis dashboard\n",
    "create_performance_analysis_dashboard(\n",
    "    models_info,\n",
    "    performance_results,\n",
    "    f\"{project_dirs['performance_benchmarks']}/performance_analysis_dashboard.png\"\n",
    ")\n",
    "```\n",
    "\n",
    "## 7. Advanced Transfer Learning Experiments\n",
    "\n",
    "```python\n",
    "# Prepare CIFAR-10 dataset for transfer learning experiments\n",
    "print(\"\\nüì• Preparing CIFAR-10 Dataset for Transfer Learning Experiments:\")\n",
    "\n",
    "# Advanced data transformations for transfer learning\n",
    "transform_train_transfer = transforms.Compose([\n",
    "    transforms.Resize(224),  # Resize for pretrained models\n",
    "    transforms.RandomCrop(224, padding=28),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNet normalization\n",
    "])\n",
    "\n",
    "transform_test_transfer = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Load datasets with subset for faster experimentation\n",
    "try:\n",
    "    trainset_full = torchvision.datasets.CIFAR10(\n",
    "        root='../data/modern_cnn_architectures/cifar10', \n",
    "        train=True, download=True, transform=transform_train_transfer\n",
    "    )\n",
    "    testset_full = torchvision.datasets.CIFAR10(\n",
    "        root='../data/modern_cnn_architectures/cifar10', \n",
    "        train=False, download=True, transform=transform_test_transfer\n",
    "    )\n",
    "    \n",
    "    # Create strategic subsets for transfer learning\n",
    "    train_subset = Subset(trainset_full, range(0, 8000))  # 8000 training samples\n",
    "    test_subset = Subset(testset_full, range(0, 2000))    # 2000 test samples\n",
    "    \n",
    "    train_loader_transfer = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    test_loader_transfer = DataLoader(test_subset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    print(f\"‚úÖ Training samples: {len(train_subset):,}\")\n",
    "    print(f\"‚úÖ Test samples: {len(test_subset):,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading CIFAR-10: {e}\")\n",
    "    print(\"üìù Creating dummy data for demonstration...\")\n",
    "    \n",
    "    class DummyCIFAR10Dataset:\n",
    "        def __init__(self, size=1000):\n",
    "            self.data = [(torch.randn(3, 224, 224), np.random.randint(0, 10)) for _ in range(size)]\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx]\n",
    "    \n",
    "    train_subset = DummyCIFAR10Dataset(1000)\n",
    "    test_subset = DummyCIFAR10Dataset(200)\n",
    "    train_loader_transfer = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    test_loader_transfer = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "# CIFAR-10 class names\n",
    "cifar10_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "class AdvancedTransferLearningExperiment:\n",
    "    \"\"\"\n",
    "    Comprehensive transfer learning experiment framework with multiple strategies\n",
    "    \n",
    "    Implements various transfer learning approaches:\n",
    "    - Feature extraction (frozen backbone)\n",
    "    - Fine-tuning (unfrozen backbone) \n",
    "    - Progressive unfreezing\n",
    "    - Layer-wise learning rates\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, pretrained_model, num_classes=10):\n",
    "        self.model_name = model_name\n",
    "        self.model = pretrained_model\n",
    "        self.num_classes = num_classes\n",
    "        self.results = {}\n",
    "        self.training_history = {}\n",
    "        \n",
    "        # Adapt classifier for target dataset\n",
    "        self._adapt_classifier()\n",
    "        \n",
    "        # Store original state for reinitialization\n",
    "        self.original_state = self.model.state_dict().copy()\n",
    "    \n",
    "    def _adapt_classifier(self):\n",
    "        \"\"\"Intelligently adapt the classifier layer for target dataset\"\"\"\n",
    "        if hasattr(self.model, 'fc'):  # ResNet-style\n",
    "            in_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(in_features, self.num_classes)\n",
    "            print(f\"   üîß Adapted ResNet classifier: {in_features} ‚Üí {self.num_classes}\")\n",
    "            \n",
    "        elif hasattr(self.model, 'classifier'):  # DenseNet/EfficientNet-style\n",
    "            if isinstance(self.model.classifier, nn.Linear):\n",
    "                in_features = self.model.classifier.in_features\n",
    "                self.model.classifier = nn.Linear(in_features, self.num_classes)\n",
    "                print(f\"   üîß Adapted classifier: {in_features} ‚Üí {self.num_classes}\")\n",
    "            else:\n",
    "                # Sequential classifier\n",
    "                last_layer = None\n",
    "                for layer in reversed(self.model.classifier):\n",
    "                    if isinstance(layer, nn.Linear):\n",
    "                        last_layer = layer\n",
    "                        break\n",
    "                if last_layer is not None:\n",
    "                    in_features = last_layer.in_features\n",
    "                    # Replace the last linear layer\n",
    "                    for i, layer in enumerate(self.model.classifier):\n",
    "                        if layer is last_layer:\n",
    "                            self.model.classifier[i] = nn.Linear(in_features, self.num_classes)\n",
    "                            break\n",
    "                    print(f\"   üîß Adapted sequential classifier: {in_features} ‚Üí {self.num_classes}\")\n",
    "                    \n",
    "        elif hasattr(self.model, 'head'):  # ViT-style\n",
    "            in_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(in_features, self.num_classes)\n",
    "            print(f\"   üîß Adapted ViT head: {in_features} ‚Üí {self.num_classes}\")\n",
    "    \n",
    "    def set_transfer_strategy(self, strategy='frozen'):\n",
    "        \"\"\"\n",
    "        Set transfer learning strategy\n",
    "        \n",
    "        Args:\n",
    "            strategy: 'frozen', 'fine_tuning', 'progressive', 'layer_wise'\n",
    "        \"\"\"\n",
    "        if strategy == 'frozen':\n",
    "            self._freeze_features(freeze=True)\n",
    "            \n",
    "        elif strategy == 'fine_tuning':\n",
    "            self._freeze_features(freeze=False)\n",
    "            \n",
    "        elif strategy == 'progressive':\n",
    "            # Start with frozen features, will unfreeze progressively\n",
    "            self._freeze_features(freeze=True)\n",
    "            \n",
    "        elif strategy == 'layer_wise':\n",
    "            # Different learning rates for different layers\n",
    "            self._freeze_features(freeze=False)\n",
    "        \n",
    "        print(f\"üéØ Transfer strategy set to: {strategy}\")\n",
    "    \n",
    "    def _freeze_features(self, freeze=True):\n",
    "        \"\"\"Freeze or unfreeze feature extraction layers\"\"\"\n",
    "        frozen_count = 0\n",
    "        trainable_count = 0\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            # Keep classifier layers trainable\n",
    "            if any(classifier_name in name for classifier_name in ['fc', 'classifier', 'head']):\n",
    "                param.requires_grad = True\n",
    "                trainable_count += 1\n",
    "            else:\n",
    "                param.requires_grad = not freeze\n",
    "                if freeze:\n",
    "                    frozen_count += 1\n",
    "                else:\n",
    "                    trainable_count += 1\n",
    "        \n",
    "        status = \"frozen\" if freeze else \"unfrozen\"\n",
    "        print(f\"   üîí Feature layers {status}: {frozen_count} frozen, {trainable_count} trainable\")\n",
    "    \n",
    "    def train_with_strategy(self, train_loader, test_loader, strategy='frozen', \n",
    "                           epochs=10, base_lr=0.001):\n",
    "        \"\"\"\n",
    "        Train model with specified transfer learning strategy\n",
    "        \n",
    "        Args:\n",
    "            train_loader: Training data loader\n",
    "            test_loader: Test data loader  \n",
    "            strategy: Transfer learning strategy\n",
    "            epochs: Number of training epochs\n",
    "            base_lr: Base learning rate\n",
    "        \"\"\"\n",
    "        print(f\"\\nüöÄ Training {self.model_name} with {strategy} strategy for {epochs} epochs\")\n",
    "        \n",
    "        # Reset model to original state\n",
    "        self.model.load_state_dict(self.original_state)\n",
    "        self._adapt_classifier()\n",
    "        self.model = self.model.to(device)\n",
    "        \n",
    "        # Set transfer strategy\n",
    "        self.set_transfer_strategy(strategy)\n",
    "        \n",
    "        # Setup optimizer based on strategy\n",
    "        if strategy == 'layer_wise':\n",
    "            optimizer = self._setup_layer_wise_optimizer(base_lr)\n",
    "        else:\n",
    "            # Standard optimizer for trainable parameters\n",
    "            trainable_params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "            lr = base_lr if strategy == 'frozen' else base_lr * 0.1  # Lower LR for fine-tuning\n",
    "            optimizer = optim.AdamW(trainable_params, lr=lr, weight_decay=0.01)\n",
    "        \n",
    "        # Setup learning rate scheduler\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=base_lr * 5, \n",
    "            epochs=epochs, steps_per_epoch=len(train_loader),\n",
    "            pct_start=0.3\n",
    "        )\n",
    "        \n",
    "        # Loss function with label smoothing\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        # Training history tracking\n",
    "        history = {\n",
    "            'train_loss': [], 'train_acc': [], \n",
    "            'test_loss': [], 'test_acc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        best_test_acc = 0.0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Progressive unfreezing for progressive strategy\n",
    "            if strategy == 'progressive' and epoch == epochs // 2:\n",
    "                print(f\"   üîì Progressive unfreezing at epoch {epoch + 1}\")\n",
    "                self._freeze_features(freeze=False)\n",
    "                # Reduce learning rate for newly unfrozen layers\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] *= 0.1\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_acc = self._train_epoch(\n",
    "                train_loader, optimizer, criterion, scheduler\n",
    "            )\n",
    "            \n",
    "            # Evaluation phase\n",
    "            test_loss, test_acc = self._evaluate_epoch(test_loader, criterion)\n",
    "            \n",
    "            # Record metrics\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['test_loss'].append(test_loss)\n",
    "            history['test_acc'].append(test_acc)\n",
    "            history['learning_rates'].append(scheduler.get_last_lr()[0])\n",
    "            \n",
    "            # Track best performance\n",
    "            if test_acc > best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "            \n",
    "            # Progress reporting\n",
    "            print(f\"   Epoch {epoch+1}/{epochs}: \"\n",
    "                  f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, \"\n",
    "                  f\"Test Loss={test_loss:.4f}, Test Acc={test_acc:.2f}%\")\n",
    "        \n",
    "        # Store results\n",
    "        self.results[strategy] = history\n",
    "        self.training_history[strategy] = {\n",
    "            'final_test_acc': test_acc,\n",
    "            'best_test_acc': best_test_acc,\n",
    "            'epochs': epochs\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ {strategy} training completed. Best test accuracy: {best_test_acc:.2f}%\")\n",
    "        return history\n",
    "    \n",
    "    def _setup_layer_wise_optimizer(self, base_lr):\n",
    "        \"\"\"Setup optimizer with layer-wise learning rates\"\"\"\n",
    "        param_groups = []\n",
    "        \n",
    "        # Different learning rates for different parts of the network\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "                \n",
    "            if 'classifier' in name or 'fc' in name or 'head' in name:\n",
    "                # Higher learning rate for new classifier\n",
    "                param_groups.append({'params': param, 'lr': base_lr})\n",
    "            elif any(early_layer in name for early_layer in ['conv1', 'bn1', 'layer1']):\n",
    "                # Lower learning rate for early layers\n",
    "                param_groups.append({'params': param, 'lr': base_lr * 0.01})\n",
    "            else:\n",
    "                # Medium learning rate for middle layers\n",
    "                param_groups.append({'params': param, 'lr': base_lr * 0.1})\n",
    "        \n",
    "        return optim.AdamW(param_groups, weight_decay=0.01)\n",
    "    \n",
    "    def _train_epoch(self, train_loader, optimizer, criterion, scheduler):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc='Training', leave=False)\n",
    "        for inputs, targets in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Handle different model outputs\n",
    "            if 'ViT' in self.model_name:\n",
    "                outputs, _ = self.model(inputs)\n",
    "            else:\n",
    "                outputs = self.model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def _evaluate_epoch(self, test_loader, criterion):\n",
    "        \"\"\"Evaluate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                # Handle different model outputs\n",
    "                if 'ViT' in self.model_name:\n",
    "                    outputs, _ = self.model(inputs)\n",
    "                else:\n",
    "                    outputs = self.model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        avg_loss = test_loss / len(test_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def create_transfer_learning_analysis(self, save_path):\n",
    "        \"\"\"Create comprehensive transfer learning analysis visualization\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"‚ö†Ô∏è No results to analyze. Train the model first.\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        strategies = list(self.results.keys())\n",
    "        colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "        \n",
    "        # 1. Learning Curves Comparison\n",
    "        for i, (strategy, history) in enumerate(self.results.items()):\n",
    "            epochs = range(1, len(history['train_loss']) + 1)\n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            axes[0, 0].plot(epochs, history['train_loss'], f'{color}-', \n",
    "                           label=f'{strategy} (train)', linewidth=2, alpha=0.8)\n",
    "            axes[0, 0].plot(epochs, history['test_loss'], f'{color}--', \n",
    "                           label=f'{strategy} (test)', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        axes[0, 0].set_title(f'{self.model_name} - Loss Curves', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Accuracy Progression\n",
    "        for i, (strategy, history) in enumerate(self.results.items()):\n",
    "            epochs = range(1, len(history['train_acc']) + 1)\n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            axes[0, 1].plot(epochs, history['test_acc'], f'{color}-', \n",
    "                           label=strategy, linewidth=3, alpha=0.8)\n",
    "        \n",
    "        axes[0, 1].set_title('Test Accuracy Progression', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Test Accuracy (%)')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Final Performance Comparison\n",
    "        final_accs = [self.training_history[strategy]['final_test_acc'] \n",
    "                     for strategy in strategies]\n",
    "        best_accs = [self.training_history[strategy]['best_test_acc'] \n",
    "                    for strategy in strategies]\n",
    "        \n",
    "        x = np.arange(len(strategies))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = axes[0, 2].bar(x - width/2, final_accs, width, label='Final Accuracy', \n",
    "                              color=colors[:len(strategies)], alpha=0.8)\n",
    "        bars2 = axes[0, 2].bar(x + width/2, best_accs, width, label='Best Accuracy',\n",
    "                              color=colors[:len(strategies)], alpha=0.6)\n",
    "        \n",
    "        axes[0, 2].set_title('Performance Comparison', fontsize=14, fontweight='bold')\n",
    "        axes[0, 2].set_ylabel('Accuracy (%)')\n",
    "        axes[0, 2].set_xticks(x)\n",
    "        axes[0, 2].set_xticklabels(strategies, rotation=45)\n",
    "        axes[0, 2].legend()\n",
    "        \n",
    "        # Add value labels\n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                axes[0, 2].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                               f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # 4. Learning Rate Schedules\n",
    "        for i, (strategy, history) in enumerate(self.results.items()):\n",
    "            epochs = range(1, len(history['learning_rates']) + 1)\n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            axes[1, 0].plot(epochs, history['learning_rates'], f'{color}-', \n",
    "                           label=strategy, linewidth=2, alpha=0.8)\n",
    "        \n",
    "        axes[1, 0].set_title('Learning Rate Schedules', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Training Efficiency Analysis\n",
    "        convergence_epochs = []\n",
    "        for strategy in strategies:\n",
    "            history = self.results[strategy]\n",
    "            final_acc = history['test_acc'][-1]\n",
    "            \n",
    "            # Find epoch where accuracy reaches 95% of final performance\n",
    "            target_acc = final_acc * 0.95\n",
    "            for epoch, acc in enumerate(history['test_acc']):\n",
    "                if acc >= target_acc:\n",
    "                    convergence_epochs.append(epoch + 1)\n",
    "                    break\n",
    "            else:\n",
    "                convergence_epochs.append(len(history['test_acc']))\n",
    "        \n",
    "        bars3 = axes[1, 1].bar(strategies, convergence_epochs, \n",
    "                              color=colors[:len(strategies)], alpha=0.8)\n",
    "        axes[1, 1].set_title('Training Efficiency\\n(Epochs to 95% Performance)', \n",
    "                            fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Epochs')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        for bar, epochs in zip(bars3, convergence_epochs):\n",
    "            height = bar.get_height()\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                           f'{epochs}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 6. Strategy Recommendations\n",
    "        axes[1, 2].axis('off')\n",
    "        \n",
    "        recommendations = [\n",
    "            \"üéØ Transfer Learning Strategy Guide:\",\n",
    "            \"\",\n",
    "            \"‚úÖ Feature Extraction (Frozen):\",\n",
    "            \"   ‚Ä¢ Fast training, low compute\",\n",
    "            \"   ‚Ä¢ Good for small datasets\",\n",
    "            \"   ‚Ä¢ Preserves pretrained features\",\n",
    "            \"\",\n",
    "            \"üî• Fine-tuning (Unfrozen):\",\n",
    "            \"   ‚Ä¢ Better final performance\", \n",
    "            \"   ‚Ä¢ Adapts to target domain\",\n",
    "            \"   ‚Ä¢ Requires more data & compute\",\n",
    "            \"\",\n",
    "            \"üîÑ Progressive Unfreezing:\",\n",
    "            \"   ‚Ä¢ Balanced approach\",\n",
    "            \"   ‚Ä¢ Gradual adaptation\",\n",
    "            \"   ‚Ä¢ Good convergence stability\",\n",
    "            \"\",\n",
    "            \"‚öñÔ∏è Layer-wise Learning Rates:\",\n",
    "            \"   ‚Ä¢ Optimized for each layer\",\n",
    "            \"   ‚Ä¢ Careful hyperparameter tuning\",\n",
    "            \"   ‚Ä¢ Best for complex adaptations\"\n",
    "        ]\n",
    "        \n",
    "        y_pos = 0.95\n",
    "        for text in recommendations:\n",
    "            if text.startswith('üéØ'):\n",
    "                weight = 'bold'\n",
    "                size = 12\n",
    "            elif text.startswith(('‚úÖ', 'üî•', 'üîÑ', '‚öñÔ∏è')):\n",
    "                weight = 'bold'\n",
    "                size = 11\n",
    "            else:\n",
    "                weight = 'normal'\n",
    "                size = 10\n",
    "            \n",
    "            axes[1, 2].text(0.05, y_pos, text, fontsize=size, fontweight=weight, \n",
    "                           transform=axes[1, 2].transAxes)\n",
    "            y_pos -= 0.04\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üíæ Transfer learning analysis saved to: {save_path}\")\n",
    "\n",
    "# Load pretrained models for transfer learning\n",
    "print(\"\\nüîÑ Loading Pretrained Models for Transfer Learning:\")\n",
    "\n",
    "transfer_experiments = {}\n",
    "\n",
    "try:\n",
    "    # ResNet-18 pretrained\n",
    "    pretrained_resnet18 = models.resnet18(weights='IMAGENET1K_V1')\n",
    "    transfer_experiments['ResNet-18'] = AdvancedTransferLearningExperiment(\n",
    "        'ResNet-18-Pretrained', pretrained_resnet18\n",
    "    )\n",
    "    print(\"‚úÖ ResNet-18 pretrained model loaded\")\n",
    "    \n",
    "    # EfficientNet-B0 pretrained (if available)\n",
    "    try:\n",
    "        pretrained_efficientnet = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "        transfer_experiments['EfficientNet-B0'] = AdvancedTransferLearningExperiment(\n",
    "            'EfficientNet-B0-Pretrained', pretrained_efficientnet\n",
    "        )\n",
    "        print(\"‚úÖ EfficientNet-B0 pretrained model loaded\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è EfficientNet-B0 not available, using DenseNet-121 instead\")\n",
    "        pretrained_densenet = models.densenet121(weights='IMAGENET1K_V1')\n",
    "        transfer_experiments['DenseNet-121'] = AdvancedTransferLearningExperiment(\n",
    "            'DenseNet-121-Pretrained', pretrained_densenet\n",
    "        )\n",
    "        print(\"‚úÖ DenseNet-121 pretrained model loaded\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading pretrained models: {e}\")\n",
    "    print(\"üìù Creating dummy experiment for demonstration\")\n",
    "    \n",
    "    # Create dummy experiment with our custom models\n",
    "    transfer_experiments['ResNet-18'] = AdvancedTransferLearningExperiment(\n",
    "        'ResNet-18-Custom', resnet_models['ResNet-18']\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ Transfer learning experiments initialized: {list(transfer_experiments.keys())}\")\n",
    "```\n",
    "\n",
    "## 8. Execution of Transfer Learning Experiments\n",
    "\n",
    "```python\n",
    "# Execute comprehensive transfer learning experiments\n",
    "print(\"\\nüß™ Executing Advanced Transfer Learning Experiments:\")\n",
    "\n",
    "# Define strategies to test\n",
    "transfer_strategies = ['frozen', 'fine_tuning', 'progressive']\n",
    "experiment_results = {}\n",
    "\n",
    "for model_name, experiment in transfer_experiments.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üéØ Running experiments for {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    experiment_results[model_name] = {}\n",
    "    \n",
    "    for strategy in transfer_strategies:\n",
    "        print(f\"\\nüî¨ Testing {strategy} strategy...\")\n",
    "        try:\n",
    "            # Run experiment with reduced epochs for demonstration\n",
    "            history = experiment.train_with_strategy(\n",
    "                train_loader_transfer, \n",
    "                test_loader_transfer,\n",
    "                strategy=strategy,\n",
    "                epochs=5,  # Reduced for demo\n",
    "                base_lr=0.001\n",
    "            )\n",
    "            experiment_results[model_name][strategy] = history\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in {strategy} experiment: {e}\")\n",
    "            # Create dummy results for demonstration\n",
    "            experiment_results[model_name][strategy] = {\n",
    "                'train_loss': [0.8, 0.6, 0.4, 0.3, 0.25],\n",
    "                'train_acc': [70, 75, 80, 85, 87],\n",
    "                'test_loss': [0.9, 0.7, 0.5, 0.4, 0.35],\n",
    "                'test_acc': [65, 70, 75, 78, 80],\n",
    "                'learning_rates': [0.001, 0.008, 0.005, 0.002, 0.0005]\n",
    "            }\n",
    "    \n",
    "    # Create individual model analysis\n",
    "    experiment.create_transfer_learning_analysis(\n",
    "        f\"{project_dirs['transfer_learning']}/{model_name}_transfer_analysis.png\"\n",
    "    )\n",
    "\n",
    "def create_comprehensive_transfer_learning_summary(experiments, save_path):\n",
    "    \"\"\"Create comprehensive summary of all transfer learning experiments\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    \n",
    "    # Collect results from all experiments\n",
    "    all_results = {}\n",
    "    model_names = list(experiments.keys())\n",
    "    strategies = ['frozen', 'fine_tuning', 'progressive']\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        for strategy in strategies:\n",
    "            key = f\"{model_name}_{strategy}\"\n",
    "            if strategy in experiments[model_name].results:\n",
    "                all_results[key] = experiments[model_name].results[strategy]\n",
    "    \n",
    "    # 1. Final Accuracy Comparison Across All Models and Strategies\n",
    "    model_strategy_combinations = []\n",
    "    final_accuracies = []\n",
    "    colors = []\n",
    "    \n",
    "    color_map = {'frozen': '#FF6B6B', 'fine_tuning': '#4ECDC4', 'progressive': '#45B7D1'}\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        for strategy in strategies:\n",
    "            if strategy in experiments[model_name].results:\n",
    "                model_strategy_combinations.append(f\"{model_name}\\n{strategy}\")\n",
    "                final_acc = experiments[model_name].results[strategy]['test_acc'][-1]\n",
    "                final_accuracies.append(final_acc)\n",
    "                colors.append(color_map[strategy])\n",
    "    \n",
    "    bars1 = axes[0, 0].bar(range(len(model_strategy_combinations)), final_accuracies, \n",
    "                          color=colors, alpha=0.8)\n",
    "    axes[0, 0].set_title('Final Test Accuracy: All Models & Strategies', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "    axes[0, 0].set_xticks(range(len(model_strategy_combinations)))\n",
    "    axes[0, 0].set_xticklabels(model_strategy_combinations, rotation=45, ha='right', fontsize=10)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, acc in zip(bars1, final_accuracies):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                       f'{acc:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Create legend for strategies\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=color_map[strategy], label=strategy.title()) \n",
    "                      for strategy in strategies]\n",
    "    axes[0, 0].legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # 2. Learning Curves Comparison (Best Models Only)\n",
    "    best_models = {}\n",
    "    for model_name in model_names:\n",
    "        best_acc = 0\n",
    "        best_strategy = None\n",
    "        for strategy in strategies:\n",
    "            if strategy in experiments[model_name].results:\n",
    "                acc = experiments[model_name].results[strategy]['test_acc'][-1]\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_strategy = strategy\n",
    "        if best_strategy:\n",
    "            best_models[model_name] = (best_strategy, experiments[model_name].results[best_strategy])\n",
    "    \n",
    "    model_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "    for i, (model_name, (strategy, history)) in enumerate(best_models.items()):\n",
    "        epochs = range(1, len(history['test_acc']) + 1)\n",
    "        color = model_colors[i % len(model_colors)]\n",
    "        axes[0, 1].plot(epochs, history['test_acc'], color=color, linewidth=3, \n",
    "                       label=f'{model_name} ({strategy})', alpha=0.8)\n",
    "    \n",
    "    axes[0, 1].set_title('Best Strategy Learning Curves by Model', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Test Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Strategy Performance Analysis\n",
    "    strategy_performance = {}\n",
    "    for strategy in strategies:\n",
    "        accuracies = []\n",
    "        for model_name in model_names:\n",
    "            if strategy in experiments[model_name].results:\n",
    "                accuracies.append(experiments[model_name].results[strategy]['test_acc'][-1])\n",
    "        if accuracies:\n",
    "            strategy_performance[strategy] = {\n",
    "                'mean': np.mean(accuracies),\n",
    "                'std': np.std(accuracies),\n",
    "                'max': np.max(accuracies),\n",
    "                'min': np.min(accuracies)\n",
    "            }\n",
    "    \n",
    "    strategy_names = list(strategy_performance.keys())\n",
    "    means = [strategy_performance[s]['mean'] for s in strategy_names]\n",
    "    stds = [strategy_performance[s]['std'] for s in strategy_names]\n",
    "    strategy_colors = [color_map[s] for s in strategy_names]\n",
    "    \n",
    "    bars2 = axes[1, 0].bar(strategy_names, means, yerr=stds, capsize=5,\n",
    "                          color=strategy_colors, alpha=0.8)\n",
    "    axes[1, 0].set_title('Strategy Performance Summary\\n(Mean ¬± Std across models)', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Test Accuracy (%)')\n",
    "    \n",
    "    for bar, mean, std in zip(bars2, means, stds):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + std + 1,\n",
    "                       f'{mean:.1f}¬±{std:.1f}%', ha='center', va='bottom', \n",
    "                       fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # 4. Training Efficiency Comparison\n",
    "    efficiency_data = {}\n",
    "    for model_name in model_names:\n",
    "        efficiency_data[model_name] = {}\n",
    "        for strategy in strategies:\n",
    "            if strategy in experiments[model_name].results:\n",
    "                history = experiments[model_name].results[strategy]\n",
    "                # Calculate epochs to reach 90% of final performance\n",
    "                final_acc = history['test_acc'][-1]\n",
    "                target_acc = final_acc * 0.9\n",
    "                \n",
    "                epochs_to_target = len(history['test_acc'])\n",
    "                for epoch, acc in enumerate(history['test_acc']):\n",
    "                    if acc >= target_acc:\n",
    "                        epochs_to_target = epoch + 1\n",
    "                        break\n",
    "                \n",
    "                efficiency_data[model_name][strategy] = epochs_to_target\n",
    "    \n",
    "    # Create efficiency heatmap\n",
    "    efficiency_matrix = []\n",
    "    for model_name in model_names:\n",
    "        row = []\n",
    "        for strategy in strategies:\n",
    "            if strategy in efficiency_data[model_name]:\n",
    "                row.append(efficiency_data[model_name][strategy])\n",
    "            else:\n",
    "                row.append(np.nan)\n",
    "        efficiency_matrix.append(row)\n",
    "    \n",
    "    im = axes[1, 1].imshow(efficiency_matrix, cmap='RdYlGn_r', aspect='auto')\n",
    "    axes[1, 1].set_title('Training Efficiency Heatmap\\n(Epochs to 90% Performance)', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xticks(range(len(strategies)))\n",
    "    axes[1, 1].set_xticklabels(strategies)\n",
    "    axes[1, 1].set_yticks(range(len(model_names)))\n",
    "    axes[1, 1].set_yticklabels(model_names)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(len(strategies)):\n",
    "            if not np.isnan(efficiency_matrix[i][j]):\n",
    "                text = axes[1, 1].text(j, i, f'{efficiency_matrix[i][j]:.0f}',\n",
    "                                     ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im, ax=axes[1, 1], label='Epochs')\n",
    "    \n",
    "    # 5. Resource Utilization Analysis\n",
    "    # Estimate computational cost (proxy based on model size and strategy)\n",
    "    resource_costs = {}\n",
    "    for model_name in model_names:\n",
    "        model_params = models_info[model_name]['total_parameters'] / 1e6\n",
    "        for strategy in strategies:\n",
    "            if strategy in experiments[model_name].results:\n",
    "                # Cost factors: frozen < progressive < fine_tuning\n",
    "                cost_multiplier = {'frozen': 0.3, 'progressive': 0.7, 'fine_tuning': 1.0}\n",
    "                epochs_trained = len(experiments[model_name].results[strategy]['train_loss'])\n",
    "                \n",
    "                # Relative computational cost\n",
    "                cost = model_params * cost_multiplier[strategy] * epochs_trained\n",
    "                \n",
    "                key = f\"{model_name}_{strategy}\"\n",
    "                resource_costs[key] = cost\n",
    "    \n",
    "    cost_labels = list(resource_costs.keys())\n",
    "    cost_values = list(resource_costs.values())\n",
    "    cost_colors = [color_map[label.split('_')[1]] for label in cost_labels]\n",
    "    \n",
    "    bars3 = axes[2, 0].bar(range(len(cost_labels)), cost_values, color=cost_colors, alpha=0.8)\n",
    "    axes[2, 0].set_title('Estimated Computational Cost\\n(Relative Units)', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "    axes[2, 0].set_ylabel('Computational Cost')\n",
    "    axes[2, 0].set_xticks(range(len(cost_labels)))\n",
    "    axes[2, 0].set_xticklabels([label.replace('_', '\\n') for label in cost_labels], \n",
    "                              rotation=45, ha='right', fontsize=9)\n",
    "    \n",
    "    # 6. Recommendations and Best Practices\n",
    "    axes[2, 1].axis('off')\n",
    "    \n",
    "    # Generate data-driven recommendations\n",
    "    best_overall = max(model_strategy_combinations, \n",
    "                      key=lambda x: final_accuracies[model_strategy_combinations.index(x)])\n",
    "    best_acc = max(final_accuracies)\n",
    "    \n",
    "    most_efficient_strategy = min(strategy_performance.keys(), \n",
    "                                 key=lambda s: strategy_performance[s]['mean'] / \n",
    "                                 np.mean([resource_costs[k] for k in resource_costs.keys() \n",
    "                                         if k.endswith(f'_{s}')]))\n",
    "    \n",
    "    recommendations = [\n",
    "        \"üéØ Transfer Learning Insights & Recommendations:\",\n",
    "        \"\",\n",
    "        f\"üèÜ Best Overall Performance:\",\n",
    "        f\"   {best_overall}: {best_acc:.1f}% accuracy\",\n",
    "        \"\",\n",
    "        f\"‚ö° Most Efficient Strategy:\",\n",
    "        f\"   {most_efficient_strategy.title()} balances performance & cost\",\n",
    "        \"\",\n",
    "        \"üìã Key Findings:\",\n",
    "        f\"   ‚Ä¢ Fine-tuning generally achieves highest accuracy\",\n",
    "        f\"   ‚Ä¢ Frozen features provide fastest training\",\n",
    "        f\"   ‚Ä¢ Progressive unfreezing offers good balance\",\n",
    "        f\"   ‚Ä¢ Model choice significantly impacts results\",\n",
    "        \"\",\n",
    "        \"üí° Best Practices:\",\n",
    "        \"   ‚Ä¢ Start with frozen features for rapid prototyping\",\n",
    "        \"   ‚Ä¢ Use fine-tuning for production deployments\",\n",
    "        \"   ‚Ä¢ Consider progressive for limited compute\",\n",
    "        \"   ‚Ä¢ Match strategy to dataset size and domain gap\",\n",
    "        \"\",\n",
    "        \"‚ö†Ô∏è Important Considerations:\",\n",
    "        \"   ‚Ä¢ Larger models benefit more from fine-tuning\",\n",
    "        \"   ‚Ä¢ Small datasets favor feature extraction\",\n",
    "        \"   ‚Ä¢ Domain similarity affects optimal strategy\",\n",
    "        \"   ‚Ä¢ Computational budget constrains choices\"\n",
    "    ]\n",
    "    \n",
    "    y_pos = 0.98\n",
    "    for text in recommendations:\n",
    "        if text.startswith('üéØ'):\n",
    "            weight = 'bold'\n",
    "            size = 12\n",
    "        elif text.startswith(('üèÜ', '‚ö°', 'üìã', 'üí°', '‚ö†Ô∏è')):\n",
    "            weight = 'bold'\n",
    "            size = 11\n",
    "        else:\n",
    "            weight = 'normal'\n",
    "            size = 10\n",
    "        \n",
    "        axes[2, 1].text(0.05, y_pos, text, fontsize=size, fontweight=weight, \n",
    "                       transform=axes[2, 1].transAxes)\n",
    "        y_pos -= 0.04\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üíæ Comprehensive transfer learning summary saved to: {save_path}\")\n",
    "\n",
    "# Create comprehensive transfer learning summary\n",
    "print(\"\\nüìä Creating Comprehensive Transfer Learning Summary...\")\n",
    "create_comprehensive_transfer_learning_summary(\n",
    "    transfer_experiments,\n",
    "    f\"{project_dirs['transfer_learning']}/comprehensive_transfer_learning_summary.png\"\n",
    ")\n",
    "```\n",
    "\n",
    "## 9. Vision Transformer Attention Visualization\n",
    "\n",
    "```python\n",
    "def visualize_vision_transformer_attention(model, sample_image, save_path, layer_indices=[-1, -3, -6]):\n",
    "    \"\"\"\n",
    "    Comprehensive attention visualization for Vision Transformer\n",
    "    \n",
    "    Args:\n",
    "        model: Vision Transformer model\n",
    "        sample_image: Input image tensor\n",
    "        save_path: Path to save visualization\n",
    "        layer_indices: Which transformer layers to visualize\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get model outputs and attention weights\n",
    "        if sample_image.dim() == 3:\n",
    "            sample_image = sample_image.unsqueeze(0)\n",
    "        \n",
    "        sample_image = sample_image.to(device)\n",
    "        outputs, attention_weights = model(sample_image)\n",
    "        \n",
    "        if not attention_weights:\n",
    "            print(\"‚ö†Ô∏è No attention weights available for visualization\")\n",
    "            return\n",
    "        \n",
    "        num_layers = len(layer_indices)\n",
    "        fig, axes = plt.subplots(3, num_layers + 1, figsize=(6*(num_layers + 1), 18))\n",
    "        \n",
    "        # Original image\n",
    "        img_display = sample_image.squeeze().cpu().numpy()\n",
    "        if img_display.shape[0] == 3:  # RGB\n",
    "            img_display = img_display.transpose(1, 2, 0)\n",
    "            # Denormalize for display (ImageNet normalization)\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            img_display = img_display * std + mean\n",
    "            img_display = np.clip(img_display, 0, 1)\n",
    "        \n",
    "        axes[0, 0].imshow(img_display, cmap='gray' if len(img_display.shape) == 2 else None)\n",
    "        axes[0, 0].set_title('Original Image\\n(Input)', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Process each selected layer\n",
    "        for col, layer_idx in enumerate(layer_indices):\n",
    "            if abs(layer_idx) > len(attention_weights):\n",
    "                continue\n",
    "                \n",
    "            # Get attention weights for this layer\n",
    "            attn = attention_weights[layer_idx].squeeze(0)  # Remove batch dimension\n",
    "            \n",
    "            # Average across heads for main visualization\n",
    "            attn_avg = attn.mean(dim=0)  # Shape: [seq_len, seq_len]\n",
    "            \n",
    "            # Extract attention from CLS token to image patches\n",
    "            cls_attn = attn_avg[0, 1:]  # Skip CLS-to-CLS attention\n",
    "            \n",
    "            # Calculate grid size for patches\n",
    "            num_patches = len(cls_attn)\n",
    "            grid_size = int(np.sqrt(num_patches))\n",
    "            \n",
    "            if grid_size * grid_size == num_patches:\n",
    "                # Reshape to spatial grid\n",
    "                attn_map = cls_attn.cpu().numpy().reshape(grid_size, grid_size)\n",
    "                \n",
    "                # 1. CLS token attention map\n",
    "                im1 = axes[0, col + 1].imshow(attn_map, cmap='viridis', interpolation='bilinear')\n",
    "                layer_name = f'Layer {layer_idx}' if layer_idx >= 0 else f'Layer {len(attention_weights) + layer_idx}'\n",
    "                axes[0, col + 1].set_title(f'{layer_name}\\nCLS Attention Map', \n",
    "                                          fontsize=12, fontweight='bold')\n",
    "                axes[0, col + 1].axis('off')\n",
    "                plt.colorbar(im1, ax=axes[0, col + 1], fraction=0.046)\n",
    "                \n",
    "                # 2. Attention overlay on original image\n",
    "                # Resize attention map to match image size\n",
    "                attn_resized = torch.nn.functional.interpolate(\n",
    "                    torch.tensor(attn_map).unsqueeze(0).unsqueeze(0), \n",
    "                    size=(img_display.shape[0], img_display.shape[1]), \n",
    "                    mode='bilinear'\n",
    "                ).squeeze().numpy()\n",
    "                \n",
    "                axes[1, col + 1].imshow(img_display, cmap='gray' if len(img_display.shape) == 2 else None)\n",
    "                axes[1, col + 1].imshow(attn_resized, alpha=0.6, cmap='jet', vmin=0, vmax=attn_resized.max())\n",
    "                axes[1, col + 1].set_title(f'{layer_name}\\nAttention Overlay', \n",
    "                                          fontsize=12, fontweight='bold')\n",
    "                axes[1, col + 1].axis('off')\n",
    "                \n",
    "                # 3. Multi-head attention analysis\n",
    "                num_heads = min(4, attn.shape[0])\n",
    "                head_attns = []\n",
    "                \n",
    "                for head in range(num_heads):\n",
    "                    head_attn = attn[head, 0, 1:].cpu().numpy().reshape(grid_size, grid_size)\n",
    "                    head_attns.append(head_attn)\n",
    "                \n",
    "                # Create combined multi-head visualization\n",
    "                combined_heads = np.concatenate(head_attns[:2], axis=1) if len(head_attns) >= 2 else head_attns[0]\n",
    "                if len(head_attns) >= 4:\n",
    "                    bottom_heads = np.concatenate(head_attns[2:4], axis=1)\n",
    "                    combined_heads = np.concatenate([combined_heads, bottom_heads], axis=0)\n",
    "                \n",
    "                im3 = axes[2, col + 1].imshow(combined_heads, cmap='viridis')\n",
    "                axes[2, col + 1].set_title(f'{layer_name}\\nMulti-Head Attention\\n(First {num_heads} heads)', \n",
    "                                          fontsize=12, fontweight='bold')\n",
    "                axes[2, col + 1].axis('off')\n",
    "                plt.colorbar(im3, ax=axes[2, col + 1], fraction=0.046)\n",
    "        \n",
    "        # Attention statistics in the leftmost column\n",
    "        axes[1, 0].axis('off')\n",
    "        axes[2, 0].axis('off')\n",
    "        \n",
    "        # Calculate attention statistics\n",
    "        stats_text = \"üîç Attention Statistics:\\n\\n\"\n",
    "        \n",
    "        for i, layer_idx in enumerate(layer_indices):\n",
    "            if abs(layer_idx) > len(attention_weights):\n",
    "                continue\n",
    "                \n",
    "            attn = attention_weights[layer_idx].squeeze(0)\n",
    "            cls_attn = attn.mean(dim=0)[0, 1:]  # Average across heads, CLS to patches\n",
    "            \n",
    "            layer_name = f'Layer {layer_idx}' if layer_idx >= 0 else f'Layer {len(attention_weights) + layer_idx}'\n",
    "            stats_text += f\"{layer_name}:\\n\"\n",
    "            stats_text += f\"  Mean: {cls_attn.mean().item():.4f}\\n\"\n",
    "            stats_text += f\"  Std: {cls_attn.std().item():.4f}\\n\"\n",
    "            stats_text += f\"  Max: {cls_attn.max().item():.4f}\\n\"\n",
    "            stats_text += f\"  Entropy: {(-cls_attn * torch.log(cls_attn + 1e-8)).sum().item():.4f}\\n\\n\"\n",
    "        \n",
    "        axes[1, 0].text(0.1, 0.9, stats_text, fontsize=11, verticalalignment='top',\n",
    "                       transform=axes[1, 0].transAxes, \n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8))\n",
    "        \n",
    "        # Attention evolution analysis\n",
    "        evolution_text = \"üìà Attention Evolution:\\n\\n\"\n",
    "        evolution_text += \"Early Layers:\\n\"\n",
    "        evolution_text += \"‚Ä¢ Focus on local patterns\\n\"\n",
    "        evolution_text += \"‚Ä¢ High attention entropy\\n\"\n",
    "        evolution_text += \"‚Ä¢ Distributed attention\\n\\n\"\n",
    "        evolution_text += \"Middle Layers:\\n\"\n",
    "        evolution_text += \"‚Ä¢ Semantic grouping\\n\"\n",
    "        evolution_text += \"‚Ä¢ Moderate specificity\\n\"\n",
    "        evolution_text += \"‚Ä¢ Object-part relationships\\n\\n\"\n",
    "        evolution_text += \"Later Layers:\\n\"\n",
    "        evolution_text += \"‚Ä¢ Global object focus\\n\"\n",
    "        evolution_text += \"‚Ä¢ Sharp attention peaks\\n\"\n",
    "        evolution_text += \"‚Ä¢ Task-relevant regions\\n\\n\"\n",
    "        evolution_text += \"Key Insights:\\n\"\n",
    "        evolution_text += \"‚Ä¢ Hierarchical processing\\n\"\n",
    "        evolution_text += \"‚Ä¢ Progressive specialization\\n\"\n",
    "        evolution_text += \"‚Ä¢ Context aggregation\"\n",
    "        \n",
    "        axes[2, 0].text(0.1, 0.9, evolution_text, fontsize=10, verticalalignment='top',\n",
    "                       transform=axes[2, 0].transAxes,\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üíæ Vision Transformer attention visualization saved to: {save_path}\")\n",
    "        \n",
    "        return cls_attn\n",
    "\n",
    "def compare_attention_across_transformer_layers(model, sample_image, save_path):\n",
    "    \"\"\"Compare attention patterns across all transformer layers\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if sample_image.dim() == 3:\n",
    "            sample_image = sample_image.unsqueeze(0)\n",
    "        \n",
    "        sample_image = sample_image.to(device)\n",
    "        outputs, attention_weights = model(sample_image)\n",
    "        \n",
    "        if not attention_weights:\n",
    "            print(\"‚ö†Ô∏è No attention weights available\")\n",
    "            return\n",
    "        \n",
    "        # Select layers to compare (every 2nd layer + first and last)\n",
    "        layer_indices = [0, 2, 4, 6, 8, 10, -1]\n",
    "        layer_indices = [i for i in layer_indices if i < len(attention_weights) or i == -1]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, len(layer_indices), figsize=(4*len(layer_indices), 8))\n",
    "        if len(layer_indices) == 1:\n",
    "            axes = axes.reshape(2, 1)\n",
    "        \n",
    "        attention_evolution = []\n",
    "        entropy_evolution = []\n",
    "        \n",
    "        for col, layer_idx in enumerate(layer_indices):\n",
    "            actual_idx = layer_idx if layer_idx >= 0 else len(attention_weights) + layer_idx\n",
    "            \n",
    "            # Get attention weights\n",
    "            attn = attention_weights[layer_idx].squeeze(0).mean(dim=0)  # Average across heads\n",
    "            cls_attn = attn[0, 1:]  # CLS to patches attention\n",
    "            \n",
    "            # Calculate spatial dimensions\n",
    "            num_patches = len(cls_attn)\n",
    "            grid_size = int(np.sqrt(num_patches))\n",
    "            \n",
    "            if grid_size * grid_size == num_patches:\n",
    "                attn_map = cls_attn.cpu().numpy().reshape(grid_size, grid_size)\n",
    "                attention_evolution.append(attn_map)\n",
    "                \n",
    "                # Calculate attention entropy\n",
    "                entropy = (-cls_attn * torch.log(cls_attn + 1e-8)).sum().item()\n",
    "                entropy_evolution.append(entropy)\n",
    "                \n",
    "                # Plot attention map\n",
    "                im1 = axes[0, col].imshow(attn_map, cmap='viridis', interpolation='bilinear')\n",
    "                axes[0, col].set_title(f'Layer {actual_idx}\\nEntropy: {entropy:.2f}', \n",
    "                                     fontsize=12, fontweight='bold')\n",
    "                axes[0, col].axis('off')\n",
    "                plt.colorbar(im1, ax=axes[0, col], fraction=0.046)\n",
    "                \n",
    "                # Plot attention distribution\n",
    "                axes[1, col].hist(cls_attn.cpu().numpy(), bins=20, alpha=0.7, \n",
    "                                color='skyblue', edgecolor='black')\n",
    "                axes[1, col].set_title(f'Attention Distribution\\nLayer {actual_idx}', \n",
    "                                     fontsize=10, fontweight='bold')\n",
    "                axes[1, col].set_xlabel('Attention Weight')\n",
    "                axes[1, col].set_ylabel('Frequency')\n",
    "                axes[1, col].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Attention Evolution Across Transformer Layers', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Create entropy evolution plot\n",
    "        if len(entropy_evolution) > 1:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            layer_numbers = [i if i >= 0 else len(attention_weights) + i for i in layer_indices]\n",
    "            plt.plot(layer_numbers, entropy_evolution, 'o-', linewidth=3, markersize=8, color='darkblue')\n",
    "            plt.title('Attention Entropy Evolution Through Transformer Layers', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Layer Index')\n",
    "            plt.ylabel('Attention Entropy')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add trend analysis\n",
    "            if len(entropy_evolution) >= 3:\n",
    "                z = np.polyfit(layer_numbers, entropy_evolution, 1)\n",
    "                p = np.poly1d(z)\n",
    "                plt.plot(layer_numbers, p(layer_numbers), \"--\", alpha=0.8, color='red', \n",
    "                        label=f'Trend: slope={z[0]:.3f}')\n",
    "                plt.legend()\n",
    "            \n",
    "            entropy_save_path = save_path.replace('.png', '_entropy_evolution.png')\n",
    "            plt.savefig(entropy_save_path, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"üíæ Attention entropy evolution saved to: {entropy_save_path}\")\n",
    "        \n",
    "        print(f\"üíæ Layer comparison saved to: {save_path}\")\n",
    "        \n",
    "        return attention_evolution\n",
    "\n",
    "# Execute Vision Transformer attention analysis\n",
    "if 'ViT-Tiny' in vit_models:\n",
    "    print(\"\\nüëÅÔ∏è Analyzing Vision Transformer Attention Mechanisms:\")\n",
    "    \n",
    "    # Prepare sample image for attention analysis\n",
    "    transform_vit_analysis = transforms.Compose([\n",
    "        transforms.Resize(32),  # Match ViT input size\n",
    "        transforms.CenterCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # Load a sample from CIFAR-10 test set\n",
    "        test_dataset_vit = torchvision.datasets.CIFAR10(\n",
    "            root='../data/modern_cnn_architectures/cifar10', \n",
    "            train=False, download=False, transform=transform_vit_analysis\n",
    "        )\n",
    "        sample_image_vit, sample_label_vit = test_dataset_vit[42]  # Fixed sample for consistency\n",
    "        print(f\"   Sample image class: {cifar10_classes[sample_label_vit]}\")\n",
    "        \n",
    "    except:\n",
    "        # Create dummy sample\n",
    "        sample_image_vit = torch.randn(3, 32, 32)\n",
    "        sample_label_vit = 0\n",
    "        print(\"   Using dummy sample image\")\n",
    "    \n",
    "    # Comprehensive attention visualization\n",
    "    print(\"\\nüîç Creating comprehensive attention visualization...\")\n",
    "    visualize_vision_transformer_attention(\n",
    "        vit_models['ViT-Tiny'],\n",
    "        sample_image_vit,\n",
    "        f\"{project_dirs['attention_visualization']}/vit_comprehensive_attention_analysis.png\"\n",
    "    )\n",
    "    \n",
    "    # Layer-wise attention comparison\n",
    "    print(\"\\nüìä Comparing attention across transformer layers...\")\n",
    "    compare_attention_across_transformer_layers(\n",
    "        vit_models['ViT-Tiny'],\n",
    "        sample_image_vit,\n",
    "        f\"{project_dirs['attention_visualization']}/vit_layer_attention_comparison.png\"\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Vision Transformer model not available for attention analysis\")\n",
    "```\n",
    "\n",
    "## 10. Final Results Summary and Model Comparison\n",
    "\n",
    "```python\n",
    "def create_comprehensive_results_summary():\n",
    "    \"\"\"Generate comprehensive summary of all experiments and analyses\"\"\"\n",
    "    \n",
    "    summary_data = {\n",
    "        'experiment_info': {\n",
    "            'title': 'Modern CNN Architectures: From ResNet to Vision Transformers',\n",
    "            'completion_timestamp': datetime.now().isoformat(),\n",
    "            'device_used': str(device),\n",
    "            'pytorch_version': torch.__version__,\n",
    "            'total_models_analyzed': len(models_info)\n",
    "        },\n",
    "        'architecture_families': {\n",
    "            'ResNet': {\n",
    "                'key_innovation': 'Skip connections and residual learning',\n",
    "                'advantages': ['Deep network training', 'Gradient flow', 'Proven performance'],\n",
    "                'models_implemented': list(resnet_models.keys()),\n",
    "                'parameter_range': f\"{min(models_info[k]['total_parameters'] for k in models_info if 'ResNet' in k):,} - {max(models_info[k]['total_parameters'] for k in models_info if 'ResNet' in k):,}\"\n",
    "            },\n",
    "            'DenseNet': {\n",
    "                'key_innovation': 'Dense connectivity and feature reuse',\n",
    "                'advantages': ['Parameter efficiency', 'Feature reuse', 'Gradient flow'],\n",
    "                'models_implemented': list(densenet_models.keys()),\n",
    "                'parameter_range': f\"{min(models_info[k]['total_parameters'] for k in models_info if 'DenseNet' in k):,} - {max(models_info[k]['total_parameters'] for k in models_info if 'DenseNet' in k):,}\"\n",
    "            },\n",
    "            'EfficientNet': {\n",
    "                'key_innovation': 'Compound scaling and mobile optimization',\n",
    "                'advantages': ['Efficiency', 'Mobile deployment', 'Scalability'],\n",
    "                'models_implemented': list(efficientnet_models.keys()),\n",
    "                'parameter_range': f\"{min(models_info[k]['total_parameters'] for k in models_info if 'EfficientNet' in k):,} - {max(models_info[k]['total_parameters'] for k in models_info if 'EfficientNet' in k):,}\"\n",
    "            },\n",
    "            'Vision Transformer': {\n",
    "                'key_innovation': 'Self-attention for computer vision',\n",
    "                'advantages': ['Global context', 'Scalability', 'Transfer learning'],\n",
    "                'models_implemented': list(vit_models.keys()),\n",
    "                'parameter_range': f\"{min(models_info[k]['total_parameters'] for k in models_info if 'ViT' in k):,} - {max(models_info[k]['total_parameters'] for k in models_info if 'ViT' in k):,}\"\n",
    "            }\n",
    "        },\n",
    "        'performance_analysis': performance_results if 'performance_results' in locals() else {},\n",
    "        'transfer_learning_insights': {\n",
    "            'strategies_tested': ['frozen', 'fine_tuning', 'progressive'],\n",
    "            'models_evaluated': list(transfer_experiments.keys()) if 'transfer_experiments' in locals() else [],\n",
    "            'key_findings': [\n",
    "                'Fine-tuning generally achieves highest accuracy',\n",
    "                'Frozen features provide fastest training',\n",
    "                'Progressive unfreezing offers good balance',\n",
    "                'Model architecture significantly impacts transfer success'\n",
    "            ]\n",
    "        },\n",
    "        'technical_achievements': {\n",
    "            'implemented_from_scratch': [\n",
    "                'ResNet with skip connections',\n",
    "                'DenseNet with dense connectivity', \n",
    "                'EfficientNet with compound scaling',\n",
    "                'Vision Transformer with self-attention'\n",
    "            ],\n",
    "            'analysis_capabilities': [\n",
    "                'Comprehensive architecture comparison',\n",
    "                'Performance benchmarking',\n",
    "                'Transfer learning experiments',\n",
    "                'Attention mechanism visualization',\n",
    "                'Feature extraction and analysis'\n",
    "            ]\n",
    "        },\n",
    "        'model_recommendations': {\n",
    "            'high_accuracy_large_datasets': ['ResNet-50', 'ViT-Small'],\n",
    "            'mobile_deployment': ['EfficientNet-B0', 'EfficientNet-B1'],\n",
    "            'memory_constrained': ['ResNet-18', 'EfficientNet-B0'],\n",
    "            'research_experimentation': ['DenseNet-121', 'ViT-Tiny'],\n",
    "            'transfer_learning': ['ResNet-18', 'EfficientNet-B0', 'ViT-Tiny']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return summary_data\n",
    "\n",
    "def create_final_dashboard(models_info, performance_results, save_path):\n",
    "    \"\"\"Create comprehensive final dashboard summarizing all analyses\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 24))\n",
    "    gs = fig.add_gridspec(6, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Model Overview - Parameters and Size (Top Row)\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    model_names = list(models_info.keys())\n",
    "    param_counts = [info['total_parameters'] / 1e6 for info in models_info.values()]\n",
    "    model_sizes = [info['model_size_mb'] for info in models_info.values()]\n",
    "    \n",
    "    # Color by family\n",
    "    colors = []\n",
    "    for name in model_names:\n",
    "        if 'ResNet' in name: colors.append('#FF6B6B')\n",
    "        elif 'DenseNet' in name: colors.append('#4ECDC4')\n",
    "        elif 'EfficientNet' in name: colors.append('#45B7D1')\n",
    "        elif 'ViT' in name: colors.append('#96CEB4')\n",
    "        else: colors.append('#FFEAA7')\n",
    "    \n",
    "    bars1 = ax1.bar(range(len(model_names)), param_counts, color=colors, alpha=0.8)\n",
    "    ax1.set_title('Model Parameter Count Comparison', fontsize=16, fontweight='bold')\n",
    "    ax1.set_ylabel('Parameters (Millions)', fontsize=12)\n",
    "    ax1.set_xticks(range(len(model_names)))\n",
    "    ax1.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars1, param_counts):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + max(param_counts)*0.01,\n",
    "                f'{count:.1f}M', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 2. Performance Comparison (if available)\n",
    "    ax2 = fig.add_subplot(gs[0, 2:])\n",
    "    if performance_results:\n",
    "        perf_models = list(performance_results.keys())\n",
    "        inference_times = [performance_results[model]['avg_inference_time_ms'] for model in perf_models]\n",
    "        throughputs = [performance_results[model]['throughput_fps'] for model in perf_models]\n",
    "        \n",
    "        ax2_twin = ax2.twinx()\n",
    "        \n",
    "        line1 = ax2.bar(range(len(perf_models)), inference_times, alpha=0.7, color='lightcoral', label='Inference Time (ms)')\n",
    "        line2 = ax2_twin.plot(range(len(perf_models)), throughputs, 'o-', color='darkblue', linewidth=3, markersize=8, label='Throughput (FPS)')\n",
    "        \n",
    "        ax2.set_title('Performance Metrics Comparison', fontsize=16, fontweight='bold')\n",
    "        ax2.set_ylabel('Inference Time (ms)', fontsize=12, color='darkred')\n",
    "        ax2_twin.set_ylabel('Throughput (FPS)', fontsize=12, color='darkblue')\n",
    "        ax2.set_xticks(range(len(perf_models)))\n",
    "        ax2.set_xticklabels(perf_models, rotation=45, ha='right')\n",
    "        \n",
    "        # Combined legend\n",
    "        lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
    "        ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Performance Analysis\\nNot Available', ha='center', va='center',\n",
    "                transform=ax2.transAxes, fontsize=14, bbox=dict(boxstyle=\"round\", facecolor='lightgray'))\n",
    "        ax2.set_title('Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 3. Architecture Timeline (Second Row)\n",
    "    ax3 = fig.add_subplot(gs[1, :2])\n",
    "    innovation_timeline = {\n",
    "        'AlexNet': 2012, 'VGG': 2014, 'ResNet': 2015, 'DenseNet': 2016, \n",
    "        'MobileNet': 2017, 'EfficientNet': 2019, 'Vision Transformer': 2020, 'ConvNeXt': 2022\n",
    "    }\n",
    "    \n",
    "    timeline_years = list(innovation_timeline.values())\n",
    "    timeline_names = list(innovation_timeline.keys())\n",
    "    implemented_models = ['ResNet', 'DenseNet', 'EfficientNet', 'Vision Transformer']\n",
    "    \n",
    "    colors_timeline = ['red' if name in implemented_models else 'lightgray' for name in timeline_names]\n",
    "    sizes = [150 if name in implemented_models else 50 for name in timeline_names]\n",
    "    \n",
    "    scatter = ax3.scatter(timeline_years, range(len(timeline_names)), c=colors_timeline, s=sizes, alpha=0.8)\n",
    "    for i, (name, year) in enumerate(innovation_timeline.items()):\n",
    "        weight = 'bold' if name in implemented_models else 'normal'\n",
    "        ax3.annotate(name, (year, i), xytext=(10, 0), textcoords='offset points',\n",
    "                    fontsize=11, fontweight=weight)\n",
    "    \n",
    "    ax3.set_title('CNN Architecture Innovation Timeline', fontsize=16, fontweight='bold')\n",
    "    ax3.set_xlabel('Year')\n",
    "    ax3.set_yticks(range(len(timeline_names)))\n",
    "    ax3.set_yticklabels(timeline_names)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.text(0.02, 0.98, 'Red: Implemented\\nGray: Reference', transform=ax3.transAxes, \n",
    "             va='top', bbox=dict(boxstyle=\"round\", facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 4. Key Innovations Summary\n",
    "    ax4 = fig.add_subplot(gs[1, 2:])\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    innovations_text = \"\"\"\n",
    "üîó ResNet (2015): Skip Connections\n",
    "   ‚Ä¢ Solves vanishing gradient problem\n",
    "   ‚Ä¢ Enables very deep networks (50-152 layers)\n",
    "   ‚Ä¢ Residual learning framework\n",
    "\n",
    "üåü DenseNet (2016): Dense Connectivity  \n",
    "   ‚Ä¢ Every layer connects to every other layer\n",
    "   ‚Ä¢ Maximum information flow between layers\n",
    "   ‚Ä¢ Parameter efficient architecture\n",
    "\n",
    "‚ö° EfficientNet (2019): Compound Scaling\n",
    "   ‚Ä¢ Systematically scales depth, width, resolution\n",
    "   ‚Ä¢ Mobile-optimized with SE blocks\n",
    "   ‚Ä¢ Best accuracy-efficiency trade-offs\n",
    "\n",
    "üëÅÔ∏è Vision Transformer (2020): Self-Attention\n",
    "   ‚Ä¢ Applies transformer to computer vision\n",
    "   ‚Ä¢ Global context through attention\n",
    "   ‚Ä¢ Patch-based image processing\n",
    "    \"\"\"\n",
    "    \n",
    "    ax4.text(0.05, 0.95, innovations_text, fontsize=11, verticalalignment='top',\n",
    "             transform=ax4.transAxes)\n",
    "    ax4.set_title('Key Architecture Innovations', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 5. Transfer Learning Results (if available)\n",
    "    ax5 = fig.add_subplot(gs[2, :])\n",
    "    if 'transfer_experiments' in locals() and transfer_experiments:\n",
    "        # Create transfer learning summary\n",
    "        strategies = ['frozen', 'fine_tuning', 'progressive']\n",
    "        models_tl = list(transfer_experiments.keys())\n",
    "        \n",
    "        # Create heatmap-style visualization\n",
    "        tl_data = []\n",
    "        for model in models_tl:\n",
    "            row = []\n",
    "            for strategy in strategies:\n",
    "                if strategy in transfer_experiments[model].results:\n",
    "                    final_acc = transfer_experiments[model].results[strategy]['test_acc'][-1]\n",
    "                    row.append(final_acc)\n",
    "                else:\n",
    "                    row.append(np.nan)\n",
    "            tl_data.append(row)\n",
    "        \n",
    "        im = ax5.imshow(tl_data, cmap='RdYlGn', aspect='auto', vmin=60, vmax=90)\n",
    "        ax5.set_title('Transfer Learning Results: Test Accuracy (%)', fontsize=16, fontweight='bold')\n",
    "        ax5.set_xticks(range(len(strategies)))\n",
    "        ax5.set_xticklabels([s.replace('_', ' ').title() for s in strategies])\n",
    "        ax5.set_yticks(range(len(models_tl)))\n",
    "        ax5.set_yticklabels(models_tl)\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(len(models_tl)):\n",
    "            for j in range(len(strategies)):\n",
    "                if not np.isnan(tl_data[i][j]):\n",
    "                    text = ax5.text(j, i, f'{tl_data[i][j]:.1f}%',\n",
    "                                   ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "        \n",
    "        plt.colorbar(im, ax=ax5, label='Test Accuracy (%)')\n",
    "    else:\n",
    "        ax5.text(0.5, 0.5, 'Transfer Learning Analysis\\nNot Available', ha='center', va='center',\n",
    "                transform=ax5.transAxes, fontsize=14, bbox=dict(boxstyle=\"round\", facecolor='lightgray'))\n",
    "        ax5.set_title('Transfer Learning Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 6. Model Selection Guide\n",
    "    ax6 = fig.add_subplot(gs[3, :2])\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    selection_guide = \"\"\"\n",
    "üéØ Model Selection Guide:\n",
    "\n",
    "üì± Mobile & Edge Deployment:\n",
    "   ‚Üí EfficientNet-B0: Best efficiency\n",
    "   ‚Üí MobileNet variants: Ultra-lightweight\n",
    "   \n",
    "üèÜ High Accuracy Applications:\n",
    "   ‚Üí ResNet-50: Proven performance\n",
    "   ‚Üí ViT-Small: State-of-the-art accuracy\n",
    "   \n",
    "üíæ Memory Constrained:\n",
    "   ‚Üí ResNet-18: Balanced choice\n",
    "   ‚Üí EfficientNet-B0: Optimized efficiency\n",
    "   \n",
    "üî¨ Research & Experimentation:\n",
    "   ‚Üí DenseNet-121: Novel connectivity\n",
    "   ‚Üí ViT-Tiny: Attention mechanisms\n",
    "   \n",
    "üéØ Transfer Learning:\n",
    "   ‚Üí ResNet-18: Versatile backbone\n",
    "   ‚Üí EfficientNet-B0: Efficient transfer\n",
    "    \"\"\"\n",
    "    \n",
    "    ax6.text(0.05, 0.95, selection_guide, fontsize=11, verticalalignment='top',\n",
    "             transform=ax6.transAxes)\n",
    "    ax6.set_title('Model Selection Guidelines', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 7. Performance Trade-offs\n",
    "    ax7 = fig.add_subplot(gs[3, 2:])\n",
    "    ax7.axis('off')\n",
    "    \n",
    "    tradeoffs_text = \"\"\"\n",
    "‚öñÔ∏è Architecture Trade-offs:\n",
    "\n",
    "üéØ Accuracy vs Efficiency:\n",
    "   High Accuracy: ViT > ResNet > DenseNet > EfficientNet\n",
    "   High Efficiency: EfficientNet > ResNet > DenseNet > ViT\n",
    "\n",
    "‚ö° Speed vs Quality:\n",
    "   Fast Inference: EfficientNet > ResNet > DenseNet > ViT\n",
    "   High Quality: ViT > ResNet > DenseNet > EfficientNet\n",
    "\n",
    "üíæ Memory vs Performance:\n",
    "   Low Memory: EfficientNet > ResNet > ViT > DenseNet\n",
    "   High Performance: ViT > ResNet > DenseNet > EfficientNet\n",
    "\n",
    "üîß Training vs Inference:\n",
    "   Training Efficient: ResNet (balanced across metrics)\n",
    "   Inference Optimized: EfficientNet (mobile-first design)\n",
    "\n",
    "üì± Mobile vs Desktop:\n",
    "   Mobile Optimized: EfficientNet series\n",
    "   Desktop/Server: ResNet, ViT series\n",
    "    \"\"\"\n",
    "    \n",
    "    ax7.text(0.05, 0.95, tradeoffs_text, fontsize=11, verticalalignment='top',\n",
    "             transform=ax7.transAxes)\n",
    "    ax7.set_title('Performance Trade-offs Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 8. Key Findings and Insights\n",
    "    ax8 = fig.add_subplot(gs[4, :])\n",
    "    ax8.axis('off')\n",
    "    \n",
    "    findings_text = \"\"\"\n",
    "üîç Key Findings and Insights:\n",
    "\n",
    "üìä Architecture Comparison:\n",
    "   ‚Ä¢ ResNet family provides excellent balance of performance, training stability, and computational efficiency\n",
    "   ‚Ä¢ DenseNet achieves high parameter efficiency but requires significant memory for dense connections\n",
    "   ‚Ä¢ EfficientNet demonstrates superior efficiency through compound scaling and mobile optimization\n",
    "   ‚Ä¢ Vision Transformers show exceptional scalability and transfer learning capabilities\n",
    "\n",
    "üéØ Transfer Learning Insights:\n",
    "   ‚Ä¢ Fine-tuning consistently outperforms feature extraction for sufficient data scenarios\n",
    "   ‚Ä¢ Progressive unfreezing offers compromise between computational cost and performance\n",
    "   ‚Ä¢ Architecture choice significantly impacts transfer learning success rates\n",
    "   ‚Ä¢ Pretrained weights provide substantial performance boost across all architectures\n",
    "\n",
    "‚ö° Performance Characteristics:\n",
    "   ‚Ä¢ EfficientNet variants achieve best inference speed for mobile deployment scenarios\n",
    "   ‚Ä¢ ResNet architectures provide most consistent performance across different hardware configurations\n",
    "   ‚Ä¢ Vision Transformers require significant computational resources but offer superior global context modeling\n",
    "   ‚Ä¢ DenseNet models show excellent feature reuse but suffer from memory bottlenecks during training\n",
    "\n",
    "üí° Practical Recommendations:\n",
    "   ‚Ä¢ Use EfficientNet for mobile applications requiring real-time performance\n",
    "   ‚Ä¢ Choose ResNet for general-purpose applications requiring proven stability\n",
    "   ‚Ä¢ Consider Vision Transformers for tasks requiring global context understanding\n",
    "   ‚Ä¢ Apply DenseNet when parameter efficiency is critical constraint\n",
    "    \"\"\"\n",
    "    \n",
    "    ax8.text(0.02, 0.95, findings_text, fontsize=11, verticalalignment='top',\n",
    "             transform=ax8.transAxes)\n",
    "    ax8.set_title('Key Findings and Practical Insights', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # 9. Future Directions\n",
    "    ax9 = fig.add_subplot(gs[5, :])\n",
    "    ax9.axis('off')\n",
    "    \n",
    "    future_text = \"\"\"\n",
    "üöÄ Future Directions and Next Steps:\n",
    "\n",
    "üî¨ Advanced Architecture Exploration:\n",
    "   ‚Ä¢ ConvNeXt: Modernized CNN design inspired by Vision Transformers\n",
    "   ‚Ä¢ Swin Transformer: Hierarchical attention for computer vision\n",
    "   ‚Ä¢ RegNet: Design space exploration for optimal architectures\n",
    "   ‚Ä¢ NFNet: Normalizer-free networks for improved training\n",
    "\n",
    "üéØ Specialized Applications:\n",
    "   ‚Ä¢ Object Detection: YOLO, R-CNN family integration\n",
    "   ‚Ä¢ Semantic Segmentation: UNet, DeepLab adaptations  \n",
    "   ‚Ä¢ Video Understanding: 3D convolutions and temporal modeling\n",
    "   ‚Ä¢ Multi-modal Learning: CLIP-style vision-language models\n",
    "\n",
    "‚ö° Optimization and Deployment:\n",
    "   ‚Ä¢ Model Compression: Pruning, quantization, knowledge distillation\n",
    "   ‚Ä¢ Neural Architecture Search: Automated design optimization\n",
    "   ‚Ä¢ Edge Deployment: TensorRT, ONNX optimization pipelines\n",
    "   ‚Ä¢ Distributed Training: Multi-GPU and multi-node scaling\n",
    "\n",
    "üîß Advanced Training Techniques:\n",
    "   ‚Ä¢ Self-supervised Learning: MAE, SimCLR, DINO approaches\n",
    "   ‚Ä¢ Meta-learning: Few-shot adaptation strategies\n",
    "   ‚Ä¢ Continual Learning: Avoiding catastrophic forgetting\n",
    "   ‚Ä¢ Adversarial Robustness: Defending against attacks\n",
    "\n",
    "üíæ Implementation Improvements:\n",
    "   ‚Ä¢ Mixed Precision Training: FP16/BF16 acceleration\n",
    "   ‚Ä¢ Gradient Checkpointing: Memory-efficient training\n",
    "   ‚Ä¢ Dynamic Batching: Adaptive computational graphs\n",
    "   ‚Ä¢ Custom CUDA Kernels: Hardware-specific optimizations\n",
    "    \"\"\"\n",
    "    \n",
    "    ax9.text(0.02, 0.95, future_text, fontsize=11, verticalalignment='top',\n",
    "             transform=ax9.transAxes)\n",
    "    ax9.set_title('Future Directions and Advanced Topics', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üíæ Comprehensive final dashboard saved to: {save_path}\")\n",
    "\n",
    "# Generate comprehensive results summary\n",
    "print(\"\\nüìã Generating Comprehensive Results Summary...\")\n",
    "final_summary = create_comprehensive_results_summary()\n",
    "\n",
    "# Save comprehensive summary to JSON\n",
    "summary_save_path = f\"{project_dirs['analysis']}/comprehensive_project_summary.json\"\n",
    "with open(summary_save_path, 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"üíæ Project summary saved to: {summary_save_path}\")\n",
    "\n",
    "# Create final comprehensive dashboard\n",
    "print(\"\\nüé® Creating Final Comprehensive Dashboard...\")\n",
    "create_final_dashboard(\n",
    "    models_info,\n",
    "    performance_results if 'performance_results' in locals() else {},\n",
    "    f\"{project_dirs['analysis']}/final_comprehensive_dashboard.png\"\n",
    ")\n",
    "\n",
    "# Save all model states\n",
    "print(\"\\nüíæ Saving Model Checkpoints...\")\n",
    "models_to_save = {**resnet_models, **densenet_models, **efficientnet_models, **vit_models}\n",
    "\n",
    "for model_name, model in models_to_save.items():\n",
    "    checkpoint_path = f\"{project_dirs['final']}/{model_name.lower().replace('-', '_')}_final.pth\"\n",
    "    \n",
    "    checkpoint_data = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_info': models_info[model_name],\n",
    "        'architecture_type': model_name.split('-')[0],\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'pytorch_version': torch.__version__,\n",
    "        'device_trained': str(device)\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint_data, checkpoint_path)\n",
    "    print(f\"   ‚úÖ {model_name} saved to {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ MODERN CNN ARCHITECTURES PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüèÜ Project Achievements:\")\n",
    "print(f\"   ‚úÖ {len(models_info)} architecture variants implemented from scratch\")\n",
    "print(f\"   ‚úÖ 4 major architecture families covered (ResNet, DenseNet, EfficientNet, ViT)\")\n",
    "print(f\"   ‚úÖ Comprehensive performance benchmarking completed\")\n",
    "print(f\"   ‚úÖ Advanced transfer learning experiments executed\")\n",
    "print(f\"   ‚úÖ Vision Transformer attention mechanisms analyzed\")\n",
    "print(f\"   ‚úÖ Professional visualizations and analysis reports generated\")\n",
    "\n",
    "print(f\"\\nüìä Technical Highlights:\")\n",
    "print(f\"   üîó ResNet: Skip connections and deep network training\")\n",
    "print(f\"   üåü DenseNet: Dense connectivity and feature reuse\")\n",
    "print(f\"   ‚ö° EfficientNet: Compound scaling and mobile optimization\") \n",
    "print(f\"   üëÅÔ∏è ViT: Self-attention and global context modeling\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Deliverables:\")\n",
    "print(f\"   üé® Comprehensive architecture analysis and comparisons\")\n",
    "print(f\"   ‚ö° Performance benchmarking and efficiency analysis\")\n",
    "print(f\"   üéØ Transfer learning experimental results\")\n",
    "print(f\"   üëÅÔ∏è Vision Transformer attention visualizations\")\n",
    "print(f\"   üíæ Complete model implementations and checkpoints\")\n",
    "print(f\"   üìã Detailed analysis reports and recommendations\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready for Advanced Topics:\")\n",
    "print(f\"   ‚Ä¢ Object Detection and Segmentation\")\n",
    "print(f\"   ‚Ä¢ Neural Architecture Search\")\n",
    "print(f\"   ‚Ä¢ Model Compression and Optimization\")\n",
    "print(f\"   ‚Ä¢ Multi-modal Learning\")\n",
    "print(f\"   ‚Ä¢ Production Deployment Strategies\")\n",
    "\n",
    "print(f\"\\nüìö All materials saved to: ../results/modern_cnn_architectures/\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
