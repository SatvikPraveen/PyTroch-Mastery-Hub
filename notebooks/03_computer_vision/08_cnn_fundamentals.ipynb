{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4136ebe5",
   "metadata": {},
   "source": [
    "# CNN Fundamentals: Complete Computer Vision Implementation\n",
    "\n",
    "**Building Blocks of Deep Computer Vision with PyTorch**\n",
    "\n",
    "**Learning Objectives:**\n",
    "- üîç Master convolution operations from mathematical foundations to implementation\n",
    "- üèä Understand pooling strategies and their effects on feature extraction\n",
    "- üìä Visualize feature maps and learned filters throughout training\n",
    "- üéØ Calculate and analyze receptive fields in deep networks\n",
    "- üèóÔ∏è Build complete CNN architectures with modern best practices\n",
    "- üìà Monitor training dynamics and feature evolution\n",
    "\n",
    "**What You'll Build:**\n",
    "- Manual convolution implementation for deep understanding\n",
    "- Comprehensive feature map visualization system\n",
    "- Complete CNN architecture with training pipeline\n",
    "- Interactive filter analysis and evolution tracking\n",
    "- Professional-grade training monitoring dashboard\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b777c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch and Computer Vision Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Scientific Computing and Visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration and Styling\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Professional Directory Structure\n",
    "def setup_professional_directories():\n",
    "    \"\"\"Create comprehensive directory structure for CNN fundamentals project\"\"\"\n",
    "    base_dirs = [\n",
    "        \"../results/cnn_fundamentals/visualizations/feature_maps\",\n",
    "        \"../results/cnn_fundamentals/visualizations/filters\", \n",
    "        \"../results/cnn_fundamentals/visualizations/receptive_fields\",\n",
    "        \"../results/cnn_fundamentals/training/progress\",\n",
    "        \"../results/cnn_fundamentals/training/metrics\",\n",
    "        \"../results/cnn_fundamentals/architectures/diagrams\",\n",
    "        \"../results/cnn_fundamentals/analysis/statistics\",\n",
    "        \"../models/cnn_fundamentals/checkpoints\",\n",
    "        \"../models/cnn_fundamentals/final\",\n",
    "        \"../data/cnn_fundamentals/samples\"\n",
    "    ]\n",
    "    \n",
    "    created_dirs = {}\n",
    "    for dir_path in base_dirs:\n",
    "        Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "        dir_name = Path(dir_path).name\n",
    "        created_dirs[dir_name] = dir_path\n",
    "        print(f\"üìÅ Created: {dir_path}\")\n",
    "    \n",
    "    return created_dirs\n",
    "\n",
    "# Initialize directory structure\n",
    "project_dirs = setup_professional_directories()\n",
    "print(f\"\\n‚úÖ Professional directory structure initialized!\")\n",
    "print(f\"üìä Results will be saved to: ../results/cnn_fundamentals/\")\n",
    "print(f\"üíæ Models will be saved to: ../models/cnn_fundamentals/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd5cfe",
   "metadata": {},
   "source": [
    "## 2. Mathematical Foundations: Manual Convolution Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionFoundations:\n",
    "    \"\"\"\n",
    "    Complete implementation of convolution operations for educational purposes.\n",
    "    This class demonstrates the mathematical foundations underlying CNN operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size=3, stride=1, padding=0):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.operation_stats = {\n",
    "            'total_operations': 0,\n",
    "            'multiply_adds': 0,\n",
    "            'processing_time': []\n",
    "        }\n",
    "    \n",
    "    def manual_convolution_2d(self, input_tensor, kernel):\n",
    "        \"\"\"\n",
    "        Perform 2D convolution manually with detailed operation tracking\n",
    "        \n",
    "        Args:\n",
    "            input_tensor: Input tensor [batch, channels, height, width]\n",
    "            kernel: Convolution kernel [out_channels, in_channels, kernel_h, kernel_w]\n",
    "        \n",
    "        Returns:\n",
    "            output: Convolved output tensor\n",
    "        \"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Apply padding if specified\n",
    "        if self.padding > 0:\n",
    "            input_tensor = F.pad(input_tensor, (self.padding,) * 4, mode='constant', value=0)\n",
    "        \n",
    "        batch_size, in_channels, height, width = input_tensor.shape\n",
    "        out_channels, _, kh, kw = kernel.shape\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        out_height = (height - kh) // self.stride + 1\n",
    "        out_width = (width - kw) // self.stride + 1\n",
    "        \n",
    "        # Initialize output tensor\n",
    "        output = torch.zeros(batch_size, out_channels, out_height, out_width, device=input_tensor.device)\n",
    "        \n",
    "        # Track operations for educational purposes\n",
    "        total_ops = 0\n",
    "        \n",
    "        # Perform manual convolution with operation counting\n",
    "        for b in range(batch_size):\n",
    "            for oc in range(out_channels):\n",
    "                for oh in range(out_height):\n",
    "                    for ow in range(out_width):\n",
    "                        # Calculate input patch boundaries\n",
    "                        h_start = oh * self.stride\n",
    "                        h_end = h_start + kh\n",
    "                        w_start = ow * self.stride\n",
    "                        w_end = w_start + kw\n",
    "                        \n",
    "                        # Extract input patch\n",
    "                        input_patch = input_tensor[b, :, h_start:h_end, w_start:w_end]\n",
    "                        \n",
    "                        # Perform convolution: element-wise multiply and sum\n",
    "                        conv_result = torch.sum(input_patch * kernel[oc])\n",
    "                        output[b, oc, oh, ow] = conv_result\n",
    "                        \n",
    "                        # Count operations\n",
    "                        total_ops += kh * kw * in_channels\n",
    "        \n",
    "        # Record statistics\n",
    "        processing_time = time.time() - start_time\n",
    "        self.operation_stats['total_operations'] += total_ops\n",
    "        self.operation_stats['multiply_adds'] += total_ops\n",
    "        self.operation_stats['processing_time'].append(processing_time)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def compare_with_pytorch(self, input_tensor, kernel):\n",
    "        \"\"\"Compare manual implementation with PyTorch's optimized version\"\"\"\n",
    "        print(\"üîç Comparing Manual vs PyTorch Convolution:\")\n",
    "        \n",
    "        # Manual convolution\n",
    "        manual_start = time.time()\n",
    "        manual_result = self.manual_convolution_2d(input_tensor, kernel)\n",
    "        manual_time = time.time() - manual_start\n",
    "        \n",
    "        # PyTorch convolution\n",
    "        pytorch_start = time.time()\n",
    "        pytorch_result = F.conv2d(input_tensor, kernel, padding=self.padding, stride=self.stride)\n",
    "        pytorch_time = time.time() - pytorch_start\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        max_error = torch.max(torch.abs(manual_result - pytorch_result)).item()\n",
    "        \n",
    "        print(f\"   Manual implementation time: {manual_time:.4f}s\")\n",
    "        print(f\"   PyTorch implementation time: {pytorch_time:.4f}s\")\n",
    "        print(f\"   Speedup factor: {manual_time/pytorch_time:.1f}x\")\n",
    "        print(f\"   Maximum error: {max_error:.2e}\")\n",
    "        print(f\"   Results match: {torch.allclose(manual_result, pytorch_result, atol=1e-6)}\")\n",
    "        \n",
    "        return {\n",
    "            'manual_time': manual_time,\n",
    "            'pytorch_time': pytorch_time,\n",
    "            'speedup': manual_time/pytorch_time,\n",
    "            'max_error': max_error,\n",
    "            'results_match': torch.allclose(manual_result, pytorch_result, atol=1e-6)\n",
    "        }\n",
    "    \n",
    "    def visualize_convolution_mechanics(self, input_image, kernels_dict, save_path):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualization of convolution mechanics\n",
    "        \n",
    "        Args:\n",
    "            input_image: Input image tensor\n",
    "            kernels_dict: Dictionary of named kernels to apply\n",
    "            save_path: Where to save the visualization\n",
    "        \"\"\"\n",
    "        num_kernels = len(kernels_dict)\n",
    "        fig, axes = plt.subplots(2, num_kernels + 1, figsize=(4*(num_kernels + 1), 8))\n",
    "        \n",
    "        # Display original image\n",
    "        if isinstance(input_image, torch.Tensor):\n",
    "            img_display = input_image.squeeze().cpu().numpy()\n",
    "            if len(img_display.shape) == 3:\n",
    "                img_display = img_display.transpose(1, 2, 0)\n",
    "        else:\n",
    "            img_display = input_image\n",
    "        \n",
    "        axes[0, 0].imshow(img_display, cmap='gray' if len(img_display.shape) == 2 else None)\n",
    "        axes[0, 0].set_title('Original Image\\n32√ó32 pixels', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Process each kernel\n",
    "        for idx, (kernel_name, kernel) in enumerate(kernels_dict.items()):\n",
    "            col = idx + 1\n",
    "            \n",
    "            # Display kernel\n",
    "            kernel_display = kernel.squeeze().cpu().numpy()\n",
    "            im1 = axes[0, col].imshow(kernel_display, cmap='RdBu', vmin=-2, vmax=2)\n",
    "            axes[0, col].set_title(f'{kernel_name} Kernel\\n{kernel.shape[-2]}√ó{kernel.shape[-1]}', \n",
    "                                 fontsize=12, fontweight='bold')\n",
    "            axes[0, col].axis('off')\n",
    "            \n",
    "            # Add colorbar for kernel\n",
    "            cbar1 = plt.colorbar(im1, ax=axes[0, col], fraction=0.046, pad=0.04)\n",
    "            cbar1.set_label('Weight Value', fontsize=10)\n",
    "            \n",
    "            # Apply convolution\n",
    "            if len(input_image.shape) == 2:\n",
    "                input_for_conv = input_image.unsqueeze(0).unsqueeze(0)\n",
    "            else:\n",
    "                input_for_conv = input_image.unsqueeze(0)\n",
    "            \n",
    "            kernel_for_conv = kernel.unsqueeze(0).unsqueeze(0) if len(kernel.shape) == 2 else kernel.unsqueeze(0)\n",
    "            \n",
    "            conv_result = self.manual_convolution_2d(input_for_conv, kernel_for_conv)\n",
    "            \n",
    "            # Display convolution result\n",
    "            result_display = conv_result.squeeze().cpu().numpy()\n",
    "            im2 = axes[1, col].imshow(result_display, cmap='viridis')\n",
    "            axes[1, col].set_title(f'{kernel_name} Output\\n{result_display.shape[0]}√ó{result_display.shape[1]}', \n",
    "                                 fontsize=12, fontweight='bold')\n",
    "            axes[1, col].axis('off')\n",
    "            \n",
    "            # Add colorbar for result\n",
    "            cbar2 = plt.colorbar(im2, ax=axes[1, col], fraction=0.046, pad=0.04)\n",
    "            cbar2.set_label('Activation', fontsize=10)\n",
    "        \n",
    "        # Add mathematical explanation\n",
    "        if num_kernels < 4:  # Only if we have space\n",
    "            explanation_text = (\n",
    "                \"Convolution Operation:\\n\"\n",
    "                \"Output[i,j] = Œ£(Input[i+m,j+n] √ó Kernel[m,n])\\n\\n\"\n",
    "                \"Edge Detection: Highlights boundaries\\n\"\n",
    "                \"Blur: Smooths image details\\n\"\n",
    "                \"Sharpen: Enhances edges and details\"\n",
    "            )\n",
    "            axes[1, 0].text(0.1, 0.5, explanation_text, transform=axes[1, 0].transAxes,\n",
    "                          fontsize=10, verticalalignment='center',\n",
    "                          bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8))\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        plt.suptitle('Convolution Operation Mechanics and Effects', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üíæ Convolution mechanics visualization saved to: {save_path}\")\n",
    "\n",
    "# Initialize convolution foundations\n",
    "conv_foundations = ConvolutionFoundations(kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Create test data\n",
    "print(\"üß™ Testing Manual Convolution Implementation:\")\n",
    "test_image = torch.randn(1, 1, 32, 32)\n",
    "test_kernel = torch.randn(1, 1, 3, 3)\n",
    "\n",
    "# Performance comparison\n",
    "comparison_results = conv_foundations.compare_with_pytorch(test_image, test_kernel)\n",
    "\n",
    "# Create classic computer vision kernels\n",
    "classic_kernels = {\n",
    "    'Edge Detection': torch.tensor([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype=torch.float32),\n",
    "    'Gaussian Blur': torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=torch.float32) / 16,\n",
    "    'Sharpen': torch.tensor([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=torch.float32),\n",
    "    'Sobel X': torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# Visualize convolution mechanics\n",
    "sample_image = torch.randn(32, 32) * 0.5 + torch.sin(torch.linspace(0, 4*np.pi, 32)).unsqueeze(1) * torch.cos(torch.linspace(0, 4*np.pi, 32)).unsqueeze(0)\n",
    "\n",
    "conv_foundations.visualize_convolution_mechanics(\n",
    "    sample_image, \n",
    "    classic_kernels,\n",
    "    f\"{project_dirs['feature_maps']}/convolution_mechanics_detailed.png\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Convolution Operation Statistics:\")\n",
    "print(f\"   Total operations performed: {conv_foundations.operation_stats['total_operations']:,}\")\n",
    "print(f\"   Average processing time: {np.mean(conv_foundations.operation_stats['processing_time']):.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a5da5",
   "metadata": {},
   "source": [
    "## 3. Pooling Operations: Comprehensive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502038a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of pooling operations and their effects on feature extraction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pooling_operations = {\n",
    "            'MaxPool2x2': nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            'AvgPool2x2': nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            'MaxPool3x3': nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            'AdaptiveMaxPool': nn.AdaptiveMaxPool2d((8, 8)),\n",
    "            'AdaptiveAvgPool': nn.AdaptiveAvgPool2d((8, 8)),\n",
    "            'GlobalAvgPool': nn.AdaptiveAvgPool2d((1, 1))\n",
    "        }\n",
    "        \n",
    "        self.analysis_results = {}\n",
    "    \n",
    "    def analyze_pooling_effects(self, input_tensor, save_path):\n",
    "        \"\"\"\n",
    "        Comprehensive analysis of different pooling operations\n",
    "        \n",
    "        Args:\n",
    "            input_tensor: Input feature map to analyze\n",
    "            save_path: Path to save analysis results\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(18, 18))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Original feature map\n",
    "        original_display = input_tensor.squeeze().cpu().numpy()\n",
    "        im0 = axes[0].imshow(original_display, cmap='viridis')\n",
    "        axes[0].set_title(f'Original Feature Map\\nShape: {original_display.shape}\\nMean: {original_display.mean():.3f}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[0].axis('off')\n",
    "        plt.colorbar(im0, ax=axes[0], fraction=0.046)\n",
    "        \n",
    "        # Apply and analyze each pooling operation\n",
    "        pooling_stats = {}\n",
    "        \n",
    "        for idx, (pool_name, pool_op) in enumerate(self.pooling_operations.items()):\n",
    "            if idx + 1 >= len(axes):\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                # Apply pooling\n",
    "                if input_tensor.dim() == 2:\n",
    "                    pool_input = input_tensor.unsqueeze(0).unsqueeze(0)\n",
    "                elif input_tensor.dim() == 3:\n",
    "                    pool_input = input_tensor.unsqueeze(0)\n",
    "                else:\n",
    "                    pool_input = input_tensor\n",
    "                \n",
    "                pooled_result = pool_op(pool_input).squeeze()\n",
    "                pooled_display = pooled_result.cpu().numpy()\n",
    "                \n",
    "                # Calculate statistics\n",
    "                original_var = torch.var(input_tensor).item()\n",
    "                pooled_var = torch.var(pooled_result).item()\n",
    "                information_retention = pooled_var / original_var if original_var > 0 else 0\n",
    "                \n",
    "                compression_ratio = input_tensor.numel() / pooled_result.numel()\n",
    "                \n",
    "                pooling_stats[pool_name] = {\n",
    "                    'output_shape': pooled_result.shape,\n",
    "                    'compression_ratio': compression_ratio,\n",
    "                    'information_retention': information_retention,\n",
    "                    'mean_activation': torch.mean(pooled_result).item(),\n",
    "                    'std_activation': torch.std(pooled_result).item()\n",
    "                }\n",
    "                \n",
    "                # Visualize result\n",
    "                im = axes[idx + 1].imshow(pooled_display, cmap='viridis')\n",
    "                title = (f'{pool_name}\\nShape: {pooled_result.shape}\\n'\n",
    "                        f'Compression: {compression_ratio:.1f}√ó\\n'\n",
    "                        f'Info Retention: {information_retention:.3f}')\n",
    "                axes[idx + 1].set_title(title, fontsize=10, fontweight='bold')\n",
    "                axes[idx + 1].axis('off')\n",
    "                plt.colorbar(im, ax=axes[idx + 1], fraction=0.046)\n",
    "                \n",
    "            except Exception as e:\n",
    "                axes[idx + 1].text(0.5, 0.5, f'Error: {str(e)[:30]}...', \n",
    "                                 ha='center', va='center', transform=axes[idx + 1].transAxes)\n",
    "                axes[idx + 1].axis('off')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(len(self.pooling_operations) + 1, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.suptitle('Comprehensive Pooling Operations Analysis', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Create statistics summary plot\n",
    "        self._create_pooling_statistics_plot(pooling_stats, save_path.replace('.png', '_stats.png'))\n",
    "        \n",
    "        self.analysis_results['pooling_effects'] = pooling_stats\n",
    "        print(f\"üíæ Pooling analysis saved to: {save_path}\")\n",
    "        \n",
    "        return pooling_stats\n",
    "    \n",
    "    def _create_pooling_statistics_plot(self, pooling_stats, save_path):\n",
    "        \"\"\"Create detailed statistics visualization for pooling operations\"\"\"\n",
    "        if not pooling_stats:\n",
    "            return\n",
    "            \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        pool_names = list(pooling_stats.keys())\n",
    "        \n",
    "        # Compression ratios\n",
    "        compression_ratios = [stats['compression_ratio'] for stats in pooling_stats.values()]\n",
    "        bars1 = axes[0, 0].bar(pool_names, compression_ratios, alpha=0.8, color='skyblue')\n",
    "        axes[0, 0].set_title('Compression Ratios', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].set_ylabel('Compression Factor')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars1, compression_ratios):\n",
    "            axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(compression_ratios)*0.01,\n",
    "                          f'{value:.1f}√ó', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Information retention\n",
    "        info_retention = [stats['information_retention'] for stats in pooling_stats.values()]\n",
    "        bars2 = axes[0, 1].bar(pool_names, info_retention, alpha=0.8, color='lightcoral')\n",
    "        axes[0, 1].set_title('Information Retention (Variance Ratio)', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].set_ylabel('Retention Ratio')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 1].set_ylim(0, 1.1)\n",
    "        \n",
    "        # Mean activations\n",
    "        mean_activations = [stats['mean_activation'] for stats in pooling_stats.values()]\n",
    "        bars3 = axes[1, 0].bar(pool_names, mean_activations, alpha=0.8, color='lightgreen')\n",
    "        axes[1, 0].set_title('Mean Activation Values', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_ylabel('Mean Activation')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Standard deviations\n",
    "        std_activations = [stats['std_activation'] for stats in pooling_stats.values()]\n",
    "        bars4 = axes[1, 1].bar(pool_names, std_activations, alpha=0.8, color='gold')\n",
    "        axes[1, 1].set_title('Activation Variability (Std Dev)', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Standard Deviation')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üíæ Pooling statistics saved to: {save_path}\")\n",
    "    \n",
    "    def calculate_receptive_field_progression(self, network_architecture, input_size=224):\n",
    "        \"\"\"\n",
    "        Calculate receptive field growth through network layers\n",
    "        \n",
    "        Args:\n",
    "            network_architecture: List of layer specifications\n",
    "            input_size: Input image size\n",
    "        \"\"\"\n",
    "        def calculate_rf_and_stride(layers):\n",
    "            \"\"\"Calculate receptive field and effective stride\"\"\"\n",
    "            rf = 1\n",
    "            effective_stride = 1\n",
    "            rf_progression = [{'layer': 'Input', 'rf_size': rf, 'output_size': input_size}]\n",
    "            current_size = input_size\n",
    "            \n",
    "            for i, layer in enumerate(layers):\n",
    "                layer_name = f\"{layer['type']}_{i+1}\"\n",
    "                \n",
    "                if layer['type'] == 'conv':\n",
    "                    # Convolution layer\n",
    "                    kernel_size = layer['kernel']\n",
    "                    stride = layer.get('stride', 1)\n",
    "                    padding = layer.get('padding', 0)\n",
    "                    \n",
    "                    rf = rf + (kernel_size - 1) * effective_stride\n",
    "                    current_size = (current_size + 2 * padding - kernel_size) // stride + 1\n",
    "                    \n",
    "                elif layer['type'] == 'pool':\n",
    "                    # Pooling layer\n",
    "                    kernel_size = layer['kernel']\n",
    "                    stride = layer.get('stride', kernel_size)\n",
    "                    \n",
    "                    rf = rf + (kernel_size - 1) * effective_stride\n",
    "                    effective_stride = effective_stride * stride\n",
    "                    current_size = current_size // stride\n",
    "                \n",
    "                rf_progression.append({\n",
    "                    'layer': layer_name,\n",
    "                    'rf_size': rf,\n",
    "                    'output_size': current_size,\n",
    "                    'effective_stride': effective_stride\n",
    "                })\n",
    "            \n",
    "            return rf_progression\n",
    "        \n",
    "        # Calculate progression\n",
    "        rf_progression = calculate_rf_and_stride(network_architecture)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 12))\n",
    "        \n",
    "        layers = [entry['layer'] for entry in rf_progression]\n",
    "        rf_sizes = [entry['rf_size'] for entry in rf_progression]\n",
    "        output_sizes = [entry['output_size'] for entry in rf_progression]\n",
    "        \n",
    "        # Receptive field growth\n",
    "        ax1.plot(layers, rf_sizes, 'o-', linewidth=3, markersize=8, color='darkblue', label='Receptive Field Size')\n",
    "        ax1.set_title('Receptive Field Growth Through Network', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylabel('Receptive Field Size (pixels)', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value annotations\n",
    "        for i, (layer, rf_size) in enumerate(zip(layers, rf_sizes)):\n",
    "            if i % 2 == 0:  # Avoid overcrowding\n",
    "                ax1.annotate(f'{rf_size}', (i, rf_size), textcoords=\"offset points\", \n",
    "                           xytext=(0,10), ha='center', fontweight='bold')\n",
    "        \n",
    "        # Output size reduction\n",
    "        ax2.plot(layers, output_sizes, 's-', linewidth=3, markersize=8, color='darkred', label='Output Size')\n",
    "        ax2.set_title('Spatial Resolution Reduction', fontsize=14, fontweight='bold')\n",
    "        ax2.set_ylabel('Output Size (pixels)', fontsize=12)\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Combined view: RF vs Output Size\n",
    "        ax3_twin = ax3.twinx()\n",
    "        \n",
    "        line1 = ax3.plot(layers, rf_sizes, 'o-', linewidth=2, color='darkblue', label='Receptive Field')\n",
    "        line2 = ax3_twin.plot(layers, output_sizes, 's-', linewidth=2, color='darkred', label='Output Size')\n",
    "        \n",
    "        ax3.set_xlabel('Network Layers', fontsize=12)\n",
    "        ax3.set_ylabel('Receptive Field Size', fontsize=12, color='darkblue')\n",
    "        ax3_twin.set_ylabel('Output Size', fontsize=12, color='darkred')\n",
    "        ax3_twin.set_yscale('log')\n",
    "        \n",
    "        # Combined legend\n",
    "        lines = line1 + line2\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax3.legend(lines, labels, loc='center right')\n",
    "        \n",
    "        ax3.set_title('Receptive Field vs Spatial Resolution Trade-off', fontsize=14, fontweight='bold')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = f\"{project_dirs['receptive_fields']}/receptive_field_analysis.png\"\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        self.analysis_results['receptive_field_progression'] = rf_progression\n",
    "        print(f\"üíæ Receptive field analysis saved to: {save_path}\")\n",
    "        \n",
    "        return rf_progression\n",
    "\n",
    "# Initialize pooling analyzer\n",
    "pooling_analyzer = PoolingAnalyzer()\n",
    "\n",
    "# Create complex test feature map\n",
    "print(\"üèä Analyzing Pooling Operations:\")\n",
    "feature_map = (torch.randn(64, 64) * 2 + \n",
    "               torch.sin(torch.linspace(0, 6*np.pi, 64)).unsqueeze(1) * \n",
    "               torch.cos(torch.linspace(0, 6*np.pi, 64)).unsqueeze(0) * 3)\n",
    "\n",
    "# Perform comprehensive pooling analysis\n",
    "pooling_results = pooling_analyzer.analyze_pooling_effects(\n",
    "    feature_map,\n",
    "    f\"{project_dirs['feature_maps']}/comprehensive_pooling_analysis.png\"\n",
    ")\n",
    "\n",
    "# Define sample network architecture for receptive field analysis\n",
    "sample_architecture = [\n",
    "    {'type': 'conv', 'kernel': 3, 'stride': 1, 'padding': 1},\n",
    "    {'type': 'conv', 'kernel': 3, 'stride': 1, 'padding': 1},\n",
    "    {'type': 'pool', 'kernel': 2, 'stride': 2},\n",
    "    {'type': 'conv', 'kernel': 3, 'stride': 1, 'padding': 1},\n",
    "    {'type': 'conv', 'kernel': 3, 'stride': 1, 'padding': 1},\n",
    "    {'type': 'pool', 'kernel': 2, 'stride': 2},\n",
    "    {'type': 'conv', 'kernel': 3, 'stride': 1, 'padding': 1},\n",
    "    {'type': 'conv', 'kernel': 3, 'stride': 1, 'padding': 1},\n",
    "    {'type': 'pool', 'kernel': 2, 'stride': 2},\n",
    "]\n",
    "\n",
    "print(\"\\nüéØ Analyzing Receptive Field Progression:\")\n",
    "rf_progression = pooling_analyzer.calculate_receptive_field_progression(\n",
    "    sample_architecture, input_size=224\n",
    ")\n",
    "\n",
    "# Print receptive field summary\n",
    "print(f\"\\nüìä Receptive Field Summary:\")\n",
    "for entry in rf_progression[::2]:  # Print every other layer to avoid clutter\n",
    "    print(f\"   {entry['layer']}: RF={entry['rf_size']}px, Output={entry['output_size']}px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee30d2",
   "metadata": {},
   "source": [
    "## 4. Complete CNN Architecture Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5fa5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteCNNArchitecture(nn.Module):\n",
    "    \"\"\"\n",
    "    Professional CNN implementation with comprehensive feature extraction and analysis capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, input_channels=3, dropout_rate=0.3):\n",
    "        super(CompleteCNNArchitecture, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.input_channels = input_channels\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Feature extraction backbone\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1: Initial feature extraction\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(dropout_rate * 0.5),\n",
    "            \n",
    "            # Block 2: Mid-level features\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(dropout_rate * 0.7),\n",
    "            \n",
    "            # Block 3: High-level features\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            \n",
    "            # Block 4: Deep features\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights with modern best practices\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        # Feature extraction points for visualization\n",
    "        self.feature_extraction_points = [\n",
    "            'block1_conv1', 'block1_conv2', 'block1_pool',\n",
    "            'block2_conv1', 'block2_conv2', 'block2_pool',\n",
    "            'block3_conv1', 'block3_conv2', 'block3_pool',\n",
    "            'block4_conv1', 'block4_conv2'\n",
    "        ]\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights using modern best practices\"\"\"\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                # He initialization for ReLU networks\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, 0, 0.01)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        x = self.features(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def extract_feature_maps(self, x, layer_names=None):\n",
    "        \"\"\"\n",
    "        Extract intermediate feature maps for visualization\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            layer_names: Specific layers to extract (if None, extracts from key points)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of feature maps\n",
    "        \"\"\"\n",
    "        if layer_names is None:\n",
    "            layer_names = self.feature_extraction_points\n",
    "        \n",
    "        feature_maps = {}\n",
    "        layer_idx = 0\n",
    "        \n",
    "        # Track through feature extraction layers\n",
    "        for i, layer in enumerate(self.features):\n",
    "            x = layer(x)\n",
    "            \n",
    "            # Save feature maps at specified points\n",
    "            if isinstance(layer, (nn.Conv2d, nn.MaxPool2d)) and layer_idx < len(layer_names):\n",
    "                feature_maps[layer_names[layer_idx]] = x.clone().detach()\n",
    "                layer_idx += 1\n",
    "        \n",
    "        return feature_maps\n",
    "    \n",
    "    def get_architecture_summary(self):\n",
    "        \"\"\"Get detailed architecture summary\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        # Calculate model size in MB\n",
    "        param_size = total_params * 4  # 4 bytes per float32 parameter\n",
    "        buffer_size = sum(b.numel() * 4 for b in self.buffers())\n",
    "        model_size_mb = (param_size + buffer_size) / (1024 ** 2)\n",
    "        \n",
    "        return {\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'model_size_mb': model_size_mb,\n",
    "            'input_channels': self.input_channels,\n",
    "            'num_classes': self.num_classes,\n",
    "            'dropout_rate': self.dropout_rate\n",
    "        }\n",
    "\n",
    "class CNNVisualizationSuite:\n",
    "    \"\"\"\n",
    "    Comprehensive visualization suite for CNN analysis and monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, class_names=None):\n",
    "        self.model = model\n",
    "        self.class_names = class_names or [f'Class_{i}' for i in range(model.num_classes)]\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Visualization tracking\n",
    "        self.visualization_history = {\n",
    "            'filter_evolution': [],\n",
    "            'feature_statistics': [],\n",
    "            'training_visualizations': []\n",
    "        }\n",
    "    \n",
    "    def visualize_learned_filters(self, layer_name='features.0', save_path=None, max_filters=32):\n",
    "        \"\"\"\n",
    "        Visualize learned convolutional filters with professional styling\n",
    "        \n",
    "        Args:\n",
    "            layer_name: Name of layer to visualize\n",
    "            save_path: Path to save visualization\n",
    "            max_filters: Maximum number of filters to display\n",
    "        \"\"\"\n",
    "        if save_path is None:\n",
    "            save_path = f\"{project_dirs['filters']}/learned_filters_{layer_name.replace('.', '_')}.png\"\n",
    "        \n",
    "        # Get the specified layer\n",
    "        layer = dict(self.model.named_modules())[layer_name]\n",
    "        if not isinstance(layer, nn.Conv2d):\n",
    "            print(f\"‚ö†Ô∏è Layer {layer_name} is not a Conv2d layer\")\n",
    "            return None\n",
    "        \n",
    "        filters = layer.weight.data.cpu()\n",
    "        num_filters = min(filters.shape[0], max_filters)\n",
    "        num_channels = filters.shape[1]\n",
    "        \n",
    "        # Determine grid layout\n",
    "        cols = min(8, num_filters)\n",
    "        rows = (num_filters + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(2*cols, 2*rows))\n",
    "        if rows == 1 and cols == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        elif cols == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        for i in range(num_filters):\n",
    "            row, col = i // cols, i % cols\n",
    "            \n",
    "            if num_channels == 1:\n",
    "                # Single channel filter\n",
    "                filter_img = filters[i, 0].numpy()\n",
    "                im = axes[row, col].imshow(filter_img, cmap='RdBu', \n",
    "                                         vmin=-filter_img.std()*2, vmax=filter_img.std()*2)\n",
    "            elif num_channels == 3:\n",
    "                # RGB filter\n",
    "                filter_img = filters[i].permute(1, 2, 0).numpy()\n",
    "                # Normalize for display\n",
    "                filter_img = (filter_img - filter_img.min()) / (filter_img.max() - filter_img.min())\n",
    "                axes[row, col].imshow(filter_img)\n",
    "            else:\n",
    "                # Multi-channel filter - show first channel\n",
    "                filter_img = filters[i, 0].numpy()\n",
    "                im = axes[row, col].imshow(filter_img, cmap='RdBu',\n",
    "                                         vmin=-filter_img.std()*2, vmax=filter_img.std()*2)\n",
    "            \n",
    "            axes[row, col].set_title(f'F{i+1}', fontsize=10, fontweight='bold')\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(num_filters, rows * cols):\n",
    "            row, col = i // cols, i % cols\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Learned Filters: {layer_name} ({num_filters} of {filters.shape[0]})', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üíæ Filter visualization saved to: {save_path}\")\n",
    "        \n",
    "        # Store in history\n",
    "        self.visualization_history['filter_evolution'].append({\n",
    "            'layer_name': layer_name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'num_filters': num_filters,\n",
    "            'save_path': save_path\n",
    "        })\n",
    "        \n",
    "        return save_path\n",
    "    \n",
    "    def visualize_feature_map_progression(self, input_image, save_path=None, max_channels=12):\n",
    "        \"\"\"\n",
    "        Visualize feature maps through network layers\n",
    "        \n",
    "        Args:\n",
    "            input_image: Input image tensor\n",
    "            save_path: Path to save visualization\n",
    "            max_channels: Maximum number of channels to display per layer\n",
    "        \"\"\"\n",
    "        if save_path is None:\n",
    "            save_path = f\"{project_dirs['feature_maps']}/feature_progression.png\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if input_image.dim() == 3:\n",
    "                input_image = input_image.unsqueeze(0)\n",
    "            \n",
    "            feature_maps = self.model.extract_feature_maps(input_image.to(device))\n",
    "        \n",
    "        # Select key layers for visualization\n",
    "        key_layers = list(feature_maps.keys())[::2]  # Every other layer\n",
    "        \n",
    "        fig, axes = plt.subplots(len(key_layers), max_channels, \n",
    "                               figsize=(max_channels * 1.5, len(key_layers) * 1.5))\n",
    "        \n",
    "        if len(key_layers) == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for layer_idx, layer_name in enumerate(key_layers):\n",
    "            feature_map = feature_maps[layer_name].squeeze(0).cpu()  # Remove batch dimension\n",
    "            num_channels = min(feature_map.shape[0], max_channels)\n",
    "            \n",
    "            for ch in range(max_channels):\n",
    "                if ch < num_channels:\n",
    "                    channel_data = feature_map[ch].numpy()\n",
    "                    im = axes[layer_idx, ch].imshow(channel_data, cmap='viridis')\n",
    "                    \n",
    "                    # Calculate statistics\n",
    "                    mean_val = channel_data.mean()\n",
    "                    std_val = channel_data.std()\n",
    "                    sparsity = (channel_data == 0).mean()\n",
    "                    \n",
    "                    title = f'Ch{ch+1}\\nŒº={mean_val:.2f}\\nœÉ={std_val:.2f}'\n",
    "                    axes[layer_idx, ch].set_title(title, fontsize=8)\n",
    "                else:\n",
    "                    axes[layer_idx, ch].axis('off')\n",
    "                \n",
    "                axes[layer_idx, ch].set_xticks([])\n",
    "                axes[layer_idx, ch].set_yticks([])\n",
    "            \n",
    "            # Add layer label\n",
    "            axes[layer_idx, 0].set_ylabel(f'{layer_name}\\n{feature_map.shape}', \n",
    "                                        rotation=90, fontsize=10, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle('Feature Map Progression Through Network Layers', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üíæ Feature progression visualization saved to: {save_path}\")\n",
    "        return save_path\n",
    "    \n",
    "    def analyze_feature_statistics(self, dataloader, num_batches=10, save_path=None):\n",
    "        \"\"\"\n",
    "        Comprehensive feature statistics analysis across dataset\n",
    "        \n",
    "        Args:\n",
    "            dataloader: DataLoader for analysis\n",
    "            num_batches: Number of batches to analyze\n",
    "            save_path: Path to save analysis results\n",
    "        \"\"\"\n",
    "        if save_path is None:\n",
    "            save_path = f\"{project_dirs['statistics']}/feature_statistics_analysis.png\"\n",
    "        \n",
    "        # Collect feature statistics\n",
    "        feature_stats = {}\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"üìä Analyzing feature statistics across {num_batches} batches...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, _) in enumerate(dataloader):\n",
    "                if batch_idx >= num_batches:\n",
    "                    break\n",
    "                \n",
    "                data = data.to(device)\n",
    "                features = self.model.extract_feature_maps(data)\n",
    "                \n",
    "                for layer_name, feature_map in features.items():\n",
    "                    if layer_name not in feature_stats:\n",
    "                        feature_stats[layer_name] = {\n",
    "                            'means': [], 'stds': [], 'sparsity': [], 'max_vals': []\n",
    "                        }\n",
    "                    \n",
    "                    # Calculate batch statistics\n",
    "                    feature_stats[layer_name]['means'].append(feature_map.mean().cpu().item())\n",
    "                    feature_stats[layer_name]['stds'].append(feature_map.std().cpu().item())\n",
    "                    feature_stats[layer_name]['sparsity'].append((feature_map == 0).float().mean().cpu().item())\n",
    "                    feature_stats[layer_name]['max_vals'].append(feature_map.max().cpu().item())\n",
    "        \n",
    "        # Create comprehensive statistics visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        layers = list(feature_stats.keys())\n",
    "        \n",
    "        # Mean activations\n",
    "        mean_activations = [np.mean(feature_stats[layer]['means']) for layer in layers]\n",
    "        bars1 = axes[0, 0].bar(range(len(layers)), mean_activations, alpha=0.8, color='skyblue')\n",
    "        axes[0, 0].set_title('Average Activation Values Across Layers', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].set_ylabel('Mean Activation')\n",
    "        axes[0, 0].set_xticks(range(len(layers)))\n",
    "        axes[0, 0].set_xticklabels([layer.replace('_', '\\n') for layer in layers], rotation=45, ha='right')\n",
    "        \n",
    "        # Activation variability\n",
    "        std_activations = [np.mean(feature_stats[layer]['stds']) for layer in layers]\n",
    "        bars2 = axes[0, 1].bar(range(len(layers)), std_activations, alpha=0.8, color='lightcoral')\n",
    "        axes[0, 1].set_title('Activation Variability (Standard Deviation)', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].set_ylabel('Standard Deviation')\n",
    "        axes[0, 1].set_xticks(range(len(layers)))\n",
    "        axes[0, 1].set_xticklabels([layer.replace('_', '\\n') for layer in layers], rotation=45, ha='right')\n",
    "        \n",
    "        # Sparsity analysis\n",
    "        sparsity_vals = [np.mean(feature_stats[layer]['sparsity']) for layer in layers]\n",
    "        bars3 = axes[1, 0].bar(range(len(layers)), sparsity_vals, alpha=0.8, color='lightgreen')\n",
    "        axes[1, 0].set_title('Activation Sparsity (Fraction of Zeros)', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_ylabel('Sparsity Ratio')\n",
    "        axes[1, 0].set_xticks(range(len(layers)))\n",
    "        axes[1, 0].set_xticklabels([layer.replace('_', '\\n') for layer in layers], rotation=45, ha='right')\n",
    "        \n",
    "        # Maximum activations\n",
    "        max_activations = [np.mean(feature_stats[layer]['max_vals']) for layer in layers]\n",
    "        bars4 = axes[1, 1].bar(range(len(layers)), max_activations, alpha=0.8, color='gold')\n",
    "        axes[1, 1].set_title('Maximum Activation Values', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Max Activation')\n",
    "        axes[1, 1].set_xticks(range(len(layers)))\n",
    "        axes[1, 1].set_xticklabels([layer.replace('_', '\\n') for layer in layers], rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Store statistics\n",
    "        self.visualization_history['feature_statistics'].append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'num_batches_analyzed': num_batches,\n",
    "            'statistics': feature_stats,\n",
    "            'save_path': save_path\n",
    "        })\n",
    "        \n",
    "        print(f\"üíæ Feature statistics analysis saved to: {save_path}\")\n",
    "        return feature_stats\n",
    "\n",
    "# Initialize the complete CNN architecture\n",
    "print(\"üèóÔ∏è Initializing Complete CNN Architecture:\")\n",
    "\n",
    "# Create model instance\n",
    "cnn_model = CompleteCNNArchitecture(\n",
    "    num_classes=10, \n",
    "    input_channels=3, \n",
    "    dropout_rate=0.3\n",
    ").to(device)\n",
    "\n",
    "# Get and display architecture summary\n",
    "arch_summary = cnn_model.get_architecture_summary()\n",
    "print(f\"\\nüìä Model Architecture Summary:\")\n",
    "print(f\"   Total parameters: {arch_summary['total_parameters']:,}\")\n",
    "print(f\"   Trainable parameters: {arch_summary['trainable_parameters']:,}\")\n",
    "print(f\"   Model size: {arch_summary['model_size_mb']:.2f} MB\")\n",
    "print(f\"   Input channels: {arch_summary['input_channels']}\")\n",
    "print(f\"   Output classes: {arch_summary['num_classes']}\")\n",
    "print(f\"   Dropout rate: {arch_summary['dropout_rate']}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\nüîç Testing Forward Pass:\")\n",
    "dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "print(f\"   Input shape: {dummy_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = cnn_model(dummy_input)\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    print(f\"   Output range: [{output.min().item():.3f}, {output.max().item():.3f}]\")\n",
    "\n",
    "# Initialize visualization suite\n",
    "visualizer = CNNVisualizationSuite(cnn_model, class_names=[f'Class_{i}' for i in range(10)])\n",
    "\n",
    "# Visualize initial (random) filters\n",
    "print(f\"\\nüé® Visualizing Initial Filters:\")\n",
    "visualizer.visualize_learned_filters(\n",
    "    layer_name='features.0',\n",
    "    save_path=f\"{project_dirs['filters']}/initial_filters_block1.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5883fa",
   "metadata": {},
   "source": [
    "## 5. Dataset Preparation and Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professional dataset preparation\n",
    "print(\"üì• Preparing CIFAR-10 Dataset with Advanced Augmentations:\")\n",
    "\n",
    "# Define comprehensive data transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.02, 0.33), ratio=(0.3, 3.3))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 datasets\n",
    "try:\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='../data/cnn_fundamentals/cifar10', \n",
    "        train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    trainloader = DataLoader(\n",
    "        trainset, batch_size=128, shuffle=True, \n",
    "        num_workers=4, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='../data/cnn_fundamentals/cifar10', \n",
    "        train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    testloader = DataLoader(\n",
    "        testset, batch_size=128, shuffle=False, \n",
    "        num_workers=4, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # CIFAR-10 class names\n",
    "    cifar10_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    print(f\"‚úÖ Training samples: {len(trainset):,}\")\n",
    "    print(f\"‚úÖ Test samples: {len(testset):,}\")\n",
    "    print(f\"‚úÖ Classes: {cifar10_classes}\")\n",
    "    print(f\"‚úÖ Batch size: {trainloader.batch_size}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading CIFAR-10: {e}\")\n",
    "    # Create dummy data for demonstration\n",
    "    print(\"üìù Creating dummy data for demonstration...\")\n",
    "    \n",
    "    class DummyCIFAR10:\n",
    "        def __init__(self, size=1000):\n",
    "            self.data = [(torch.randn(3, 32, 32), np.random.randint(0, 10)) for _ in range(size)]\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx]\n",
    "    \n",
    "    trainset = DummyCIFAR10(1000)\n",
    "    testset = DummyCIFAR10(200)\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "    cifar10_classes = ('dummy_class_' + str(i) for i in range(10))\n",
    "\n",
    "class ProfessionalTrainer:\n",
    "    \"\"\"\n",
    "    Professional training pipeline with comprehensive monitoring and visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, class_names):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "        self.training_history = {\n",
    "            'train_loss': [], 'train_acc': [],\n",
    "            'val_loss': [], 'val_acc': [],\n",
    "            'learning_rates': [], 'epoch_times': []\n",
    "        }\n",
    "        self.best_val_acc = 0.0\n",
    "        self.visualizer = CNNVisualizationSuite(model, class_names)\n",
    "        \n",
    "    def setup_training(self, learning_rate=0.001, weight_decay=0.01, epochs=10):\n",
    "        \"\"\"Setup training components with modern best practices\"\"\"\n",
    "        \n",
    "        # Loss function with label smoothing\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        # Optimizer with weight decay\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=learning_rate, \n",
    "            weight_decay=weight_decay,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer, \n",
    "            max_lr=learning_rate * 10,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=len(trainloader),\n",
    "            pct_start=0.3,\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "        \n",
    "        print(f\"üìã Training Setup Complete:\")\n",
    "        print(f\"   Optimizer: AdamW (lr={learning_rate}, wd={weight_decay})\")\n",
    "        print(f\"   Scheduler: OneCycleLR (max_lr={learning_rate * 10})\")\n",
    "        print(f\"   Loss: CrossEntropyLoss (label_smoothing=0.1)\")\n",
    "        \n",
    "    def train_epoch(self, epoch, total_epochs):\n",
    "        \"\"\"Train for one epoch with detailed monitoring\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{total_epochs}')\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%',\n",
    "                'LR': f'{self.scheduler.get_last_lr()[0]:.6f}'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate model performance\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in testloader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        epoch_val_loss = val_loss / len(testloader)\n",
    "        epoch_val_acc = 100. * correct / total\n",
    "        \n",
    "        return epoch_val_loss, epoch_val_acc\n",
    "    \n",
    "    def train_with_visualization(self, epochs=10, visualize_every=2):\n",
    "        \"\"\"\n",
    "        Complete training loop with comprehensive visualization and monitoring\n",
    "        \"\"\"\n",
    "        print(f\"\\nüöÄ Starting Training for {epochs} epochs with visualization every {visualize_every} epochs\")\n",
    "        \n",
    "        import time\n",
    "        training_start_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_acc = self.train_epoch(epoch, epochs)\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_acc = self.validate_epoch()\n",
    "            \n",
    "            # Record metrics\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            current_lr = self.scheduler.get_last_lr()[0]\n",
    "            \n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_acc'].append(train_acc)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_acc'].append(val_acc)\n",
    "            self.training_history['learning_rates'].append(current_lr)\n",
    "            self.training_history['epoch_times'].append(epoch_time)\n",
    "            \n",
    "            # Print epoch summary\n",
    "            print(f\"\\nüìä Epoch {epoch+1}/{epochs} Summary:\")\n",
    "            print(f\"   Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"   Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            print(f\"   Learning Rate: {current_lr:.6f}\")\n",
    "            print(f\"   Epoch Time: {epoch_time:.2f}s\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                self.save_checkpoint(epoch, is_best=True)\n",
    "                print(f\"   üéØ New best validation accuracy: {val_acc:.2f}%\")\n",
    "            \n",
    "            # Visualization at specified intervals\n",
    "            if (epoch + 1) % visualize_every == 0 or epoch == epochs - 1:\n",
    "                print(f\"\\nüé® Creating visualizations for epoch {epoch+1}...\")\n",
    "                self.create_epoch_visualizations(epoch)\n",
    "            \n",
    "            # Regular checkpoint\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_checkpoint(epoch, is_best=False)\n",
    "        \n",
    "        total_training_time = time.time() - training_start_time\n",
    "        \n",
    "        print(f\"\\nüéâ Training Complete!\")\n",
    "        print(f\"   Total time: {total_training_time/60:.2f} minutes\")\n",
    "        print(f\"   Best validation accuracy: {self.best_val_acc:.2f}%\")\n",
    "        print(f\"   Average epoch time: {np.mean(self.training_history['epoch_times']):.2f}s\")\n",
    "        \n",
    "        # Final comprehensive analysis\n",
    "        self.create_final_analysis()\n",
    "        \n",
    "        return self.training_history\n",
    "    \n",
    "    def create_epoch_visualizations(self, epoch):\n",
    "        \"\"\"Create visualizations for current epoch\"\"\"\n",
    "        \n",
    "        # Get sample batch for visualization\n",
    "        sample_batch, _ = next(iter(testloader))\n",
    "        sample_image = sample_batch[0].to(self.device)\n",
    "        \n",
    "        # Visualize learned filters\n",
    "        filter_save_path = f\"{project_dirs['filters']}/epoch_{epoch+1:02d}_learned_filters.png\"\n",
    "        self.visualizer.visualize_learned_filters(\n",
    "            layer_name='features.0',\n",
    "            save_path=filter_save_path\n",
    "        )\n",
    "        \n",
    "        # Visualize feature progression\n",
    "        feature_save_path = f\"{project_dirs['feature_maps']}/epoch_{epoch+1:02d}_feature_progression.png\"\n",
    "        self.visualizer.visualize_feature_map_progression(\n",
    "            sample_image,\n",
    "            save_path=feature_save_path\n",
    "        )\n",
    "        \n",
    "    def save_checkpoint(self, epoch, is_best=False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'training_history': self.training_history,\n",
    "            'best_val_acc': self.best_val_acc,\n",
    "            'architecture_summary': self.model.get_architecture_summary()\n",
    "        }\n",
    "        \n",
    "        if is_best:\n",
    "            save_path = f\"{project_dirs['checkpoints']}/best_model.pth\"\n",
    "            print(f\"   üíæ Saving best model checkpoint to: {save_path}\")\n",
    "        else:\n",
    "            save_path = f\"{project_dirs['checkpoints']}/checkpoint_epoch_{epoch+1:02d}.pth\"\n",
    "            print(f\"   üíæ Saving checkpoint to: {save_path}\")\n",
    "        \n",
    "        torch.save(checkpoint, save_path)\n",
    "    \n",
    "    def create_training_progress_plots(self, save_path=None):\n",
    "        \"\"\"Create comprehensive training progress visualization\"\"\"\n",
    "        if save_path is None:\n",
    "            save_path = f\"{project_dirs['progress']}/training_progress_comprehensive.png\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        epochs = range(1, len(self.training_history['train_loss']) + 1)\n",
    "        \n",
    "        # Loss curves\n",
    "        axes[0, 0].plot(epochs, self.training_history['train_loss'], 'b-', label='Training Loss', linewidth=2, marker='o')\n",
    "        axes[0, 0].plot(epochs, self.training_history['val_loss'], 'r-', label='Validation Loss', linewidth=2, marker='s')\n",
    "        axes[0, 0].set_title('Loss Curves', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy curves\n",
    "        axes[0, 1].plot(epochs, self.training_history['train_acc'], 'b-', label='Training Accuracy', linewidth=2, marker='o')\n",
    "        axes[0, 1].plot(epochs, self.training_history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2, marker='s')\n",
    "        axes[0, 1].set_title('Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate schedule\n",
    "        axes[0, 2].plot(epochs, self.training_history['learning_rates'], 'g-', linewidth=2, marker='d')\n",
    "        axes[0, 2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "        axes[0, 2].set_xlabel('Epoch')\n",
    "        axes[0, 2].set_ylabel('Learning Rate')\n",
    "        axes[0, 2].set_yscale('log')\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training time per epoch\n",
    "        axes[1, 0].plot(epochs, self.training_history['epoch_times'], 'purple', linewidth=2, marker='x')\n",
    "        axes[1, 0].set_title('Training Time per Epoch', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Time (seconds)')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Validation accuracy improvement\n",
    "        val_acc_smooth = []\n",
    "        best_so_far = 0\n",
    "        for acc in self.training_history['val_acc']:\n",
    "            best_so_far = max(best_so_far, acc)\n",
    "            val_acc_smooth.append(best_so_far)\n",
    "        \n",
    "        axes[1, 1].plot(epochs, val_acc_smooth, 'orange', linewidth=3, label='Best Validation Accuracy')\n",
    "        axes[1, 1].plot(epochs, self.training_history['val_acc'], 'lightcoral', alpha=0.7, label='Current Validation Accuracy')\n",
    "        axes[1, 1].set_title('Validation Accuracy Progress', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training metrics summary\n",
    "        final_train_acc = self.training_history['train_acc'][-1]\n",
    "        final_val_acc = self.training_history['val_acc'][-1]\n",
    "        best_val_acc = max(self.training_history['val_acc'])\n",
    "        generalization_gap = final_train_acc - final_val_acc\n",
    "        \n",
    "        metrics = ['Final Train Acc', 'Final Val Acc', 'Best Val Acc', 'Generalization Gap']\n",
    "        values = [final_train_acc, final_val_acc, best_val_acc, generalization_gap]\n",
    "        colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']\n",
    "        \n",
    "        bars = axes[1, 2].bar(metrics, values, color=colors, alpha=0.8)\n",
    "        axes[1, 2].set_title('Training Summary Metrics', fontsize=14, fontweight='bold')\n",
    "        axes[1, 2].set_ylabel('Accuracy (%) / Gap')\n",
    "        axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,\n",
    "                          f'{value:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"üíæ Training progress plots saved to: {save_path}\")\n",
    "        \n",
    "        return save_path\n",
    "    \n",
    "    def create_final_analysis(self):\n",
    "        \"\"\"Create final comprehensive analysis and visualizations\"\"\"\n",
    "        print(\"\\nüî¨ Creating Final Analysis and Visualizations...\")\n",
    "        \n",
    "        # Training progress plots\n",
    "        self.create_training_progress_plots()\n",
    "        \n",
    "        # Feature statistics analysis\n",
    "        self.visualizer.analyze_feature_statistics(testloader, num_batches=20)\n",
    "        \n",
    "        # Final model state visualization\n",
    "        sample_batch, _ = next(iter(testloader))\n",
    "        sample_image = sample_batch[0].to(self.device)\n",
    "        \n",
    "        # Visualize final learned filters\n",
    "        self.visualizer.visualize_learned_filters(\n",
    "            layer_name='features.0',\n",
    "            save_path=f\"{project_dirs['filters']}/final_learned_filters_block1.png\"\n",
    "        )\n",
    "        \n",
    "        # Visualize final feature progression\n",
    "        self.visualizer.visualize_feature_map_progression(\n",
    "            sample_image,\n",
    "            save_path=f\"{project_dirs['feature_maps']}/final_feature_progression.png\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Final analysis complete!\")\n",
    "\n",
    "# Initialize professional trainer\n",
    "print(\"\\nüéì Initializing Professional Training Pipeline:\")\n",
    "trainer = ProfessionalTrainer(cnn_model, device, cifar10_classes)\n",
    "\n",
    "# Setup training with modern best practices\n",
    "trainer.setup_training(learning_rate=0.001, weight_decay=0.01, epochs=10)\n",
    "\n",
    "# Start training with comprehensive monitoring\n",
    "print(\"\\nüöÄ Beginning Training with Full Visualization Pipeline:\")\n",
    "final_training_history = trainer.train_with_visualization(epochs=10, visualize_every=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa8b42",
   "metadata": {},
   "source": [
    "## 6. Post-Training Analysis and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e1ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation and analysis suite\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, class_names):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "        self.model.eval()\n",
    "    \n",
    "    def evaluate_model_performance(self, dataloader, save_path=None):\n",
    "        \"\"\"Comprehensive model performance evaluation\"\"\"\n",
    "        if save_path is None:\n",
    "            save_path = f\"{project_dirs['statistics']}/model_performance_evaluation.png\"\n",
    "        \n",
    "        # Detailed evaluation metrics\n",
    "        class_correct = [0] * len(self.class_names)\n",
    "        class_total = [0] * len(self.class_names)\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        confidence_scores = []\n",
    "        \n",
    "        print(\"üìä Evaluating model performance across all test data...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                # Get predictions and confidence scores\n",
    "                probabilities = F.softmax(outputs, dim=1)\n",
    "                confidence, predicted = torch.max(probabilities, 1)\n",
    "                \n",
    "                # Store for analysis\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "                confidence_scores.extend(confidence.cpu().numpy())\n",
    "                \n",
    "                # Per-class accuracy\n",
    "                correct = predicted.eq(targets)\n",
    "                for i in range(targets.size(0)):\n",
    "                    label = targets[i].item()\n",
    "                    class_correct[label] += correct[i].item()\n",
    "                    class_total[label] += 1\n",
    "        \n",
    "        # Calculate metrics\n",
    "        overall_accuracy = 100. * sum(class_correct) / sum(class_total)\n",
    "        class_accuracies = [100. * class_correct[i] / max(class_total[i], 1) for i in range(len(self.class_names))]\n",
    "        \n",
    "        # Create comprehensive evaluation visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Per-class accuracy\n",
    "        bars1 = axes[0, 0].bar(self.class_names, class_accuracies, alpha=0.8, color=plt.cm.Set3(range(len(self.class_names))))\n",
    "        axes[0, 0].set_title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 0].axhline(y=overall_accuracy, color='red', linestyle='--', alpha=0.8, label=f'Overall: {overall_accuracy:.2f}%')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars1, class_accuracies):\n",
    "            axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                          f'{acc:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Confidence score distribution\n",
    "        axes[0, 1].hist(confidence_scores, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0, 1].set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Confidence Score')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].axvline(x=np.mean(confidence_scores), color='red', linestyle='--', \n",
    "                         label=f'Mean: {np.mean(confidence_scores):.3f}')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Confusion matrix\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import seaborn as sns\n",
    "        \n",
    "        cm = confusion_matrix(all_targets, all_predictions)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "                   xticklabels=self.class_names, yticklabels=self.class_names, ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Normalized Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Predicted Label')\n",
    "        axes[1, 0].set_ylabel('True Label')\n",
    "        \n",
    "        # Performance summary\n",
    "        summary_metrics = {\n",
    "            'Overall Accuracy': overall_accuracy,\n",
    "            'Best Class Acc': max(class_accuracies),\n",
    "            'Worst Class Acc': min(class_accuracies),\n",
    "            'Std Dev Acc': np.std(class_accuracies),\n",
    "            'Mean Confidence': np.mean(confidence_scores) * 100,\n",
    "            'Low Confidence (<0.8)': (np.array(confidence_scores) < 0.8).mean() * 100\n",
    "        }\n",
    "        \n",
    "        metrics_names = list(summary_metrics.keys())\n",
    "        metrics_values = list(summary_metrics.values())\n",
    "        \n",
    "        bars2 = axes[1, 1].bar(range(len(metrics_names)), metrics_values, \n",
    "                             alpha=0.8, color=['green', 'blue', 'red', 'orange', 'purple', 'brown'])\n",
    "        axes[1, 1].set_title('Performance Summary Metrics', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Percentage / Value')\n",
    "        axes[1, 1].set_xticks(range(len(metrics_names)))\n",
    "        axes[1, 1].set_xticklabels([name.replace(' ', '\\n') for name in metrics_names], rotation=0, ha='center')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars2, metrics_values):\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(metrics_values)*0.01,\n",
    "                          f'{value:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(f\"\\nüìà Model Evaluation Results:\")\n",
    "        print(f\"   Overall Accuracy: {overall_accuracy:.2f}%\")\n",
    "        print(f\"   Best performing class: {self.class_names[np.argmax(class_accuracies)]} ({max(class_accuracies):.2f}%)\")\n",
    "        print(f\"   Worst performing class: {self.class_names[np.argmin(class_accuracies)]} ({min(class_accuracies):.2f}%)\")\n",
    "        print(f\"   Mean confidence: {np.mean(confidence_scores):.3f}\")\n",
    "        print(f\"   Predictions with low confidence (<0.8): {(np.array(confidence_scores) < 0.8).mean()*100:.1f}%\")\n",
    "        \n",
    "        evaluation_results = {\n",
    "            'overall_accuracy': overall_accuracy,\n",
    "            'class_accuracies': dict(zip(self.class_names, class_accuracies)),\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'confidence_stats': {\n",
    "                'mean': float(np.mean(confidence_scores)),\n",
    "                'std': float(np.std(confidence_scores)),\n",
    "                'low_confidence_percentage': float((np.array(confidence_scores) < 0.8).mean() * 100)\n",
    "            },\n",
    "            'summary_metrics': summary_metrics\n",
    "        }\n",
    "        \n",
    "        return evaluation_results\n",
    "\n",
    "# Perform comprehensive model evaluation\n",
    "print(\"\\nüî¨ Performing Comprehensive Model Evaluation:\")\n",
    "evaluator = ModelEvaluator(cnn_model, device, cifar10_classes)\n",
    "evaluation_results = evaluator.evaluate_model_performance(testloader)\n",
    "\n",
    "# Save final model\n",
    "final_model_path = f\"{project_dirs['final']}/cnn_fundamentals_final_model.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': cnn_model.state_dict(),\n",
    "    'architecture_summary': cnn_model.get_architecture_summary(),\n",
    "    'training_history': final_training_history,\n",
    "    'evaluation_results': evaluation_results,\n",
    "    'class_names': cifar10_classes,\n",
    "    'model_config': {\n",
    "        'num_classes': 10,\n",
    "        'input_channels': 3,\n",
    "        'dropout_rate': 0.3\n",
    "    }\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"üíæ Final model saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d1eb7",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Results Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baddc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_summary():\n",
    "    \"\"\"Generate comprehensive summary of the entire CNN fundamentals project\"\"\"\n",
    "    \n",
    "    summary_data = {\n",
    "        'project_info': {\n",
    "            'title': 'CNN Fundamentals: Complete Computer Vision Implementation',\n",
    "            'completion_timestamp': datetime.now().isoformat(),\n",
    "            'total_execution_time': 'N/A',  # Would be calculated in actual execution\n",
    "            'pytorch_version': torch.__version__,\n",
    "            'device_used': str(device)\n",
    "        },\n",
    "        'learning_objectives_achieved': {\n",
    "            'manual_convolution_implementation': {\n",
    "                'status': 'completed',\n",
    "                'key_insights': [\n",
    "                    'Implemented convolution from mathematical foundations',\n",
    "                    'Compared manual implementation with PyTorch optimized version',\n",
    "                    'Analyzed computational complexity and operation counting',\n",
    "                    'Visualized effects of different kernel types'\n",
    "                ]\n",
    "            },\n",
    "            'pooling_analysis': {\n",
    "                'status': 'completed',\n",
    "                'key_insights': [\n",
    "                    'Comprehensive analysis of 6 different pooling operations',\n",
    "                    'Quantified information retention and compression ratios',\n",
    "                    'Calculated receptive field progression through network layers',\n",
    "                    'Demonstrated trade-offs between spatial resolution and receptive field size'\n",
    "                ]\n",
    "            },\n",
    "            'cnn_architecture': {\n",
    "                'status': 'completed',\n",
    "                'key_insights': [\n",
    "                    'Built professional 4-block CNN architecture with modern best practices',\n",
    "                    'Implemented proper weight initialization and regularization',\n",
    "                    'Created comprehensive feature extraction and visualization capabilities',\n",
    "                    'Achieved modular design for easy modification and extension'\n",
    "                ]\n",
    "            },\n",
    "            'training_pipeline': {\n",
    "                'status': 'completed',\n",
    "                'key_insights': [\n",
    "                    'Professional training pipeline with advanced data augmentation',\n",
    "                    'Modern optimization techniques (AdamW, OneCycleLR, Label Smoothing)',\n",
    "                    'Comprehensive monitoring and checkpoint management',\n",
    "                    'Real-time visualization of training dynamics'\n",
    "                ]\n",
    "            },\n",
    "            'visualization_and_analysis': {\n",
    "                'status': 'completed',\n",
    "                'key_insights': [\n",
    "                    'Filter evolution tracking throughout training',\n",
    "                    'Feature map progression visualization',\n",
    "                    'Statistical analysis of activations across layers',\n",
    "                    'Comprehensive model evaluation and performance metrics'\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        'technical_achievements': {\n",
    "            'model_architecture': cnn_model.get_architecture_summary(),\n",
    "            'training_performance': {\n",
    "                'final_train_accuracy': final_training_history['train_acc'][-1] if final_training_history['train_acc'] else 'N/A',\n",
    "                'final_val_accuracy': final_training_history['val_acc'][-1] if final_training_history['val_acc'] else 'N/A',\n",
    "                'best_val_accuracy': max(final_training_history['val_acc']) if final_training_history['val_acc'] else 'N/A',\n",
    "                'total_epochs_trained': len(final_training_history['train_loss']) if final_training_history['train_loss'] else 0\n",
    "            },\n",
    "            'model_evaluation': evaluation_results if 'evaluation_results' in locals() else 'Evaluation pending',\n",
    "            'visualizations_created': {\n",
    "                'convolution_mechanics': 'Detailed visualization of convolution operations and kernel effects',\n",
    "                'pooling_analysis': 'Comprehensive comparison of pooling operations',\n",
    "                'receptive_field_analysis': 'RF progression through network layers',\n",
    "                'filter_evolution': 'Learned filter visualization throughout training',\n",
    "                'feature_maps': 'Feature map progression and statistics',\n",
    "                'training_monitoring': 'Real-time training progress and metrics'\n",
    "            }\n",
    "        },\n",
    "        'educational_value': {\n",
    "            'concepts_demonstrated': [\n",
    "                'Mathematical foundations of convolution operations',\n",
    "                'Effects of different pooling strategies on information retention',\n",
    "                'Receptive field calculation and growth analysis',\n",
    "                'Modern CNN architecture design principles',\n",
    "                'Professional training pipeline implementation',\n",
    "                'Comprehensive model evaluation techniques',\n",
    "                'Advanced visualization and analysis methods'\n",
    "            ],\n",
    "            'practical_skills_developed': [\n",
    "                'Manual implementation of core operations for deep understanding',\n",
    "                'Professional PyTorch coding practices and project organization',\n",
    "                'Advanced data augmentation and regularization techniques',\n",
    "                'Modern optimization and learning rate scheduling',\n",
    "                'Comprehensive model monitoring and visualization',\n",
    "                'Statistical analysis of neural network behavior',\n",
    "                'Research-quality documentation and presentation'\n",
    "            ]\n",
    "        },\n",
    "        'files_generated': {\n",
    "            'models': [\n",
    "                'Final trained CNN model with comprehensive metadata',\n",
    "                'Training checkpoints at regular intervals',\n",
    "                'Best model checkpoint based on validation performance'\n",
    "            ],\n",
    "            'visualizations': [\n",
    "                'Convolution mechanics and kernel effects',\n",
    "                'Pooling operations comprehensive analysis',\n",
    "                'Receptive field progression diagrams',\n",
    "                'Filter evolution throughout training',\n",
    "                'Feature map progression and statistics',\n",
    "                'Training progress and performance metrics',\n",
    "                'Model evaluation and confusion matrices'\n",
    "            ],\n",
    "            'analysis_results': [\n",
    "                'Feature statistics across training batches',\n",
    "                'Performance evaluation metrics by class',\n",
    "                'Training history and optimization dynamics',\n",
    "                'Comprehensive project summary and insights'\n",
    "            ]\n",
    "        },\n",
    "        'next_steps_recommendations': [\n",
    "            'Experiment with modern architectures (ResNet, EfficientNet, Vision Transformers)',\n",
    "            'Implement attention mechanisms and advanced regularization techniques',\n",
    "            'Explore transfer learning and fine-tuning strategies',\n",
    "            'Apply techniques to real-world computer vision problems',\n",
    "            'Investigate interpretability and explainable AI methods',\n",
    "            'Scale to larger datasets and implement distributed training'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return summary_data\n",
    "\n",
    "# Generate comprehensive summary\n",
    "project_summary = generate_comprehensive_summary()\n",
    "\n",
    "# Save comprehensive summary\n",
    "summary_save_path = f\"{project_dirs['statistics']}/comprehensive_project_summary.json\"\n",
    "with open(summary_save_path, 'w') as f:\n",
    "    json.dump(project_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéì CNN FUNDAMENTALS: COMPREHENSIVE PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìö Project: {project_summary['project_info']['title']}\")\n",
    "print(f\"üïê Completed: {project_summary['project_info']['completion_timestamp']}\")\n",
    "print(f\"üîß PyTorch Version: {project_summary['project_info']['pytorch_version']}\")\n",
    "print(f\"üíª Device: {project_summary['project_info']['device_used']}\")\n",
    "\n",
    "print(f\"\\nüéØ Learning Objectives Status:\")\n",
    "for objective, details in project_summary['learning_objectives_achieved'].items():\n",
    "    status_emoji = \"‚úÖ\" if details['status'] == 'completed' else \"‚è≥\"\n",
    "    print(f\"   {status_emoji} {objective.replace('_', ' ').title()}\")\n",
    "\n",
    "print(f\"\\nüèóÔ∏è Model Architecture Summary:\")\n",
    "arch_summary = project_summary['technical_achievements']['model_architecture']\n",
    "print(f\"   üìä Total Parameters: {arch_summary['total_parameters']:,}\")\n",
    "print(f\"   üíæ Model Size: {arch_summary['model_size_mb']:.2f} MB\")\n",
    "print(f\"   üî¢ Input Channels: {arch_summary['input_channels']}\")\n",
    "print(f\"   üéØ Output Classes: {arch_summary['num_classes']}\")\n",
    "\n",
    "training_perf = project_summary['technical_achievements']['training_performance']\n",
    "if training_perf['final_val_accuracy'] != 'N/A':\n",
    "    print(f\"\\nüìà Training Performance:\")\n",
    "    print(f\"   üéØ Final Validation Accuracy: {training_perf['final_val_accuracy']:.2f}%\")\n",
    "    print(f\"   üèÜ Best Validation Accuracy: {training_perf['best_val_accuracy']:.2f}%\")\n",
    "    print(f\"   üìä Total Epochs: {training_perf['total_epochs_trained']}\")\n",
    "\n",
    "print(f\"\\nüé® Key Visualizations Created:\")\n",
    "for viz_name, description in project_summary['technical_achievements']['visualizations_created'].items():\n",
    "    print(f\"   üìä {viz_name.replace('_', ' ').title()}: {description}\")\n",
    "\n",
    "print(f\"\\nüí° Educational Concepts Demonstrated:\")\n",
    "for i, concept in enumerate(project_summary['educational_value']['concepts_demonstrated'][:5], 1):\n",
    "    print(f\"   {i}. {concept}\")\n",
    "\n",
    "print(f\"\\nüõ†Ô∏è Practical Skills Developed:\")\n",
    "for i, skill in enumerate(project_summary['educational_value']['practical_skills_developed'][:5], 1):\n",
    "    print(f\"   {i}. {skill}\")\n",
    "\n",
    "print(f\"\\nüìÅ Project Deliverables:\")\n",
    "print(f\"   üìä Visualizations: {len(project_summary['files_generated']['visualizations'])} comprehensive analyses\")\n",
    "print(f\"   üíæ Models: {len(project_summary['files_generated']['models'])} saved model states\")\n",
    "print(f\"   üìà Analysis: {len(project_summary['files_generated']['analysis_results'])} detailed reports\")\n",
    "\n",
    "print(f\"\\nüöÄ Recommended Next Steps:\")\n",
    "for i, recommendation in enumerate(project_summary['next_steps_recommendations'][:4], 1):\n",
    "    print(f\"   {i}. {recommendation}\")\n",
    "\n",
    "print(f\"\\nüíæ Complete summary saved to: {summary_save_path}\")\n",
    "\n",
    "# List all generated files and directories\n",
    "print(f\"\\nüìÇ Project Structure Summary:\")\n",
    "for dir_name, dir_path in project_dirs.items():\n",
    "    if Path(dir_path).exists():\n",
    "        file_count = len(list(Path(dir_path).glob('*')))\n",
    "        print(f\"   üìÅ {dir_name}: {file_count} files in {dir_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ CNN FUNDAMENTALS PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"   ‚úÖ All learning objectives achieved\")\n",
    "print(\"   ‚úÖ Professional-grade implementation completed\") \n",
    "print(\"   ‚úÖ Comprehensive analysis and documentation generated\")\n",
    "print(\"   ‚úÖ Ready for advanced computer vision topics\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ac1fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc99782b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "033068b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab78dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22aab1f8",
   "metadata": {},
   "source": [
    "## üéØ Project Summary and Key Achievements\n",
    "\n",
    "### **üìö What We Built**\n",
    "- **Manual Convolution Implementation**: From mathematical foundations to optimized comparisons\n",
    "- **Comprehensive Pooling Analysis**: 6 different pooling operations with quantitative analysis\n",
    "- **Professional CNN Architecture**: 4-block network with modern best practices\n",
    "- **Advanced Training Pipeline**: With real-time monitoring and visualization\n",
    "- **Complete Evaluation Suite**: Statistical analysis and performance metrics\n",
    "\n",
    "### **üéì Learning Outcomes Achieved**\n",
    "\n",
    "**Technical Mastery:**\n",
    "- Deep understanding of convolution mathematics and implementation\n",
    "- Mastery of pooling strategies and receptive field calculations\n",
    "- Professional PyTorch development practices\n",
    "- Modern training techniques and optimization strategies\n",
    "- Comprehensive model evaluation and analysis methods\n",
    "\n",
    "**Practical Skills:**\n",
    "- Building CNNs from mathematical foundations\n",
    "- Professional project organization and documentation\n",
    "- Advanced visualization and monitoring techniques\n",
    "- Research-quality analysis and presentation\n",
    "- Industry-standard model development workflow\n",
    "\n",
    "### **üèÜ Key Results**\n",
    "- ‚úÖ **Manual convolution implementation** matching PyTorch accuracy to 1e-6 precision\n",
    "- ‚úÖ **Comprehensive pooling analysis** with quantified information retention metrics\n",
    "- ‚úÖ **Professional CNN architecture** with {arch_summary['total_parameters']:,} parameters\n",
    "- ‚úÖ **Complete training pipeline** with advanced optimization and monitoring\n",
    "- ‚úÖ **Extensive visualization suite** covering all aspects of CNN behavior\n",
    "\n",
    "### **üìä Educational Impact**\n",
    "This notebook provides a **complete foundation** for understanding CNNs from mathematical principles to practical implementation. It bridges the gap between theoretical understanding and professional practice, preparing students for advanced computer vision topics and real-world applications.\n",
    "\n",
    "### **üöÄ Next Steps in Your CNN Journey**\n",
    "1. **Modern Architectures**: ResNet, EfficientNet, Vision Transformers\n",
    "2. **Advanced Techniques**: Attention mechanisms, advanced regularization\n",
    "3. **Transfer Learning**: Pre-trained models and fine-tuning strategies\n",
    "4. **Real Applications**: Object detection, segmentation, medical imaging\n",
    "5. **Research Topics**: Interpretability, few-shot learning, neural architecture search\n",
    "\n",
    "**üéâ Congratulations! You've mastered the fundamentals of CNNs and are ready to tackle advanced computer vision challenges!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
