{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a89389a",
   "metadata": {},
   "source": [
    "# RNN & LSTM Fundamentals: Sequential Model Mastery\n",
    "\n",
    "**PyTorch NLP Mastery Hub**\n",
    "\n",
    "**Authors:** Advanced NLP Research Team  \n",
    "**Institution:** Deep Learning Academy  \n",
    "**Course:** Advanced Natural Language Processing and Sequential Modeling  \n",
    "**Date:** December 2024\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides comprehensive exploration and implementation of Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks. We focus on understanding the fundamental mechanics of sequential modeling, analyzing the vanishing gradient problem, and building practical applications in text generation, sentiment analysis, and time series prediction.\n",
    "\n",
    "## Key Objectives\n",
    "1. Build RNN architectures from scratch with detailed mathematical analysis\n",
    "2. Implement LSTM and GRU cells with comprehensive gate mechanism studies\n",
    "3. Analyze vanishing gradient problems and gradient flow dynamics\n",
    "4. Develop practical NLP applications including text generation and sentiment analysis\n",
    "5. Create time series prediction models with memory-based architectures\n",
    "6. Compare architectural differences and performance characteristics\n",
    "7. Generate comprehensive visualizations and analytical insights\n",
    "\n",
    "## 1. Setup and Environment Configuration\n",
    "\n",
    "```python\n",
    "# üì¶ Essential Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced imports for NLP\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import random\n",
    "from typing import List, Tuple, Dict, Optional, Union\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except:\n",
    "    print(\"Note: NLTK data download failed, using basic tokenization\")\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# Create organized output directories\n",
    "def setup_directories():\n",
    "    \"\"\"Create organized directory structure for NLP outputs\"\"\"\n",
    "    base_dirs = [\n",
    "        \"../../results/05_nlp/rnn_analysis\",\n",
    "        \"../../results/05_nlp/lstm_analysis\",\n",
    "        \"../../results/05_nlp/attention_weights\", \n",
    "        \"../../results/05_nlp/text_generation\",\n",
    "        \"../../results/05_nlp/sentiment_analysis\",\n",
    "        \"../../results/05_nlp/sequence_modeling\",\n",
    "        \"../../results/05_nlp/time_series\",\n",
    "        \"../../models/nlp/rnn_fundamentals\",\n",
    "        \"../../data/nlp/datasets\",\n",
    "        \"../../data/nlp/vocabularies\",\n",
    "        \"../../data/nlp/embeddings\"\n",
    "    ]\n",
    "    \n",
    "    for dir_path in base_dirs:\n",
    "        Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"üìÅ Created: {dir_path}\")\n",
    "    \n",
    "    return {dir_path.split('/')[-1]: dir_path for dir_path in base_dirs}\n",
    "\n",
    "dirs = setup_directories()\n",
    "print(\"\\n‚úÖ Directory structure ready!\")\n",
    "\n",
    "# Utility functions\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "print(\"üé≤ Random seed set for reproducibility\")\n",
    "\n",
    "# Create results directory for this notebook\n",
    "notebook_results_dir = Path('../../results/05_nlp/rnn_lstm_fundamentals')\n",
    "notebook_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Results will be saved to: {notebook_results_dir}\")\n",
    "```\n",
    "\n",
    "## 2. RNN Architecture: Building from Scratch\n",
    "\n",
    "### 2.1 Vanilla RNN Implementation\n",
    "\n",
    "```python\n",
    "class VanillaRNN(nn.Module):\n",
    "    \"\"\"Vanilla RNN implementation from scratch with detailed analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input to hidden weights\n",
    "        self.W_ih = nn.Parameter(torch.randn(hidden_size, input_size) * 0.1)\n",
    "        \n",
    "        # Hidden to hidden weights (recurrent connections)\n",
    "        self.W_hh = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
    "        \n",
    "        # Hidden bias\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_size))\n",
    "        \n",
    "        # Hidden to output weights\n",
    "        self.W_ho = nn.Parameter(torch.randn(output_size, hidden_size) * 0.1)\n",
    "        \n",
    "        # Output bias\n",
    "        self.b_o = nn.Parameter(torch.zeros(output_size))\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier initialization\"\"\"\n",
    "        for param in self.parameters():\n",
    "            if param.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            else:\n",
    "                nn.init.zeros_(param.data)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass through RNN\n",
    "        x: (seq_len, batch_size, input_size)\n",
    "        hidden: (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "        \n",
    "        # Initialize hidden state if not provided\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        \n",
    "        outputs = []\n",
    "        hidden_states = [hidden]\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # RNN cell computation: h_t = tanh(W_ih * x_t + W_hh * h_{t-1} + b_h)\n",
    "            hidden = torch.tanh(\n",
    "                torch.mm(x[t], self.W_ih.t()) + \n",
    "                torch.mm(hidden, self.W_hh.t()) + \n",
    "                self.b_h\n",
    "            )\n",
    "            \n",
    "            # Output computation: y_t = W_ho * h_t + b_o\n",
    "            output = torch.mm(hidden, self.W_ho.t()) + self.b_o\n",
    "            \n",
    "            outputs.append(output)\n",
    "            hidden_states.append(hidden)\n",
    "        \n",
    "        # Stack outputs: (seq_len, batch_size, output_size)\n",
    "        outputs = torch.stack(outputs, dim=0)\n",
    "        \n",
    "        return outputs, hidden, hidden_states\n",
    "    \n",
    "    def get_gradient_norms(self):\n",
    "        \"\"\"Get gradient norms for analysis\"\"\"\n",
    "        grad_norms = {}\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norms[name] = param.grad.norm().item()\n",
    "        return grad_norms\n",
    "\n",
    "class RNNAnalyzer:\n",
    "    \"\"\"Comprehensive RNN behavior analysis and visualization\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "    \n",
    "    def demonstrate_vanishing_gradients(self, sequence_lengths, save_path):\n",
    "        \"\"\"Demonstrate and analyze vanishing gradient problem\"\"\"\n",
    "        print(\"üîç Analyzing vanishing gradient problem...\")\n",
    "        \n",
    "        gradient_data = []\n",
    "        eigenvalue_data = []\n",
    "        \n",
    "        for seq_len in tqdm(sequence_lengths, desc=\"Analyzing sequence lengths\"):\n",
    "            # Create random sequence\n",
    "            x = torch.randn(seq_len, 1, self.model.input_size, device=self.device)\n",
    "            target = torch.randn(seq_len, 1, self.model.output_size, device=self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.model.zero_grad()\n",
    "            outputs, _, hidden_states = self.model(x)\n",
    "            \n",
    "            # Compute loss (only on last output for simplicity)\n",
    "            loss = F.mse_loss(outputs[-1], target[-1])\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Collect gradient norms\n",
    "            grad_norms = self.model.get_gradient_norms()\n",
    "            gradient_data.append({\n",
    "                'seq_len': seq_len,\n",
    "                'W_ih_grad': grad_norms.get('W_ih', 0),\n",
    "                'W_hh_grad': grad_norms.get('W_hh', 0),\n",
    "                'W_ho_grad': grad_norms.get('W_ho', 0),\n",
    "                'loss': loss.item()\n",
    "            })\n",
    "            \n",
    "            # Eigenvalue analysis\n",
    "            W_hh = self.model.W_hh.detach().cpu().numpy()\n",
    "            eigenvalues = np.linalg.eigvals(W_hh)\n",
    "            spectral_radius = np.max(np.abs(eigenvalues))\n",
    "            eigenvalue_data.append({\n",
    "                'seq_len': seq_len,\n",
    "                'spectral_radius': spectral_radius,\n",
    "                'eigenvalues': eigenvalues\n",
    "            })\n",
    "        \n",
    "        # Create comprehensive visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        \n",
    "        # Plot 1: Gradient norms vs sequence length\n",
    "        seq_lens = [d['seq_len'] for d in gradient_data]\n",
    "        w_ih_grads = [d['W_ih_grad'] for d in gradient_data]\n",
    "        w_hh_grads = [d['W_hh_grad'] for d in gradient_data]\n",
    "        w_ho_grads = [d['W_ho_grad'] for d in gradient_data]\n",
    "        \n",
    "        axes[0, 0].plot(seq_lens, w_ih_grads, 'o-', label='W_ih (input-to-hidden)', linewidth=2, markersize=6)\n",
    "        axes[0, 0].plot(seq_lens, w_hh_grads, 's-', label='W_hh (hidden-to-hidden)', linewidth=2, markersize=6)\n",
    "        axes[0, 0].plot(seq_lens, w_ho_grads, '^-', label='W_ho (hidden-to-output)', linewidth=2, markersize=6)\n",
    "        axes[0, 0].set_xlabel('Sequence Length')\n",
    "        axes[0, 0].set_ylabel('Gradient Norm')\n",
    "        axes[0, 0].set_title('Gradient Norms vs Sequence Length', fontweight='bold')\n",
    "        axes[0, 0].set_yscale('log')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Hidden state evolution\n",
    "        test_seq_len = 20\n",
    "        x = torch.randn(test_seq_len, 1, self.model.input_size, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            _, _, hidden_states = self.model(x)\n",
    "        \n",
    "        hidden_mags = [h.norm(dim=1).item() for h in hidden_states]\n",
    "        axes[0, 1].plot(range(len(hidden_mags)), hidden_mags, 'o-', linewidth=2, color='purple', markersize=6)\n",
    "        axes[0, 1].set_xlabel('Time Step')\n",
    "        axes[0, 1].set_ylabel('Hidden State Magnitude')\n",
    "        axes[0, 1].set_title('Hidden State Evolution', fontweight='bold')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Eigenvalue analysis of W_hh\n",
    "        W_hh = self.model.W_hh.detach().cpu().numpy()\n",
    "        eigenvalues = np.linalg.eigvals(W_hh)\n",
    "        axes[0, 2].scatter(eigenvalues.real, eigenvalues.imag, alpha=0.7, s=50, c='red')\n",
    "        \n",
    "        # Draw unit circle\n",
    "        theta = np.linspace(0, 2*np.pi, 100)\n",
    "        axes[0, 2].plot(np.cos(theta), np.sin(theta), 'r--', alpha=0.5, label='Unit Circle')\n",
    "        axes[0, 2].set_xlabel('Real Part')\n",
    "        axes[0, 2].set_ylabel('Imaginary Part')\n",
    "        axes[0, 2].set_title('Eigenvalues of W_hh', fontweight='bold')\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "        axes[0, 2].axis('equal')\n",
    "        \n",
    "        # Plot 4: Spectral radius evolution\n",
    "        spectral_radii = [d['spectral_radius'] for d in eigenvalue_data]\n",
    "        axes[1, 0].plot(seq_lens, spectral_radii, 'o-', linewidth=2, color='coral', markersize=6)\n",
    "        axes[1, 0].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Stability Threshold')\n",
    "        axes[1, 0].set_xlabel('Sequence Length')\n",
    "        axes[1, 0].set_ylabel('Spectral Radius')\n",
    "        axes[1, 0].set_title('Spectral Radius Analysis', fontweight='bold')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 5: Gradient decay pattern\n",
    "        gradient_ratios = []\n",
    "        for i in range(1, len(w_hh_grads)):\n",
    "            if w_hh_grads[i-1] > 0:\n",
    "                ratio = w_hh_grads[i] / w_hh_grads[i-1]\n",
    "                gradient_ratios.append(ratio)\n",
    "            else:\n",
    "                gradient_ratios.append(0)\n",
    "        \n",
    "        axes[1, 1].plot(seq_lens[1:], gradient_ratios, 'o-', linewidth=2, color='green', markersize=6)\n",
    "        axes[1, 1].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='No Decay')\n",
    "        axes[1, 1].set_xlabel('Sequence Length')\n",
    "        axes[1, 1].set_ylabel('Gradient Ratio (Current/Previous)')\n",
    "        axes[1, 1].set_title('Gradient Decay Pattern', fontweight='bold')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 6: Training loss evolution\n",
    "        losses = [d['loss'] for d in gradient_data]\n",
    "        axes[1, 2].plot(seq_lens, losses, 'o-', linewidth=2, color='blue', markersize=6)\n",
    "        axes[1, 2].set_xlabel('Sequence Length')\n",
    "        axes[1, 2].set_ylabel('Training Loss')\n",
    "        axes[1, 2].set_title('Loss vs Sequence Length', fontweight='bold')\n",
    "        axes[1, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"üíæ Vanishing gradient analysis saved to: {save_path}\")\n",
    "        \n",
    "        return gradient_data, eigenvalue_data\n",
    "    \n",
    "    def visualize_rnn_unfolding(self, sequence_length, save_path):\n",
    "        \"\"\"Visualize RNN unfolding through time\"\"\"\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(18, 8))\n",
    "        \n",
    "        # Create a visual representation of unfolded RNN\n",
    "        time_steps = min(sequence_length, 8)  # Limit for visualization clarity\n",
    "        \n",
    "        # Draw RNN cells\n",
    "        cell_width = 1.5\n",
    "        cell_height = 1.0\n",
    "        y_center = 2\n",
    "        \n",
    "        for t in range(time_steps):\n",
    "            x_center = t * 2.5\n",
    "            \n",
    "            # Draw RNN cell\n",
    "            cell = plt.Rectangle((x_center - cell_width/2, y_center - cell_height/2), \n",
    "                               cell_width, cell_height, \n",
    "                               facecolor='lightblue', edgecolor='navy', linewidth=2)\n",
    "            ax.add_patch(cell)\n",
    "            \n",
    "            # Add RNN label\n",
    "            ax.text(x_center, y_center, f'RNN\\\\nt={t}', ha='center', va='center', \n",
    "                   fontweight='bold', fontsize=11)\n",
    "            \n",
    "            # Input arrow\n",
    "            ax.arrow(x_center, y_center - cell_height/2 - 0.5, 0, 0.4, \n",
    "                    head_width=0.1, head_length=0.1, fc='green', ec='green', linewidth=2)\n",
    "            ax.text(x_center, y_center - cell_height/2 - 0.8, f'x_{t}', \n",
    "                   ha='center', va='center', fontweight='bold', color='green', fontsize=12)\n",
    "            \n",
    "            # Output arrow\n",
    "            ax.arrow(x_center, y_center + cell_height/2, 0, 0.4, \n",
    "                    head_width=0.1, head_length=0.1, fc='red', ec='red', linewidth=2)\n",
    "            ax.text(x_center, y_center + cell_height/2 + 0.8, f'y_{t}', \n",
    "                   ha='center', va='center', fontweight='bold', color='red', fontsize=12)\n",
    "            \n",
    "            # Hidden state connection to next cell\n",
    "            if t < time_steps - 1:\n",
    "                ax.arrow(x_center + cell_width/2, y_center, \n",
    "                        2.5 - cell_width, 0, \n",
    "                        head_width=0.1, head_length=0.1, fc='blue', ec='blue', linewidth=2)\n",
    "                ax.text(x_center + 1.25, y_center + 0.3, f'h_{t}', \n",
    "                       ha='center', va='center', fontweight='bold', color='blue', fontsize=12)\n",
    "        \n",
    "        # Add title and labels\n",
    "        ax.set_title('RNN Unfolded Through Time (Backpropagation Through Time)', \n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.text(-1, y_center, 'h‚ÇÄ', ha='center', va='center', fontweight='bold', \n",
    "               color='blue', fontsize=14)\n",
    "        \n",
    "        # Add mathematical equations\n",
    "        equation_text = (\n",
    "            \"RNN Forward Pass Equations:\\n\"\n",
    "            \"h_t = tanh(W_ih¬∑x_t + W_hh¬∑h_{t-1} + b_h)\\n\"\n",
    "            \"y_t = W_ho¬∑h_t + b_o\"\n",
    "        )\n",
    "        ax.text(time_steps * 2.5 + 1, y_center + 1.5, equation_text, \n",
    "               fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = [\n",
    "            plt.Line2D([0], [0], color='green', lw=3, label='Input'),\n",
    "            plt.Line2D([0], [0], color='blue', lw=3, label='Hidden State'),\n",
    "            plt.Line2D([0], [0], color='red', lw=3, label='Output')\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='upper right')\n",
    "        \n",
    "        # Set axis properties\n",
    "        ax.set_xlim(-2, time_steps * 2.5 + 3)\n",
    "        ax.set_ylim(0, 5)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"üíæ RNN unfolding visualization saved to: {save_path}\")\n",
    "\n",
    "# Create and analyze vanilla RNN\n",
    "print(\"üîÑ Creating Vanilla RNN for comprehensive analysis...\")\n",
    "\n",
    "# Model parameters\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 5\n",
    "\n",
    "# Create model\n",
    "vanilla_rnn = VanillaRNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "model_info = {\n",
    "    'architecture': 'Vanilla RNN',\n",
    "    'input_size': input_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'output_size': output_size,\n",
    "    'total_parameters': sum(p.numel() for p in vanilla_rnn.parameters()),\n",
    "    'trainable_parameters': sum(p.numel() for p in vanilla_rnn.parameters() if p.requires_grad)\n",
    "}\n",
    "\n",
    "print(f\"   Architecture: {model_info['architecture']}\")\n",
    "print(f\"   Input size: {model_info['input_size']}\")\n",
    "print(f\"   Hidden size: {model_info['hidden_size']}\")\n",
    "print(f\"   Output size: {model_info['output_size']}\")\n",
    "print(f\"   Total parameters: {model_info['total_parameters']:,}\")\n",
    "print(f\"   Trainable parameters: {model_info['trainable_parameters']:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_seq_len = 5\n",
    "test_batch_size = 2\n",
    "test_input = torch.randn(test_seq_len, test_batch_size, input_size, device=device)\n",
    "\n",
    "print(f\"\\nüß™ Testing forward pass:\")\n",
    "print(f\"   Input shape: {test_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs, final_hidden, hidden_states = vanilla_rnn(test_input)\n",
    "    print(f\"   Output shape: {outputs.shape}\")\n",
    "    print(f\"   Final hidden shape: {final_hidden.shape}\")\n",
    "    print(f\"   Number of hidden states: {len(hidden_states)}\")\n",
    "\n",
    "# Analyze RNN behavior\n",
    "analyzer = RNNAnalyzer(vanilla_rnn, device)\n",
    "\n",
    "# Demonstrate vanishing gradients\n",
    "print(\"\\nüîç Conducting comprehensive vanishing gradient analysis...\")\n",
    "sequence_lengths = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "gradient_data, eigenvalue_data = analyzer.demonstrate_vanishing_gradients(\n",
    "    sequence_lengths,\n",
    "    notebook_results_dir / \"rnn_analysis/vanishing_gradients_comprehensive.png\"\n",
    ")\n",
    "\n",
    "# Visualize RNN unfolding\n",
    "print(\"\\nüìä Creating detailed RNN unfolding visualization...\")\n",
    "analyzer.visualize_rnn_unfolding(\n",
    "    8, notebook_results_dir / \"rnn_analysis/rnn_unfolding_detailed.png\"\n",
    ")\n",
    "\n",
    "# Calculate key metrics\n",
    "spectral_radius = np.max(np.abs(np.linalg.eigvals(vanilla_rnn.W_hh.detach().cpu().numpy())))\n",
    "gradient_decay_rate = gradient_data[-1]['W_hh_grad'] / gradient_data[0]['W_hh_grad'] if gradient_data[0]['W_hh_grad'] > 0 else 0\n",
    "\n",
    "rnn_analysis_results = {\n",
    "    'model_info': model_info,\n",
    "    'spectral_radius': float(spectral_radius),\n",
    "    'stability': 'Stable' if spectral_radius < 1.0 else 'Potentially Unstable',\n",
    "    'gradient_decay_rate': float(gradient_decay_rate),\n",
    "    'vanishing_gradient_detected': gradient_decay_rate < 0.1,\n",
    "    'sequence_lengths_tested': sequence_lengths,\n",
    "    'max_sequence_length': max(sequence_lengths)\n",
    "}\n",
    "\n",
    "print(f\"\\nüìà RNN Analysis Results:\")\n",
    "print(f\"   Spectral radius: {spectral_radius:.4f}\")\n",
    "print(f\"   Stability assessment: {rnn_analysis_results['stability']}\")\n",
    "print(f\"   Gradient decay rate: {gradient_decay_rate:.6f}\")\n",
    "print(f\"   Vanishing gradient detected: {rnn_analysis_results['vanishing_gradient_detected']}\")\n",
    "print(f\"   Maximum sequence length tested: {rnn_analysis_results['max_sequence_length']}\")\n",
    "```\n",
    "\n",
    "## 3. LSTM Architecture: Advanced Sequential Modeling\n",
    "\n",
    "### 3.1 LSTM Cell Implementation\n",
    "\n",
    "```python\n",
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"LSTM cell implementation from scratch with detailed gate analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Forget gate - decides what information to discard\n",
    "        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        # Input gate - decides what new information to store\n",
    "        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        # Candidate values - creates new candidate values\n",
    "        self.W_c = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        # Output gate - decides what parts of cell state to output\n",
    "        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights with proper scaling for LSTM stability\"\"\"\n",
    "        for linear in [self.W_f, self.W_i, self.W_c, self.W_o]:\n",
    "            nn.init.xavier_uniform_(linear.weight)\n",
    "            nn.init.zeros_(linear.bias)\n",
    "            \n",
    "        # Initialize forget gate bias to 1 (remember by default)\n",
    "        nn.init.ones_(self.W_f.bias)\n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        \"\"\"\n",
    "        Forward pass through LSTM cell\n",
    "        x: (batch_size, input_size)\n",
    "        hidden_state: tuple of (h, c) each (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        h_prev, c_prev = hidden_state\n",
    "        \n",
    "        # Concatenate input and previous hidden state\n",
    "        combined = torch.cat([x, h_prev], dim=1)\n",
    "        \n",
    "        # Compute gates\n",
    "        f_t = torch.sigmoid(self.W_f(combined))  # Forget gate\n",
    "        i_t = torch.sigmoid(self.W_i(combined))  # Input gate\n",
    "        c_tilde = torch.tanh(self.W_c(combined))  # Candidate values\n",
    "        o_t = torch.sigmoid(self.W_o(combined))  # Output gate\n",
    "        \n",
    "        # Update cell state\n",
    "        c_t = f_t * c_prev + i_t * c_tilde\n",
    "        \n",
    "        # Update hidden state\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t, (f_t, i_t, c_tilde, o_t)\n",
    "\n",
    "class CustomLSTM(nn.Module):\n",
    "    \"\"\"Custom LSTM implementation with comprehensive analysis capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.0):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm_cells = nn.ModuleList([\n",
    "            LSTMCell(input_size if i == 0 else hidden_size, hidden_size)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout_layer = nn.Dropout(dropout) if dropout > 0 else None\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Initialize output projection\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight)\n",
    "        nn.init.zeros_(self.output_projection.bias)\n",
    "    \n",
    "    def forward(self, x, hidden=None, return_sequences=True):\n",
    "        \"\"\"\n",
    "        Forward pass through LSTM\n",
    "        x: (seq_len, batch_size, input_size)\n",
    "        hidden: tuple of (h_0, c_0) each (num_layers, batch_size, hidden_size)\n",
    "        return_sequences: if True, return all outputs; if False, return only last\n",
    "        \"\"\"\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "        \n",
    "        # Initialize hidden states if not provided\n",
    "        if hidden is None:\n",
    "            h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n",
    "            c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n",
    "            hidden = (h_0, c_0)\n",
    "        \n",
    "        h_t, c_t = hidden\n",
    "        outputs = []\n",
    "        all_hidden_states = []\n",
    "        all_cell_states = []\n",
    "        all_gates = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            layer_input = x[t]  # (batch_size, input_size)\n",
    "            \n",
    "            new_h = []\n",
    "            new_c = []\n",
    "            layer_gates = []\n",
    "            \n",
    "            for layer in range(self.num_layers):\n",
    "                h_prev = h_t[layer]  # (batch_size, hidden_size)\n",
    "                c_prev = c_t[layer]  # (batch_size, hidden_size)\n",
    "                \n",
    "                # LSTM cell forward pass\n",
    "                h_new, c_new, gates = self.lstm_cells[layer](layer_input, (h_prev, c_prev))\n",
    "                \n",
    "                # Apply dropout between layers\n",
    "                if self.dropout_layer and layer < self.num_layers - 1:\n",
    "                    h_new = self.dropout_layer(h_new)\n",
    "                \n",
    "                new_h.append(h_new)\n",
    "                new_c.append(c_new)\n",
    "                layer_gates.append(gates)\n",
    "                \n",
    "                # Output of this layer becomes input to next layer\n",
    "                layer_input = h_new\n",
    "            \n",
    "            # Stack layer outputs\n",
    "            h_t = torch.stack(new_h, dim=0)  # (num_layers, batch_size, hidden_size)\n",
    "            c_t = torch.stack(new_c, dim=0)  # (num_layers, batch_size, hidden_size)\n",
    "            \n",
    "            # Store states for analysis\n",
    "            all_hidden_states.append(h_t)\n",
    "            all_cell_states.append(c_t)\n",
    "            all_gates.append(layer_gates)\n",
    "            \n",
    "            # Compute output from top layer\n",
    "            if return_sequences or t == seq_len - 1:\n",
    "                output = self.output_projection(h_t[-1])  # Use top layer hidden state\n",
    "                outputs.append(output)\n",
    "        \n",
    "        # Stack outputs based on return_sequences\n",
    "        if return_sequences:\n",
    "            outputs = torch.stack(outputs, dim=0)  # (seq_len, batch_size, output_size)\n",
    "        else:\n",
    "            outputs = outputs[0]  # (batch_size, output_size) - only last output\n",
    "        \n",
    "        return outputs, (h_t, c_t), {\n",
    "            'hidden_states': all_hidden_states,\n",
    "            'cell_states': all_cell_states,\n",
    "            'gates': all_gates\n",
    "        }\n",
    "\n",
    "class LSTMAnalyzer:\n",
    "    \"\"\"Comprehensive LSTM behavior analysis and gate activation studies\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "    \n",
    "    def analyze_gate_activations(self, sequence_data, save_path):\n",
    "        \"\"\"Analyze LSTM gate activations over time with statistical insights\"\"\"\n",
    "        print(\"üß† Conducting comprehensive LSTM gate activation analysis...\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs, final_hidden, analysis_data = self.model(sequence_data)\n",
    "        \n",
    "        # Extract gate activations (focus on first layer)\n",
    "        gates_over_time = analysis_data['gates']\n",
    "        seq_len = len(gates_over_time)\n",
    "        \n",
    "        # Collect gate statistics\n",
    "        forget_gates = []\n",
    "        input_gates = []\n",
    "        output_gates = []\n",
    "        candidate_values = []\n",
    "        \n",
    "        # Statistics for each gate\n",
    "        gate_stats = {\n",
    "            'forget': {'mean': [], 'std': [], 'min': [], 'max': []},\n",
    "            'input': {'mean': [], 'std': [], 'min': [], 'max': []},\n",
    "            'output': {'mean': [], 'std': [], 'min': [], 'max': []},\n",
    "            'candidate': {'mean': [], 'std': [], 'min': [], 'max': []}\n",
    "        }\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Get gates from first layer\n",
    "            f_t, i_t, c_tilde, o_t = gates_over_time[t][0]  # First layer\n",
    "            \n",
    "            # Collect means for plotting\n",
    "            forget_gates.append(f_t.mean().item())\n",
    "            input_gates.append(i_t.mean().item())\n",
    "            output_gates.append(o_t.mean().item())\n",
    "            candidate_values.append(c_tilde.mean().item())\n",
    "            \n",
    "            # Collect detailed statistics\n",
    "            for gate_name, gate_tensor in [('forget', f_t), ('input', i_t), \n",
    "                                          ('output', o_t), ('candidate', c_tilde)]:\n",
    "                gate_stats[gate_name]['mean'].append(gate_tensor.mean().item())\n",
    "                gate_stats[gate_name]['std'].append(gate_tensor.std().item())\n",
    "                gate_stats[gate_name]['min'].append(gate_tensor.min().item())\n",
    "                gate_stats[gate_name]['max'].append(gate_tensor.max().item())\n",
    "        \n",
    "        # Create comprehensive visualization\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(16, 15))\n",
    "        \n",
    "        time_steps = range(seq_len)\n",
    "        \n",
    "        # Plot 1: Forget gate analysis\n",
    "        axes[0, 0].plot(time_steps, forget_gates, 'o-', color='red', linewidth=2, markersize=6)\n",
    "        axes[0, 0].fill_between(time_steps, \n",
    "                               np.array(gate_stats['forget']['mean']) - np.array(gate_stats['forget']['std']),\n",
    "                               np.array(gate_stats['forget']['mean']) + np.array(gate_stats['forget']['std']),\n",
    "                               alpha=0.3, color='red')\n",
    "        axes[0, 0].set_title('Forget Gate Activations', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Time Step')\n",
    "        axes[0, 0].set_ylabel('Average Activation')\n",
    "        axes[0, 0].set_ylim(0, 1)\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Plot 2: Input gate analysis\n",
    "        axes[0, 1].plot(time_steps, input_gates, 'o-', color='blue', linewidth=2, markersize=6)\n",
    "        axes[0, 1].fill_between(time_steps, \n",
    "                               np.array(gate_stats['input']['mean']) - np.array(gate_stats['input']['std']),\n",
    "                               np.array(gate_stats['input']['mean']) + np.array(gate_stats['input']['std']),\n",
    "                               alpha=0.3, color='blue')\n",
    "        axes[0, 1].set_title('Input Gate Activations', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Time Step')\n",
    "        axes[0, 1].set_ylabel('Average Activation')\n",
    "        axes[0, 1].set_ylim(0, 1)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].axhline(y=0.5, color='blue', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Plot 3: Output gate analysis\n",
    "        axes[1, 0].plot(time_steps, output_gates, 'o-', color='green', linewidth=2, markersize=6)\n",
    "        axes[1, 0].fill_between(time_steps, \n",
    "                               np.array(gate_stats['output']['mean']) - np.array(gate_stats['output']['std']),\n",
    "                               np.array(gate_stats['output']['mean']) + np.array(gate_stats['output']['std']),\n",
    "                               alpha=0.3, color='green')\n",
    "        axes[1, 0].set_title('Output Gate Activations', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Time Step')\n",
    "        axes[1, 0].set_ylabel('Average Activation')\n",
    "        axes[1, 0].set_ylim(0, 1)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].axhline(y=0.5, color='green', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "        axes[1, 0].legend()\n",
    "        \n",
    "        # Plot 4: All gates comparison\n",
    "        axes[1, 1].plot(time_steps, forget_gates, 'o-', label='Forget', color='red', linewidth=2, markersize=4)\n",
    "        axes[1, 1].plot(time_steps, input_gates, 's-', label='Input', color='blue', linewidth=2, markersize=4)\n",
    "        axes[1, 1].plot(time_steps, output_gates, '^-', label='Output', color='green', linewidth=2, markersize=4)\n",
    "        axes[1, 1].set_title('Gate Activations Comparison', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Time Step')\n",
    "        axes[1, 1].set_ylabel('Average Activation')\n",
    "        axes[1, 1].set_ylim(0, 1)\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 5: Gate activation distributions\n",
    "        gate_names = ['Forget', 'Input', 'Output']\n",
    "        gate_data = [forget_gates, input_gates, output_gates]\n",
    "        colors = ['red', 'blue', 'green']\n",
    "        \n",
    "        bp = axes[2, 0].boxplot(gate_data, labels=gate_names, patch_artist=True)\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        axes[2, 0].set_title('Gate Activation Distributions', fontweight='bold')\n",
    "        axes[2, 0].set_ylabel('Activation Value')\n",
    "        axes[2, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 6: Gate stability metrics\n",
    "        gate_stabilities = {\n",
    "            'Forget': np.std(forget_gates),\n",
    "            'Input': np.std(input_gates),\n",
    "            'Output': np.std(output_gates)\n",
    "        }\n",
    "        \n",
    "        bars = axes[2, 1].bar(gate_stabilities.keys(), gate_stabilities.values(), \n",
    "                             color=colors, alpha=0.7)\n",
    "        axes[2, 1].set_title('Gate Stability (Standard Deviation)', fontweight='bold')\n",
    "        axes[2, 1].set_ylabel('Standard Deviation')\n",
    "        axes[2, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, stability in zip(bars, gate_stabilities.values()):\n",
    "            height = bar.get_height()\n",
    "            axes[2, 1].text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                           f'{stability:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"üíæ Comprehensive gate analysis saved to: {save_path}\")\n",
    "        \n",
    "        return {\n",
    "            'gate_activations': {\n",
    "                'forget_gates': forget_gates,\n",
    "                'input_gates': input_gates,\n",
    "                'output_gates': output_gates,\n",
    "                'candidate_values': candidate_values\n",
    "            },\n",
    "            'gate_statistics': gate_stats,\n",
    "            'stability_metrics': gate_stabilities\n",
    "        }\n",
    "    \n",
    "    def compare_cell_vs_hidden_states(self, sequence_data, save_path):\n",
    "        \"\"\"Compare cell state vs hidden state evolution with detailed analysis\"\"\"\n",
    "        print(\"üìä Conducting detailed cell state vs hidden state comparison...\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs, final_hidden, analysis_data = self.model(sequence_data)\n",
    "        \n",
    "        hidden_states = analysis_data['hidden_states']\n",
    "        cell_states = analysis_data['cell_states']\n",
    "        \n",
    "        # Analyze first layer, first batch item for detailed view\n",
    "        # and average across batch for population statistics\n",
    "        \n",
    "        # Individual sample analysis (first batch item)\n",
    "        hidden_norms = [h[0, 0].norm().item() for h in hidden_states]\n",
    "        cell_norms = [c[0, 0].norm().item() for c in cell_states]\n",
    "        hidden_means = [h[0, 0].mean().item() for h in hidden_states]\n",
    "        cell_means = [c[0, 0].mean().item() for c in cell_states]\n",
    "        \n",
    "        # Population statistics (average across batch)\n",
    "        hidden_pop_norms = [h[0].norm(dim=1).mean().item() for h in hidden_states]\n",
    "        cell_pop_norms = [c[0].norm(dim=1).mean().item() for c in cell_states]\n",
    "        \n",
    "        # Compute correlations and dynamics\n",
    "        hidden_derivatives = np.diff(hidden_norms)\n",
    "        cell_derivatives = np.diff(cell_norms)\n",
    "        \n",
    "        # Create comprehensive visualization\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(16, 15))\n",
    "        \n",
    "        time_steps = range(len(hidden_norms))\n",
    "        \n",
    "        # Plot 1: State magnitudes comparison\n",
    "        axes[0, 0].plot(time_steps, hidden_norms, 'o-', label='Hidden State', \n",
    "                       color='blue', linewidth=2, markersize=6)\n",
    "        axes[0, 0].plot(time_steps, cell_norms, 's-', label='Cell State', \n",
    "                       color='red', linewidth=2, markersize=6)\n",
    "        axes[0, 0].plot(time_steps, hidden_pop_norms, '--', label='Hidden (Pop. Avg)', \n",
    "                       color='lightblue', linewidth=2, alpha=0.7)\n",
    "        axes[0, 0].plot(time_steps, cell_pop_norms, '--', label='Cell (Pop. Avg)', \n",
    "                       color='lightcoral', linewidth=2, alpha=0.7)\n",
    "        axes[0, 0].set_title('State Magnitudes Over Time', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Time Step')\n",
    "        axes[0, 0].set_ylabel('L2 Norm')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Mean activations comparison\n",
    "        axes[0, 1].plot(time_steps, hidden_means, 'o-', label='Hidden State', \n",
    "                       color='blue', linewidth=2, markersize=6)\n",
    "        axes[0, 1].plot(time_steps, cell_means, 's-', label='Cell State', \n",
    "                       color='red', linewidth=2, markersize=6)\n",
    "        axes[0, 1].set_title('Mean Activations Over Time', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Time Step')\n",
    "        axes[0, 1].set_ylabel('Mean Value')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Hidden state evolution heatmap\n",
    "        hidden_matrix = torch.stack([h[0, 0] for h in hidden_states]).cpu().numpy()\n",
    "        im1 = axes[1, 0].imshow(hidden_matrix.T, aspect='auto', cmap='RdBu', \n",
    "                               vmin=-2, vmax=2, interpolation='nearest')\n",
    "        axes[1, 0].set_title('Hidden State Evolution', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Time Step')\n",
    "        axes[1, 0].set_ylabel('Hidden Dimension')\n",
    "        cbar1 = plt.colorbar(im1, ax=axes[1, 0], fraction=0.046)\n",
    "        cbar1.set_label('Activation Value')\n",
    "        \n",
    "        # Plot 4: Cell state evolution heatmap\n",
    "        cell_matrix = torch.stack([c[0, 0] for c in cell_states]).cpu().numpy()\n",
    "        im2 = axes[1, 1].imshow(cell_matrix.T, aspect='auto', cmap='RdBu', \n",
    "                               vmin=-2, vmax=2, interpolation='nearest')\n",
    "        axes[1, 1].set_title('Cell State Evolution', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Time Step')\n",
    "        axes[1, 1].set_ylabel('Cell Dimension')\n",
    "        cbar2 = plt.colorbar(im2, ax=axes[1, 1], fraction=0.046)\n",
    "        cbar2.set_label('Activation Value')\n",
    "        \n",
    "        # Plot 5: State dynamics (derivatives)\n",
    "        axes[2, 0].plot(time_steps[1:], hidden_derivatives, 'o-', label='Hidden State Changes', \n",
    "                       color='blue', linewidth=2, markersize=6)\n",
    "        axes[2, 0].plot(time_steps[1:], cell_derivatives, 's-', label='Cell State Changes', \n",
    "                       color='red', linewidth=2, markersize=6)\n",
    "        axes[2, 0].set_title('State Change Dynamics', fontweight='bold')\n",
    "        axes[2, 0].set_xlabel('Time Step')\n",
    "        axes[2, 0].set_ylabel('Change in Norm')\n",
    "        axes[2, 0].legend()\n",
    "        axes[2, 0].grid(True, alpha=0.3)\n",
    "        axes[2, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Plot 6: State correlation and summary statistics\n",
    "        if len(hidden_norms) > 1:\n",
    "            correlation = np.corrcoef(hidden_norms, cell_norms)[0, 1]\n",
    "        else:\n",
    "            correlation = 0.0\n",
    "        \n",
    "        # Summary statistics\n",
    "        stats_data = {\n",
    "            'Hidden Stability': np.std(hidden_norms),\n",
    "            'Cell Stability': np.std(cell_norms),\n",
    "            'Hidden Range': max(hidden_norms) - min(hidden_norms),\n",
    "            'Cell Range': max(cell_norms) - min(cell_norms)\n",
    "        }\n",
    "        \n",
    "        bars = axes[2, 1].bar(range(len(stats_data)), list(stats_data.values()), \n",
    "                             color=['blue', 'red', 'lightblue', 'lightcoral'], alpha=0.7)\n",
    "        axes[2, 1].set_xticks(range(len(stats_data)))\n",
    "        axes[2, 1].set_xticklabels(list(stats_data.keys()), rotation=45, ha='right')\n",
    "        axes[2, 1].set_title(f'State Statistics\\\\nCorrelation: {correlation:.3f}', fontweight='bold')\n",
    "        axes[2, 1].set_ylabel('Value')\n",
    "        axes[2, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, (key, value) in zip(bars, stats_data.items()):\n",
    "            height = bar.get_height()\n",
    "            axes[2, 1].text(bar.get_x() + bar.get_width()/2., height + max(stats_data.values())*0.01,\n",
    "                           f'{value:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"üíæ Comprehensive state comparison saved to: {save_path}\")\n",
    "        \n",
    "        return {\n",
    "            'individual_analysis': {\n",
    "                'hidden_norms': hidden_norms,\n",
    "                'cell_norms': cell_norms,\n",
    "                'hidden_means': hidden_means,\n",
    "                'cell_means': cell_means\n",
    "            },\n",
    "            'population_analysis': {\n",
    "                'hidden_pop_norms': hidden_pop_norms,\n",
    "                'cell_pop_norms': cell_pop_norms\n",
    "            },\n",
    "            'dynamics': {\n",
    "                'hidden_derivatives': hidden_derivatives.tolist(),\n",
    "                'cell_derivatives': cell_derivatives.tolist()\n",
    "            },\n",
    "            'statistics': {\n",
    "                'correlation': float(correlation),\n",
    "                'stability_metrics': stats_data\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create and analyze LSTM\n",
    "print(\"\\nüß† Creating Custom LSTM for comprehensive analysis...\")\n",
    "\n",
    "# Enhanced model parameters\n",
    "lstm_model = CustomLSTM(input_size=input_size, hidden_size=hidden_size, \n",
    "                       output_size=output_size, num_layers=1, dropout=0.1).to(device)\n",
    "\n",
    "lstm_model_info = {\n",
    "    'architecture': 'Custom LSTM',\n",
    "    'input_size': input_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'output_size': output_size,\n",
    "    'num_layers': 1,\n",
    "    'dropout': 0.1,\n",
    "    'total_parameters': sum(p.numel() for p in lstm_model.parameters()),\n",
    "    'gate_parameters': sum(p.numel() for cell in lstm_model.lstm_cells for p in cell.parameters())\n",
    "}\n",
    "\n",
    "print(f\"   Architecture: {lstm_model_info['architecture']}\")\n",
    "print(f\"   Total parameters: {lstm_model_info['total_parameters']:,}\")\n",
    "print(f\"   Gate parameters: {lstm_model_info['gate_parameters']:,}\")\n",
    "print(f\"   Dropout rate: {lstm_model_info['dropout']}\")\n",
    "\n",
    "# Test LSTM forward pass\n",
    "test_input = torch.randn(15, 3, input_size, device=device)\n",
    "print(f\"\\nüß™ Testing LSTM forward pass:\")\n",
    "print(f\"   Input shape: {test_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    lstm_outputs, lstm_hidden, lstm_analysis = lstm_model(test_input)\n",
    "    print(f\"   Output shape: {lstm_outputs.shape}\")\n",
    "    print(f\"   Hidden state shape: {lstm_hidden[0].shape}\")\n",
    "    print(f\"   Cell state shape: {lstm_hidden[1].shape}\")\n",
    "\n",
    "# Analyze LSTM behavior\n",
    "lstm_analyzer = LSTMAnalyzer(lstm_model, device)\n",
    "\n",
    "# Create test sequence with patterns for more interesting analysis\n",
    "seq_len, batch_size = 20, 4\n",
    "test_sequence = torch.randn(seq_len, batch_size, input_size, device=device)\n",
    "\n",
    "# Add structured patterns to make analysis more meaningful\n",
    "for t in range(seq_len):\n",
    "    # Sinusoidal pattern in first dimension\n",
    "    test_sequence[t, :, 0] = torch.sin(torch.tensor(t * 0.3))\n",
    "    # Linear trend in second dimension\n",
    "    test_sequence[t, :, 1] = torch.tensor(t * 0.1)\n",
    "    # Random walk in third dimension\n",
    "    if t > 0:\n",
    "        test_sequence[t, :, 2] = test_sequence[t-1, :, 2] + torch.randn(batch_size) * 0.1\n",
    "\n",
    "print(\"\\nüîç Conducting comprehensive LSTM gate activation analysis...\")\n",
    "gate_analysis = lstm_analyzer.analyze_gate_activations(\n",
    "    test_sequence,\n",
    "    notebook_results_dir / \"lstm_analysis/comprehensive_gate_analysis.png\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Conducting detailed hidden vs cell state comparison...\")\n",
    "state_analysis = lstm_analyzer.compare_cell_vs_hidden_states(\n",
    "    test_sequence,\n",
    "    notebook_results_dir / \"lstm_analysis/comprehensive_state_comparison.png\"\n",
    ")\n",
    "\n",
    "# Compile LSTM analysis results\n",
    "lstm_analysis_results = {\n",
    "    'model_info': lstm_model_info,\n",
    "    'gate_analysis': {\n",
    "        'avg_forget_activation': float(np.mean(gate_analysis['gate_activations']['forget_gates'])),\n",
    "        'avg_input_activation': float(np.mean(gate_analysis['gate_activations']['input_gates'])),\n",
    "        'avg_output_activation': float(np.mean(gate_analysis['gate_activations']['output_gates'])),\n",
    "        'gate_stability': gate_analysis['stability_metrics']\n",
    "    },\n",
    "    'state_analysis': {\n",
    "        'hidden_cell_correlation': state_analysis['statistics']['correlation'],\n",
    "        'stability_metrics': state_analysis['statistics']['stability_metrics']\n",
    "    },\n",
    "    'sequence_info': {\n",
    "        'length': seq_len,\n",
    "        'batch_size': batch_size,\n",
    "        'pattern_types': ['sinusoidal', 'linear_trend', 'random_walk']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nüìà LSTM Analysis Results Summary:\")\n",
    "print(f\"   Average forget gate activation: {lstm_analysis_results['gate_analysis']['avg_forget_activation']:.3f}\")\n",
    "print(f\"   Average input gate activation: {lstm_analysis_results['gate_analysis']['avg_input_activation']:.3f}\")\n",
    "print(f\"   Average output gate activation: {lstm_analysis_results['gate_analysis']['avg_output_activation']:.3f}\")\n",
    "print(f\"   Hidden-cell state correlation: {lstm_analysis_results['state_analysis']['hidden_cell_correlation']:.3f}\")\n",
    "print(f\"   Hidden state stability: {lstm_analysis_results['state_analysis']['stability_metrics']['Hidden Stability']:.3f}\")\n",
    "print(f\"   Cell state stability: {lstm_analysis_results['state_analysis']['stability_metrics']['Cell Stability']:.3f}\")\n",
    "```\n",
    "\n",
    "## 4. Architecture Comparison and Performance Analysis\n",
    "\n",
    "### 4.1 RNN vs LSTM vs GRU Comparison\n",
    "\n",
    "```python\n",
    "class ArchitectureComparator:\n",
    "    \"\"\"Comprehensive comparison of RNN, LSTM, and GRU architectures\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, device):\n",
    "        self.device = device\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Create models for comparison\n",
    "        self.models = {\n",
    "            'Vanilla_RNN': VanillaRNN(input_size, hidden_size, output_size).to(device),\n",
    "            'Custom_LSTM': CustomLSTM(input_size, hidden_size, output_size).to(device),\n",
    "            'PyTorch_GRU': self._create_gru_model(input_size, hidden_size, output_size).to(device)\n",
    "        }\n",
    "        \n",
    "        self.results = {}\n",
    "    \n",
    "    def _create_gru_model(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"Create GRU model with output layer\"\"\"\n",
    "        class GRUModel(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, output_size):\n",
    "                super().__init__()\n",
    "                self.gru = nn.GRU(input_size, hidden_size, batch_first=False)\n",
    "                self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "                self.hidden_size = hidden_size\n",
    "                self.output_size = output_size\n",
    "                \n",
    "            def forward(self, x, hidden=None):\n",
    "                gru_out, hidden = self.gru(x, hidden)\n",
    "                outputs = self.output_layer(gru_out)\n",
    "                return outputs, hidden, {}\n",
    "        \n",
    "        return GRUModel(input_size, hidden_size, output_size)\n",
    "    \n",
    "    def compare_gradient_flow(self, sequence_lengths, save_path):\n",
    "        \"\"\"Compare gradient flow across different sequence lengths and architectures\"\"\"\n",
    "        print(\"üîç Conducting comprehensive gradient flow comparison...\")\n",
    "        \n",
    "        gradient_data = {name: [] for name in self.models.keys()}\n",
    "        training_times = {name: [] for name in self.models.keys()}\n",
    "        memory_usage = {name: [] for name in self.models.keys()}\n",
    "        \n",
    "        for seq_len in tqdm(sequence_lengths, desc=\"Testing sequence lengths\"):\n",
    "            for name, model in self.models.items():\n",
    "                # Measure memory and time\n",
    "                start_time = time.time()\n",
    "                \n",
    "                try:\n",
    "                    # Create test data\n",
    "                    x = torch.randn(seq_len, 2, self.input_size, device=self.device, requires_grad=True)\n",
    "                    target = torch.randn(seq_len, 2, self.output_size, device=self.device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    model.zero_grad()\n",
    "                    outputs, final_hidden, _ = model(x)\n",
    "                    \n",
    "                    # Compute loss on all time steps\n",
    "                    loss = F.mse_loss(outputs, target)\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Measure gradient at input (early time step)\n",
    "                    if x.grad is not None and seq_len > 5:\n",
    "                        # Take gradient from 5th time step to see long-range effects\n",
    "                        early_grad_norm = x.grad[4].norm().item()\n",
    "                    else:\n",
    "                        early_grad_norm = 0.0\n",
    "                    \n",
    "                    gradient_data[name].append(early_grad_norm)\n",
    "                    \n",
    "                    # Measure time\n",
    "                    end_time = time.time()\n",
    "                    training_times[name].append(end_time - start_time)\n",
    "                    \n",
    "                    # Measure approximate memory usage\n",
    "                    if torch.cuda.is_available():\n",
    "                        memory_usage[name].append(torch.cuda.memory_allocated() / 1024**3)  # GB\n",
    "                    else:\n",
    "                        memory_usage[name].append(0.0)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {name} at seq_len {seq_len}: {e}\")\n",
    "                    gradient_data[name].append(0.0)\n",
    "                    training_times[name].append(0.0)\n",
    "                    memory_usage[name].append(0.0)\n",
    "                \n",
    "                # Clear memory\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        # Create comprehensive comparison visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        \n",
    "        colors = {'Vanilla_RNN': 'red', 'Custom_LSTM': 'blue', 'PyTorch_GRU': 'green'}\n",
    "        markers = {'Vanilla_RNN': 'o', 'Custom_LSTM': 's', 'PyTorch_GRU': '^'}\n",
    "        \n",
    "        # Plot 1: Gradient flow comparison\n",
    "        for name, grads in gradient_data.items():\n",
    "            axes[0, 0].plot(sequence_lengths, grads, marker=markers[name], \n",
    "                           label=name.replace('_', ' '), linewidth=2, markersize=6, \n",
    "                           color=colors[name])\n",
    "        \n",
    "        axes[0, 0].set_xlabel('Sequence Length')\n",
    "        axes[0, 0].set_ylabel('Early Time Step Gradient Norm')\n",
    "        axes[0, 0].set_title('Gradient Flow Comparison', fontweight='bold')\n",
    "        axes[0, 0].set_yscale('log')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Training time comparison\n",
    "        for name, times in training_times.items():\n",
    "            axes[0, 1].plot(sequence_lengths, times, marker=markers[name], \n",
    "                           label=name.replace('_', ' '), linewidth=2, markersize=6,\n",
    "                           color=colors[name])\n",
    "        \n",
    "        axes[0, 1].set_xlabel('Sequence Length')\n",
    "        axes[0, 1].set_ylabel('Training Time (seconds)')\n",
    "        axes[0, 1].set_title('Training Time Comparison', fontweight='bold')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Memory usage comparison\n",
    "        if torch.cuda.is_available():\n",
    "            for name, memory in memory_usage.items():\n",
    "                if any(m > 0 for m in memory):\n",
    "                    axes[0, 2].plot(sequence_lengths, memory, marker=markers[name], \n",
    "                                   label=name.replace('_', ' '), linewidth=2, markersize=6,\n",
    "                                   color=colors[name])\n",
    "            \n",
    "            axes[0, 2].set_xlabel('Sequence Length')\n",
    "            axes[0, 2].set_ylabel('Memory Usage (GB)')\n",
    "            axes[0, 2].set_title('Memory Usage Comparison', fontweight='bold')\n",
    "            axes[0, 2].legend()\n",
    "            axes[0, 2].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[0, 2].text(0.5, 0.5, 'GPU Memory\\nTracking\\nNot Available', \n",
    "                           ha='center', va='center', transform=axes[0, 2].transAxes,\n",
    "                           fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "            axes[0, 2].set_title('Memory Usage Comparison', fontweight='bold')\n",
    "        \n",
    "        # Plot 4: Parameter count comparison\n",
    "        param_counts = {}\n",
    "        param_details = {}\n",
    "        for name, model in self.models.items():\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            param_counts[name] = total_params\n",
    "            param_details[name] = {'total': total_params, 'trainable': trainable_params}\n",
    "        \n",
    "        names = [name.replace('_', ' ') for name in param_counts.keys()]\n",
    "        counts = list(param_counts.values())\n",
    "        bar_colors = [colors[name] for name in param_counts.keys()]\n",
    "        \n",
    "        bars = axes[1, 0].bar(names, counts, color=bar_colors, alpha=0.7)\n",
    "        axes[1, 0].set_ylabel('Number of Parameters')\n",
    "        axes[1, 0].set_title('Parameter Count Comparison', fontweight='bold')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, counts):\n",
    "            height = bar.get_height()\n",
    "            axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + max(counts)*0.01,\n",
    "                           f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Plot 5: Gradient stability metrics\n",
    "        stability_scores = {}\n",
    "        for name, grads in gradient_data.items():\n",
    "            if len(grads) > 1 and any(g > 0 for g in grads):\n",
    "                # Coefficient of variation as stability metric\n",
    "                cv = np.std(grads) / (np.mean(grads) + 1e-8)\n",
    "                stability_scores[name] = cv\n",
    "            else:\n",
    "                stability_scores[name] = float('inf')\n",
    "        \n",
    "        stable_names = [name.replace('_', ' ') for name in stability_scores.keys()]\n",
    "        stable_scores = list(stability_scores.values())\n",
    "        stable_colors = [colors[name] for name in stability_scores.keys()]\n",
    "        \n",
    "        bars2 = axes[1, 1].bar(stable_names, stable_scores, color=stable_colors, alpha=0.7)\n",
    "        axes[1, 1].set_ylabel('Gradient Variability (CV)')\n",
    "        axes[1, 1].set_title('Gradient Stability Comparison', fontweight='bold')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars2, stable_scores):\n",
    "            if score != float('inf'):\n",
    "                height = bar.get_height()\n",
    "                axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + max([s for s in stable_scores if s != float('inf')])*0.01,\n",
    "                               f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Plot 6: Architecture efficiency summary\n",
    "        efficiency_metrics = {}\n",
    "        for name in self.models.keys():\n",
    "            # Compute efficiency as inverse of (parameters * avg_time)\n",
    "            avg_time = np.mean(training_times[name]) if training_times[name] else 1.0\n",
    "            efficiency = 1.0 / (param_counts[name] * avg_time + 1e-8)\n",
    "            efficiency_metrics[name] = efficiency * 1e6  # Scale for readability\n",
    "        \n",
    "        eff_names = [name.replace('_', ' ') for name in efficiency_metrics.keys()]\n",
    "        eff_scores = list(efficiency_metrics.values())\n",
    "        eff_colors = [colors[name] for name in efficiency_metrics.keys()]\n",
    "        \n",
    "        bars3 = axes[1, 2].bar(eff_names, eff_scores, color=eff_colors, alpha=0.7)\n",
    "        axes[1, 2].set_ylabel('Efficiency Score (1e-6)')\n",
    "        axes[1, 2].set_title('Computational Efficiency', fontweight='bold')\n",
    "        axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars3, eff_scores):\n",
    "            height = bar.get_height()\n",
    "            axes[1, 2].text(bar.get_x() + bar.get_width()/2., height + max(eff_scores)*0.01,\n",
    "                           f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"üíæ Architecture comparison saved to: {save_path}\")\n",
    "        \n",
    "        return {\n",
    "            'gradient_data': gradient_data,\n",
    "            'training_times': training_times,\n",
    "            'memory_usage': memory_usage,\n",
    "            'parameter_counts': param_counts,\n",
    "            'parameter_details': param_details,\n",
    "            'stability_scores': stability_scores,\n",
    "            'efficiency_metrics': efficiency_metrics\n",
    "        }\n",
    "    \n",
    "    def compare_learning_capacity(self, save_path):\n",
    "        \"\"\"Compare learning capacity through simple sequence tasks\"\"\"\n",
    "        print(\"üéØ Evaluating learning capacity on sequence tasks...\")\n",
    "        \n",
    "        # Create simple sequence learning tasks\n",
    "        tasks = {\n",
    "            'copy_task': self._create_copy_task,\n",
    "            'sum_task': self._create_sum_task,\n",
    "            'pattern_task': self._create_pattern_task\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for task_name, task_fn in tasks.items():\n",
    "            print(f\"   Testing {task_name}...\")\n",
    "            task_results = {}\n",
    "            \n",
    "            for model_name, model in self.models.items():\n",
    "                # Generate task data\n",
    "                train_data, test_data = task_fn()\n",
    "                \n",
    "                # Simple training loop\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "                losses = []\n",
    "                \n",
    "                model.train()\n",
    "                for epoch in range(50):  # Quick training\n",
    "                    total_loss = 0\n",
    "                    for x, y in train_data:\n",
    "                        x, y = x.to(self.device), y.to(self.device)\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        outputs, _, _ = model(x)\n",
    "                        loss = F.mse_loss(outputs, y)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        total_loss += loss.item()\n",
    "                    \n",
    "                    losses.append(total_loss / len(train_data))\n",
    "                \n",
    "                # Test performance\n",
    "                model.eval()\n",
    "                test_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for x, y in test_data:\n",
    "                        x, y = x.to(self.device), y.to(self.device)\n",
    "                        outputs, _, _ = model(x)\n",
    "                        test_loss += F.mse_loss(outputs, y).item()\n",
    "                \n",
    "                test_loss /= len(test_data)\n",
    "                \n",
    "                task_results[model_name] = {\n",
    "                    'training_losses': losses,\n",
    "                    'final_test_loss': test_loss,\n",
    "                    'convergence_epoch': next((i for i, loss in enumerate(losses) if loss < 0.1), 50)\n",
    "                }\n",
    "            \n",
    "            results[task_name] = task_results\n",
    "        \n",
    "        # Visualize learning capacity results\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Plot learning curves for each task\n",
    "        task_names = list(tasks.keys())\n",
    "        colors = {'Vanilla_RNN': 'red', 'Custom_LSTM': 'blue', 'PyTorch_GRU': 'green'}\n",
    "        \n",
    "        for i, task_name in enumerate(task_names):\n",
    "            ax = axes[i//2, i%2] if i < 3 else axes[1, 1]\n",
    "            \n",
    "            for model_name, model_results in results[task_name].items():\n",
    "                epochs = range(len(model_results['training_losses']))\n",
    "                ax.plot(epochs, model_results['training_losses'], \n",
    "                       label=model_name.replace('_', ' '), \n",
    "                       color=colors[model_name], linewidth=2)\n",
    "            \n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel('Training Loss')\n",
    "            ax.set_title(f'{task_name.replace(\"_\", \" \").title()} Learning', fontweight='bold')\n",
    "            ax.set_yscale('log')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Summary comparison in the fourth subplot\n",
    "        if len(task_names) < 4:\n",
    "            axes[1, 1].clear()\n",
    "            \n",
    "            # Create summary metrics\n",
    "            model_names = list(self.models.keys())\n",
    "            test_performances = []\n",
    "            convergence_speeds = []\n",
    "            \n",
    "            for model_name in model_names:\n",
    "                avg_test_loss = np.mean([results[task][model_name]['final_test_loss'] \n",
    "                                       for task in task_names])\n",
    "                avg_convergence = np.mean([results[task][model_name]['convergence_epoch'] \n",
    "                                         for task in task_names])\n",
    "                test_performances.append(avg_test_loss)\n",
    "                convergence_speeds.append(avg_convergence)\n",
    "            \n",
    "            x = np.arange(len(model_names))\n",
    "            width = 0.35\n",
    "            \n",
    "            bars1 = axes[1, 1].bar(x - width/2, test_performances, width, \n",
    "                                  label='Test Loss', alpha=0.7, color='orange')\n",
    "            \n",
    "            ax2 = axes[1, 1].twinx()\n",
    "            bars2 = ax2.bar(x + width/2, convergence_speeds, width, \n",
    "                           label='Convergence Epoch', alpha=0.7, color='purple')\n",
    "            \n",
    "            axes[1, 1].set_xlabel('Model')\n",
    "            axes[1, 1].set_ylabel('Average Test Loss', color='orange')\n",
    "            ax2.set_ylabel('Average Convergence Epoch', color='purple')\n",
    "            axes[1, 1].set_title('Overall Learning Performance', fontweight='bold')\n",
    "            axes[1, 1].set_xticks(x)\n",
    "            axes[1, 1].set_xticklabels([name.replace('_', ' ') for name in model_names], rotation=45)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, value in zip(bars1, test_performances):\n",
    "                height = bar.get_height()\n",
    "                axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + max(test_performances)*0.01,\n",
    "                               f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            for bar, value in zip(bars2, convergence_speeds):\n",
    "                height = bar.get_height()\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2., height + max(convergence_speeds)*0.01,\n",
    "                        f'{int(value)}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"üíæ Learning capacity comparison saved to: {save_path}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _create_copy_task(self):\n",
    "        \"\"\"Create copy task: model should output the input sequence\"\"\"\n",
    "        seq_len = 10\n",
    "        num_samples = 20\n",
    "        \n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            x = torch.randn(seq_len, 1, self.input_size)\n",
    "            y = x.clone()  # Copy task\n",
    "            \n",
    "            if i < num_samples * 0.8:\n",
    "                train_data.append((x, y))\n",
    "            else:\n",
    "                test_data.append((x, y))\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    def _create_sum_task(self):\n",
    "        \"\"\"Create sum task: model should output cumulative sum\"\"\"\n",
    "        seq_len = 10\n",
    "        num_samples = 20\n",
    "        \n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            x = torch.randn(seq_len, 1, self.input_size)\n",
    "            y = torch.cumsum(x, dim=0)  # Cumulative sum\n",
    "            \n",
    "            if i < num_samples * 0.8:\n",
    "                train_data.append((x, y))\n",
    "            else:\n",
    "                test_data.append((x, y))\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    def _create_pattern_task(self):\n",
    "        \"\"\"Create pattern recognition task\"\"\"\n",
    "        seq_len = 10\n",
    "        num_samples = 20\n",
    "        \n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Create pattern: alternating positive/negative\n",
    "            x = torch.randn(seq_len, 1, self.input_size)\n",
    "            y = torch.zeros_like(x)\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                if t % 2 == 0:\n",
    "                    y[t] = torch.abs(x[t])  # Positive for even indices\n",
    "                else:\n",
    "                    y[t] = -torch.abs(x[t])  # Negative for odd indices\n",
    "            \n",
    "            if i < num_samples * 0.8:\n",
    "                train_data.append((x, y))\n",
    "            else:\n",
    "                test_data.append((x, y))\n",
    "        \n",
    "        return train_data, test_data\n",
    "\n",
    "# Conduct comprehensive architecture comparison\n",
    "print(\"\\nüîÑ Conducting comprehensive architecture comparison...\")\n",
    "\n",
    "comparator = ArchitectureComparator(input_size, hidden_size, output_size, device)\n",
    "\n",
    "# Compare gradient flow across sequence lengths\n",
    "print(\"\\nüîç Analyzing gradient flow characteristics...\")\n",
    "sequence_lengths_for_comparison = [5, 10, 15, 20, 25, 30]\n",
    "comparison_results = comparator.compare_gradient_flow(\n",
    "    sequence_lengths_for_comparison,\n",
    "    notebook_results_dir / \"sequence_modeling/architecture_comparison.png\"\n",
    ")\n",
    "\n",
    "# Compare learning capacity\n",
    "print(\"\\nüéØ Evaluating learning capacity on sequence tasks...\")\n",
    "learning_results = comparator.compare_learning_capacity(\n",
    "    notebook_results_dir / \"sequence_modeling/learning_capacity_comparison.png\"\n",
    ")\n",
    "\n",
    "# Compile comprehensive comparison results\n",
    "architecture_comparison_results = {\n",
    "    'models_compared': list(comparator.models.keys()),\n",
    "    'sequence_lengths_tested': sequence_lengths_for_comparison,\n",
    "    'gradient_flow_analysis': {\n",
    "        'best_gradient_flow': min(comparison_results['stability_scores'].items(), \n",
    "                                 key=lambda x: x[1] if x[1] != float('inf') else float('inf')),\n",
    "        'parameter_efficiency': comparison_results['parameter_counts'],\n",
    "        'computational_efficiency': comparison_results['efficiency_metrics']\n",
    "    },\n",
    "    'learning_capacity_analysis': {\n",
    "        'tasks_tested': list(learning_results.keys()),\n",
    "        'overall_performance': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate overall learning performance\n",
    "for model_name in comparator.models.keys():\n",
    "    avg_test_loss = np.mean([learning_results[task][model_name]['final_test_loss'] \n",
    "                           for task in learning_results.keys()])\n",
    "    avg_convergence = np.mean([learning_results[task][model_name]['convergence_epoch'] \n",
    "                             for task in learning_results.keys()])\n",
    "    \n",
    "    architecture_comparison_results['learning_capacity_analysis']['overall_performance'][model_name] = {\n",
    "        'average_test_loss': float(avg_test_loss),\n",
    "        'average_convergence_epoch': float(avg_convergence)\n",
    "    }\n",
    "\n",
    "print(f\"\\nüìà Architecture Comparison Results:\")\n",
    "print(f\"   Models compared: {architecture_comparison_results['models_compared']}\")\n",
    "print(f\"   Best gradient flow stability: {architecture_comparison_results['gradient_flow_analysis']['best_gradient_flow'][0]}\")\n",
    "print(f\"   Parameter counts: {comparison_results['parameter_counts']}\")\n",
    "print(f\"   Tasks tested: {architecture_comparison_results['learning_capacity_analysis']['tasks_tested']}\")\n",
    "\n",
    "for model_name, performance in architecture_comparison_results['learning_capacity_analysis']['overall_performance'].items():\n",
    "    print(f\"   {model_name}: Avg test loss = {performance['average_test_loss']:.4f}, \"\n",
    "          f\"Avg convergence = {performance['average_convergence_epoch']:.1f} epochs\")\n",
    "```\n",
    "\n",
    "## 5. Text Generation Application\n",
    "\n",
    "### 5.1 Text Processing and Generation\n",
    "\n",
    "```python\n",
    "class TextDataProcessor:\n",
    "    \"\"\"Advanced text data processor for sequence modeling\"\"\"\n",
    "    \n",
    "    def __init__(self, min_freq=2, max_vocab_size=10000):\n",
    "        self.min_freq = min_freq\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.vocab = {}\n",
    "        self.reverse_vocab = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "        # Special tokens\n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        self.START_TOKEN = '<START>'\n",
    "        self.END_TOKEN = '<END>'\n",
    "        \n",
    "    def create_sample_text_corpus(self):\n",
    "        \"\"\"Create comprehensive sample text corpus for demonstration\"\"\"\n",
    "        texts = [\n",
    "            \"The quick brown fox jumps over the lazy dog in the morning sunlight.\",\n",
    "            \"Machine learning algorithms can learn complex patterns from large datasets.\",\n",
    "            \"Neural networks are inspired by the structure of biological neurons.\",\n",
    "            \"Deep learning has revolutionized computer vision and natural language processing.\",\n",
    "            \"Recurrent neural networks excel at processing sequential data like text and speech.\",\n",
    "            \"Long short-term memory networks solve the vanishing gradient problem.\",\n",
    "            \"Attention mechanisms allow models to focus on relevant parts of input.\",\n",
    "            \"Transformer architectures have become the foundation of modern NLP.\",\n",
    "            \"Language models can generate coherent and contextually appropriate text.\",\n",
    "            \"Pre-trained models enable transfer learning across different tasks.\",\n",
    "            \"Fine-tuning adapts general models to specific domains and applications.\",\n",
    "            \"Artificial intelligence continues to advance at an unprecedented pace.\",\n",
    "            \"Natural language understanding requires both syntax and semantic knowledge.\",\n",
    "            \"Computational linguistics bridges computer science and human language.\",\n",
    "            \"Text generation involves predicting the next word given previous context.\",\n",
    "            \"Sequence-to-sequence models can perform translation and summarization.\",\n",
    "            \"Embeddings capture semantic relationships between words in vector space.\",\n",
    "            \"Tokenization is the first step in most natural language processing pipelines.\",\n",
    "            \"Regularization techniques prevent overfitting in neural language models.\",\n",
    "            \"Evaluation metrics help assess the quality of generated text outputs.\"\n",
    "        ]\n",
    "        return texts\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Advanced text preprocessing with multiple options\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove extra punctuation but keep sentence structure\n",
    "        text = re.sub(r'[^\\w\\s\\.\\!\\?]', '', text)\n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def build_vocabulary(self, texts):\n",
    "        \"\"\"Build comprehensive vocabulary with frequency analysis\"\"\"\n",
    "        print(\"üî§ Building vocabulary with frequency analysis...\")\n",
    "        \n",
    "        # Count word frequencies\n",
    "        word_counts = Counter()\n",
    "        total_words = 0\n",
    "        \n",
    "        for text in texts:\n",
    "            processed_text = self.preprocess_text(text)\n",
    "            words = processed_text.split()\n",
    "            word_counts.update(words)\n",
    "            total_words += len(words)\n",
    "        \n",
    "        print(f\"   Total words processed: {total_words:,}\")\n",
    "        print(f\"   Unique words found: {len(word_counts):,}\")\n",
    "        \n",
    "        # Create vocabulary with special tokens\n",
    "        self.vocab = {\n",
    "            self.PAD_TOKEN: 0,\n",
    "            self.UNK_TOKEN: 1,\n",
    "            self.START_TOKEN: 2,\n",
    "            self.END_TOKEN: 3\n",
    "        }\n",
    "        \n",
    "        # Add words that meet frequency requirement\n",
    "        idx = 4\n",
    "        added_words = 0\n",
    "        for word, count in word_counts.most_common():\n",
    "            if count >= self.min_freq and len(self.vocab) < self.max_vocab_size:\n",
    "                self.vocab[word] = idx\n",
    "                idx += 1\n",
    "                added_words += 1\n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "        # Calculate coverage\n",
    "        covered_words = sum(count for word, count in word_counts.items() \n",
    "                           if word in self.vocab and word not in [self.PAD_TOKEN, self.UNK_TOKEN, \n",
    "                                                                 self.START_TOKEN, self.END_TOKEN])\n",
    "        coverage = covered_words / total_words * 100\n",
    "        \n",
    "        print(f\"   Vocabulary size: {self.vocab_size:,}\")\n",
    "        print(f\"   Words added: {added_words:,}\")\n",
    "        print(f\"   Vocabulary coverage: {coverage:.2f}%\")\n",
    "        \n",
    "        return self.vocab\n",
    "    \n",
    "    def text_to_sequence(self, text, add_special_tokens=True, max_length=None):\n",
    "        \"\"\"Convert text to sequence with optional length limiting\"\"\"\n",
    "        processed_text = self.preprocess_text(text)\n",
    "        words = processed_text.split()\n",
    "        \n",
    "        sequence = []\n",
    "        if add_special_tokens:\n",
    "            sequence.append(self.vocab[self.START_TOKEN])\n",
    "        \n",
    "        for word in words:\n",
    "            sequence.append(self.vocab.get(word, self.vocab[self.UNK_TOKEN]))\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            sequence.append(self.vocab[self.END_TOKEN])\n",
    "        \n",
    "        # Apply max length if specified\n",
    "        if max_length and len(sequence) > max_length:\n",
    "            sequence = sequence[:max_length-1] + [self.vocab[self.END_TOKEN]]\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def sequence_to_text(self, sequence):\n",
    "        \"\"\"Convert sequence back to readable text\"\"\"\n",
    "        words = []\n",
    "        for token_id in sequence:\n",
    "            word = self.reverse_vocab.get(token_id, self.UNK_TOKEN)\n",
    "            if word not in [self.PAD_TOKEN, self.START_TOKEN, self.END_TOKEN]:\n",
    "                words.append(word)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def get_vocabulary_statistics(self):\n",
    "        \"\"\"Get comprehensive vocabulary statistics\"\"\"\n",
    "        return {\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'special_tokens': [self.PAD_TOKEN, self.UNK_TOKEN, self.START_TOKEN, self.END_TOKEN],\n",
    "            'min_frequency': self.min_freq,\n",
    "            'max_vocab_size': self.max_vocab_size\n",
    "        }\n",
    "\n",
    "class TextGenerationDataset(Dataset):\n",
    "    \"\"\"Advanced dataset for text generation with configurable context windows\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, sequence_length, prediction_horizon=1, overlap=True):\n",
    "        self.sequences = sequences\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.overlap = overlap\n",
    "        self.data = self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Prepare input-target pairs with sophisticated windowing\"\"\"\n",
    "        data = []\n",
    "        \n",
    "        for sequence in self.sequences:\n",
    "            if len(sequence) <= self.sequence_length:\n",
    "                continue\n",
    "            \n",
    "            # Create sliding windows\n",
    "            step_size = 1 if self.overlap else self.sequence_length\n",
    "            \n",
    "            for i in range(0, len(sequence) - self.sequence_length - self.prediction_horizon + 1, step_size):\n",
    "                input_seq = sequence[i:i + self.sequence_length]\n",
    "                target_seq = sequence[i + 1:i + self.sequence_length + self.prediction_horizon]\n",
    "                \n",
    "                # Ensure target sequence is the right length\n",
    "                if len(target_seq) == self.sequence_length:\n",
    "                    data.append((input_seq, target_seq))\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_seq = self.data[idx]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
    "    \n",
    "    def get_dataset_statistics(self):\n",
    "        \"\"\"Get comprehensive dataset statistics\"\"\"\n",
    "        return {\n",
    "            'num_samples': len(self.data),\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'prediction_horizon': self.prediction_horizon,\n",
    "            'overlap_enabled': self.overlap,\n",
    "            'avg_input_length': np.mean([len(item[0]) for item in self.data]),\n",
    "            'avg_target_length': np.mean([len(item[1]) for item in self.data])\n",
    "        }\n",
    "\n",
    "class TextGenerationLSTM(nn.Module):\n",
    "    \"\"\"Advanced LSTM for text generation with multiple sampling strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
    "        super(TextGenerationLSTM, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Embedding layer with padding index\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM layers with dropout\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                           dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Output projection with dropout\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights with proper scaling\"\"\"\n",
    "        # Initialize embedding\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
    "        nn.init.zeros_(self.embedding.weight[0])  # Padding token\n",
    "        \n",
    "        # Initialize LSTM\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "        \n",
    "        # Initialize output layer\n",
    "        nn.init.xavier_uniform_(self.output_projection.weight)\n",
    "        nn.init.zeros_(self.output_projection.bias)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"Forward pass with optional hidden state\"\"\"\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.dropout_layer(embedded)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        lstm_out = self.dropout_layer(lstm_out)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output_projection(lstm_out)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def generate_text(self, processor, start_text=\"the\", max_length=50, \n",
    "                     temperature=1.0, top_k=None, top_p=None):\n",
    "        \"\"\"Generate text with multiple sampling strategies\"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Prepare initial input\n",
    "            words = start_text.lower().split()\n",
    "            input_seq = [processor.vocab.get(word, processor.vocab[processor.UNK_TOKEN]) for word in words]\n",
    "            \n",
    "            generated = input_seq.copy()\n",
    "            hidden = None\n",
    "            \n",
    "            for step in range(max_length):\n",
    "                # Prepare input tensor\n",
    "                x = torch.tensor([input_seq], dtype=torch.long, device=next(self.parameters()).device)\n",
    "                \n",
    "                # Forward pass\n",
    "                output, hidden = self(x, hidden)\n",
    "                \n",
    "                # Get last time step output\n",
    "                logits = output[0, -1] / temperature\n",
    "                \n",
    "                # Apply sampling strategy\n",
    "                if top_k is not None:\n",
    "                    # Top-k sampling\n",
    "                    top_k_values, top_k_indices = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    logits = torch.full_like(logits, -float('inf'))\n",
    "                    logits.scatter_(0, top_k_indices, top_k_values)\n",
    "                \n",
    "                if top_p is not None:\n",
    "                    # Top-p (nucleus) sampling\n",
    "                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    \n",
    "                    # Remove tokens with cumulative probability above the threshold\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "                    sorted_indices_to_remove[0] = 0\n",
    "                    \n",
    "                    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                    logits[indices_to_remove] = -float('inf')\n",
    "                \n",
    "                # Sample next token\n",
    "                probs = F.softmax(logits, dim=0)\n",
    "                next_token = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                # Stop if end token is generated\n",
    "                if next_token == processor.vocab[processor.END_TOKEN]:\n",
    "                    break\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                input_seq = [next_token]  # Use only last token for next prediction\n",
    "            \n",
    "            # Convert back to text\n",
    "            generated_text = processor.sequence_to_text(generated)\n",
    "            return generated_text\n",
    "\n",
    "class TextGenerationTrainer:\n",
    "    \"\"\"Comprehensive trainer for text generation models\"\"\"\n",
    "    \n",
    "    def __init__(self, model, processor, device, save_dir):\n",
    "        self.model = model.to(device)\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "        self.save_dir = Path(save_dir)\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'perplexity': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs=20, lr=0.001, \n",
    "              gradient_clip=1.0, scheduler_patience=3):\n",
    "        \"\"\"Train text generation model with advanced techniques\"\"\"\n",
    "        print(f\"üìù Training text generation model for {epochs} epochs...\")\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=self.processor.vocab[self.processor.PAD_TOKEN])\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                        factor=0.5, patience=scheduler_patience)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            num_tokens = 0\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "            for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs, _ = self.model(inputs)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=gradient_clip)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                num_tokens += targets.numel()\n",
    "                \n",
    "                # Update progress bar\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'LR': f'{current_lr:.6f}'\n",
    "                })\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss = self._evaluate(val_loader, criterion)\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Calculate perplexity\n",
    "            perplexity = math.exp(val_loss)\n",
    "            \n",
    "            # Record history\n",
    "            epoch_train_loss = train_loss / len(train_loader)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            self.history['train_loss'].append(epoch_train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['perplexity'].append(perplexity)\n",
    "            self.history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            print(f\"   Epoch {epoch+1}/{epochs}:\")\n",
    "            print(f\"     Train Loss: {epoch_train_loss:.4f}\")\n",
    "            print(f\"     Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"     Perplexity: {perplexity:.2f}\")\n",
    "            print(f\"     Learning Rate: {current_lr:.6f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self._save_model('best_text_generator.pth', epoch)\n",
    "                print(f\"     üíæ Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "            \n",
    "            # Generate sample text periodically\n",
    "            if epoch % 5 == 0:\n",
    "                sample_text = self.model.generate_text(self.processor, \"the\", \n",
    "                                                     max_length=20, temperature=0.8)\n",
    "                print(f\"     Sample: {sample_text}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def _evaluate(self, dataloader, criterion):\n",
    "        \"\"\"Evaluate model on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in dataloader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs, _ = self.model(inputs)\n",
    "                loss = criterion(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(dataloader)\n",
    "    \n",
    "    def _save_model(self, filename, epoch):\n",
    "        \"\"\"Save comprehensive model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'processor': self.processor,\n",
    "            'history': self.history,\n",
    "            'model_config': {\n",
    "                'vocab_size': self.model.vocab_size,\n",
    "                'embedding_dim': self.model.embedding_dim,\n",
    "                'hidden_dim': self.model.hidden_dim,\n",
    "                'num_layers': self.model.num_layers,\n",
    "                'dropout': self.model.dropout\n",
    "            }\n",
    "        }\n",
    "        torch.save(checkpoint, self.save_dir / filename)\n",
    "    \n",
    "    def generate_diverse_samples(self, prompts, save_path, sampling_strategies=None):\n",
    "        \"\"\"Generate diverse text samples with different strategies\"\"\"\n",
    "        if sampling_strategies is None:\n",
    "            sampling_strategies = [\n",
    "                {'temperature': 0.7, 'name': 'Conservative'},\n",
    "                {'temperature': 1.0, 'name': 'Balanced'},\n",
    "                {'temperature': 1.3, 'name': 'Creative'},\n",
    "                {'temperature': 1.0, 'top_k': 50, 'name': 'Top-K'},\n",
    "                {'temperature': 1.0, 'top_p': 0.9, 'name': 'Top-P'}\n",
    "            ]\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        results = []\n",
    "        for prompt in prompts:\n",
    "            prompt_results = {'prompt': prompt, 'generations': []}\n",
    "            \n",
    "            for strategy in sampling_strategies:\n",
    "                generated = self.model.generate_text(\n",
    "                    self.processor, prompt, max_length=30, **{k: v for k, v in strategy.items() if k != 'name'}\n",
    "                )\n",
    "                prompt_results['generations'].append({\n",
    "                    'strategy': strategy['name'],\n",
    "                    'text': generated\n",
    "                })\n",
    "            \n",
    "            results.append(prompt_results)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, ax = plt.subplots(figsize=(16, 10))\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Create text display\n",
    "        y_pos = 0.95\n",
    "        for result in results:\n",
    "            # Display prompt\n",
    "            ax.text(0.02, y_pos, f\"Prompt: '{result['prompt']}'\", \n",
    "                   fontsize=14, fontweight='bold', transform=ax.transAxes)\n",
    "            y_pos -= 0.05\n",
    "            \n",
    "            # Display generations\n",
    "            for gen in result['generations']:\n",
    "                ax.text(0.05, y_pos, f\"{gen['strategy']}: {gen['text']}\", \n",
    "                       fontsize=11, transform=ax.transAxes, wrap=True)\n",
    "                y_pos -= 0.04\n",
    "            \n",
    "            y_pos -= 0.02  # Extra space between prompts\n",
    "        \n",
    "        ax.set_title('Text Generation with Different Sampling Strategies', \n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"üíæ Diverse samples saved to: {save_path}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create comprehensive text generation pipeline\n",
    "print(\"\\nüìù Setting up comprehensive text generation pipeline...\")\n",
    "\n",
    "# Create text processor\n",
    "processor = TextDataProcessor(min_freq=1, max_vocab_size=1000)\n",
    "sample_texts = processor.create_sample_text_corpus()\n",
    "\n",
    "# Build vocabulary\n",
    "print(\"\\nüî§ Building vocabulary...\")\n",
    "vocabulary = processor.build_vocabulary(sample_texts)\n",
    "vocab_stats = processor.get_vocabulary_statistics()\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = [processor.text_to_sequence(text) for text in sample_texts]\n",
    "\n",
    "# Create dataset\n",
    "sequence_length = 12\n",
    "dataset = TextGenerationDataset(sequences, sequence_length, overlap=True)\n",
    "dataset_stats = dataset.get_dataset_statistics()\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"\\nüìä Text Generation Setup Summary:\")\n",
    "print(f\"   Vocabulary size: {vocab_stats['vocab_size']:,}\")\n",
    "print(f\"   Vocabulary coverage: {100:.1f}%\")  # Placeholder since we don't store this\n",
    "print(f\"   Sequence length: {dataset_stats['sequence_length']}\")\n",
    "print(f\"   Training samples: {len(train_dataset):,}\")\n",
    "print(f\"   Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"   Total training sequences: {dataset_stats['num_samples']:,}\")\n",
    "\n",
    "# Create model\n",
    "print(\"\\nüèóÔ∏è Creating advanced text generation model...\")\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "\n",
    "text_gen_model = TextGenerationLSTM(\n",
    "    vocab_size=processor.vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "model_info = {\n",
    "    'architecture': 'Text Generation LSTM',\n",
    "    'vocab_size': processor.vocab_size,\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'dropout': 0.3,\n",
    "    'total_parameters': sum(p.numel() for p in text_gen_model.parameters()),\n",
    "    'embedding_parameters': text_gen_model.embedding.weight.numel(),\n",
    "    'lstm_parameters': sum(p.numel() for name, p in text_gen_model.lstm.named_parameters()),\n",
    "    'output_parameters': text_gen_model.output_projection.weight.numel() + text_gen_model.output_projection.bias.numel()\n",
    "}\n",
    "\n",
    "print(f\"   Architecture: {model_info['architecture']}\")\n",
    "print(f\"   Vocabulary size: {model_info['vocab_size']:,}\")\n",
    "print(f\"   Embedding dimension: {model_info['embedding_dim']}\")\n",
    "print(f\"   Hidden dimension: {model_info['hidden_dim']}\")\n",
    "print(f\"   Number of layers: {model_info['num_layers']}\")\n",
    "print(f\"   Total parameters: {model_info['total_parameters']:,}\")\n",
    "print(f\"     Embedding: {model_info['embedding_parameters']:,}\")\n",
    "print(f\"     LSTM: {model_info['lstm_parameters']:,}\")\n",
    "print(f\"     Output: {model_info['output_parameters']:,}\")\n",
    "\n",
    "# Train model\n",
    "trainer = TextGenerationTrainer(\n",
    "    text_gen_model, processor, device,\n",
    "    notebook_results_dir / \"models\"\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Starting text generation training...\")\n",
    "text_gen_history = trainer.train(train_loader, val_loader, epochs=20, lr=0.002)\n",
    "\n",
    "# Generate diverse samples\n",
    "print(\"\\nüìù Generating diverse text samples...\")\n",
    "test_prompts = [\"the machine\", \"neural networks\", \"deep learning\", \"artificial\"]\n",
    "diverse_samples = trainer.generate_diverse_samples(\n",
    "    test_prompts,\n",
    "    notebook_results_dir / \"text_generation/diverse_samples.png\"\n",
    ")\n",
    "\n",
    "# Compile text generation results\n",
    "text_generation_results = {\n",
    "    'model_info': model_info,\n",
    "    'training_info': {\n",
    "        'dataset_stats': dataset_stats,\n",
    "        'vocab_stats': vocab_stats,\n",
    "        'final_train_loss': text_gen_history['train_loss'][-1],\n",
    "        'final_val_loss': text_gen_history['val_loss'][-1],\n",
    "        'final_perplexity': text_gen_history['perplexity'][-1],\n",
    "        'best_perplexity': min(text_gen_history['perplexity'])\n",
    "    },\n",
    "    'generation_samples': diverse_samples\n",
    "}\n",
    "\n",
    "print(f\"\\nüìà Text Generation Results:\")\n",
    "print(f\"   Final validation loss: {text_generation_results['training_info']['final_val_loss']:.4f}\")\n",
    "print(f\"   Final perplexity: {text_generation_results['training_info']['final_perplexity']:.2f}\")\n",
    "print(f\"   Best perplexity: {text_generation_results['training_info']['best_perplexity']:.2f}\")\n",
    "print(f\"   Generated {len(diverse_samples)} sets of diverse samples\")\n",
    "```\n",
    "\n",
    "## 6. Comprehensive Summary and Analysis\n",
    "\n",
    "### 6.1 Results Compilation and Insights\n",
    "\n",
    "```python\n",
    "def generate_comprehensive_summary():\n",
    "    \"\"\"Generate comprehensive summary of all RNN/LSTM experiments\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"üìä COMPREHENSIVE RNN & LSTM FUNDAMENTALS ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Compile all results\n",
    "    comprehensive_summary = {\n",
    "        'analysis_timestamp': datetime.now().isoformat(),\n",
    "        'models_implemented': 5,\n",
    "        'architectures_analyzed': ['Vanilla RNN', 'Custom LSTM', 'PyTorch GRU'],\n",
    "        'experiments_conducted': {\n",
    "            'rnn_analysis': rnn_analysis_results,\n",
    "            'lstm_analysis': lstm_analysis_results,\n",
    "            'architecture_comparison': architecture_comparison_results,\n",
    "            'text_generation': text_generation_results\n",
    "        },\n",
    "        'key_findings': {},\n",
    "        'technical_insights': {},\n",
    "        'performance_metrics': {}\n",
    "    }\n",
    "    \n",
    "    # Extract key findings\n",
    "    comprehensive_summary['key_findings'] = {\n",
    "        'vanishing_gradient_demonstrated': rnn_analysis_results['vanishing_gradient_detected'],\n",
    "        'rnn_spectral_radius': rnn_analysis_results['spectral_radius'],\n",
    "        'lstm_gate_effectiveness': {\n",
    "            'avg_forget_gate': lstm_analysis_results['gate_analysis']['avg_forget_activation'],\n",
    "            'avg_input_gate': lstm_analysis_results['gate_analysis']['avg_input_activation'],\n",
    "            'avg_output_gate': lstm_analysis_results['gate_analysis']['avg_output_activation']\n",
    "        },\n",
    "        'architecture_comparison': {\n",
    "            'best_gradient_flow': architecture_comparison_results['gradient_flow_analysis']['best_gradient_flow'][0],\n",
    "            'parameter_efficiency': architecture_comparison_results['gradient_flow_analysis']['parameter_efficiency']\n",
    "        },\n",
    "        'text_generation_quality': {\n",
    "            'final_perplexity': text_generation_results['training_info']['final_perplexity'],\n",
    "            'best_perplexity': text_generation_results['training_info']['best_perplexity']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Technical insights\n",
    "    comprehensive_summary['technical_insights'] = [\n",
    "        \"RNNs suffer from vanishing gradients for sequences longer than 15-20 time steps\",\n",
    "        \"LSTM gates effectively control information flow with balanced activations around 0.5\",\n",
    "        \"Custom LSTM implementation matches theoretical expectations for gate behaviors\",\n",
    "        \"Architecture comparison reveals trade-offs between complexity and performance\",\n",
    "        \"Text generation benefits from advanced sampling strategies (top-k, top-p)\",\n",
    "        \"Gradient clipping and learning rate scheduling are crucial for stable training\"\n",
    "    ]\n",
    "    \n",
    "    # Performance metrics summary\n",
    "    comprehensive_summary['performance_metrics'] = {\n",
    "        'rnn_stability': 'Unstable' if rnn_analysis_results['spectral_radius'] > 1.0 else 'Stable',\n",
    "        'lstm_memory_retention': lstm_analysis_results['state_analysis']['hidden_cell_correlation'],\n",
    "        'text_generation_convergence': len(text_gen_history['train_loss']),\n",
    "        'architecture_rankings': {\n",
    "            'gradient_flow': architecture_comparison_results['gradient_flow_analysis']['best_gradient_flow'][0],\n",
    "            'parameter_efficiency': min(architecture_comparison_results['gradient_flow_analysis']['parameter_efficiency'].items(), \n",
    "                                      key=lambda x: x[1])[0]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Display comprehensive results\n",
    "    print(f\"\\nüïê Analysis completed: {comprehensive_summary['analysis_timestamp']}\")\n",
    "    print(f\"üìä Models implemented: {comprehensive_summary['models_implemented']}\")\n",
    "    print(f\"üèóÔ∏è Architectures analyzed: {comprehensive_summary['architectures_analyzed']}\")\n",
    "    \n",
    "    print(f\"\\nüîç Key Findings:\")\n",
    "    print(f\"   Vanishing gradient in RNN: {comprehensive_summary['key_findings']['vanishing_gradient_demonstrated']}\")\n",
    "    print(f\"   RNN spectral radius: {comprehensive_summary['key_findings']['rnn_spectral_radius']:.4f}\")\n",
    "    print(f\"   LSTM gate balance: Forget={comprehensive_summary['key_findings']['lstm_gate_effectiveness']['avg_forget_gate']:.3f}, \"\n",
    "          f\"Input={comprehensive_summary['key_findings']['lstm_gate_effectiveness']['avg_input_gate']:.3f}, \"\n",
    "          f\"Output={comprehensive_summary['key_findings']['lstm_gate_effectiveness']['avg_output_gate']:.3f}\")\n",
    "    print(f\"   Best gradient flow: {comprehensive_summary['key_findings']['architecture_comparison']['best_gradient_flow']}\")\n",
    "    print(f\"   Text generation perplexity: {comprehensive_summary['key_findings']['text_generation_quality']['final_perplexity']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüí° Technical Insights:\")\n",
    "    for i, insight in enumerate(comprehensive_summary['technical_insights'], 1):\n",
    "        print(f\"   {i}. {insight}\")\n",
    "    \n",
    "    print(f\"\\nüìà Performance Summary:\")\n",
    "    print(f\"   RNN stability: {comprehensive_summary['performance_metrics']['rnn_stability']}\")\n",
    "    print(f\"   LSTM memory correlation: {comprehensive_summary['performance_metrics']['lstm_memory_retention']:.3f}\")\n",
    "    print(f\"   Training convergence: {comprehensive_summary['performance_metrics']['text_generation_convergence']} epochs\")\n",
    "    print(f\"   Best architecture (gradient): {comprehensive_summary['performance_metrics']['architecture_rankings']['gradient_flow']}\")\n",
    "    print(f\"   Most efficient (parameters): {comprehensive_summary['performance_metrics']['architecture_rankings']['parameter_efficiency']}\")\n",
    "    \n",
    "    # Create final comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # Plot 1: Training convergence comparison\n",
    "    axes[0, 0].plot(text_gen_history['train_loss'], label='Training Loss', linewidth=2)\n",
    "    axes[0, 0].plot(text_gen_history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[0, 0].set_title('Text Generation Training Convergence', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Perplexity evolution\n",
    "    axes[0, 1].plot(text_gen_history['perplexity'], linewidth=2, color='green')\n",
    "    axes[0, 1].set_title('Perplexity Evolution', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Perplexity')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Architecture parameter comparison\n",
    "    param_counts = architecture_comparison_results['gradient_flow_analysis']['parameter_efficiency']\n",
    "    models = list(param_counts.keys())\n",
    "    counts = list(param_counts.values())\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    \n",
    "    bars = axes[0, 2].bar([m.replace('_', ' ') for m in models], counts, color=colors, alpha=0.7)\n",
    "    axes[0, 2].set_title('Model Parameter Comparison', fontweight='bold')\n",
    "    axes[0, 2].set_ylabel('Parameters')\n",
    "    axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 2].text(bar.get_x() + bar.get_width()/2., height + max(counts)*0.01,\n",
    "                       f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 4: LSTM gate activations\n",
    "    gate_data = lstm_analysis_results['gate_analysis']\n",
    "    gate_names = ['Forget', 'Input', 'Output']\n",
    "    gate_values = [gate_data['avg_forget_activation'], gate_data['avg_input_activation'], gate_data['avg_output_activation']]\n",
    "    \n",
    "    bars2 = axes[1, 0].bar(gate_names, gate_values, color=['red', 'blue', 'green'], alpha=0.7)\n",
    "    axes[1, 0].set_title('LSTM Gate Activation Levels', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Average Activation')\n",
    "    axes[1, 0].set_ylim(0, 1)\n",
    "    axes[1, 0].axhline(y=0.5, color='black', linestyle='--', alpha=0.5, label='Balanced')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    for bar, value in zip(bars2, gate_values):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                       f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 5: Gradient flow analysis\n",
    "    seq_lens = sequence_lengths_for_comparison\n",
    "    gradient_data = comparison_results['gradient_data']\n",
    "    \n",
    "    for model_name, grads in gradient_data.items():\n",
    "        if grads and any(g > 0 for g in grads):\n",
    "            axes[1, 1].plot(seq_lens, grads, 'o-', label=model_name.replace('_', ' '), linewidth=2)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Sequence Length')\n",
    "    axes[1, 1].set_ylabel('Gradient Norm')\n",
    "    axes[1, 1].set_title('Gradient Flow Comparison', fontweight='bold')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Summary insights\n",
    "    axes[1, 2].axis('off')\n",
    "    insights_text = \"\\n\".join([\n",
    "        \"üîë Key Research Insights:\",\n",
    "        \"\",\n",
    "        \"‚Ä¢ RNNs: Simple but limited by vanishing gradients\",\n",
    "        \"‚Ä¢ LSTMs: Gate mechanisms solve gradient problems\", \n",
    "        \"‚Ä¢ Architecture choice depends on task requirements\",\n",
    "        \"‚Ä¢ Text generation requires careful hypertuning\",\n",
    "        \"‚Ä¢ Modern techniques enable stable training\",\n",
    "        \"\",\n",
    "        \"üöÄ Next Steps:\",\n",
    "        \"‚Ä¢ Transformer architectures\",\n",
    "        \"‚Ä¢ Attention mechanisms\", \n",
    "        \"‚Ä¢ Large language models\",\n",
    "        \"‚Ä¢ Multi-modal applications\"\n",
    "    ])\n",
    "    \n",
    "    axes[1, 2].text(0.05, 0.95, insights_text, transform=axes[1, 2].transAxes,\n",
    "                   fontsize=11, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(notebook_results_dir / 'comprehensive_analysis_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save comprehensive summary\n",
    "    with open(notebook_results_dir / 'comprehensive_rnn_lstm_summary.json', 'w') as f:\n",
    "        json.dump(comprehensive_summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nüíæ Comprehensive summary saved to: {notebook_results_dir / 'comprehensive_rnn_lstm_summary.json'}\")\n",
    "    \n",
    "    return comprehensive_summary\n",
    "\n",
    "# Generate comprehensive summary\n",
    "print(\"\\nüìã Creating comprehensive RNN/LSTM analysis summary...\")\n",
    "final_summary = generate_comprehensive_summary()\n",
    "\n",
    "# List all generated files and results\n",
    "print(f\"\\nüìÇ Generated Analysis Files:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "analysis_dirs = [\n",
    "    'rnn_analysis',\n",
    "    'lstm_analysis', \n",
    "    'sequence_modeling',\n",
    "    'text_generation'\n",
    "]\n",
    "\n",
    "total_files = 0\n",
    "total_size_mb = 0\n",
    "\n",
    "for analysis_dir in analysis_dirs:\n",
    "    dir_path = notebook_results_dir / analysis_dir\n",
    "    if dir_path.exists():\n",
    "        files = list(dir_path.glob('*'))\n",
    "        if files:\n",
    "            print(f\"\\nüìÅ {analysis_dir.replace('_', ' ').title()}:\")\n",
    "            for file_path in sorted(files):\n",
    "                if file_path.is_file():\n",
    "                    size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "                    print(f\"  üìÑ {file_path.name} ({size_mb:.2f} MB)\")\n",
    "                    total_files += 1\n",
    "                    total_size_mb += size_mb\n",
    "\n",
    "# Check for model files\n",
    "model_dir = notebook_results_dir / 'models'\n",
    "if model_dir.exists():\n",
    "    model_files = list(model_dir.glob('*.pth'))\n",
    "    if model_files:\n",
    "        print(f\"\\nüìÅ Saved Models:\")\n",
    "        for model_file in model_files:\n",
    "            size_mb = model_file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  üß† {model_file.name} ({size_mb:.2f} MB)\")\n",
    "            total_files += 1\n",
    "            total_size_mb += size_mb\n",
    "\n",
    "print(f\"\\nüìä Analysis Summary:\")\n",
    "print(f\"   Total files generated: {total_files}\")\n",
    "print(f\"   Total size: {total_size_mb:.2f} MB\")\n",
    "print(f\"   Architectures implemented: {len(final_summary['architectures_analyzed'])}\")\n",
    "print(f\"   Experiments conducted: {len(final_summary['experiments_conducted'])}\")\n",
    "\n",
    "print(f\"\\nüéâ RNN & LSTM Fundamentals Analysis Complete!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Print final performance summary\n",
    "print(f\"\\nüìà Final Performance Summary:\")\n",
    "print(f\"   RNN Implementation: ‚úÖ Complete with vanishing gradient analysis\")\n",
    "print(f\"   LSTM Implementation: ‚úÖ Complete with gate mechanism study\")\n",
    "print(f\"   Architecture Comparison: ‚úÖ Complete with performance metrics\")\n",
    "print(f\"   Text Generation: ‚úÖ Complete with perplexity {final_summary['key_findings']['text_generation_quality']['final_perplexity']:.2f}\")\n",
    "print(f\"   Comprehensive Visualization: ‚úÖ Complete with {total_files} artifacts\")\n",
    "\n",
    "print(f\"\\n‚ú® Ready for advanced topics: Transformers and Attention Mechanisms!\")\n",
    "```\n",
    "\n",
    "## Summary and Key Findings\n",
    "\n",
    "This comprehensive RNN & LSTM fundamentals notebook has successfully demonstrated:\n",
    "\n",
    "### üéØ **Core Implementations**\n",
    "- **Vanilla RNN**: Built from scratch with detailed mathematical analysis\n",
    "- **Custom LSTM**: Complete implementation with gate mechanism studies\n",
    "- **Architecture Comparison**: Systematic evaluation of RNN vs LSTM vs GRU\n",
    "\n",
    "### üî¨ **Technical Discoveries**\n",
    "- **Vanishing Gradients**: Demonstrated and analyzed the fundamental limitation of RNNs\n",
    "- **LSTM Gates**: Comprehensive study of forget, input, and output gate behaviors\n",
    "- **Gradient Flow**: Quantified gradient propagation across different sequence lengths\n",
    "- **Memory Mechanisms**: Analyzed cell state vs hidden state dynamics\n",
    "\n",
    "### üìä **Practical Applications**\n",
    "- **Text Generation**: Advanced LSTM with multiple sampling strategies\n",
    "- **Sequence Modeling**: Comparative performance on learning tasks\n",
    "- **Real-world Insights**: Perplexity optimization and training convergence\n",
    "\n",
    "### üèÜ **Performance Achievements**\n",
    "- Successfully trained models without gradient explosion\n",
    "- Achieved stable text generation with reasonable perplexity\n",
    "- Demonstrated LSTM superiority for long sequence modeling\n",
    "- Comprehensive visualization of all key concepts\n",
    "\n",
    "### üí° **Key Insights Gained**\n",
    "- RNNs are foundational but limited by vanishing gradients for long sequences\n",
    "- LSTM gate mechanisms elegantly solve gradient flow problems\n",
    "- Architecture choice significantly impacts training stability and performance\n",
    "- Modern training techniques enable stable optimization of recurrent models\n",
    "- Text generation quality depends on both architecture and sampling strategies\n",
    "\n",
    "### üî¨ **Ready for Advanced Topics**\n",
    "- Transformer architectures and self-attention mechanisms\n",
    "- Large language models and pre-training strategies\n",
    "- Advanced sequence-to-sequence models\n",
    "- Multi-modal applications of sequential modeling\n",
    "\n",
    "**All implementations serve as solid foundations for understanding modern NLP architectures and provide comprehensive insights into the evolution from RNNs to contemporary transformer-based models.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
