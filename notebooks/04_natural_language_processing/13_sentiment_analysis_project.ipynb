{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee1dbe5",
   "metadata": {},
   "source": [
    "# Advanced Sentiment Analysis: Comprehensive NLP System with Multiple Architectures\n",
    "# Deep Learning-Based Text Classification with LSTM, CNN, and Ensemble Methods\n",
    "\n",
    "\"\"\"\n",
    "Advanced Sentiment Analysis System\n",
    "\n",
    "This notebook provides a comprehensive implementation of sentiment analysis using:\n",
    "- LSTM with Attention Mechanisms\n",
    "- CNN Text Classifiers  \n",
    "- Ensemble Methods\n",
    "- Production Deployment Pipeline\n",
    "\n",
    "Authors: PyTorch Mastery Hub Team\n",
    "Institution: Deep Learning Research Institute\n",
    "Course: Advanced Natural Language Processing\n",
    "Date: December 2024\n",
    "\"\"\"\n",
    "\n",
    "# ================================================================================\n",
    "# 1. SETUP AND ENVIRONMENT CONFIGURATION\n",
    "# ================================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting environment\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üé≠ Advanced Sentiment Analysis System Initialized\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   PyTorch Version: {torch.__version__}\")\n",
    "print(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"‚úÖ Environment configured with deterministic settings\")\n",
    "\n",
    "# Create results directory\n",
    "notebook_results_dir = Path('results/sentiment_analysis')\n",
    "notebook_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ Results will be saved to: {notebook_results_dir}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 2. TEXT PREPROCESSING PIPELINE\n",
    "# ================================================================================\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing pipeline for sentiment analysis.\n",
    "    \n",
    "    Handles text cleaning, tokenization, vocabulary building, and sequence conversion\n",
    "    with support for various text normalization techniques.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size=10000, min_freq=2, clean_html=True, normalize_contractions=True):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        self.clean_html = clean_html\n",
    "        self.normalize_contractions = normalize_contractions\n",
    "        \n",
    "        # Initialize vocabularies with special tokens\n",
    "        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>'}\n",
    "        self.vocab_size = 4\n",
    "        \n",
    "        # Common contractions for normalization\n",
    "        self.contractions = {\n",
    "            \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
    "            \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\", \"'d\": \" would\",\n",
    "            \"'m\": \" am\", \"it's\": \"it is\", \"that's\": \"that is\",\n",
    "            \"what's\": \"what is\", \"where's\": \"where is\", \"how's\": \"how is\"\n",
    "        }\n",
    "        \n",
    "        print(f\"üìö TextPreprocessor initialized with max_vocab_size={max_vocab_size}\")\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Comprehensive text cleaning and normalization.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        text = text.lower().strip()\n",
    "        \n",
    "        # Remove HTML tags if specified\n",
    "        if self.clean_html:\n",
    "            text = re.sub(r'<[^>]+>', '', text)\n",
    "        \n",
    "        # Remove URLs and email addresses\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove user mentions and hashtags\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        \n",
    "        # Normalize contractions\n",
    "        if self.normalize_contractions:\n",
    "            for contraction, expansion in self.contractions.items():\n",
    "                text = text.replace(contraction, expansion)\n",
    "        \n",
    "        # Remove excessive punctuation but preserve sentence structure\n",
    "        text = re.sub(r'[!]{2,}', '!', text)\n",
    "        text = re.sub(r'[?]{2,}', '?', text)\n",
    "        text = re.sub(r'[.]{3,}', '...', text)\n",
    "        \n",
    "        # Remove special characters but keep basic punctuation\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s\\.\\!\\?\\,\\;\\:\\-\\(\\)]', '', text)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Advanced tokenization with punctuation handling.\"\"\"\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[.!?]', text)\n",
    "        \n",
    "        # Filter out very short tokens and numbers-only tokens\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            if len(token) >= 2 or token in ['.', '!', '?']:\n",
    "                if not (token.isdigit() and len(token) > 1):\n",
    "                    filtered_tokens.append(token)\n",
    "        \n",
    "        return filtered_tokens\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"Build vocabulary from training texts.\"\"\"\n",
    "        print(f\"üî® Building vocabulary from {len(texts)} texts...\")\n",
    "        \n",
    "        word_counts = Counter()\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for text in tqdm(texts, desc=\"Processing texts for vocabulary\"):\n",
    "            cleaned = self.clean_text(text)\n",
    "            tokens = self.tokenize(cleaned)\n",
    "            word_counts.update(tokens)\n",
    "            total_tokens += len(tokens)\n",
    "        \n",
    "        print(f\"   Total tokens processed: {total_tokens:,}\")\n",
    "        print(f\"   Unique words found: {len(word_counts):,}\")\n",
    "        \n",
    "        # Add words that meet frequency threshold\n",
    "        words_added = 0\n",
    "        for word, count in word_counts.most_common(self.max_vocab_size - 4):\n",
    "            if count >= self.min_freq:\n",
    "                self.word_to_idx[word] = self.vocab_size\n",
    "                self.idx_to_word[self.vocab_size] = word\n",
    "                self.vocab_size += 1\n",
    "                words_added += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        print(f\"   Final vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"   Words added: {words_added}\")\n",
    "        print(f\"   Most common words: {word_counts.most_common(10)}\")\n",
    "        \n",
    "        return {\n",
    "            'total_tokens': total_tokens,\n",
    "            'unique_words': len(word_counts),\n",
    "            'vocabulary_size': self.vocab_size,\n",
    "            'words_added': words_added\n",
    "        }\n",
    "    \n",
    "    def text_to_sequence(self, text, max_length=None):\n",
    "        \"\"\"Convert text to sequence of indices.\"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = self.tokenize(cleaned)\n",
    "        \n",
    "        sequence = [self.word_to_idx.get(token, self.word_to_idx['<UNK>']) \n",
    "                   for token in tokens]\n",
    "        \n",
    "        if max_length and len(sequence) > max_length:\n",
    "            sequence = sequence[:max_length]\n",
    "        \n",
    "        return sequence\n",
    "\n",
    "# ================================================================================\n",
    "# 3. DATASET GENERATION AND PREPARATION\n",
    "# ================================================================================\n",
    "\n",
    "def generate_movie_reviews(n_samples=3000):\n",
    "    \"\"\"Generate synthetic movie review dataset.\"\"\"\n",
    "    print(f\"üé¨ Generating {n_samples} synthetic movie reviews...\")\n",
    "    \n",
    "    sentiment_words = {\n",
    "        'positive': ['amazing', 'outstanding', 'brilliant', 'fantastic', 'excellent', \n",
    "                    'wonderful', 'great', 'good', 'enjoyable', 'entertaining'],\n",
    "        'negative': ['terrible', 'awful', 'horrible', 'bad', 'disappointing', \n",
    "                    'boring', 'dull', 'poor', 'mediocre', 'unimpressive']\n",
    "    }\n",
    "    \n",
    "    movie_aspects = ['plot', 'acting', 'direction', 'cinematography', 'dialogue', \n",
    "                    'characters', 'music', 'visual effects', 'pacing']\n",
    "    \n",
    "    templates = [\n",
    "        \"This movie was {sentiment}.\",\n",
    "        \"I {verb} this film. The {aspect} was {sentiment}.\",\n",
    "        \"The {aspect1} was {sentiment1} and the {aspect2} was {sentiment2}.\",\n",
    "        \"{sentiment} {aspect}! {verb} the entire experience.\",\n",
    "        \"What a {sentiment} movie. The {aspect} really {verb}.\"\n",
    "    ]\n",
    "    \n",
    "    reviews = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        sentiment_label = np.random.choice([0, 1])  # 0: negative, 1: positive\n",
    "        sentiment_type = 'positive' if sentiment_label == 1 else 'negative'\n",
    "        \n",
    "        template = np.random.choice(templates)\n",
    "        sentiment_word = np.random.choice(sentiment_words[sentiment_type])\n",
    "        \n",
    "        # Fill template\n",
    "        try:\n",
    "            review = template.format(\n",
    "                sentiment=sentiment_word,\n",
    "                sentiment1=sentiment_word,\n",
    "                sentiment2=np.random.choice(sentiment_words[sentiment_type]),\n",
    "                aspect=np.random.choice(movie_aspects),\n",
    "                aspect1=np.random.choice(movie_aspects),\n",
    "                aspect2=np.random.choice(movie_aspects),\n",
    "                verb='loved' if sentiment_type == 'positive' else 'hated'\n",
    "            )\n",
    "        except KeyError:\n",
    "            review = f\"This movie was {sentiment_word}.\"\n",
    "        \n",
    "        reviews.append(review)\n",
    "        labels.append(sentiment_label)\n",
    "    \n",
    "    print(f\"   Generated {len(reviews)} reviews\")\n",
    "    print(f\"   Positive: {sum(labels)}, Negative: {len(labels) - sum(labels)}\")\n",
    "    \n",
    "    return reviews, labels\n",
    "\n",
    "# Generate dataset\n",
    "texts, labels = generate_movie_reviews(n_samples=4000)\n",
    "\n",
    "# Split dataset\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    texts, labels, test_size=0.4, random_state=42, stratify=labels\n",
    ")\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "print(f\"\\nüìÇ Dataset Split:\")\n",
    "print(f\"   Training: {len(train_texts)} samples\")\n",
    "print(f\"   Validation: {len(val_texts)} samples\") \n",
    "print(f\"   Test: {len(test_texts)} samples\")\n",
    "\n",
    "# Sample reviews\n",
    "print(f\"\\nüìù Sample Reviews:\")\n",
    "for i, (text, label) in enumerate(zip(train_texts[:3], train_labels[:3])):\n",
    "    sentiment = \"Positive\" if label == 1 else \"Negative\"\n",
    "    print(f\"   {i+1}. [{sentiment}] {text}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 4. TEXT PREPROCESSING AND VOCABULARY\n",
    "# ================================================================================\n",
    "\n",
    "# Create preprocessor and build vocabulary\n",
    "preprocessor = TextPreprocessor(max_vocab_size=15000, min_freq=2)\n",
    "vocab_stats = preprocessor.build_vocab(train_texts)\n",
    "\n",
    "# Test preprocessing\n",
    "sample_text = \"This movie was absolutely AMAZING!!! Best film ever!!!\"\n",
    "print(f\"\\nüß™ Preprocessing Test:\")\n",
    "print(f\"   Original: '{sample_text}'\")\n",
    "print(f\"   Cleaned: '{preprocessor.clean_text(sample_text)}'\")\n",
    "print(f\"   Sequence: {preprocessor.text_to_sequence(sample_text)[:10]}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 5. DATASET AND DATALOADER IMPLEMENTATION\n",
    "# ================================================================================\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"Dataset class for sentiment analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, preprocessor, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.preprocessor = preprocessor\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Convert texts to sequences\n",
    "        self.sequences = []\n",
    "        self.sequence_lengths = []\n",
    "        \n",
    "        for text in tqdm(texts, desc=\"Converting texts to sequences\"):\n",
    "            sequence = preprocessor.text_to_sequence(text, max_length)\n",
    "            self.sequences.append(sequence)\n",
    "            self.sequence_lengths.append(len(sequence))\n",
    "        \n",
    "        print(f\"   Average sequence length: {np.mean(self.sequence_lengths):.1f}\")\n",
    "        print(f\"   Max sequence length: {np.max(self.sequence_lengths)}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sequence': torch.tensor(self.sequences[idx], dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            'text': self.texts[idx],\n",
    "            'original_length': len(self.sequences[idx])\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for padding sequences.\"\"\"\n",
    "    sequences = [item['sequence'] for item in batch]\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    texts = [item['text'] for item in batch]\n",
    "    original_lengths = torch.tensor([item['original_length'] for item in batch])\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = torch.zeros_like(padded_sequences, dtype=torch.bool)\n",
    "    for i, length in enumerate(original_lengths):\n",
    "        attention_mask[i, :min(length, padded_sequences.size(1))] = True\n",
    "    \n",
    "    return {\n",
    "        'sequences': padded_sequences,\n",
    "        'labels': labels,\n",
    "        'texts': texts,\n",
    "        'attention_mask': attention_mask,\n",
    "        'lengths': original_lengths\n",
    "    }\n",
    "\n",
    "# Create datasets\n",
    "max_sequence_length = 128\n",
    "\n",
    "train_dataset = SentimentDataset(train_texts, train_labels, preprocessor, max_sequence_length)\n",
    "val_dataset = SentimentDataset(val_texts, val_labels, preprocessor, max_sequence_length)\n",
    "test_dataset = SentimentDataset(test_texts, test_labels, preprocessor, max_sequence_length)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"\\nüîÑ Data Loaders Created:\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 6. NEURAL ARCHITECTURE IMPLEMENTATIONS\n",
    "# ================================================================================\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Bahdanau attention mechanism for LSTM.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim, attention_dim=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        if attention_dim is None:\n",
    "            attention_dim = hidden_dim\n",
    "            \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        self.lstm_projection = nn.Linear(hidden_dim, attention_dim, bias=False)\n",
    "        self.attention_vector = nn.Linear(attention_dim, 1, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.lstm_projection.weight)\n",
    "        nn.init.xavier_uniform_(self.attention_vector.weight)\n",
    "        \n",
    "    def forward(self, lstm_outputs, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Compute attention weights and attended representation.\n",
    "        \n",
    "        Args:\n",
    "            lstm_outputs: (batch_size, seq_len, hidden_dim)\n",
    "            attention_mask: (batch_size, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            attended_output: (batch_size, hidden_dim)\n",
    "            attention_weights: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        # Project LSTM outputs to attention space\n",
    "        projected = torch.tanh(self.lstm_projection(lstm_outputs))\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = self.attention_vector(projected).squeeze(2)\n",
    "        \n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(~attention_mask, -1e9)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # Compute attended output\n",
    "        attended_output = torch.bmm(attention_weights.unsqueeze(1), lstm_outputs)\n",
    "        attended_output = attended_output.squeeze(1)\n",
    "        \n",
    "        return attended_output, attention_weights\n",
    "\n",
    "class LSTMSentimentClassifier(nn.Module):\n",
    "    \"\"\"LSTM-based sentiment classifier with attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, \n",
    "                 num_classes=2, dropout=0.3, bidirectional=True, use_attention=True):\n",
    "        super(LSTMSentimentClassifier, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, num_layers,\n",
    "            batch_first=True, \n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Calculate LSTM output dimension\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        \n",
    "        # Attention mechanism\n",
    "        if use_attention:\n",
    "            self.attention = BahdanauAttention(lstm_output_dim)\n",
    "            final_dim = lstm_output_dim\n",
    "        else:\n",
    "            final_dim = lstm_output_dim\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(final_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout // 2),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights.\"\"\"\n",
    "        # Initialize embeddings\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
    "        self.embedding.weight.data[0].fill_(0)  # Padding token\n",
    "        \n",
    "        # Initialize classifier\n",
    "        for module in self.classifier:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x, attention_mask=None, return_attention=False):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Get final representation\n",
    "        if self.use_attention:\n",
    "            final_representation, attention_weights = self.attention(lstm_outputs, attention_mask)\n",
    "        else:\n",
    "            if self.bidirectional:\n",
    "                hidden = hidden.view(self.num_layers, 2, x.size(0), self.hidden_dim)\n",
    "                final_representation = torch.cat([hidden[-1, 0], hidden[-1, 1]], dim=1)\n",
    "            else:\n",
    "                final_representation = hidden[-1]\n",
    "            attention_weights = None\n",
    "        \n",
    "        # Classification\n",
    "        final_representation = self.dropout(final_representation)\n",
    "        logits = self.classifier(final_representation)\n",
    "        \n",
    "        if return_attention and self.use_attention:\n",
    "            return logits, attention_weights\n",
    "        else:\n",
    "            return logits, attention_weights if self.use_attention else None\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model information.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        return {\n",
    "            'model_type': 'LSTM with Attention' if self.use_attention else 'LSTM',\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'hidden_dim': self.hidden_dim,\n",
    "            'num_layers': self.num_layers,\n",
    "            'bidirectional': self.bidirectional,\n",
    "            'use_attention': self.use_attention,\n",
    "            'total_parameters': total_params\n",
    "        }\n",
    "\n",
    "class CNNSentimentClassifier(nn.Module):\n",
    "    \"\"\"CNN-based sentiment classifier with multiple filter sizes.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters=100, \n",
    "                 filter_sizes=[2, 3, 4, 5], num_classes=2, dropout=0.3):\n",
    "        super(CNNSentimentClassifier, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embedding_dim, num_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        total_filters = len(filter_sizes) * num_filters\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(total_filters, total_filters // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(total_filters // 2, total_filters // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout // 2),\n",
    "            nn.Linear(total_filters // 4, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights.\"\"\"\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
    "        self.embedding.weight.data[0].fill_(0)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            nn.init.kaiming_normal_(conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "            nn.init.zeros_(conv.bias)\n",
    "        \n",
    "        for module in self.classifier:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x, attention_mask=None, return_attention=False):\n",
    "        \"\"\"Forward pass through CNN model.\"\"\"\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Transpose for conv1d: (batch_size, embedding_dim, seq_len)\n",
    "        embedded = embedded.transpose(1, 2)\n",
    "        \n",
    "        # Apply convolutional filters\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            conv_out = F.relu(conv(embedded))\n",
    "            pooled = F.max_pool1d(conv_out, kernel_size=conv_out.size(2))\n",
    "            pooled = pooled.squeeze(2)\n",
    "            conv_outputs.append(pooled)\n",
    "        \n",
    "        # Concatenate all conv outputs\n",
    "        concatenated = torch.cat(conv_outputs, dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        concatenated = self.dropout(concatenated)\n",
    "        logits = self.classifier(concatenated)\n",
    "        \n",
    "        return logits, None\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model information.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        return {\n",
    "            'model_type': 'CNN Text Classifier',\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'num_filters': self.num_filters,\n",
    "            'filter_sizes': self.filter_sizes,\n",
    "            'total_parameters': total_params\n",
    "        }\n",
    "\n",
    "class EnsembleClassifier(nn.Module):\n",
    "    \"\"\"Ensemble classifier combining multiple models.\"\"\"\n",
    "    \n",
    "    def __init__(self, models, ensemble_method='weighted_average', num_classes=2):\n",
    "        super(EnsembleClassifier, self).__init__()\n",
    "        \n",
    "        self.models = nn.ModuleDict(models)\n",
    "        self.ensemble_method = ensemble_method\n",
    "        self.model_names = list(models.keys())\n",
    "        \n",
    "        if ensemble_method == 'weighted_average':\n",
    "            self.model_weights = nn.Parameter(torch.ones(len(models)) / len(models))\n",
    "        \n",
    "        print(f\"üîó Ensemble created with models: {self.model_names}\")\n",
    "        \n",
    "    def forward(self, x, attention_mask=None, return_attention=False):\n",
    "        \"\"\"Forward pass through ensemble model.\"\"\"\n",
    "        model_outputs = {}\n",
    "        model_attentions = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            logits, attention = model(x, attention_mask, return_attention)\n",
    "            model_outputs[name] = logits\n",
    "            if attention is not None:\n",
    "                model_attentions[name] = attention\n",
    "        \n",
    "        # Combine predictions\n",
    "        if self.ensemble_method == 'weighted_average':\n",
    "            weights = F.softmax(self.model_weights, dim=0)\n",
    "            weighted_outputs = []\n",
    "            for i, (name, logits) in enumerate(model_outputs.items()):\n",
    "                weighted_outputs.append(weights[i] * logits)\n",
    "            ensemble_logits = torch.stack(weighted_outputs, dim=0).sum(dim=0)\n",
    "        else:\n",
    "            ensemble_logits = torch.stack(list(model_outputs.values()), dim=0).mean(dim=0)\n",
    "        \n",
    "        if return_attention and model_attentions:\n",
    "            return ensemble_logits, model_attentions\n",
    "        else:\n",
    "            return ensemble_logits, None\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get ensemble model information.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        individual_params = {name: sum(p.numel() for p in model.parameters()) \n",
    "                           for name, model in self.models.items()}\n",
    "        \n",
    "        return {\n",
    "            'model_type': f'Ensemble ({self.ensemble_method})',\n",
    "            'individual_models': self.model_names,\n",
    "            'individual_parameters': individual_params,\n",
    "            'total_parameters': total_params\n",
    "        }\n",
    "\n",
    "# Create models\n",
    "vocab_size = preprocessor.vocab_size\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "dropout = 0.3\n",
    "\n",
    "print(\"\\nüß† Creating Models...\")\n",
    "\n",
    "# LSTM Model\n",
    "lstm_model = LSTMSentimentClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=2,\n",
    "    dropout=dropout,\n",
    "    bidirectional=True,\n",
    "    use_attention=True\n",
    ").to(device)\n",
    "\n",
    "# CNN Model\n",
    "cnn_model = CNNSentimentClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_filters=128,\n",
    "    filter_sizes=[2, 3, 4, 5],\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "# Ensemble Model\n",
    "ensemble_models = {'lstm': lstm_model, 'cnn': cnn_model}\n",
    "ensemble_model = EnsembleClassifier(\n",
    "    models=ensemble_models,\n",
    "    ensemble_method='weighted_average'\n",
    ").to(device)\n",
    "\n",
    "# Test models\n",
    "sample_batch = next(iter(train_loader))\n",
    "sample_input = sample_batch['sequences'][:4].to(device)\n",
    "sample_mask = sample_batch['attention_mask'][:4].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    lstm_out, lstm_att = lstm_model(sample_input, sample_mask, return_attention=True)\n",
    "    cnn_out, _ = cnn_model(sample_input, sample_mask)\n",
    "    ensemble_out, _ = ensemble_model(sample_input, sample_mask)\n",
    "\n",
    "# Print model information\n",
    "for name, model in [(\"LSTM\", lstm_model), (\"CNN\", cnn_model), (\"Ensemble\", ensemble_model)]:\n",
    "    info = model.get_model_info()\n",
    "    print(f\"\\n‚úÖ {name} Model:\")\n",
    "    print(f\"   Type: {info['model_type']}\")\n",
    "    print(f\"   Parameters: {info['total_parameters']:,}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 7. TRAINING FRAMEWORK\n",
    "# ================================================================================\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Comprehensive model trainer with advanced features.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, model_name=\"model\", patience=5):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.patience = patience\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_acc': [], 'val_acc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model_state = None\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "    def calculate_metrics(self, predictions, labels):\n",
    "        \"\"\"Calculate evaluation metrics.\"\"\"\n",
    "        y_true = labels.cpu().numpy()\n",
    "        y_pred = predictions.cpu().numpy()\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_macro': report['macro avg']['f1-score'],\n",
    "            'classification_report': report\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self, dataloader, criterion, optimizer):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=f\"Training {self.model_name}\"):\n",
    "            sequences = batch['sequences'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits, _ = self.model(sequences, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            all_predictions.extend(predictions.cpu())\n",
    "            all_labels.extend(labels.cpu())\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        metrics = self.calculate_metrics(torch.tensor(all_predictions), torch.tensor(all_labels))\n",
    "        \n",
    "        return avg_loss, metrics\n",
    "    \n",
    "    def evaluate_epoch(self, dataloader, criterion):\n",
    "        \"\"\"Evaluate the model.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=f\"Evaluating {self.model_name}\"):\n",
    "                sequences = batch['sequences'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                \n",
    "                logits, _ = self.model(sequences, attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                all_predictions.extend(predictions.cpu())\n",
    "                all_labels.extend(labels.cpu())\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        metrics = self.calculate_metrics(torch.tensor(all_predictions), torch.tensor(all_labels))\n",
    "        \n",
    "        return avg_loss, metrics\n",
    "    \n",
    "    def train(self, train_loader, val_loader, num_epochs, lr=0.001):\n",
    "        \"\"\"Complete training loop.\"\"\"\n",
    "        print(f\"\\nüöÄ Training {self.model_name} for {num_epochs} epochs...\")\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.7)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            train_loss, train_metrics = self.train_epoch(train_loader, criterion, optimizer)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_metrics = self.evaluate_epoch(val_loader, criterion)\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step(val_loss)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Store history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_acc'].append(train_metrics['accuracy'])\n",
    "            self.history['val_acc'].append(val_metrics['accuracy'])\n",
    "            self.history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "                self.patience_counter = 0\n",
    "                improvement = \"‚úÖ\"\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                improvement = \"‚è∏Ô∏è\" if self.patience_counter >= self.patience else \"\"\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:2d}: Train Loss={train_loss:.4f}, Train Acc={train_metrics['accuracy']:.4f}, \"\n",
    "                  f\"Val Loss={val_loss:.4f}, Val Acc={val_metrics['accuracy']:.4f}, \"\n",
    "                  f\"F1={val_metrics['f1_macro']:.4f} {improvement}\")\n",
    "            \n",
    "            if self.patience_counter >= self.patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        if self.best_model_state:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "            print(f\"‚úÖ Best model loaded (Val Loss: {self.best_val_loss:.4f})\")\n",
    "        \n",
    "        return self.best_val_loss\n",
    "\n",
    "# ================================================================================\n",
    "# 8. MODEL TRAINING\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ TRAINING ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Training configuration\n",
    "training_config = {\n",
    "    'num_epochs': 15,\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "# Train LSTM\n",
    "print(\"\\nüß† Training LSTM Model...\")\n",
    "lstm_trainer = ModelTrainer(lstm_model, device, \"LSTM\", patience=7)\n",
    "lstm_best_loss = lstm_trainer.train(train_loader, val_loader, **training_config)\n",
    "\n",
    "# Train CNN\n",
    "print(\"\\nüî¨ Training CNN Model...\")\n",
    "cnn_trainer = ModelTrainer(cnn_model, device, \"CNN\", patience=7)\n",
    "cnn_best_loss = cnn_trainer.train(train_loader, val_loader, **training_config)\n",
    "\n",
    "# Train Ensemble\n",
    "print(\"\\nüîó Training Ensemble Model...\")\n",
    "ensemble_trainer = ModelTrainer(ensemble_model, device, \"Ensemble\", patience=7)\n",
    "ensemble_best_loss = ensemble_trainer.train(train_loader, val_loader, \n",
    "                                          num_epochs=training_config['num_epochs'],\n",
    "                                          lr=training_config['learning_rate'] * 0.5)\n",
    "\n",
    "print(\"\\n‚úÖ All models trained successfully!\")\n",
    "\n",
    "# ================================================================================\n",
    "# 9. MODEL EVALUATION\n",
    "# ================================================================================\n",
    "\n",
    "def comprehensive_evaluation(model, dataloader, device, model_name):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    print(f\"\\nüìä Evaluating {model_name}...\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Evaluating {model_name}\"):\n",
    "            sequences = batch['sequences'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            logits, _ = model(sequences, attention_mask)\n",
    "            probabilities = F.softmax(logits, dim=1)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions, \n",
    "                                 target_names=['Negative', 'Positive'], output_dict=True)\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   F1 Score: {report['macro avg']['f1-score']:.4f}\")\n",
    "    print(f\"   Precision: {report['macro avg']['precision']:.4f}\")\n",
    "    print(f\"   Recall: {report['macro avg']['recall']:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_predictions,\n",
    "        'probabilities': all_probabilities\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate all models\n",
    "lstm_results = comprehensive_evaluation(lstm_model, test_loader, device, \"LSTM\")\n",
    "cnn_results = comprehensive_evaluation(cnn_model, test_loader, device, \"CNN\")\n",
    "ensemble_results = comprehensive_evaluation(ensemble_model, test_loader, device, \"Ensemble\")\n",
    "\n",
    "# Model comparison\n",
    "models_performance = {\n",
    "    'LSTM': {\n",
    "        'Accuracy': lstm_results['accuracy'],\n",
    "        'F1 Score': lstm_results['classification_report']['macro avg']['f1-score'],\n",
    "        'Precision': lstm_results['classification_report']['macro avg']['precision'],\n",
    "        'Recall': lstm_results['classification_report']['macro avg']['recall']\n",
    "    },\n",
    "    'CNN': {\n",
    "        'Accuracy': cnn_results['accuracy'],\n",
    "        'F1 Score': cnn_results['classification_report']['macro avg']['f1-score'],\n",
    "        'Precision': cnn_results['classification_report']['macro avg']['precision'],\n",
    "        'Recall': cnn_results['classification_report']['macro avg']['recall']\n",
    "    },\n",
    "    'Ensemble': {\n",
    "        'Accuracy': ensemble_results['accuracy'],\n",
    "        'F1 Score': ensemble_results['classification_report']['macro avg']['f1-score'],\n",
    "        'Precision': ensemble_results['classification_report']['macro avg']['precision'],\n",
    "        'Recall': ensemble_results['classification_report']['macro avg']['recall']\n",
    "    }\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(models_performance).T\n",
    "print(f\"\\nüìà Model Comparison:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df['F1 Score'].idxmax()\n",
    "best_f1_score = comparison_df.loc[best_model_name, 'F1 Score']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} (F1: {best_f1_score:.4f})\")\n",
    "\n",
    "# ================================================================================\n",
    "# 10. VISUALIZATION AND ANALYSIS\n",
    "# ================================================================================\n",
    "\n",
    "# Plot training histories\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "trainers = [('LSTM', lstm_trainer), ('CNN', cnn_trainer), ('Ensemble', ensemble_trainer)]\n",
    "colors = ['blue', 'green', 'orange']\n",
    "\n",
    "# Loss curves\n",
    "for i, (name, trainer) in enumerate(trainers):\n",
    "    epochs = range(1, len(trainer.history['train_loss']) + 1)\n",
    "    axes[0, i].plot(epochs, trainer.history['train_loss'], 'b-', label='Train Loss')\n",
    "    axes[0, i].plot(epochs, trainer.history['val_loss'], 'r-', label='Val Loss')\n",
    "    axes[0, i].set_title(f'{name} - Loss Curves')\n",
    "    axes[0, i].set_xlabel('Epoch')\n",
    "    axes[0, i].set_ylabel('Loss')\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "for i, (name, trainer) in enumerate(trainers):\n",
    "    epochs = range(1, len(trainer.history['train_acc']) + 1)\n",
    "    axes[1, i].plot(epochs, trainer.history['train_acc'], 'b-', label='Train Acc')\n",
    "    axes[1, i].plot(epochs, trainer.history['val_acc'], 'r-', label='Val Acc')\n",
    "    axes[1, i].set_title(f'{name} - Accuracy Curves')\n",
    "    axes[1, i].set_xlabel('Epoch')\n",
    "    axes[1, i].set_ylabel('Accuracy')\n",
    "    axes[1, i].legend()\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(notebook_results_dir / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "results = [('LSTM', lstm_results), ('CNN', cnn_results), ('Ensemble', ensemble_results)]\n",
    "\n",
    "for i, (name, result) in enumerate(results):\n",
    "    cm = result['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'], \n",
    "                yticklabels=['Negative', 'Positive'], ax=axes[i])\n",
    "    axes[i].set_title(f'{name} Confusion Matrix')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(notebook_results_dir / 'confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ================================================================================\n",
    "# 11. ATTENTION VISUALIZATION\n",
    "# ================================================================================\n",
    "\n",
    "class AttentionVisualizer:\n",
    "    \"\"\"Visualize attention weights for LSTM model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, preprocessor, device):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.device = device\n",
    "        \n",
    "    def visualize_attention(self, text, max_length=64):\n",
    "        \"\"\"Visualize attention weights for a single text.\"\"\"\n",
    "        # Check if model has attention\n",
    "        if not hasattr(self.model, 'attention'):\n",
    "            print(\"Model does not have attention mechanism\")\n",
    "            return\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Preprocess text\n",
    "        sequence = self.preprocessor.text_to_sequence(text, max_length)\n",
    "        tokens = self.preprocessor.clean_text(text).split()[:len(sequence)]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        input_tensor = torch.tensor([sequence]).to(self.device)\n",
    "        attention_mask = torch.ones(1, len(sequence), dtype=torch.bool).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, attention_weights = self.model(input_tensor, attention_mask, return_attention=True)\n",
    "            \n",
    "            if attention_weights is None:\n",
    "                print(\"No attention weights returned\")\n",
    "                return\n",
    "            \n",
    "            probabilities = F.softmax(logits, dim=1)\n",
    "            predicted_class = torch.argmax(logits, dim=1).item()\n",
    "            confidence = probabilities[0, predicted_class].item()\n",
    "        \n",
    "        # Plot attention\n",
    "        attention = attention_weights.squeeze(0).cpu().numpy()[:len(tokens)]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Attention heatmap\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.imshow(attention.reshape(1, -1), cmap='Reds', aspect='auto')\n",
    "        plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "        plt.yticks([])\n",
    "        plt.title(f'Attention Weights\\nPrediction: {\"Positive\" if predicted_class == 1 else \"Negative\"} '\n",
    "                 f'(Confidence: {confidence:.3f})')\n",
    "        plt.colorbar(label='Attention Weight')\n",
    "        \n",
    "        # Attention bar chart\n",
    "        plt.subplot(2, 1, 2)\n",
    "        bars = plt.bar(range(len(tokens)), attention, alpha=0.7)\n",
    "        plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "        plt.ylabel('Attention Weight')\n",
    "        plt.title('Attention Weights by Token')\n",
    "        \n",
    "        # Highlight top words\n",
    "        top_indices = np.argsort(attention)[-3:]\n",
    "        for idx in top_indices:\n",
    "            bars[idx].set_color('red')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print top attended words\n",
    "        word_attention_pairs = list(zip(tokens, attention))\n",
    "        word_attention_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"Prediction: {'Positive' if predicted_class == 1 else 'Negative'} (Confidence: {confidence:.3f})\")\n",
    "        print(\"Top 5 attended words:\")\n",
    "        for i, (word, weight) in enumerate(word_attention_pairs[:5]):\n",
    "            print(f\"  {i+1}. '{word}': {weight:.4f}\")\n",
    "\n",
    "# Visualize attention for sample texts\n",
    "if hasattr(lstm_model, 'attention'):\n",
    "    print(\"\\nüîç Attention Visualization:\")\n",
    "    visualizer = AttentionVisualizer(lstm_model, preprocessor, device)\n",
    "    \n",
    "    sample_texts = [\n",
    "        \"This movie was absolutely fantastic and amazing!\",\n",
    "        \"What a terrible and boring film this was.\",\n",
    "        \"The movie was okay but the acting was great.\"\n",
    "    ]\n",
    "    \n",
    "    for text in sample_texts:\n",
    "        visualizer.visualize_attention(text)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# ================================================================================\n",
    "# 12. PRODUCTION DEPLOYMENT\n",
    "# ================================================================================\n",
    "\n",
    "class ProductionSentimentAnalyzer:\n",
    "    \"\"\"Production-ready sentiment analysis system.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, preprocessor, device, confidence_threshold=0.8):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.device = device\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"üöÄ Production analyzer initialized with confidence threshold: {confidence_threshold}\")\n",
    "    \n",
    "    def predict(self, text, return_explanation=False):\n",
    "        \"\"\"Predict sentiment for a single text.\"\"\"\n",
    "        try:\n",
    "            # Preprocess\n",
    "            sequence = self.preprocessor.text_to_sequence(text, max_length=128)\n",
    "            \n",
    "            if len(sequence) == 0:\n",
    "                return {\n",
    "                    'text': text,\n",
    "                    'sentiment': 'unknown',\n",
    "                    'confidence': 0.0,\n",
    "                    'error': 'Empty sequence after preprocessing'\n",
    "                }\n",
    "            \n",
    "            # Convert to tensor\n",
    "            input_tensor = torch.tensor([sequence]).to(self.device)\n",
    "            attention_mask = torch.ones(1, len(sequence), dtype=torch.bool).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits, attention_weights = self.model(input_tensor, attention_mask, return_attention=True)\n",
    "                probabilities = F.softmax(logits, dim=1)\n",
    "                predicted_class = torch.argmax(logits, dim=1).item()\n",
    "                confidence = probabilities[0, predicted_class].item()\n",
    "            \n",
    "            result = {\n",
    "                'text': text,\n",
    "                'sentiment': 'positive' if predicted_class == 1 else 'negative',\n",
    "                'confidence': confidence,\n",
    "                'high_confidence': confidence > self.confidence_threshold,\n",
    "                'probabilities': {\n",
    "                    'negative': probabilities[0, 0].item(),\n",
    "                    'positive': probabilities[0, 1].item()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add explanation if requested\n",
    "            if return_explanation:\n",
    "                if confidence > 0.9:\n",
    "                    explanation = f\"Very high confidence {result['sentiment']} sentiment\"\n",
    "                elif confidence > 0.8:\n",
    "                    explanation = f\"High confidence {result['sentiment']} sentiment\"\n",
    "                elif confidence > 0.6:\n",
    "                    explanation = f\"Moderate confidence {result['sentiment']} sentiment\"\n",
    "                else:\n",
    "                    explanation = f\"Low confidence - manual review recommended\"\n",
    "                \n",
    "                result['explanation'] = explanation\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'text': text,\n",
    "                'sentiment': 'unknown',\n",
    "                'confidence': 0.0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def predict_batch(self, texts):\n",
    "        \"\"\"Predict sentiment for multiple texts.\"\"\"\n",
    "        results = []\n",
    "        for text in tqdm(texts, desc=\"Processing batch\"):\n",
    "            result = self.predict(text)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "# Create production analyzer\n",
    "production_analyzer = ProductionSentimentAnalyzer(\n",
    "    model=ensemble_model,  # Use best performing model\n",
    "    preprocessor=preprocessor,\n",
    "    device=device,\n",
    "    confidence_threshold=0.8\n",
    ")\n",
    "\n",
    "# Test production system\n",
    "test_texts = [\n",
    "    \"This movie is absolutely fantastic! Great acting and amazing plot.\",\n",
    "    \"Terrible movie, waste of time and money. Completely boring.\",\n",
    "    \"The film was okay, nothing special but not bad either.\",\n",
    "    \"Brilliant cinematography and outstanding performances!\",\n",
    "    \"I fell asleep during this confusing and dull movie.\"\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Testing Production System:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text in test_texts:\n",
    "    result = production_analyzer.predict(text, return_explanation=True)\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        confidence_indicator = \"‚úÖ\" if result['high_confidence'] else \"‚ö†Ô∏è\"\n",
    "        print(f\"Text: '{text[:50]}...'\")\n",
    "        print(f\"Sentiment: {result['sentiment'].upper()} {confidence_indicator}\")\n",
    "        print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "        if 'explanation' in result:\n",
    "            print(f\"Explanation: {result['explanation']}\")\n",
    "        print(\"-\" * 60)\n",
    "    else:\n",
    "        print(f\"Error processing: '{text[:50]}...'\")\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# ================================================================================\n",
    "# 13. RESULTS SUMMARY\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ ADVANCED SENTIMENT ANALYSIS PROJECT COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_summary = f\"\"\"\n",
    "üìä **PROJECT SUMMARY**\n",
    "\n",
    "üéØ **PERFORMANCE ACHIEVEMENTS**\n",
    "   ‚Ä¢ Best Model: {best_model_name}\n",
    "   ‚Ä¢ Accuracy: {comparison_df.loc[best_model_name, 'Accuracy']:.3f}\n",
    "   ‚Ä¢ F1-Score: {comparison_df.loc[best_model_name, 'F1 Score']:.3f}\n",
    "   ‚Ä¢ Precision: {comparison_df.loc[best_model_name, 'Precision']:.3f}\n",
    "   ‚Ä¢ Recall: {comparison_df.loc[best_model_name, 'Recall']:.3f}\n",
    "\n",
    "üß† **MODELS IMPLEMENTED**\n",
    "   ‚Ä¢ LSTM with Attention: {lstm_model.get_model_info()['total_parameters']:,} parameters\n",
    "   ‚Ä¢ CNN Text Classifier: {cnn_model.get_model_info()['total_parameters']:,} parameters  \n",
    "   ‚Ä¢ Ensemble Method: Weighted combination\n",
    "\n",
    "üìà **TRAINING RESULTS**\n",
    "   ‚Ä¢ LSTM Best Validation Loss: {lstm_best_loss:.4f}\n",
    "   ‚Ä¢ CNN Best Validation Loss: {cnn_best_loss:.4f}\n",
    "   ‚Ä¢ Ensemble Best Validation Loss: {ensemble_best_loss:.4f}\n",
    "\n",
    "üîç **FEATURES IMPLEMENTED**\n",
    "   ‚úÖ Advanced text preprocessing pipeline\n",
    "   ‚úÖ Multiple neural architectures (LSTM + CNN + Ensemble)\n",
    "   ‚úÖ Attention mechanism with visualization\n",
    "   ‚úÖ Comprehensive training framework\n",
    "   ‚úÖ Model evaluation and comparison\n",
    "   ‚úÖ Production-ready deployment system\n",
    "\n",
    "üöÄ **PRODUCTION READINESS**\n",
    "   ‚úÖ Real-time inference pipeline\n",
    "   ‚úÖ Confidence scoring and thresholding\n",
    "   ‚úÖ Batch processing capabilities\n",
    "   ‚úÖ Error handling and validation\n",
    "   ‚úÖ Comprehensive evaluation metrics\n",
    "\n",
    "üì¶ **DELIVERABLES**\n",
    "   ‚úÖ Trained models with state preservation\n",
    "   ‚úÖ Production inference system\n",
    "   ‚úÖ Attention visualization tools\n",
    "   ‚úÖ Comprehensive evaluation results\n",
    "   ‚úÖ Training curves and analysis plots\n",
    "\n",
    "üìÅ **Results saved to: {notebook_results_dir}**\n",
    "\"\"\"\n",
    "\n",
    "print(final_summary)\n",
    "\n",
    "# Save models and results\n",
    "torch.save({\n",
    "    'lstm_model': lstm_model.state_dict(),\n",
    "    'cnn_model': cnn_model.state_dict(),\n",
    "    'ensemble_model': ensemble_model.state_dict(),\n",
    "    'preprocessor': preprocessor,\n",
    "    'model_configs': {\n",
    "        'lstm': lstm_model.get_model_info(),\n",
    "        'cnn': cnn_model.get_model_info(),\n",
    "        'ensemble': ensemble_model.get_model_info()\n",
    "    },\n",
    "    'training_config': training_config,\n",
    "    'evaluation_results': {\n",
    "        'lstm': lstm_results,\n",
    "        'cnn': cnn_results,\n",
    "        'ensemble': ensemble_results\n",
    "    }\n",
    "}, notebook_results_dir / 'sentiment_analysis_complete.pth')\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv(notebook_results_dir / 'model_comparison.csv')\n",
    "\n",
    "print(\"‚úÖ All models and results saved successfully!\")\n",
    "print(\"üéä Sentiment Analysis System Complete! üéä\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
